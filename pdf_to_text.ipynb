{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pdf_to_text.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZA3karia/PDF2TEXT/blob/master/pdf_to_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MUxNMjw5Tle",
        "colab_type": "code",
        "outputId": "85a54ffe-5e12-4d9c-e430-5226af5ebfd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "!pip install PyPDF2\n",
        "!pip install textract\n",
        "!pip install nltk"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.6/dist-packages (1.26.0)\n",
            "Requirement already satisfied: textract in /usr/local/lib/python3.6/dist-packages (1.6.3)\n",
            "Requirement already satisfied: SpeechRecognition==3.8.1 in /usr/local/lib/python3.6/dist-packages (from textract) (3.8.1)\n",
            "Requirement already satisfied: six==1.12.0 in /usr/local/lib/python3.6/dist-packages (from textract) (1.12.0)\n",
            "Requirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.6/dist-packages (from textract) (3.0.4)\n",
            "Requirement already satisfied: argcomplete==1.10.0 in /usr/local/lib/python3.6/dist-packages (from textract) (1.10.0)\n",
            "Requirement already satisfied: extract-msg==0.23.1 in /usr/local/lib/python3.6/dist-packages (from textract) (0.23.1)\n",
            "Requirement already satisfied: xlrd==1.2.0 in /usr/local/lib/python3.6/dist-packages (from textract) (1.2.0)\n",
            "Requirement already satisfied: beautifulsoup4==4.8.0 in /usr/local/lib/python3.6/dist-packages (from textract) (4.8.0)\n",
            "Requirement already satisfied: python-pptx==0.6.18 in /usr/local/lib/python3.6/dist-packages (from textract) (0.6.18)\n",
            "Requirement already satisfied: docx2txt==0.8 in /usr/local/lib/python3.6/dist-packages (from textract) (0.8)\n",
            "Requirement already satisfied: pdfminer.six==20181108 in /usr/local/lib/python3.6/dist-packages (from textract) (20181108)\n",
            "Requirement already satisfied: EbookLib==0.17.1 in /usr/local/lib/python3.6/dist-packages (from textract) (0.17.1)\n",
            "Requirement already satisfied: olefile==0.46 in /usr/local/lib/python3.6/dist-packages (from extract-msg==0.23.1->textract) (0.46)\n",
            "Requirement already satisfied: tzlocal==1.5.1 in /usr/local/lib/python3.6/dist-packages (from extract-msg==0.23.1->textract) (1.5.1)\n",
            "Requirement already satisfied: imapclient==2.1.0 in /usr/local/lib/python3.6/dist-packages (from extract-msg==0.23.1->textract) (2.1.0)\n",
            "Requirement already satisfied: soupsieve>=1.2 in /usr/local/lib/python3.6/dist-packages (from beautifulsoup4==4.8.0->textract) (1.9.4)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from python-pptx==0.6.18->textract) (4.2.6)\n",
            "Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.6/dist-packages (from python-pptx==0.6.18->textract) (1.2.1)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.6/dist-packages (from python-pptx==0.6.18->textract) (4.3.0)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.6/dist-packages (from pdfminer.six==20181108->textract) (3.9.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.6/dist-packages (from pdfminer.six==20181108->textract) (2.1.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from tzlocal==1.5.1->extract-msg==0.23.1->textract) (2018.9)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-069aqc5cuE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Created on Aug 10, 2018\n",
        "@author: zhaosong\n",
        "This example tell you how to extract text content from a pdf file.\n",
        "'''\n",
        "\n",
        "import PyPDF2\n",
        "import textract\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# This function will extract and return the pdf file text content.\n",
        "def extractPdfText(filePath=''):\n",
        "\n",
        "    # Open the pdf file in read binary mode.\n",
        "    fileObject = open(filePath, 'rb')\n",
        "\n",
        "    # Create a pdf reader .\n",
        "    pdfFileReader = PyPDF2.PdfFileReader(fileObject)\n",
        "\n",
        "    # Get total pdf page number.\n",
        "    totalPageNumber = pdfFileReader.numPages\n",
        "\n",
        "    # Print pdf total page number.\n",
        "    print('This pdf file contains totally ' + str(totalPageNumber) + ' pages.')\n",
        "\n",
        "    currentPageNumber = 1\n",
        "    text = ''\n",
        "\n",
        "    # Loop in all the pdf pages.\n",
        "    while(currentPageNumber < totalPageNumber ):\n",
        "\n",
        "        # Get the specified pdf page object.\n",
        "        pdfPage = pdfFileReader.getPage(currentPageNumber)\n",
        "\n",
        "        # Get pdf page text.\n",
        "        text = text + pdfPage.extractText()\n",
        "\n",
        "        # Process next page.\n",
        "        currentPageNumber += 1\n",
        "\n",
        "    if(text == ''):\n",
        "        # If can not extract text then use ocr lib to extract the scanned pdf file.\n",
        "        text = textract.process(filePath, method='tesseract', encoding='utf-8')\n",
        "       \n",
        "    return text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvegPeSs6CAV",
        "colab_type": "code",
        "outputId": "c3769259-d4df-4982-8cfa-9f1c881db0c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pdfFilePath = '/content/Aurélien Géron - Hands-On Machine Learning with Scikit-Learn and TensorFlow_ Concepts, Tools, and Techniques to Build Intelligent Systems-O’Reilly Media (2017).pdf'\n",
        "\n",
        "pdfText = extractPdfText(pdfFilePath)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This pdf file contains totally 564 pages.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMzUwHOA7mio",
        "colab_type": "code",
        "outputId": "f1c45dba-40c6-4f5e-b716-99b7dfd1cb1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "pdfText\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Aur•lien G•ron\\nHands-On Machine Learning withScikit-Learn and TensorFlowConcepts, Tools, and Techniques to\\nBuild Intelligent Systems\\nBostonFarnhamSebastopolTokyoBeijingBostonFarnhamSebastopolTokyoBeijing978-1-491-96229-9[LSI]Hands-On Machine Learning with Scikit-Learn and TensorFlowby Aur•lien G•ron\\nCopyright † 2017 Aur•lien G•ron. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O‡Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\nO‡Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\\nalso available for most titles (\\nhttp://oreilly.com/safari\\n). For more information, contact our corporate/insti…\\ntutional sales department: 800-998-9938 or \\ncorporate@oreilly.com\\n.Editor: Nicole Tache\\nProduction Editor: Nicholas Adams\\nCopyeditor: Rachel Monaghan\\nProofreader: Charles RoumeliotisIndexer: Wendy Catalano\\nInterior Designer: David Futato\\nCover Designer: Randy Comer\\nIllustrator: Rebecca DemarestMarch 2017:\\n First EditionRevision History for the First Edition2017-03-10: First Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781491962299\\n for release details.The O‡Reilly logo is a registered trademark of O‡Reilly Media, Inc. \\nHands-On Machine Learning with\\nScikit-Learn and TensorFlow\\n, the cover image, and related trade dress are trademarks of O‡Reilly Media,\\nInc.While the publisher and the author have used good faith efforts to ensure that the information and\\ninstructions contained in this work are accurate, the publisher and the author disclaim all responsibility\\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\\nor reliance on this work. Use of the information and instructions contained in this work is at your own\\nrisk. If any code samples or other technology this work contains or describes is subject to open source\\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\\nthereof complies with such licenses and/or rights.\\nTable of ContentsPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xiii\\nPart I. The Fundamentals of Machine Learning1.The Machine Learning Landscape. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3\\nWhat Is Machine Learning?                                                                                           4\\nWhy Use Machine Learning?                                                                                         4\\nTypes of Machine Learning Systems                                                                             7\\nSupervised/Unsupervised Learning                                                                           8\\nBatch and Online Learning                                                                                       14\\nInstance-Based Versus Model-Based Learning                                                      17\\nMain Challenges of Machine Learning                                                                       22\\nInsufficient Quantity of Training Data                                                                   22\\nNonrepresentative Training Data                                                                            24\\nPoor-Quality Data                                                                                                      25\\nIrrelevant Features                                                                                                     25\\nOverfitting the Training Data                                                                                   26\\nUnderfitting the Training Data                                                                                28\\nStepping Back                                                                                                             28\\nTesting and Validating                                                                                                   29\\nExercises                                                                                                                          31\\n2.End-to-End Machine Learning Project. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  33\\nWorking with Real Data                                                                                                33\\nLook at the Big Picture                                                                                                  35\\nFrame the Problem                                                                                                    35\\nSelect a Performance Measure                                                                                  37\\niiiCheck the Assumptions                                                                                             40\\nGet the Data                                                                                                                    40\\nCreate the Workspace                                                                                                40\\nDownload the Data                                                                                                    43\\nTake a Quick Look at the Data Structure                                                                45\\nCreate a Test Set                                                                                                          49\\nDiscover and Visualize the Data to Gain Insights                                                     53\\nVisualizing Geographical Data                                                                                 53\\nLooking for Correlations                                                                                           55\\nExperimenting with Attribute Combinations                                                        58\\nPrepare the Data for Machine Learning Algorithms                                                59\\nData Cleaning                                                                                                             60\\nHandling Text and Categorical Attributes                                                              62\\nCustom Transformers                                                                                                64\\nFeature Scaling                                                                                                            65\\nTransformation Pipelines                                                                                          66\\nSelect and Train a Model                                                                                               68\\nTraining and Evaluating on the Training Set                                                         68\\nBetter Evaluation Using Cross-Validation                                                              69\\nFine-Tune Your Model                                                                                                  71\\nGrid Search                                                                                                                 72\\nRandomized Search                                                                                                   74\\nEnsemble Methods                                                                                                     74\\nAnalyze the Best Models and Their Errors                                                             74\\nEvaluate Your System on the Test Set                                                                      75\\nLaunch, Monitor, and Maintain Your System                                                            76\\nTry It Out!                                                                                                                       77\\nExercises                                                                                                                          77\\n3.Classi•cation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  79\\nMNIST                                                                                                                             79\\nTraining a Binary Classifier                                                                                          82\\nPerformance Measures                                                                                                  82\\nMeasuring Accuracy Using Cross-Validation                                                        83\\nConfusion Matrix                                                                                                       84\\nPrecision and Recall                                                                                                   86\\nPrecision/Recall Tradeoff                                                                                          87\\nThe ROC Curve                                                                                                          91\\nMulticlass Classification                                                                                               93\\nError Analysis                                                                                                                 96\\nMultilabel Classification                                                                                             100\\nMultioutput Classification                                                                                          101\\niv | Table of Contents\\nExercises                                                                                                                        102\\n4.Training Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  105\\nLinear Regression                                                                                                        106\\nThe Normal Equation                                                                                              108\\nComputational Complexity                                                                                    110\\nGradient Descent                                                                                                         111\\nBatch Gradient Descent                                                                                           114\\nStochastic Gradient Descent                                                                                   117\\nMini-batch Gradient Descent                                                                                 119\\nPolynomial Regression                                                                                                121\\nLearning Curves                                                                                                           123\\nRegularized Linear Models                                                                                         127\\nRidge Regression                                                                                                      127\\nLasso Regression                                                                                                      130\\nElastic Net                                                                                                                 132\\nEarly Stopping                                                                                                          133\\nLogistic Regression                                                                                                      134\\nEstimating Probabilities                                                                                          134\\nTraining and Cost Function                                                                                   135\\nDecision Boundaries                                                                                                136\\nSoftmax Regression                                                                                                  139\\nExercises                                                                                                                        142\\n5.Support Vector Machines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  145\\nLinear SVM Classification                                                                                          145\\nSoft Margin Classification                                                                                       146\\nNonlinear SVM Classification                                                                                   149\\nPolynomial Kernel                                                                                                   150\\nAdding Similarity Features                                                                                     151\\nGaussian RBF Kernel                                                                                               152\\nComputational Complexity                                                                                    153\\nSVM Regression                                                                                                           154\\nUnder the Hood                                                                                                           156\\nDecision Function and Predictions                                                                       156\\nTraining Objective                                                                                                   157\\nQuadratic Programming                                                                                         159\\nThe Dual Problem                                                                                                    160\\nKernelized SVM                                                                                                       161\\nOnline SVMs                                                                                                            164\\nExercises                                                                                                                        165\\nTable of Contents | v\\n6.Decision Trees. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  167\\nTraining and Visualizing a Decision Tree                                                                167\\nMaking Predictions                                                                                                     169\\nEstimating Class Probabilities                                                                                   171\\nThe CART Training Algorithm                                                                                 171\\nComputational Complexity                                                                                        172\\nGini Impurity or Entropy?                                                                                         172\\nRegularization Hyperparameters                                                                              173\\nRegression                                                                                                                     175\\nInstability                                                                                                                      177\\nExercises                                                                                                                        178\\n7.Ensemble Learning and Random Forests. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  181\\nVoting Classifiers                                                                                                         181\\nBagging and Pasting                                                                                                    185\\nBagging and Pasting in Scikit-Learn                                                                     186\\nOut-of-Bag Evaluation                                                                                            187\\nRandom Patches and Random Subspaces                                                                188\\nRandom Forests                                                                                                           189\\nExtra-Trees                                                                                                                190\\nFeature Importance                                                                                                  190\\nBoosting                                                                                                                        191\\nAdaBoost                                                                                                                   192\\nGradient Boosting                                                                                                    195\\nStacking                                                                                                                         200\\nExercises                                                                                                                        202\\n8.Dimensionality Reduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  205\\nThe Curse of Dimensionality                                                                                     206\\nMain Approaches for Dimensionality Reduction                                                   207\\nProjection                                                                                                                  207\\nManifold Learning                                                                                                   210\\nPCA                                                                                                                                211\\nPreserving the Variance                                                                                          211\\nPrincipal Components                                                                                            212\\nProjecting Down to d Dimensions                                                                        213\\nUsing Scikit-Learn                                                                                                    214\\nExplained Variance Ratio                                                                                        214\\nChoosing the Right Number of Dimensions                                                       215\\nPCA for Compression                                                                                             216\\nIncremental PCA                                                                                                     217\\nRandomized PCA                                                                                                    218\\nvi | Table of Contents\\nKernel PCA                                                                                                                   218\\nSelecting a Kernel and Tuning Hyperparameters                                                219\\nLLE                                                                                                                                 221\\nOther Dimensionality Reduction Techniques                                                         223\\nExercises                                                                                                                        224\\nPart II. \\nNeural Networks and Deep Learning9.Up and Running with TensorFlow. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  229\\nInstallation                                                                                                                    232\\nCreating Your First Graph and Running It in a Session                                         232\\nManaging Graphs                                                                                                        234\\nLifecycle of a Node Value                                                                                            235\\nLinear Regression with TensorFlow                                                                          235\\nImplementing Gradient Descent                                                                               237\\nManually Computing the Gradients                                                                     237\\nUsing autodiff                                                                                                           238\\nUsing an Optimizer                                                                                                  239\\nFeeding Data to the Training Algorithm                                                                  239\\nSaving and Restoring Models                                                                                     241\\nVisualizing the Graph and Training Curves Using TensorBoard                         242\\nName Scopes                                                                                                                245\\nModularity                                                                                                                    246\\nSharing Variables                                                                                                         248\\nExercises                                                                                                                        251\\n10.Introduction to Arti•cial Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  253\\nFrom Biological to Artificial Neurons                                                                      254\\nBiological Neurons                                                                                                   255\\nLogical Computations with Neurons                                                                    256\\nThe Perceptron                                                                                                         257\\nMulti-Layer Perceptron and Backpropagation                                                    261\\nTraining an MLP with TensorFlow‡s High-Level API                                            264\\nTraining a DNN Using Plain TensorFlow                                                                265\\nConstruction Phase                                                                                                  265\\nExecution Phase                                                                                                       269\\nUsing the Neural Network                                                                                      270\\nFine-Tuning Neural Network Hyperparameters                                                     270\\nNumber of Hidden Layers                                                                                      270\\nNumber of Neurons per Hidden Layer                                                                 272\\nActivation Functions                                                                                               272\\nTable of Contents | vii\\nExercises                                                                                                                        273\\n11.Training Deep Neural Nets. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  275\\nVanishing/Exploding Gradients Problems                                                              275\\nXavier and He Initialization                                                                                   277\\nNonsaturating Activation Functions                                                                     279\\nBatch Normalization                                                                                                282\\nGradient Clipping                                                                                                    286\\nReusing Pretrained Layers                                                                                          286\\nReusing a TensorFlow Model                                                                                 287\\nReusing Models from Other Frameworks                                                            288\\nFreezing the Lower Layers                                                                                      289\\nCaching the Frozen Layers                                                                                     290\\nTweaking, Dropping, or Replacing the Upper Layers                                        290\\nModel Zoos                                                                                                               291\\nUnsupervised Pretraining                                                                                       291\\nPretraining on an Auxiliary Task                                                                           292\\nFaster Optimizers                                                                                                         293\\nMomentum optimization                                                                                        294\\nNesterov Accelerated Gradient                                                                              295\\nAdaGrad                                                                                                                    296\\nRMSProp                                                                                                                   298\\nAdam Optimization                                                                                                 298\\nLearning Rate Scheduling                                                                                       300\\nAvoiding Overfitting Through Regularization                                                        302\\nEarly Stopping                                                                                                          303\\n—1 and —2 Regularization                                                                                           303\\nDropout                                                                                                                     304\\nMax-Norm Regularization                                                                                      307\\nData Augmentation                                                                                                  309\\nPractical Guidelines                                                                                                     310\\nExercises                                                                                                                        311\\n12.Distributing TensorFlow Across Devices and Servers. . . . . . . . . . . . . . . . . . . . . . . . . . .  313\\nMultiple Devices on a Single Machine                                                                      314\\nInstallation                                                                                                                314\\nManaging the GPU RAM                                                                                       317\\nPlacing Operations on Devices                                                                              318\\nParallel Execution                                                                                                    321\\nControl Dependencies                                                                                             323\\nMultiple Devices Across Multiple Servers                                                               323\\nOpening a Session                                                                                                    325\\nviii | Table of Contents\\nThe Master and Worker Services                                                                           325\\nPinning Operations Across Tasks                                                                          326\\nSharding Variables Across Multiple Parameter Servers                                     327\\nSharing State Across Sessions Using Resource Containers                                328\\nAsynchronous Communication Using TensorFlow Queues                             329\\nLoading Data Directly from the Graph                                                                 335\\nParallelizing Neural Networks on a TensorFlow Cluster                                       342\\nOne Neural Network per Device                                                                           342\\nIn-Graph Versus Between-Graph Replication                                                     343\\nModel Parallelism                                                                                                     345\\nData Parallelism                                                                                                        347\\nExercises                                                                                                                        352\\n13.Convolutional Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  353\\nThe Architecture of the Visual Cortex                                                                     354\\nConvolutional Layer                                                                                                    355\\nFilters                                                                                                                         357\\nStacking Multiple Feature Maps                                                                             358\\nTensorFlow Implementation                                                                                  360\\nMemory Requirements                                                                                           362\\nPooling Layer                                                                                                                363\\nCNN Architectures                                                                                                      365\\nLeNet-5                                                                                                                      366\\nAlexNet                                                                                                                      367\\nGoogLeNet                                                                                                                368\\nResNet                                                                                                                        372\\nExercises                                                                                                                        376\\n14.Recurrent Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  379\\nRecurrent Neurons                                                                                                      380\\nMemory Cells                                                                                                           382\\nInput and Output Sequences                                                                                  382\\nBasic RNNs in TensorFlow                                                                                         384\\nStatic Unrolling Through Time                                                                              385\\nDynamic Unrolling Through Time                                                                       387\\nHandling Variable Length Input Sequences                                                         387\\nHandling Variable-Length Output Sequences                                                     388\\nTraining RNNs                                                                                                             389\\nTraining a Sequence Classifier                                                                               389\\nTraining to Predict Time Series                                                                              392\\nCreative RNN                                                                                                           396\\nDeep RNNs                                                                                                                   396\\nTable of Contents | ix\\nDistributing a Deep RNN Across Multiple GPUs                                               397\\nApplying Dropout                                                                                                    399\\nThe Difficulty of Training over Many Time Steps                                               400\\nLSTM Cell                                                                                                                     401\\nPeephole Connections                                                                                             403\\nGRU Cell                                                                                                                       404\\nNatural Language Processing                                                                                     405\\nWord Embeddings                                                                                                   405\\nAn Encoder–Decoder Network for Machine Translation                                  407\\nExercises                                                                                                                        410\\n15.Autoencoders. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  411\\nEfficient Data Representations                                                                                   412\\nPerforming PCA with an Undercomplete Linear Autoencoder                           413\\nStacked Autoencoders                                                                                                 415\\nTensorFlow Implementation                                                                                  416\\nTying Weights                                                                                                           417\\nTraining One Autoencoder at a Time                                                                   418\\nVisualizing the Reconstructions                                                                            420\\nVisualizing Features                                                                                                 421\\nUnsupervised Pretraining Using Stacked Autoencoders                                       422\\nDenoising Autoencoders                                                                                            424\\nTensorFlow Implementation                                                                                  425\\nSparse Autoencoders                                                                                                   426\\nTensorFlow Implementation                                                                                  427\\nVariational Autoencoders                                                                                           428\\nGenerating Digits                                                                                                     431\\nOther Autoencoders                                                                                                    432\\nExercises                                                                                                                        433\\n16.Reinforcement Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  437\\nLearning to Optimize Rewards                                                                                  438\\nPolicy Search                                                                                                                 440\\nIntroduction to OpenAI Gym                                                                                   441\\nNeural Network Policies                                                                                             444\\nEvaluating Actions: The Credit Assignment Problem                                           447\\nPolicy Gradients                                                                                                           448\\nMarkov Decision Processes                                                                                        453\\nTemporal Difference Learning and Q-Learning                                                     457\\nExploration Policies                                                                                                 459\\nApproximate Q-Learning                                                                                       460\\nLearning to Play Ms. Pac-Man Using Deep Q-Learning                                       460\\nx | Table of Contents\\nExercises                                                                                                                        469\\nThank You!                                                                                                                   470\\nA.Exercise Solutions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  471\\nB.Machine Learning Project Checklist. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  497\\nC.SVM Dual Problem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  503\\nD.Autodi†. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  507\\nE.Other Popular ANN Architectures. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  515\\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  525\\nTable of Contents | xi\\n1Available on Hinton‡s home page at \\nhttp://www.cs.toronto.edu/~hinton/\\n.2Despite the fact that Yann Lecun‡s deep convolutional neural networks had worked well for image recognition\\nsince the 1990s, although they were not as general purpose.PrefaceThe Machine Learning TsunamiIn 2006, Geoffrey Hinton et al. published a paper\\n1 showing how to train a deep neuralnetwork capable of recognizing handwritten digits with state-of-the-art precision\\n(>98%). They branded this technique ƒDeep Learning.⁄ Training a deep neural net\\nwas widely considered impossible at the time,\\n2 and most researchers had abandonedthe idea since the 1990s. This paper revived the interest of the scientific community\\nand before long many new papers demonstrated that Deep Learning was not only\\npossible, but capable of mind-blowing achievements that no other Machine Learning\\n(ML) technique could hope to match (with the help of tremendous computing power\\nand great amounts of data). This enthusiasm soon extended to many other areas of\\nMachine Learning.\\nFast-forward 10 years and Machine Learning has conquered the industry: it is now at\\nthe heart of much of the magic in today‡s high-tech products, ranking your web\\nsearch results, powering your smartphone‡s speech recognition, and recommending\\nvideos, beating the world champion at the game of Go. Before you know it, it will be\\ndriving your car.\\nMachine Learning in Your ProjectsSo naturally you are excited about Machine Learning and you would love to join the\\nparty!Perhaps you would like to give your homemade robot a brain of its own? Make it rec…\\nognize faces? Or learn to walk around?xiiiOr maybe your company has tons of data (user logs, financial data, production data,\\nmachine sensor data, hotline stats, HR reports, etc.), and more than likely you could\\nunearth some hidden gems if you just knew where to look; for example:\\n‹Segment customers and find the best marketing strategy for each group\\n‹Recommend products for each client based on what similar clients bought\\n‹Detect which transactions are likely to be fraudulent\\n‹Predict next year‡s revenue\\n‹And moreWhatever the reason, you have decided to learn Machine Learning and implement it\\nin your projects. Great idea!\\nObjective and ApproachThis book assumes that you know close to nothing about Machine Learning. Its goal\\nis to give you the concepts, the intuitions, and the tools you need to actually imple…\\nment programs capable of \\nlearning from data\\n.We will cover a large number of techniques, from the simplest and most commonly\\nused (such as linear regression) to some of the Deep Learning techniques that regu…\\nlarly win competitions.\\nRather than implementing our own toy versions of each algorithm, we will be using\\nactual production-ready Python frameworks:‹Scikit-Learn is very easy to use, yet it implements many Machine Learning algo…\\nrithms efficiently, so it makes for a great entry point to learn Machine Learning.\\n‹TensorFlow\\n is a more complex library for distributed numerical computation\\nusing data flow graphs. It makes it possible to train and run very large neural net…\\nworks efficiently by distributing the computations across potentially thousands\\nof multi-GPU servers. TensorFlow was created at Google and supports many of\\ntheir large-scale Machine Learning applications. It was open-sourced in Novem…\\nber 2015.The book favors a hands-on approach, growing an intuitive understanding of\\nMachine Learning through concrete working examples and just a little bit of theory.\\nWhile you can read this book without picking up your laptop, we highly recommend\\nyou experiment with the code examples available online as Jupyter notebooks at\\nhttps://github.com/ageron/handson-ml\\n.xiv | Preface\\nPrerequisitesThis book assumes that you have some Python programming experience and that you\\nare familiar with Python‡s main scientific libraries, in particular \\nNumPy\\n, Pandas\\n, andMatplotlib\\n.Also, if you care about what‡s under the hood you should have a reasonable under…\\nstanding of college-level math as well (calculus, linear algebra, probabilities, and sta…\\ntistics).If you don‡t know Python yet, \\nhttp://learnpython.org/\\n is a great place to start. The offi…\\ncial tutorial on python.org is also quite good.If you have never used Jupyter, \\nChapter 2\\n will guide you through installation and the\\nbasics: it is a great tool to have in your toolbox.\\nIf you are not familiar with Python‡s scientific libraries, the provided Jupyter note…\\nbooks include a few tutorials. There is also a quick math tutorial for linear algebra.\\nRoadmapThis book is organized in two parts. Part I, \\n•e Fundamentals of Machine Learning\\n,covers the following topics:‹What is Machine Learning? What problems does it try to solve? What are the\\nmain categories and fundamental concepts of Machine Learning systems?\\n‹The main steps in a typical Machine Learning project.\\n‹Learning by fitting a model to data.\\n‹Optimizing a cost function.‹Handling, cleaning, and preparing data.\\n‹Selecting and engineering features.\\n‹Selecting a model and tuning hyperparameters using cross-validation.\\n‹The main challenges of Machine Learning, in particular underfitting and overfit…\\nting (the bias/variance tradeoff).\\n‹Reducing the dimensionality of the training data to fight the curse of dimension…\\nality.\\n‹The most common learning algorithms: Linear and Polynomial Regression,\\nLogistic Regression, k-Nearest Neighbors, Support Vector Machines, Decision\\nTrees, Random Forests, and Ensemble methods.\\nPreface | xv\\nPart II, \\nNeural Networks and Deep Learning\\n, covers the following topics:‹What are neural nets? What are they good for?\\n‹Building and training neural nets using TensorFlow.\\n‹The most important neural net architectures: feedforward neural nets, convolu…\\ntional nets, recurrent nets, long short-term memory (LSTM) nets, and autoen…\\ncoders.‹Techniques for training deep neural nets.\\n‹Scaling neural networks for huge datasets.\\n‹Reinforcement learning.\\nThe first part is based mostly on Scikit-Learn while the second part uses TensorFlow.\\nDon‡t \\njump into deep waters too hastily: while Deep Learning is no\\ndoubt one of the most exciting areas in Machine Learning, you\\nshould master the fundamentals first. Moreover, most problems\\ncan be solved quite well using simpler techniques such as Random\\nForests and Ensemble methods (discussed in \\nPart I\\n). Deep Learn…ing is best suited for complex problems such as image recognition,\\nspeech recognition, or natural language processing, provided you\\nhave enough data, computing power, and patience.\\nOther ResourcesMany resources are available to learn about Machine Learning. Andrew Ng‡s \\nMLcourse on Coursera and Geoffrey Hinton‡s \\ncourse on neural networks and DeepLearning are amazing, although they both require a significant time investment\\n(think months).\\nThere are also many interesting websites about Machine Learning, including of\\ncourse Scikit-Learn‡s exceptional \\nUser Guide\\n. You may also enjoy \\nDataquest\\n, whichprovides very nice interactive tutorials, and ML blogs such as those listed on \\nQuora.Finally, the \\nDeep Learning website has a good list of resources to learn more.Of course there are also many other introductory books about Machine Learning, in\\nparticular:\\n‹Joel Grus, \\nData Science from Scratch\\n (O‡Reilly). This book presents the funda…\\nmentals of Machine Learning, and implements some of the main algorithms in\\npure Python (from scratch, as the name suggests).\\n‹Stephen Marsland, \\nMachine Learning: An Algorithmic Perspective\\n (Chapman and\\nHall). This book is a great introduction to Machine Learning, covering a wide\\nxvi | Preface\\nrange of topics in depth, with code examples in Python (also from scratch, but\\nusing NumPy).\\n‹Sebastian Raschka, \\nPython Machine Learning\\n (Packt Publishing). Also a great\\nintroduction to Machine Learning, this book leverages Python open source libra…\\nries (Pylearn 2 and Theano).‹Yaser S. Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin, \\nLearning from\\nData\\n (AMLBook). A rather theoretical approach to ML, this book provides deep\\ninsights, in particular on the bias/variance tradeoff (see \\nChapter 4\\n).‹Stuart Russell and Peter Norvig, \\nArti†cial Intelligence: A Modern Approach, 3rd\\nEdition\\n (Pearson). This is a great (and huge) book covering an incredible amount\\nof topics, including Machine Learning. It helps put ML into perspective.\\nFinally, a great way to learn is to join ML competition websites such as \\nKaggle.comthis will allow you to practice your skills on real-world problems, with help andinsights from some of the best ML professionals out there.\\nConventions Used in This BookThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\nConstant widthUsed for program listings, as well as within paragraphs to refer to program ele…\\nments such as variable or function names, databases, data types, environment\\nvariables, statements and keywords.\\nConstant width boldShows commands or other text that should be typed literally by the user.\\nConstant width italicShows text that should be replaced with user-supplied values or by values deter…\\nmined by context.\\nThis element signifies a tip or suggestion.\\nPreface | xvii\\nThis element signifies a general note.\\nThis element indicates a warning or caution.\\nUsing Code ExamplesSupplemental material (code examples, exercises, etc.) is available for download at\\nhttps://github.com/ageron/handson-ml\\n.This book is here to help you get your job done. In general, if example code is offered\\nwith this book, you may use it in your programs and documentation. You do not\\nneed to contact us for permission unless you‡re reproducing a significant portion of\\nthe code. For example, writing a program that uses several chunks of code from this\\nbook does not require permission. Selling or distributing a CD-ROM of examples\\nfrom O‡Reilly books does require permission. Answering a question by citing this\\nbook and quoting example code does not require permission. Incorporating a signifi…\\ncant amount of example code from this book into your product‡s documentation does\\nrequire permission.We appreciate, but do not require, attribution. An attribution usually includes the\\ntitle, author, publisher, and ISBN. For example: ƒ\\nHands-On Machine Learning with\\nScikit-Learn and TensorFlow\\n by Aur•lien G•ron (O‡Reilly). Copyright 2017 Aur•lien\\nG•ron, 978-1-491-96229-9.⁄\\nIf you feel your use of code examples falls outside fair use or the permission given\\nabove, feel free to contact us at \\npermissions@oreilly.com\\n.O‡Reilly SafariSafari\\n (formerly Safari Books Online) is a membership-based\\ntraining and reference platform for enterprise, government,\\neducators, and individuals.\\nMembers have access to thousands of books, training videos, Learning Paths, interac…\\ntive tutorials, and curated playlists from over 250 publishers, including O‡Reilly\\nMedia, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Profes…\\nsional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press,\\nxviii | Preface\\nJohn Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe\\nPress, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, and\\nCourse Technology, among others.\\nFor more information, please visit \\nhttp://oreilly.com/safari\\n.How to Contact UsPlease address comments and questions concerning this book to the publisher:\\nO‡Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472800-998-9938 (in the United States or Canada)\\n707-829-0515 (international or local)\\n707-829-0104 (fax)We have a web page for this book, where we list errata, examples, and any additional\\ninformation. You can access this page at \\nhttp://bit.ly/hands-on-machine-learning-with-scikit-learn-and-tensor‡ow.To comment or ask technical questions about this book, send email to \\nbookques…\\ntions@oreilly.com\\n.For more information about our books, courses, conferences, and news, see our web…\\nsite at \\nhttp://www.oreilly.com\\n.Find us on Facebook: \\nhttp://facebook.com/oreilly\\nFollow us on Twitter: \\nhttp://twitter.com/oreillymedia\\nWatch us on YouTube: \\nhttp://www.youtube.com/oreillymedia\\nAcknowledgmentsI would like to thank my Google colleagues, in particular the YouTube video classifi…\\ncation team, for teaching me so much about Machine Learning. I could never have\\nstarted this project without them. Special thanks to my personal ML gurus: Cl•ment\\nCourbet, Julien Dubois, Mathias Kende, Daniel Kitachewsky, James Pack, Alexander\\nPak, Anosh Raj, Vitor Sessak, Wiktor Tomczak, Ingrid von Glehn, Rich Washington,\\nand everyone at YouTube Paris.\\nI am incredibly grateful to all the amazing people who took time out of their busy\\nlives to review my book in so much detail. Thanks to Pete Warden for answering all\\nmy TensorFlow questions, reviewing \\nPart II\\n, providing many interesting insights, and\\nof course for being part of the core TensorFlow team. You should definitely check out\\nPreface | xix\\nhis blog! Many thanks to Lukas Biewald for his very thorough review of \\nPart II\\n: he leftno stone unturned, tested all the code (and caught a few errors), made many great\\nsuggestions, and his enthusiasm was contagious. You should check out \\nhis blog andhis cool robots! Thanks to Justin Francis, who also reviewed \\nPart II\\n very thoroughly,\\ncatching errors and providing great insights, in particular in \\nChapter 16\\n. Check outhis posts on TensorFlow!\\nHuge thanks as well to David Andrzejewski, who reviewed \\nPart I\\n and providedincredibly useful feedback, identifying unclear sections and suggesting how to\\nimprove them. Check out \\nhis website! Thanks to Gr•goire Mesnil, who reviewed\\nPart II\\n and contributed very interesting practical advice on training neural networks.\\nThanks as well to Eddy Hung, Salim S•maoune, Karim Matrah, Ingrid von Glehn,\\nIain Smears, and Vincent Guilbeau for reviewing \\nPart I\\n and making many useful sug…\\ngestions. And I also wish to thank my father-in-law, Michel Tessier, former mathe…\\nmatics teacher and now a great translator of Anton Chekhov, for helping me iron out\\nsome of the mathematics and notations in this book and reviewing the linear algebra\\nJupyter notebook.\\nAnd of course, a gigantic ƒthank you⁄ to my dear brother Sylvain, who reviewed every\\nsingle chapter, tested every line of code, provided feedback on virtually every section,\\nand encouraged me from the first line to the last. Love you, bro!Many thanks as well to O‡Reilly‡s fantastic staff, in particular Nicole Tache, who gave\\nme insightful feedback, always cheerful, encouraging, and helpful. Thanks as well to\\nMarie Beaugureau, Ben Lorica, Mike Loukides, and Laurel Ruma for believing in this\\nproject and helping me define its scope. Thanks to Matt Hacker and all of the Atlas\\nteam for answering all my technical questions regarding formatting, asciidoc, and\\nLaTeX, and thanks to Rachel Monaghan, Nick Adams, and all of the production team\\nfor their final review and their hundreds of corrections.\\nLast but not least, I am infinitely grateful to my beloved wife, Emmanuelle, and to our\\nthree wonderful kids, Alexandre, R•mi, and Gabrielle, for encouraging me to workhard on this book, asking many questions (who said you can‡t teach neural networks\\nto a seven-year-old?), and even bringing me cookies and coffee. What more can one\\ndream of?\\nxx | Preface\\nPART IThe Fundamentals ofMachine LearningCHAPTER 1The Machine Learning LandscapeWhen most people hear ƒMachine Learning,⁄ they picture a robot: a dependable but…\\nler or a deadly Terminator depending on who you ask. But Machine Learning is not\\njust a futuristic fantasy, it‡s already here. In fact, it has been around for decades in\\nsome specialized applications, such as \\nOptical Character Recognition\\n (OCR). \\nBut thefirst ML application that really became mainstream, improving the lives of hundreds\\nof millions of people, took over the world back in the 1990s: it was the spam \\n†lter.Not exactly a self-aware Skynet, but it does technically qualify as Machine Learning\\n(it has actually learned so well that you seldom need to flag an email as spam any…\\nmore). It was followed by hundreds of ML applications that now quietly power hun…\\ndreds of products and features that you use regularly, from better recommendations\\nto voice search.Where does Machine Learning start and where does it end? What exactly does it\\nmean for a machine to learn\\n something? If I download a copy of Wikipedia, has my\\ncomputer really ƒlearned⁄ something? Is it suddenly smarter? In this chapter we will\\nstart by clarifying what Machine Learning is and why you may want to use it.\\nThen, before we set out to explore the Machine Learning continent, we will take a\\nlook at the map and learn about the main regions and the most notable landmarks:\\nsupervised versus unsupervised learning, online versus batch learning, instance-\\nbased versus model-based learning. Then we will look at the workflow of a typical ML\\nproject, discuss the main challenges you may face, and cover how to evaluate and\\nfine-tune a Machine Learning system.\\nThis chapter introduces a lot of fundamental concepts (and jargon) that every data\\nscientist should know by heart. It will be a high-level overview (the only chapter\\nwithout much code), all rather simple, but you should make sure everything is\\ncrystal-clear to you before continuing to the rest of the book. So grab a coffee and let‡s\\nget started!3If you already know all the Machine Learning basics, you may want\\nto skip directly to Chapter 2\\n. If you are not sure, try to answer all\\nthe questions listed at the end of the chapter before moving on.\\nWhat Is Machine Learning?Machine Learning is the science (and art) of programming computers so they can\\nlearn from data\\n.Here is a slightly more general definition:\\n[Machine Learning is the] field of study that gives computers the ability to learn\\nwithout being explicitly programmed.›Arthur Samuel, \\n1959And a more engineering-oriented one:\\nA computer program is said to learn from experience E with respect to some task T\\nand some performance measure P, if its performance on T, as measured by P, improves\\nwith experience E.›Tom Mitchell, \\n1997For example, your spam filter is a Machine Learning program that can learn to flag\\nspam given examples of spam emails (e.g., flagged by users) and examples of regular\\n(nonspam, also called ƒham⁄) emails. The examples that the system uses to learn are\\ncalled the training set\\n. Each training example is called a \\ntraining instance\\n (or sample\\n).In this case, the task T is to flag spam for new emails, the experience E is the training\\ndata\\n, and the performance measure P needs to be defined; for example, you can use\\nthe ratio of correctly classified emails. This particular performance measure is called\\naccuracy\\n and it is often used in classification tasks.\\nIf you just download a copy of Wikipedia, your computer has a lot more data, but it is\\nnot suddenly better at any task. Thus, it is not Machine Learning.\\nWhy Use Machine Learning?Consider how you would write a spam filter using traditional programming techni…ques (Figure 1-1):1.First you would look at what spam typically looks like. You might notice that\\nsome words or phrases (such as ƒ4U,⁄ ƒcredit card,⁄ ƒfree,⁄ and ƒamazing⁄) tend to\\ncome up a lot in the subject. Perhaps you would also notice a few other patterns\\nin the sender‡s name, the email‡s body, and so on.\\n4 | Chapter 1: The Machine Learning Landscape\\n2.You would write a detection algorithm for each of the patterns that you noticed,\\nand your program would flag emails as spam if a number of these patterns are\\ndetected.3.You would test your program, and repeat steps 1 and 2 until it is good enough.\\nFigure 1-1. \\n•e traditional approach\\nSince the problem is not trivial, your program will likely become a long list of com…plex rules›pretty hard to maintain.\\nIn contrast, a spam filter based on Machine Learning techniques automatically learns\\nwhich words and phrases are good predictors of spam by detecting unusually fre…\\nquent patterns of words in the spam examples compared to the ham examples\\n(Figure 1-2). The program is much shorter, easier to maintain, and most likely more\\naccurate.\\nFigure 1-2. Machine Learning approach\\nWhy Use Machine Learning? | 5\\nMoreover, if spammers notice that all their emails containing ƒ4U⁄ are blocked, they\\nmight start writing ƒFor U⁄ instead. A spam filter using traditional programming\\ntechniques would need to be updated to flag ƒFor U⁄ emails. If spammers keep work…\\ning around your spam filter, you will need to keep writing new rules forever.\\nIn contrast, a spam filter based on Machine Learning techniques automatically noti…\\nces that ƒFor U⁄ has become unusually frequent in spam flagged by users, and it starts\\nflagging them without your intervention (\\nFigure 1-3).Figure 1-3. Automatically adapting to change\\nAnother area where Machine Learning shines is for problems that either are too com…\\nplex for traditional approaches or have no known algorithm. For example, consider \\nspeech recognition: say you want to start simple and write a program capable of dis…\\ntinguishing the words ƒone⁄ and ƒtwo.⁄ You might notice that the word ƒtwo⁄ starts\\nwith a high-pitch sound (ƒT⁄), so you could hardcode an algorithm that measures\\nhigh-pitch sound intensity and use that to distinguish ones and twos. Obviously this\\ntechnique will not scale to thousands of words spoken by millions of very different\\npeople in noisy environments and in dozens of languages. The best solution (at least\\ntoday) is to write an algorithm that learns by itself, given many example recordings\\nfor each word.Finally, Machine Learning can help humans learn (\\nFigure 1-4): ML algorithms can beinspected to see what they have learned (although for some algorithms this can be\\ntricky). For instance, once the spam filter has been trained on enough spam, it can\\neasily be inspected to reveal the list of words and combinations of words that it\\nbelieves are the best predictors of spam. Sometimes this will reveal unsuspected cor…relations or new trends, and thereby lead to a better understanding of the problem.\\nApplying ML techniques to dig into large amounts of data can help discover patterns\\nthat were not immediately apparent. This is called \\ndata mining\\n.6 | Chapter 1: The Machine Learning Landscape\\nFigure 1-4. Machine Learning can help humans learn\\nTo summarize, Machine Learning is great for:\\n‹Problems for which existing solutions require a lot of hand-tuning or long lists ofrules: one Machine Learning algorithm can often simplify code and perform bet…\\nter.\\n‹Complex problems for which there is no good solution at all using a traditional\\napproach: the best Machine Learning techniques can find a solution.\\n‹Fluctuating environments: a Machine Learning system can adapt to new data.\\n‹Getting insights about complex problems and large amounts of data.\\nTypes of Machine Learning SystemsThere are so many different types of Machine Learning systems that it is useful to\\nclassify them in broad categories based on:\\n‹Whether or not they are trained with human supervision (supervised, unsuper…\\nvised, semisupervised, and Reinforcement Learning)\\n‹Whether or not they can learn incrementally on the fly (online versus batch\\nlearning)‹Whether they work by simply comparing new data points to known data points,\\nor instead detect patterns in the training data and build a predictive model, much\\nlike scientists do (instance-based versus model-based learning)\\nThese criteria are not exclusive; you can combine them in any way you like. For\\nexample, a state-of-the-art spam filter may learn on the fly using a deep neural net…\\nTypes of Machine Learning Systems | 7\\n1Fun fact: this odd-sounding name is a statistics term introduced by Francis Galton while he was studying the\\nfact that the children of tall people tend to be shorter than their parents. Since children were shorter, he called\\nthis regression to the mean\\n. This name was then applied to the methods he used to analyze correlations\\nbetween variables.work model trained using examples of spam and ham; this makes it an online, model-\\nbased, supervised learning system.\\nLet‡s look at each of these criteria a bit more closely.\\nSupervised/Unsupervised LearningMachine Learning systems can be classified according to the amount and type of\\nsupervision they get during training. There are four major categories: supervised\\nlearning, unsupervised learning, semisupervised learning, and Reinforcement Learn…\\ning.Supervised learningIn supervised learning\\n, the training data you feed to the algorithm includes the desired\\nsolutions, called labels\\n (Figure 1-5).Figure 1-5. A labeled training set for supervised learning (e.g., spam \\nclassi†cation)A typical supervised learning task is \\nclassi†cation. The spam filter is a good example\\nof this: it is trained with many example emails along with their \\nclass\\n (spam or ham),and it must learn how to classify new emails.\\nAnother typical task is to predict a target\\n numeric value, such as the price of a car,\\ngiven a set of features\\n (mileage, age, brand, etc.) called predictors\\n. This sort of task is called regression\\n (\\nFigure 1-6).1 To train the system, you need to give it many examples\\nof cars, including both their predictors and their labels (i.e., their prices).8 | Chapter 1: The Machine Learning Landscape\\n2Some neural network architectures can be unsupervised, such as autoencoders and restricted Boltzmann\\nmachines. They can also be semisupervised, such as in deep belief networks and unsupervised pretraining.\\nIn Machine Learning an \\nattribute\\n is a data type (e.g., ƒMileage⁄),\\nwhile a feature\\n has several meanings depending on the context, but\\ngenerally means an attribute plus its value (e.g., ƒMileage =\\n15,000⁄). Many people use the words \\nattribute\\n and feature\\n inter…\\nchangeably, though.\\nFigure 1-6. Regression\\nNote that some regression algorithms can be used for classification as well, and vice\\nversa. For example, \\nLogistic Regression\\n is commonly used for classification, as it can\\noutput a value that corresponds to the probability of belonging to a given class (e.g.,\\n20% chance of being spam).Here are some of the most important supervised learning algorithms (covered in this\\nbook):‹k-Nearest Neighbors\\n‹Linear Regression‹Logistic Regression‹Support Vector Machines (SVMs)\\n‹Decision Trees and Random Forests\\n‹Neural networks\\n2Types of Machine Learning Systems | 9\\nUnsupervised learningIn unsupervised learning\\n, as you might guess, the training data is unlabeled\\n(Figure 1-7). The system tries to learn without a teacher.\\nFigure 1-7. An unlabeled training set for unsupervised learning\\nHere are some of the most important unsupervised learning algorithms (we will\\ncover dimensionality reduction in Chapter 8\\n):‹Clustering›k-Means\\n›Hierarchical Cluster Analysis (HCA)›Expectation Maximization\\n‹Visualization and dimensionality reduction\\n›Principal Component Analysis (PCA)\\n›Kernel PCA\\n›Locally-Linear Embedding (LLE)›t-distributed Stochastic Neighbor Embedding (t-SNE)\\n‹Association rule learning\\n›Apriori\\n›Eclat\\nFor example, say you have a lot of data about your blog‡s visitors. You may want to\\nrun a clustering\\n algorithm to try to detect groups of similar visitors (\\nFigure 1-8). At\\nno point do you tell the algorithm which group a visitor belongs to: it finds those\\nconnections without your help. For example, it might notice that 40% of your visitors\\nare males who love comic books and generally read your blog in the evening, while20% are young sci-fi lovers who visit during the weekends, and so on. If you use ahierarchical clustering\\n algorithm, it may also subdivide each group into smaller\\ngroups. This may help you target your posts for each group.\\n10 | Chapter 1: The Machine Learning Landscape\\n3Notice how animals are rather well separated from vehicles, how horses are close to deer but far from birds,\\nand so on. Figure reproduced with permission from Socher, Ganjoo, Manning, and Ng (2013), ƒT-SNE visual…\\nization of the semantic word space.⁄\\nFigure 1-8. Clustering\\nVisualization\\n algorithms \\nare also good examples of unsupervised learning algorithms:\\nyou feed them a lot of complex and unlabeled data, and they output a 2D or 3D rep…\\nresentation of your data that can easily be plotted (\\nFigure 1-9). These algorithms try\\nto preserve as much structure as they can (e.g., trying to keep separate clusters in the\\ninput space from overlapping in the visualization), so you can understand how the\\ndata is organized and perhaps identify unsuspected patterns.\\nFigure 1-9. Example of a t-SNE visualization highlighting semantic clusters\\n3Types of Machine Learning Systems | 11\\nA related task is \\ndimensionality reduction\\n, in which the goal is to simplify the data\\nwithout losing too much information. One way to do this is to merge several correla…\\nted features into one. For example, a car‡s mileage may be very correlated with its age,\\nso the dimensionality reduction algorithm will merge them into one feature that rep…\\nresents the car‡s wear and tear. This is called \\nfeature extraction\\n.It is often a good idea to try to reduce the dimension of your train…\\ning data using a dimensionality reduction algorithm before you\\nfeed it to another Machine Learning algorithm (such as a super…\\nvised learning algorithm). It will run much faster, the data will take\\nup less disk and memory space, and in some cases it may also per…\\nform better.\\nYet another important unsupervised task is \\nanomaly detection\\n›for example, detect…\\ning unusual credit card transactions to prevent fraud, catching manufacturing defects,\\nor automatically removing outliers from a dataset before feeding it to another learn…\\ning algorithm. The system is trained with normal instances, and when it sees a newinstance it can tell whether it looks like a normal one or whether it is likely an anom…aly (see Figure 1-10).Figure 1-10. Anomaly detection\\nFinally, another common unsupervised task is \\nassociation rule learning\\n, in which thegoal is to dig into large amounts of data and discover interesting relations between\\nattributes. For example, suppose you own a supermarket. Running an association rule\\non your sales logs may reveal that people who purchase barbecue sauce and potato\\nchips also tend to buy steak. Thus, you may want to place these items close to each \\nother.\\n12 | Chapter 1: The Machine Learning Landscape\\n4That‡s when the system works perfectly. In practice it often creates a few clusters per person, and sometimes\\nmixes up two people who look alike, so you need to provide a few labels per person and manually clean up\\nsome clusters.Semisupervised learningSome algorithms can deal with partially labeled training data, usually a lot of unla…\\nbeled data and a little bit of labeled data. This is called \\nsemisupervised learning\\n(Figure 1-11).Some photo-hosting services, such as \\nGoogle Photos, are good examples of this. Once\\nyou upload all your family photos to the service, it automatically recognizes that the\\nsame person A shows up in photos 1, 5, and 11, while another person B shows up inphotos 2, 5, and 7. This is the unsupervised part of the algorithm (clustering). Now all\\nthe system needs is for you to tell it who these people are. Just one label per person,\\n4and it is able to name everyone in every photo, which is useful for searching photos.\\nFigure 1-11. Semisupervised learning\\nMost semisupervised learning algorithms are combinations of unsupervised and\\nsupervised algorithms. For example, \\ndeep belief networks\\n (DBNs) are based on unsu…\\npervised components called \\nrestricted Boltzmann machines\\n (RBMs) stacked on top ofone another. RBMs are trained sequentially in an unsupervised manner, and then the\\nwhole system is fine-tuned using supervised learning techniques.\\nReinforcement LearningReinforcement Learning\\n is a very different beast. The learning system, called an \\nagent\\nin this context, can observe the environment, select and perform actions, and get\\nrewards\\n in return (or penalties\\n in the form of negative rewards, as in \\nFigure 1-12). It\\nmust then learn by itself what is the best strategy, called a \\npolicy\\n, to get the mostreward over time. A policy defines what action the agent should choose when it is in a\\ngiven situation.\\nTypes of Machine Learning Systems | 13\\nFigure 1-12. Reinforcement Learning\\nFor example, many robots implement Reinforcement Learning algorithms to learn\\nhow to walk. DeepMind‡s AlphaGo program is also a good example of Reinforcement\\nLearning: it made the headlines in March 2016 when it beat the world champion Lee\\nSedol at the game of \\nGo\\n. It learned its winning policy by analyzing millions of games,\\nand then playing many games against itself. Note that learning was turned off during\\nthe games against the champion; AlphaGo was just applying the policy it had learned.\\nBatch and Online LearningAnother criterion used to classify Machine Learning systems is whether or not the\\nsystem can learn incrementally from a stream of incoming data.\\nBatch learningIn batch learning\\n, the system is incapable of learning incrementally: it must be trained\\nusing all the available data. This will generally take a lot of time and computing\\nresources, so it is typically done offline. First the system is trained, and then it islaunched into production and runs without learning anymore; it just applies what it\\nhas learned. This is called o—ine learning\\n.If you want a batch learning system to know about new data (such as a new type of\\nspam), you need to train a new version of the system from scratch on the full dataset\\n(not just the new data, but also the old data), then stop the old system and replace it\\nwith the new one.Fortunately, the whole process of training, evaluating, and launching a Machine\\nLearning system can be automated fairly easily (as shown in \\nFigure 1-3), so even a14 | Chapter 1: The Machine Learning Landscape\\nbatch learning system can adapt to change. Simply update the data and train a new\\nversion of the system from scratch as often as needed.\\nThis solution is simple and often works fine, but training using the full set of data can\\ntake many hours, so you would typically train a new system only every 24 hours or\\neven just weekly. If your system needs to adapt to rapidly changing data (e.g., to pre…\\ndict stock prices), then you need a more reactive solution.Also, training on the full set of data requires a lot of computing resources (CPU,\\nmemory space, disk space, disk I/O, network I/O, etc.). If you have a lot of data and\\nyou automate your system to train from scratch every day, it will end up costing you a\\nlot of money. If the amount of data is huge, it may even be impossible to use a batch\\nlearning algorithm.Finally, if your system needs to be able to learn autonomously and it has limited\\nresources (e.g., a smartphone application or a rover on Mars), then carrying around\\nlarge amounts of training data and taking up a lot of resources to train for hours\\nevery day is a showstopper.\\nFortunately, a better option in all these cases is to use algorithms that are capable of\\nlearning incrementally.\\nOnline learningIn online learning\\n, you train the system incrementally by feeding it data instances\\nsequentially, either individually or by small groups called \\nmini-batches\\n. Each learningstep is fast and cheap, so the system can learn about new data on the fly, as it arrives\\n(see Figure 1-13).Figure 1-13. Online learning\\nOnline learning is great for systems that receive data as a continuous flow (e.g., stock\\nprices) and need to adapt to change rapidly or autonomously. It is also a good option\\nTypes of Machine Learning Systems | 15\\nif you have limited computing resources: once an online learning system has learned\\nabout new data instances, it does not need them anymore, so you can discard them\\n(unless you want to be able to roll back to a previous state and ƒreplay⁄ the data). This\\ncan save a huge amount of space.\\nOnline learning algorithms can also be used to train systems on huge datasets that\\ncannot fit in one machine‡s main memory (this is called \\nout-of-core\\n learning). Thealgorithm loads part of the data, runs a training step on that data, and repeats the\\nprocess until it has run on all of the data (see \\nFigure 1-14).This whole process is usually done offline (i.e., not on the live sys…tem), so online learning\\n can be a confusing name. Think of it asincremental learning\\n.Figure 1-14. Using online learning to handle huge datasets\\nOne important parameter of online learning systems is how fast they should adapt to\\nchanging data: this is called \\nthe learning rate\\n. If you set a high learning rate, then your\\nsystem will rapidly adapt to new data, but it will also tend to quickly forget the old\\ndata (you don‡t want a spam filter to flag only the latest kinds of spam it was shown).\\nConversely, if you set a low learning rate, the system will have more inertia; that is, it\\nwill learn more slowly, but it will also be less sensitive to noise in the new data or to\\nsequences of nonrepresentative data points.\\nA big challenge with online learning is that if bad data is fed to the system, the sys…\\ntem‡s performance will gradually decline. If we are talking about a live system, your\\nclients will notice. For example, bad data could come from a malfunctioning sensor\\non a robot, or from someone spamming a search engine to try to rank high in search\\n16 | Chapter 1: The Machine Learning Landscape\\nresults. To reduce this risk, you need to monitor your system closely and promptly\\nswitch learning off (and possibly revert to a previously working state) if you detect a\\ndrop in performance. You may also want to monitor the input data and react to\\nabnormal data (e.g., using an anomaly detection algorithm).\\nInstance-Based Versus Model-Based LearningOne more way to categorize Machine Learning systems is by how they \\ngeneralize\\n.Most Machine Learning tasks are about making predictions. This means that given a\\nnumber of training examples, the system needs to be able to generalize to examples it\\nhas never seen before. Having a good performance measure on the training data is\\ngood, but insufficient; the true goal is to perform well on new instances.\\nThere are two main approaches to generalization: instance-based learning and\\nmodel-based learning.Instance-based learningPossibly the most trivial form of learning is simply to learn by heart. If you were to\\ncreate a spam filter this way, it would just flag all emails that are identical to emails\\nthat have already been flagged by users›not the worst solution, but certainly not the\\nbest.Instead of just flagging emails that are identical to known spam emails, your spam\\nfilter could be programmed to also flag emails that are very similar to known spam\\nemails. This requires a measure of similarity\\n between two emails. A (very basic) simi…\\nlarity measure between two emails could be to count the number of words they have\\nin common. The system would flag an email as spam if it has many words in com…\\nmon with a known spam email.This is called instance-based learning\\n: the system learns the examples by heart, then\\ngeneralizes to new cases using a similarity measure (Figure 1-15).Figure 1-15. Instance-based learning\\nTypes of Machine Learning Systems | 17\\nModel-based learningAnother way to generalize from a set of examples is to build a model of these exam…\\nples, then use that model to make \\npredictions\\n. This is called model-based learning\\n(Figure 1-16).Figure 1-16. Model-based learning\\nFor example, suppose you want to know if money makes people happy, so you down…\\nload the Better Life Index\\n data from the \\nOECD‡s website\\n as well as stats about GDP\\nper capita from the \\nIMF‡s website\\n. Then you join the tables and sort by GDP per cap…\\nita. Table 1-1\\n shows an excerpt of what you get.\\nTable 1-1. Does money make people happier?\\nCountryGDP per capita (USD)\\nLife satisfaction\\nHungary12,2404.9Korea27,1955.8France37,6756.5Australia50,9627.3United States\\n55,8057.2Let‡s plot the data for a few random countries (\\nFigure 1-17).18 | Chapter 1: The Machine Learning Landscape\\n5By convention, the Greek letter − (theta) is frequently used to represent model parameters.\\nFigure 1-17. Do you see a trend here?\\nThere does seem to be a trend here! Although the data is \\nnoisy\\n (i.e., partly random), it\\nlooks like life satisfaction goes up more or less linearly as the country‡s GDP per cap…\\nita increases. So you decide to model life satisfaction as a linear function of GDP per\\ncapita. This step is called \\nmodel selection\\n: you selected a linear model\\n of life satisfac…\\ntion with just one attribute, GDP per capita (\\nEquation 1-1\\n).Equation 1-1. A simple linear model\\nlife\\n_satisfaction\\n=–0+–1‰GDP_per_capita\\nThis model has two model parameters\\n, –0 and –1.5 By tweaking these parameters, youcan make your model represent any linear function, as shown in \\nFigure 1-18.Figure 1-18. A few possible linear models\\nTypes of Machine Learning Systems | 19\\n6The code assumes that \\nprepare_country_stats() is already defined: it merges the GDP and life satisfaction\\ndata into a single Pandas dataframe.\\n7It‡s okay if you don‡t understand all the code yet; we will present Scikit-Learn in the following chapters.\\nBefore you can use your model, you need to define the parameter values \\n–0 and –1.How can you know which values will make your model perform best? To answer this\\nquestion, you need to specify a performance measure. You can either define a \\nutility\\nfunction\\n (or \\n†tness function\\n) that measures how \\ngood\\n your model is, or you can definea cost function\\n that measures how \\nbad\\n it is. For linear regression problems, people\\ntypically use a cost function that measures the distance between the linear model‡s\\npredictions and the training examples; the objective is to minimize this distance.\\nThis is where the Linear Regression algorithm comes in: you feed it your trainingexamples and it finds the parameters that make the linear model fit best to your data.\\nThis is called training\\n the model. In our case the algorithm finds that the optimal\\nparameter values are –0 = 4.85 and –1 = 4.91 ‰ 10–5.Now the model fits the training data as closely as possible (for a linear model), as you\\ncan see in Figure 1-19.Figure 1-19. \\n•e linear model that \\n†ts the training data best\\nYou are finally ready to run the model to make predictions. For example, say you\\nwant to know how happy Cypriots are, and the OECD data does not have the answer.\\nFortunately, you can use your model to make a good prediction: you look up Cyprus‡s\\nGDP per capita, find $22,587, and then apply your model and find that life satisfac…\\ntion is likely to be somewhere around 4.85 + 22,587 ‰ 4.91 ‰ 10-5 = 5.96.To whet your appetite, \\nExample 1-1\\n shows the Python code that loads the data, pre…\\npares it,6 creates a scatterplot for visualization, and then trains a linear model and\\nmakes a prediction.720 | Chapter 1: The Machine Learning Landscape\\nExample 1-1. Training and running a linear model using Scikit-Learn\\nimport matplotlibimport matplotlib.pyplot as pltimport numpy as npimport pandas as pdimport sklearn# Load the dataoecd_bli = pd.read_csv(\"oecd_bli_2015.csv\", thousands=•,•)gdp_per_capita = pd.read_csv(\"gdp_per_capita.csv\",thousands=•,•,delimiter=•\\\\t•,                             encoding=•latin1•, na_values=\"n/a\")# Prepare the datacountry_stats = prepare_country_stats(oecd_bli, gdp_per_capita)X = np.c_[country_stats[\"GDP per capita\"]]y = np.c_[country_stats[\"Life satisfaction\"]]# Visualize the datacountry_stats.plot(kind=•scatter•, x=\"GDP per capita\", y=•Life satisfaction•)plt.show()# Select a linear modellin_reg_model = sklearn.linear_model.LinearRegression()# Train the modellin_reg_model.fit(X, y)# Make a prediction for CyprusX_new = [[22587]]  # Cyprus• GDP per capitaprint(lin_reg_model.predict(X_new)) # outputs [[ 5.96242338]]If you had used an instance-based learning algorithm instead, youwould have found that Slovenia has the closest GDP per capita to\\nthat of Cyprus ($20,732), and since the OECD data tells us that\\nSlovenians‡ life satisfaction is 5.7, you would have predicted a life\\nsatisfaction of 5.7 for Cyprus. If you zoom out a bit and look at the\\ntwo next closest countries, you will find Portugal and Spain with\\nlife satisfactions of 5.1 and 6.5, respectively. Averaging these three\\nvalues, you get 5.77, which is pretty close to your model-based pre…diction. This simple algorithm is called \\nk-Nearest Neighbors\\n regres…sion (in this example, \\nk = 3).Replacing the Linear Regression model with k-Nearest Neighbors\\nregression in the previous code is as simple as replacing this line:\\nclf = sklearn.linear_model.LinearRegression()with this one:clf = sklearn.neighbors.KNeighborsRegressor(n_neighbors=3)Types of Machine Learning Systems | 21\\nIf all went well, your model will make good predictions. If not, you may need to use\\nmore attributes (employment rate, health, air pollution, etc.), get more or better qual…\\nity training data, or perhaps select a more powerful model (e.g., a Polynomial Regres…\\nsion model).In summary:\\n‹You studied the data.\\n‹You selected a model.\\n‹You trained it on the training data (i.e., the learning algorithm searched for the\\nmodel parameter values that minimize a cost function).\\n‹Finally, you applied the model to make predictions on new cases (this is called\\ninference\\n), hoping that this model will generalize well.\\nThis is what a typical Machine Learning project looks like. In \\nChapter 2\\n you willexperience this first-hand by going through an end-to-end project.We have covered a lot of ground so far: you now know what Machine Learning is\\nreally about, why it is useful, what some of the most common categories of ML sys…\\ntems are, and what a typical project workflow looks like. Now let‡s look at what can go\\nwrong in learning and prevent you from making accurate predictions.\\nMain Challenges of Machine LearningIn short, since your main task is to select a learning algorithm and train it on somedata, the two things that can go wrong are ƒbad algorithm⁄ and ƒbad data.⁄ Let‡s start\\nwith examples of bad data.\\nInsu…cient Quantity of Training DataFor a toddler to learn what an apple is, all it takes is for you to point to an apple and\\nsay ƒapple⁄ (possibly repeating this procedure a few times). Now the child is able to\\nrecognize apples in all sorts of colors and shapes. Genius.\\nMachine Learning is not quite there yet; it takes a lot of data for most Machine Learn…\\ning algorithms to work properly. Even for very simple problems you typically need\\nthousands of examples, and for complex problems such as image or speech recogni…\\ntion you may need millions of examples (unless you can reuse parts of an existing\\nmodel).22 | Chapter 1: The Machine Learning Landscape\\n8For example, knowing whether to write ƒto,⁄ ƒtwo,⁄ or ƒtoo⁄ depending on the context.\\n9Figure reproduced with permission from Banko and Brill (2001), ƒLearning Curves for Confusion Set Disam…\\nbiguation.⁄\\n10ƒThe Unreasonable Effectiveness of Data,⁄ Peter Norvig et al. (2009).\\nThe Unreasonable E†ectiveness of DataIn a famous paper\\n published in 2001, Microsoft researchers Michele Banko and EricBrill showed that very different Machine Learning algorithms, including fairly simple\\nones, performed almost identically well on a complex problem of natural language\\ndisambiguation\\n8 once they were given enough data (as you can see in \\nFigure 1-20).Figure 1-20. \\n•e importance of data versus algorithms\\n9As the authors put it: ƒthese results suggest that we may want to reconsider the trade-\\noff between spending time and money on algorithm development versus spending it\\non corpus development.⁄\\nThe idea that data matters more than algorithms for complex problems was further\\npopularized by Peter Norvig et al. in a paper titled \\nƒThe Unreasonable Effectiveness\\nof Data⁄\\n published in 2009.10 It should be noted, however, that small- and medium-\\nsized datasets are still very common, and it is not always easy or cheap to get extra\\ntraining data, so don‡t abandon algorithms just yet.\\nMain Challenges of Machine Learning | 23\\nNonrepresentative Training DataIn order to generalize well, it is crucial that your training data be representative of the\\nnew cases you want to generalize to. This is true whether you use instance-based\\nlearning or model-based learning.For example, the set of countries we used earlier for training the linear model was not\\nperfectly representative; a few countries were missing. \\nFigure 1-21 shows what the\\ndata looks like when you add the missing countries.\\nFigure 1-21. A more representative training sample\\nIf you train a linear model on this data, you get the solid line, while the old model is\\nrepresented by the dotted line. As you can see, not only does adding a few missing\\ncountries significantly alter the model, but it makes it clear that such a simple linear\\nmodel is probably never going to work well. It seems that very rich countries are not\\nhappier than moderately rich countries (in fact they seem unhappier), and conversely\\nsome poor countries seem happier than many rich countries.\\nBy using a nonrepresentative training set, we trained a model that is unlikely to make\\naccurate predictions, especially for very poor and very rich countries.\\nIt is crucial to use a training set that is representative of the cases you want to general…\\nize to. This is often harder than it sounds: if the sample is too small, you will \\nhave\\nsampling noise\\n (i.e., nonrepresentative data as a result of chance), but even very large\\nsamples can be nonrepresentative if the sampling method is flawed. This is called\\nsampling bias\\n.A Famous Example of Sampling BiasPerhaps the most famous example of sampling bias happened during the US presi…\\ndential election in 1936, which pitted Landon against Roosevelt: the \\nLiterary Digest\\nconducted a very large poll, sending mail to about 10 million people. It got 2.4 million\\nanswers, and predicted with high confidence that Landon would get 57% of the votes.\\n24 | Chapter 1: The Machine Learning Landscape\\nInstead, Roosevelt won with 62% of the votes. The flaw was in the \\nLiterary Digest\\n‡s\\nsampling method:\\n‹First, to obtain the addresses to send the polls to, the \\nLiterary Digest\\n used tele…phone directories, lists of magazine subscribers, club membership lists, and thelike. All of these lists tend to favor wealthier people, who are more likely to vote\\nRepublican (hence Landon).‹Second, less than 25% of the people who received the poll answered. Again, thisintroduces a sampling bias, by ruling out people who don‡t care much about poli…\\ntics, people who don‡t like the \\nLiterary Digest\\n, and other key groups. This is a spe…cial type of sampling bias called \\nnonresponse bias\\n.Here is another example: say you want to build a system to recognize funk music vid…\\neos. One way to build your training set is to search ƒfunk music⁄ on YouTube and use\\nthe resulting videos. But this assumes that YouTube‡s search engine returns a set of\\nvideos that are representative of all the funk music videos on YouTube. In reality, the\\nsearch results are likely to be biased toward popular artists (and if you live in Brazilyou will get a lot of ƒfunk carioca⁄ videos, which sound nothing like James Brown).\\nOn the other hand, how else can you get a large training set?Poor-Quality DataObviously, if your training data is full of errors, outliers, and noise (e.g., due to poor-\\nquality measurements), it will make it harder for the system to detect the underlying\\npatterns, so your system is less likely to perform well. It is often well worth the effort\\nto spend time cleaning up your training data. The truth is, most data scientists spend\\na significant part of their time doing just that. For example:\\n‹If some instances are clearly outliers, it may help to simply discard them or try to\\nfix the errors manually.\\n‹If some instances are missing a few features (e.g., 5% of your customers did not\\nspecify their age), you must decide whether you want to ignore this attribute alto…\\ngether, ignore these instances, fill in the missing values (e.g., with the median\\nage), or train one model with the feature and one model without it, and so on.\\nIrrelevant FeaturesAs the saying goes: garbage in, garbage out. Your system will only be capable of learn…\\ning if the training data contains enough relevant features and not too many irrelevant\\nones. A critical part of the success of a Machine Learning project is coming up with a\\ngood set of features to train on. This process, called \\nfeature engineering\\n, involves:\\nMain Challenges of Machine Learning | 25\\n‹Feature selection\\n: selecting the most useful features to train on among existing\\nfeatures.\\n‹Feature extraction\\n: combining existing features to produce a more useful one (as\\nwe saw earlier, dimensionality reduction algorithms can help).\\n‹Creating new features by gathering new data.\\nNow that we have looked at many examples of bad data, let‡s look at a couple of exam…\\nples of bad algorithms.Over•tting the Training DataSay you are visiting a foreign country and the taxi driver rips you off. You might be\\ntempted to say that \\nall\\n taxi drivers in that country are thieves. Overgeneralizing is\\nsomething that we humans do all too often, and unfortunately machines can fall into\\nthe same trap if we are not careful. In Machine Learning this is called \\nover†tting: itmeans that the model performs well on the training data, but it does not generalize\\nwell.Figure 1-22 shows an example of a high-degree polynomial life satisfaction model\\nthat strongly overfits the training data. Even though it performs much better on the\\ntraining data than the simple linear model, would you really trust its predictions?\\nFigure 1-22. \\nOver†tting the training data\\nComplex models such as deep neural networks can detect subtle patterns in the data,\\nbut if the training set is noisy, or if it is too small (which introduces sampling noise),\\nthen the model is likely to detect patterns in the noise itself. Obviously these patterns\\nwill not generalize to new instances. For example, say you feed your life satisfaction\\nmodel many more attributes, including uninformative ones such as the country‡s\\nname. In that case, a complex model may detect patterns like the fact that all coun…\\ntries in the training data with a \\nw in their name have a life satisfaction greater than 7:\\nNew Zealand (7.3), Norway (7.4), Sweden (7.2), and Switzerland (7.5). How confident\\n26 | Chapter 1: The Machine Learning Landscape\\nare you that the W-satisfaction rule generalizes to Rwanda or Zimbabwe? Obviously\\nthis pattern occurred in the training data by pure chance, but the model has no way\\nto tell whether a pattern is real or simply the result of noise in the data.\\nOverfitting happens when the model is too complex relative to the\\namount and noisiness of the training data. The possible solutions\\nare:‹To simplify the model by selecting one with fewer parameters\\n(e.g., a linear model rather than a high-degree polynomial\\nmodel), by reducing the number of attributes in the training\\ndata or by constraining the model\\n‹To gather more training data\\n‹To reduce the noise in the training data (e.g., fix data errors\\nand remove outliers)Constraining a model to make it simpler and reduce the risk of overfitting is \\ncalledregularization\\n. For example, the linear model we defined earlier has two parameters,\\n–0 and \\n–1. This gives the learning algorithm two degrees of freedom\\n to \\nadapt the model\\nto the training data: it can tweak both the height (\\n–0) and the slope (–1) of the line. Ifwe forced –1 = 0, the algorithm would have only one degree of freedom and would\\nhave a much harder time fitting the data properly: all it could do is move the line up\\nor down to get as close as possible to the training instances, so it would end uparound the mean. A very simple model indeed! If we allow the algorithm to modify \\n–1but we force it to keep it small, then the learning algorithm will effectively have some…\\nwhere in between one and two degrees of freedom. It will produce a simpler model\\nthan with two degrees of freedom, but more complex than with just one. You want to\\nfind the right balance between fitting the data perfectly and keeping the model simple\\nenough to ensure that it will generalize well.\\nFigure 1-23 shows three models: the dotted line represents the original model that\\nwas trained with a few countries missing, the dashed line is our second model trained\\nwith all countries, and the solid line is a linear model trained with the same data as\\nthe first model but with a regularization constraint. You can see that regularization\\nforced the model to have a smaller slope, which fits a bit less the training data that the\\nmodel was trained on, but actually allows it to generalize better to new examples.\\nMain Challenges of Machine Learning | 27\\nFigure 1-23. Regularization reduces the risk of \\nover†ttingThe amount of regularization to apply during learning can be controlled by a \\nhyper…\\nparameter\\n. A hyperparameter is a parameter of a learning algorithm (not of the\\nmodel). As such, it is not affected by the learning algorithm itself; it must be set prior\\nto training and remains constant during training. If you set the regularization hyper…\\nparameter to a very large value, you will get an almost flat model (a slope close to\\nzero); the learning algorithm will almost certainly not overfit the training data, but it\\nwill be less likely to find a good solution. Tuning hyperparameters is an important\\npart of building a Machine Learning system (you will see a detailed example in the\\nnext chapter).\\nUnder•tting the Training DataAs you might guess, \\nunder†tting is the opposite of overfitting: it occurs when yourmodel is too simple to learn the underlying structure of the data. For example, a lin…\\near model of life satisfaction is prone to underfit; reality is just more complex than\\nthe model, so its predictions are bound to be inaccurate, even on the training exam…\\nples.The main options to fix this problem are:‹Selecting a more powerful model, with more parameters‹Feeding better features to the learning algorithm (feature engineering)\\n‹Reducing the constraints on the model (e.g., reducing the regularization hyper…\\nparameter)Stepping BackBy now you already know a lot about Machine Learning. However, we went through\\nso many concepts that you may be feeling a little lost, so let‡s step back and look at the\\nbig picture:28 | Chapter 1: The Machine Learning Landscape\\n‹Machine Learning is about making machines get better at some task by learning\\nfrom data, instead of having to explicitly code rules.\\n‹There are many different types of ML systems: supervised or not, batch or online,\\ninstance-based or model-based, and so on.‹In a ML project you gather data in a training set, and you feed the training set to\\na learning algorithm. If the algorithm is model-based it tunes some parameters tofit the model to the training set (i.e., to make good predictions on the training setitself), and then hopefully it will be able to make good predictions on new cases\\nas well. If the algorithm is instance-based, it just learns the examples by heart and\\nuses a similarity measure to generalize to new instances.‹The system will not perform well if your training set is too small, or if the data is\\nnot representative, noisy, or polluted with irrelevant features (garbage in, garbage\\nout). Lastly, your model needs to be neither too simple (in which case it will\\nunderfit) nor too complex (in which case it will overfit).\\nThere‡s just one last important topic to cover: once you have trained a model, you\\ndon‡t want to just ƒhope⁄ it generalizes to new cases. You want to evaluate it, and fine-\\ntune it if necessary. Let‡s see how.\\nTesting and ValidatingThe only way to know how well a model will generalize to new cases is to actually try\\nit out on new cases. One way to do that is to put your model in production and moni…\\ntor how well it performs. This works well, but if your model is horribly bad, yourusers will complain›not the best idea.\\nA better option is to split your data into two sets: the \\ntraining set\\n and the test set\\n. Asthese names imply, you train your model using the training set, and you test it using\\nthe test set. The error rate on new cases is called the \\ngeneralization error\\n (or out-of-\\nsample error\\n), and by evaluating your model on the test set, you get an estimation of\\nthis error. This value tells you how well your model will perform on instances it has\\nnever seen before.If the training error is low (i.e., your model makes few mistakes on the training set)but the generalization error is high, it means that your model is overfitting the train…\\ning data.\\nIt is common to use 80% of the data for training and \\nhold out\\n 20%for testing.Testing and Validating | 29\\n11ƒThe Lack of A Priori Distinctions Between Learning Algorithms,⁄ D. Wolperts (1996).\\nSo evaluating a model is simple enough: just use a test set. Now suppose you are hesi…\\ntating between two models (say a linear model and a polynomial model): how can\\nyou decide? One option is to train both and compare how well they generalize using\\nthe test set.Now suppose that the linear model generalizes better, but you want to apply some \\nregularization to avoid overfitting. The question is: how do you choose the value of\\nthe regularization hyperparameter? One option is to train 100 different models using\\n100 different values for this hyperparameter. Suppose you find the best hyperparame…\\nter value that produces a model with the lowest generalization error, say just 5% error.\\nSo you launch this model into production, but unfortunately it does not perform as\\nwell as expected and produces 15% errors. What just happened?\\nThe problem is that you measured the generalization error multiple times on the test\\nset, and you adapted the model and hyperparameters to produce the best model \\nfor\\nthat set\\n. This means that the model is unlikely to perform as well on new data.\\nA common solution to this problem is to have a second holdout set called the \\nvalida…\\ntion set\\n. You train multiple models with various hyperparameters using the training\\nset, you select the model and hyperparameters that perform best on the validation set,\\nand when you‡re happy with your model you run a single final test against the test set\\nto get an estimate of the generalization error.\\nTo avoid ƒwasting⁄ too much training data in validation sets, a common technique is\\nto use cross-validation\\n: the training set is split into complementary subsets, and each\\nmodel is trained against a different combination of these subsets and validated\\nagainst the remaining parts. Once the model type and hyperparameters have been\\nselected, a final model is trained using these hyperparameters on the full training set,\\nand the generalized error is measured on the test set.No Free Lunch TheoremA model is a simplified version of the observations. The simplifications are meant to\\ndiscard the superfluous details that are unlikely to generalize to new instances. How…\\never, to decide what data to discard and what data to keep, you must make \\nassump…\\ntions\\n. For example, a linear model makes the assumption that the data is\\nfundamentally linear and that the distance between the instances and the straight line\\nis just noise, which can safely be ignored.In a famous 1996 paper\\n,11 David Wolpert demonstrated that if you make absolutely\\nno assumption about the data, then there is no reason to prefer one model over any\\nother. This is called the \\nNo Free Lunch\\n (NFL) theorem. For some datasets the best\\n30 | Chapter 1: The Machine Learning Landscape\\nmodel is a linear model, while for other datasets it is a neural network. There is no\\nmodel that is \\na priori\\n guaranteed to work better (hence the name of the theorem). The\\nonly way to know for sure which model is best is to evaluate them all. Since this is not\\npossible, in practice you make some reasonable assumptions about the data and you\\nevaluate only a few reasonable models. For example, for simple tasks you may evalu…\\nate linear models with various levels of regularization, and for a complex problem you\\nmay evaluate various neural networks.\\nExercisesIn this chapter we have covered some of the most important concepts in Machine\\nLearning. In the next chapters we will dive deeper and write more code, but before we\\ndo, make sure you know how to answer the following questions:\\n1.How would you define Machine Learning?\\n2.Can you name four types of problems where it shines?3.What is a labeled training set?\\n4.What are the two most common supervised tasks?\\n5.Can you name four common unsupervised tasks?\\n6.What type of Machine Learning algorithm would you use to allow a robot to\\nwalk in various unknown terrains?7.What type of algorithm would you use to segment your customers into multiple\\ngroups?8.Would you frame the problem of spam detection as a supervised learning prob…\\nlem or an unsupervised learning problem?\\n9.What is an online learning system?\\n10.What is out-of-core learning?\\n11.What type of learning algorithm relies on a similarity measure to make predic…\\ntions?12.What is the difference between a model parameter and a learning algorithm‡s\\nhyperparameter?\\n13.What do model-based learning algorithms search for? What is the most common\\nstrategy they use to succeed? How do they make predictions?\\n14.Can you name four of the main challenges in Machine Learning?\\n15.If your model performs great on the training data but generalizes poorly to new\\ninstances, what is happening? Can you name three possible solutions?\\n16.What is a test set and why would you want to use it?\\nExercises | 31\\n17.What is the purpose of a validation set?\\n18.What can go wrong if you tune hyperparameters using the test set?\\n19.What is cross-validation and why would you prefer it to a validation set?\\nSolutions to these exercises are available in \\nAppendix A\\n.32 | Chapter 1: The Machine Learning Landscape\\n1The example project is completely fictitious; the goal is just to illustrate the main steps of a Machine Learning\\nproject, not to learn anything about the real estate business.\\nCHAPTER 2End-to-End Machine Learning ProjectIn this chapter, you will go through an example project end to end, pretending to be a\\nrecently hired data scientist in a real estate company.\\n1 Here are the main steps you will\\ngo through:1.Look at the big picture.\\n2.Get the data.\\n3.Discover and visualize the data to gain insights.\\n4.Prepare the data for Machine Learning algorithms.\\n5.Select a model and train it.6.Fine-tune your model.7.Present your solution.\\n8.Launch, monitor, and maintain your system.\\nWorking with Real DataWhen you are learning about Machine Learning it is best to actually experiment with\\nreal-world data, not just artificial datasets. Fortunately, there are thousands of open\\ndatasets to choose from, ranging across all sorts of domains. Here are a few places\\nyou can look to get data:\\n‹Popular open data repositories:\\n332The original dataset appeared in R. Kelley Pace and Ronald Barry, ƒSparse Spatial Autoregressions,⁄ \\nStatistics\\n& Probability Letters\\n 33, no. 3 (1997): 291–297.\\n›UC Irvine Machine Learning Repository\\n›Kaggle datasets\\n›Amazon‡s AWS datasets\\n‹Meta portals (they list open data repositories):\\n›http://dataportals.org/\\n›http://opendatamonitor.eu/\\n›http://quandl.com/\\n‹Other pages listing many popular open data repositories:\\n›Wikipedia‡s list of Machine Learning datasets\\n›Quora.com question›Datasets subreddit\\nIn this chapter we chose the California Housing Prices dataset from the StatLib repos…\\nitory\\n2 (see Figure 2-1). This dataset was based on data from the 1990 California cen…\\nsus. It is not exactly recent (you could still afford a nice house in the Bay Area at the\\ntime), but it has many qualities for learning, so we will pretend it is recent data. We\\nalso added a categorical attribute and removed a few features for teaching purposes.\\nFigure 2-1. California housing prices\\n34 | Chapter 2: End-to-End Machine Learning Project\\n3A piece of information fed to a Machine Learning system is often called a \\nsignal\\n in reference to Shannon‡s\\ninformation theory: you want a high signal/noise ratio.\\nLook at the Big PictureWelcome to Machine Learning Housing Corporation! The first task you are asked to\\nperform is to build a model of housing prices in California using the California cen…sus data. This data has metrics such as the population, median income, median hous…\\ning price, and so on for each block group in California. Block groups are the smallestgeographical unit for which the US Census Bureau publishes sample data (a block\\ngroup typically has a population of 600 to 3,000 people). We will just call them ƒdis…\\ntricts⁄ for short.\\nYour model should learn from this data and be able to predict the median housing\\nprice in any district, given all the other metrics.\\nSince you are a well-organized data scientist, the first thing you do\\nis to pull out your Machine Learning project checklist. You can\\nstart with the one in Appendix B\\n; it should work reasonably wellfor most Machine Learning projects but make sure to adapt it to\\nyour needs. In this chapter we will go through many checklist\\nitems, but we will also skip a few, either because they are self-\\nexplanatory or because they will be discussed in later chapters.\\nFrame the ProblemThe first question to ask your boss is what exactly is the business objective; building a\\nmodel is probably not the end goal. How does the company expect to use and benefit\\nfrom this model? This is important because it will determine how you frame the\\nproblem, what algorithms you will select, what performance measure you will use to\\nevaluate your model, and how much effort you should spend tweaking it.\\nYour boss answers that your model‡s output (a prediction of a district‡s median hous…\\ning price) will be fed to another Machine Learning system (see \\nFigure 2-2), alongwith many other \\nsignals\\n.3 This downstream system will determine whether it is worthinvesting in a given area or not. Getting this right is critical, as it directly affects reve…\\nnue.\\nLook at the Big Picture | 35\\nFigure 2-2. A Machine Learning pipeline for real estate investments\\nPipelinesA sequence of data processing \\ncomponents\\n is called a data \\npipeline\\n. Pipelines are very\\ncommon in Machine Learning systems, since there is a lot of data to manipulate and\\nmany data transformations to apply.\\nComponents typically run asynchronously. Each component pulls in a large amount\\nof data, processes it, and spits out the result in another data store, and then some time\\nlater the next component in the pipeline pulls this data and spits out its own output,\\nand so on. Each component is fairly self-contained: the interface between components\\nis simply the data store. This makes the system quite simple to grasp (with the help of\\na data flow graph), and different teams can focus on different components. Moreover,\\nif a component breaks down, the downstream components can often continue to run\\nnormally (at least for a while) by just using the last output from the broken compo…\\nnent. This makes the architecture quite robust.\\nOn the other hand, a broken component can go unnoticed for some time if proper\\nmonitoring is not implemented. The data gets stale and the overall system‡s perfor…\\nmance drops.The next question to ask is what the current solution looks like (if any). It will often\\ngive you a reference performance, as well as insights on how to solve the problem.\\nYour boss answers that the district housing prices are currently estimated manually\\nby experts: a team gathers up-to-date information about a district (excluding median\\nhousing prices), and they use complex rules to come up with an estimate. This is\\ncostly and time-consuming, and their estimates are not great; their typical error rate\\nis about 15%.Okay, with all this information you are now ready to start designing your system.\\nFirst, you need to frame the problem: is it supervised, unsupervised, or Reinforce…\\nment Learning? Is it a classification task, a regression task, or something else? Should\\n36 | Chapter 2: End-to-End Machine Learning Project\\n4The standard deviation, generally denoted „ (the Greek letter sigma), is the square root of the \\nvariance\\n, whichis the average of the squared deviation from the mean.\\n5When a feature has a bell-shaped \\nnormal distribution\\n (also called a Gaussian distribution\\n), which is very com…\\nmon, the ƒ68-95-99.7⁄ rule applies: about 68% of the values fall within 1„ of the mean, 95% within 2„, and\\n99.7% within 3„.you use batch learning or online learning techniques? Before you read on, pause and\\ntry to answer these questions for yourself.\\nHave you found the answers? Let‡s see: it is clearly a typical supervised learning task\\nsince you are given labeled\\n training examples (each instance comes with the expected\\noutput, i.e., the district‡s median housing price). Moreover, it is also a typical regres…\\nsion task, since you are asked to predict a value. More specifically, this is a \\nmultivari…\\nate regression\\n problem since the system will use multiple features to make a prediction\\n(it will use the district‡s population, the median income, etc.). In the first chapter, you\\npredicted life satisfaction based on just one feature, the GDP per capita, so it was \\naunivariate regression\\n problem. Finally, there is no continuous flow of data coming in\\nthe system, there is no particular need to adjust to changing data rapidly, and the data\\nis small enough to fit in memory, so plain batch learning should do just fine.\\nIf the data was huge, you could either split your batch learning\\nwork across multiple servers (using the \\nMapReduce\\n technique, aswe will see later), or you could use an online learning \\ntechniqueinstead.Select a Performance MeasureYour \\nnext step is to select a performance measure. A typical performance measure forregression problems is the Root Mean Square Error (RMSE). It measures the \\nstandard\\ndeviation\\n4 of the errors the system makes in its predictions. For example, an RMSE\\nequal to 50,000 means that about 68% of the system‡s predictions fall within $50,000\\nof the actual value, and about 95% of the predictions fall within $100,000 of the actualvalue.5 Equation 2-1\\n shows the mathematical formula to compute the RMSE.\\nEquation 2-1. Root Mean Square Error (RMSE)\\nRMSE,h=1m“i=1\\nmhi”yi2Look at the Big Picture | 37\\n6Recall that the transpose operator flips a column vector into a row vector (and vice versa).\\nNotationsThis equation introduces several very common Machine Learning notations that we\\nwill use throughout this book:‹m is the number of instances in the dataset you are measuring the RMSE on.\\n›For example, if you are evaluating the RMSE on a validation set of 2,000 dis…\\ntricts, then m = 2,000.‹x(i) is a vector of all the feature values (excluding the label) of the \\nith\\n instance inthe dataset, and \\ny(i) is its label (the desired output value for that instance).\\n›For example, if the first district in the dataset is located at longitude –118.29‘,\\nlatitude 33.91‘, and it has 1,416 inhabitants with a median income of $38,372,\\nand the median house value is $156,400 (ignoring the other features for now),\\nthen:1=”118.29\\n33.91\\n1,416\\n38,372\\nand:y1=156,400\\n‹X is a matrix containing all the feature values (excluding labels) of all instances in\\nthe dataset. There is one row per instance and the \\nith\\n row is equal to the transpose\\nof x(i), noted (x(i))T.6›For example, if the first district is as just described, then the matrix \\nX lookslike this:=1T2T1999T2000T=”118.2933.911,41638,372\\n\\n38 | Chapter 2: End-to-End Machine Learning Project\\n‹h is your system‡s prediction function, also called \\na hypothesis\\n. When your systemis given an instance‡s feature vector \\nx(i), it outputs a predicted value ƒ(i) = h(x(i))for that instance (\\nƒ is pronounced ƒy-hat⁄).\\n›For example, if your system predicts that the median housing price in the first\\ndistrict is $158,400, then ƒ(1) = h(x(1)) = 158,400. The prediction error for thisdistrict is ƒ(1) – y(1) = 2,000.‹RMSE(X,h) is the cost function measured on the set of examples using your\\nhypothesis \\nh.We use lowercase italic font for scalar values (such as \\nm or y(i)) and function names(such as h), lowercase bold font for vectors (such as \\nx(i)), and uppercase bold font for\\nmatrices (such as \\nX).Even though the RMSE is generally the preferred performance measure for regression\\ntasks, in some contexts you may prefer to use another function. For example, suppose\\nthat there are many outlier districts. In that case, you may consider using \\nthe Mean\\nAbsolute Error\\n (also called the Average Absolute Deviation; see \\nEquation 2-2\\n):Equation 2-2. Mean Absolute Error\\nMAE,h=1m“i=1\\nmhi”yiBoth the RMSE and the MAE are ways to measure the distance between two vectors:\\nthe vector of predictions and the vector of target values. Various distance measures,\\nor norms\\n, are possible:‹Computing the root of a sum of squares (RMSE) corresponds to the \\nEuclidian\\nnorm\\n: it is the notion of distance you are familiar with. It is also called the —\\n2norm\\n, noted  ’ 2 (or just  ’ ).‹Computing the sum of absolutes (MAE) corresponds to the —\\n1 norm\\n, noted \\n ’ 1.It is sometimes called the \\nManhattan norm\\n because it measures the distance\\nbetween two points in a city if you can only travel along orthogonal city blocks.\\n‹More generally, the —\\nk norm\\n of a vector v containing \\nn elements is defined as\\n\\nk=v0k+v1k++vnk1k. —0 just \\ngives the cardinality of the vector (i.e.,the number of elements), and —\\n‚ gives the maximum absolute value in the vector.\\n‹The higher the norm index, the more it focuses on large values and neglects smallones. This is why the RMSE is more sensitive to outliers than the MAE. But when\\nLook at the Big Picture | 39\\n7The latest version of Python 3 is recommended. Python 2.7+ should work fine too, but it is deprecated.\\noutliers are exponentially rare (like in a bell-shaped curve), the RMSE performs\\nvery well and is generally preferred.\\nCheck the AssumptionsLastly, it is good practice to list and verify the assumptions that were made so far (by\\nyou or others); this can catch serious issues early on. For example, the district prices\\nthat your system outputs are going to be fed into a downstream Machine Learning\\nsystem, and we assume that these prices are going to be used as such. But what if the\\ndownstream system actually converts the prices into categories (e.g., ƒcheap,⁄\\nƒmedium,⁄ or ƒexpensive⁄) and then uses those categories instead of the prices them…\\nselves? In this case, getting the price perfectly right is not important at all; your sys…\\ntem just needs to get the category right. If that‡s so, then the problem should have\\nbeen framed as a classification task, not a regression task. You don‡t want to find this\\nout after working on a regression system for months.\\nFortunately, after talking with the team in charge of the downstream system, you are\\nconfident that they do indeed need the actual prices, not just categories. Great! You‡re\\nall set, the lights are green, and you can start coding now!\\nGet the DataIt‡s time to get your hands dirty. Don‡t hesitate to pick up your laptop and walk\\nthrough the following code examples in a Jupyter notebook. The full Jupyter note…\\nbook is available at \\nhttps://github.com/ageron/handson-ml\\n.Create the WorkspaceFirst you will need to have Python installed. It is probably already installed on your\\nsystem. If not, you can get it at \\nhttps://www.python.org/\\n.7Next you need to create a workspace directory for your Machine Learning code and\\ndatasets. Open a terminal and type the following commands (after the \\n$ prompts):\\n$ export ML_PATH=\"$HOME/ml\"      # You can change the path if you prefer$ mkdir -p $ML_PATHYou will need a number of Python modules: Jupyter, NumPy, Pandas, Matplotlib, and\\nScikit-Learn. If you already have Jupyter running with all these modules installed,\\nyou can safely skip to ƒDownload the Data⁄ on page \\n43. If you don‡t have them yet,\\nthere are many ways to install them (and their dependencies). You can use your sys…\\ntem‡s packaging system (e.g., apt-get on Ubuntu, or MacPorts or HomeBrew on\\n40 | Chapter 2: End-to-End Machine Learning Project\\n8We will show the installation steps using pip in a bash shell on a Linux or macOS system. You may need to\\nadapt these commands to your own system. On Windows, we recommend installing Anaconda instead.\\n9You may need to have administrator rights to run this command; if so, try prefixing it with \\nsudo.macOS), install a Scientific Python distribution such as Anaconda and use its packag…\\ning system, or just use Python‡s own packaging system, pip, which is included by\\ndefault with the Python binary installers (since Python 2.7.9).\\n8 You can check to see if\\npip is installed by typing the following command:$ pip3 --versionpip 9.0.1 from [...]/lib/python3.5/site-packages (python 3.5)You should make sure you have a recent version of pip installed, at the very least >1.4\\nto support binary module installation (a.k.a. wheels). To upgrade the pip module,\\ntype:9$ pip3 install --upgrade pipCollecting pip[...]Successfully installed pip-9.0.1Creating an Isolated EnvironmentIf you would like to work in an isolated environment (which is strongly recom…\\nmended so you can work on different projects without having conflicting library ver…\\nsions), install virtualenv by running the following pip command:\\n$ pip3 install --user --upgrade virtualenvCollecting virtualenv[...]Successfully installed virtualenvNow you can create an isolated Python environment by typing:\\n$ cd $ML_PATH$ virtualenv envUsing base prefix •[...]•New python executable in [...]/ml/env/bin/python3.5Also creating executable in [...]/ml/env/bin/pythonInstalling setuptools, pip, wheel...done.Now every time you want to activate this environment, just open a terminal and type:\\n$ cd $ML_PATH$ source env/bin/activateWhile the environment is active, any package you install using pip will be installed in\\nthis isolated environment, and Python will only have access to these packages (if you\\nalso want access to the system‡s site packages, you should create the environment\\nGet the Data | 41\\n10Note that Jupyter can handle multiple versions of Python, and even many other languages such as R or\\nOctave.\\nusing virtualenv‡s \\n--system-site-packages option). Check out virtualenv‡s docu…\\nmentation for more information.\\nNow you can install all the required modules and their dependencies using this sim…\\nple pip command:$ pip3 install --upgrade jupyter matplotlib numpy pandas scipy scikit-learnCollecting jupyter  Downloading jupyter-1.0.0-py2.py3-none-any.whlCollecting matplotlib  [...]To check your installation, try to import every module like this:\\n$ python3 -c \"import jupyter, matplotlib, numpy, pandas, scipy, sklearn\"There should be no output and no error. Now you can fire up Jupyter by typing:\\n$ jupyter notebook[I 15:24 NotebookApp] Serving notebooks from local directory: [...]/ml[I 15:24 NotebookApp] 0 active kernels[I 15:24 NotebookApp] The Jupyter Notebook is running at: http://localhost:8888/[I 15:24 NotebookApp] Use Control-C to stop this server and shut down allkernels (twice to skip confirmation).A Jupyter server is now running in your terminal, listening to port 8888. You can visit\\nthis server by opening your web browser to \\nhttp://localhost:8888/\\n (this usually hap…\\npens automatically when the server starts). You should see your empty workspace\\ndirectory (containing only the \\nenv\\n directory if you followed the preceding virtualenv\\ninstructions).Now create a new Python notebook by clicking on the New button and selecting the\\nappropriate Python version\\n10 (see Figure 2-3).This does three things: first, it creates a new notebook file called \\nUntitled.ipynb\\n inyour workspace; second, it starts a Jupyter Python kernel to run this notebook; and\\nthird, it opens this notebook in a new tab. You should start by renaming this note…\\nbook to ƒHousing⁄ (this will automatically rename the file to \\nHousing.ipynb\\n) by click…ing Untitled and typing the new name.\\n42 | Chapter 2: End-to-End Machine Learning Project\\nFigure 2-3. Your workspace in Jupyter\\nA notebook contains a list of cells. Each cell can contain executable code or formatted\\ntext. Right now the notebook contains only one empty code cell, labeled ƒIn [1]:⁄. Try\\ntyping print(\"Hello world!\") in the cell, and click on the play button (see\\nFigure 2-4) or press Shift-Enter. This sends the current cell to this notebook‡s Python\\nkernel, which runs it and returns the output. The result is displayed below the cell,\\nand since we reached the end of the notebook, a new cell is automatically created. Go\\nthrough the User Interface Tour from Jupyter‡s Help menu to learn the basics.\\nFigure 2-4. Hello world Python notebook\\nDownload the DataIn typical environments your data would be available in a relational database (or\\nsome other common datastore) and spread across multiple tables/documents/files. To\\nGet the Data | 43\\n11You might also need to check legal constraints, such as private fields that should never be copied to unsafe\\ndatastores.\\n12In a real project you would save this code in a Python file, but for now you can just write it in your Jupyter\\nnotebook.access it, you would first need to get your credentials and access authorizations,\\n11 and\\nfamiliarize yourself with the data schema. In this project, however, things are much\\nsimpler: you will just download a single compressed file, \\nhousing.tgz\\n, which contains a\\ncomma-separated value (CSV) file called \\nhousing.csv\\n with all the data.\\nYou could use your web browser to download it, and run \\ntar xzf housing.tgz todecompress the file and extract the CSV file, but it is preferable to create a small func…\\ntion to do that. It is useful in particular if data changes regularly, as it allows you to\\nwrite a small script that you can run whenever you need to fetch the latest data (or\\nyou can set up a scheduled job to do that automatically at regular intervals). Auto…\\nmating the process of fetching the data is also useful if you need to install the dataset\\non multiple machines.\\nHere is the function to fetch the data:\\n12import osimport tarfilefrom six.moves import urllibDOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"HOUSING_PATH = \"datasets/housing\"HOUSING_URL = DOWNLOAD_ROOT + HOUSING_PATH + \"/housing.tgz\"def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):    if not os.path.isdir(housing_path):        os.makedirs(housing_path)    tgz_path = os.path.join(housing_path, \"housing.tgz\")    urllib.request.urlretrieve(housing_url, tgz_path)    housing_tgz = tarfile.open(tgz_path)    housing_tgz.extractall(path=housing_path)    housing_tgz.close()Now when you call \\nfetch_housing_data(), it creates a \\ndatasets/housing\\n directory in\\nyour workspace, downloads the housing.tgz\\n file, and extracts the \\nhousing.csv\\n from it in\\nthis directory.\\nNow let‡s load the data using Pandas. Once again you should write a small function to\\nload the data:\\nimport pandas as pddef load_housing_data(housing_path=HOUSING_PATH):    csv_path = os.path.join(housing_path, \"housing.csv\")    return pd.read_csv(csv_path)44 | Chapter 2: End-to-End Machine Learning Project\\nThis function returns a Pandas DataFrame object containing all the data.\\nTake a Quick Look at the Data StructureLet‡s take a look at the top five rows using the DataFrame‡s \\nhead() method (seeFigure 2-5).Figure 2-5. Top \\n†ve rows in the dataset\\nEach row represents one district. There are 10 attributes (you can see the first 6 in the\\nscreenshot): longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, median_house_value, andocean_proximity.The info() method is useful to get a quick description of the data, in particular the\\ntotal number of rows, and each attribute‡s type and number of non-null values (see\\nFigure 2-6).Figure 2-6. Housing info\\nGet the Data | 45\\nThere are 20,640 instances in the dataset, which means that it is fairly small by\\nMachine Learning standards, but it‡s perfect to get started. Notice that the \\ntotal_bedrooms attribute has only 20,433 non-null values, meaning that 207 districts are miss…\\ning this feature. We will need to take care of this later.\\nAll attributes are numerical, except the \\nocean_proximity field. Its type is \\nobject, so itcould hold any kind of Python object, but since you loaded this data from a CSV file\\nyou know that it must be a text attribute. When you looked at the top five rows, you\\nprobably noticed that the values in that column were repetitive, which means that it is\\nprobably a categorical attribute. You can find out what categories exist and how many\\ndistricts belong to each category by using the \\nvalue_counts() method:>>> housing[\"ocean_proximity\"].value_counts()<1H OCEAN     9136INLAND        6551NEAR OCEAN    2658NEAR BAY      2290ISLAND           5Name: ocean_proximity, dtype: int64Let‡s look at the other fields. The \\ndescribe() method shows a summary of the\\nnumerical attributes (\\nFigure 2-7).Figure 2-7. Summary of each numerical attribute\\nThe count, mean, min, and max rows are self-explanatory. Note that the null values are\\nignored (so, for example, \\ncount of total_bedrooms is 20,433, not 20,640). The stdrow shows the standard deviation (which measures how dispersed the values are).\\nThe 25%, 50%, and 75% rows show the corresponding percentiles\\n: a percentile indi…\\ncates the value below which a given percentage of observations in a group of observa…\\ntions falls. For example, 25% of the districts have a \\nhousing_median_age lower than46 | Chapter 2: End-to-End Machine Learning Project\\n18, while 50% are lower than 29 and 75% are lower than 37. These are often called the25th percentile (or 1\\nst quartile\\n), the median, and the 75th percentile (or 3\\nrd quartile).Another quick way to get a feel of the type of data you are dealing with is to plot a \\nhistogram for each numerical attribute. A histogram shows the number of instances\\n(on the vertical axis) that have a given value range (on the horizontal axis). You can\\neither plot this one attribute at a time, or you can call the \\nhist() method on thewhole dataset, and it will plot a histogram for each numerical attribute (see\\nFigure 2-8). For example, you can see that slightly over 800 districts have a\\nmedian_house_value equal to about $500,000.%matplotlib inline   # only in a Jupyter notebookimport matplotlib.pyplot as plthousing.hist(bins=50, figsize=(20,15))plt.show()Figure 2-8. A histogram for each numerical attribute\\nGet the Data | 47\\nThe hist() method relies on Matplotlib, which in turn relies on a\\nuser-specified graphical backend to draw on your screen. So before\\nyou can plot anything, you need to specify which backend Matplot…\\nlib should use. The simplest option is to use Jupyter‡s magic com…\\nmand %matplotlib inline. This tells Jupyter to set up Matplotlib\\nso it uses Jupyter‡s own backend. Plots are then rendered within the\\nnotebook itself. Note that calling \\nshow() is optional in a Jupyter\\nnotebook, as Jupyter will automatically display plots when a cell is\\nexecuted.Notice a few things in these histograms:\\n1.First, the median income attribute does not look like it is expressed in US dollars\\n(USD). After checking with the team that collected the data, you are told that the\\ndata has been scaled and capped at 15 (actually 15.0001) for higher median\\nincomes, and at 0.5 (actually 0.4999) for lower median incomes. Working with \\npreprocessed attributes is common in Machine Learning, and it is not necessarily\\na problem, but you should try to understand how the data was computed.\\n2.The housing median age and the median house value were also capped. The lat…\\nter may be a serious problem since it is your target attribute (your labels). Your\\nMachine Learning algorithms may learn that prices never go beyond that limit.\\nYou need to check with your client team (the team that will use your system‡s out…\\nput) to see if this is a problem or not. If they tell you that they need precise pre…\\ndictions even beyond $500,000, then you have mainly two options:\\na.Collect proper labels for the districts whose labels were capped.\\nb.\\nRemove those districts from the training set (and also from the test set, sinceyour system should not be evaluated poorly if it predicts values beyond\\n$500,000).3.These attributes have very different scales. We will discuss this later in this chap…\\nter when we explore feature scaling.\\n4.Finally, many histograms are \\ntail heavy\\n: they extend much farther to the right of\\nthe median than to the left. This may make it a bit harder for some Machine\\nLearning algorithms to detect patterns. We will try transforming these attributes\\nlater on to have more bell-shaped distributions.\\nHopefully you now have a better understanding of the kind of data you \\nare dealingwith.48 | Chapter 2: End-to-End Machine Learning Project\\n13You will often see people set the random seed to 42. This number has no special property, other than to be\\nThe Answer to the Ultimate Question of Life, the Universe, and Everything.\\nWait! Before you look at the data any further, you need to create a\\ntest set, put it aside, and never look at it.\\nCreate a Test SetIt may sound strange to voluntarily set aside part of the data at this stage. After all,\\nyou have only taken a quick glance at the data, and surely you should learn a whole\\nlot more about it before you decide what algorithms to use, right? This is true, but\\nyour brain is an amazing pattern detection system, which means that it is highly\\nprone to overfitting: if you look at the test set, you may stumble upon some seemingly\\ninteresting pattern in the test data that leads you to select a particular kind of\\nMachine Learning model. When you estimate the generalization error using the test\\nset, your estimate will be too optimistic and you will launch a system that will not\\nperform as well as expected. This is called data snooping\\n bias.Creating a test set is theoretically quite simple: just pick some instances randomly,\\ntypically 20% of the dataset, and set them aside:\\nimport numpy as npdef split_train_test(data, test_ratio):    shuffled_indices = np.random.permutation(len(data))    test_set_size = int(len(data) * test_ratio)    test_indices = shuffled_indices[:test_set_size]    train_indices = shuffled_indices[test_set_size:]    return data.iloc[train_indices], data.iloc[test_indices]You can then use this function like this:\\n>>> train_set, test_set = split_train_test(housing, 0.2)>>> print(len(train_set), \"train +\", len(test_set), \"test\")16512 train + 4128 testWell, this works, but it is not perfect: if you run the program again, it will generate a\\ndifferent test set! Over time, you (or your Machine Learning algorithms) will get to\\nsee the whole dataset, which is what you want to avoid.\\nOne solution is to save the test set on the first run and then load it in subsequent\\nruns. Another option is to set the random number generator‡s seed (e.g., \\nnp.random.seed(42))13 before calling np.random.permutation(), so that it always generates\\nthe same shuffled indices.\\nGet the Data | 49\\n14The location information is actually quite coarse, and as a result many districts will have the exact same ID, so\\nthey will end up in the same set (test or train). This introduces some unfortunate sampling bias.\\nBut both these solutions will break next time you fetch an updated dataset. A com…\\nmon solution is to use each instance‡s identifier to decide whether or not it should go\\nin the test set (assuming instances have a unique and immutable identifier). For\\nexample, you could compute a hash of each instance‡s identifier, keep only the last\\nbyte of the hash, and put the instance in the test set if this value is lower or equal to51 (~20% of 256). This ensures that the test set will remain consistent across multiple\\nruns, even if you refresh the dataset. The new test set will contain 20% of the new\\ninstances, but it will not contain any instance that was previously in the training set.\\nHere is a possible implementation:\\nimport hashlibdef test_set_check(identifier, test_ratio, hash):    return hash(np.int64(identifier)).digest()[-1] < 256 * test_ratiodef split_train_test_by_id(data, test_ratio, id_column, hash=hashlib.md5):    ids = data[id_column]    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio, hash))    return data.loc[~in_test_set], data.loc[in_test_set]Unfortunately, the housing dataset does not have an identifier column. The simplest\\nsolution is to use the row index as the ID:housing_with_id = housing.reset_index()   # adds an †index† columntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")If you use the row index as a unique identifier, you need to make sure that new data\\ngets appended to the end of the dataset, and no row ever gets deleted. If this is not\\npossible, then you can try to use the most stable features to build a unique identifier.\\nFor example, a district‡s latitude and longitude are guaranteed to be stable for a few\\nmillion years, so you could combine them into an ID like so:\\n14housing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"id\")Scikit-Learn provides a few functions to split datasets into multiple subsets in various\\nways. The simplest function is \\ntrain_test_split, which does pretty much the same\\nthing as the function split_train_test defined earlier, with a couple of additional\\nfeatures. First there is a \\nrandom_state parameter that allows you to set the random\\ngenerator seed as explained previously, and second you can pass it multiple datasets\\nwith an identical number of rows, and it will split them on the same indices (this is\\nvery useful, for example, if you have a separate DataFrame for labels):\\n50 | Chapter 2: End-to-End Machine Learning Project\\nfrom sklearn.model_selection import train_test_splittrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)So far we have considered purely random sampling methods. This is generally fine if\\nyour dataset is large enough (especially relative to the number of attributes), but if it\\nis not, you run the risk of introducing a significant sampling bias. When a survey\\ncompany decides to call 1,000 people to ask them a few questions, they don‡t just pick\\n1,000 people randomly in a phone booth. They try to ensure that these 1,000 people\\nare representative of the whole population. For example, the US population is com…\\nposed of 51.3% female and 48.7% male, so a well-conducted survey in the US would\\ntry to maintain this ratio in the sample: 513 female and 487 male. This is called \\nstrati…\\n†ed sampling\\n: the population is divided into homogeneous subgroups called \\nstrata\\n,and the right number of instances is sampled from each stratum to guarantee that the\\ntest set is representative of the overall population. If they used purely random sam…\\npling, there would be about 12% chance of sampling a skewed test set with either less\\nthan 49% female or more than 54% female. Either way, the survey results would be\\nsignificantly biased.\\nSuppose you chatted with experts who told you that the median income is a very\\nimportant attribute to predict median housing prices. You may want to ensure that\\nthe test set is representative of the various categories of incomes in the whole dataset.\\nSince the median income is a continuous numerical attribute, you first need to create\\nan income category attribute. Let‡s look at the median income histogram more closely\\n(see Figure 2-9):Figure 2-9. Histogram of income categories\\nMost median income values are clustered around 2–5 (tens of thousands of dollars),\\nbut some median incomes go far beyond 6. It is important to have a sufficient num…\\nGet the Data | 51\\nber of instances in your dataset for each stratum, or else the estimate of the stratum‡s\\nimportance may be biased. This means that you should not have too many strata, and\\neach stratum should be large enough. The following code creates an income category\\nattribute by dividing the median income by 1.5 (to limit the number of income cate…\\ngories), and rounding up using ceil (to have discrete categories), and then merging\\nall the categories greater than 5 into category 5:\\nhousing[\"income_cat\"] = np.ceil(housing[\"median_income\"] / 1.5)housing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace=True)Now you are ready to do stratified sampling based on the income category. For this\\nyou can use Scikit-Learn‡s \\nStratifiedShuffleSplit class:from sklearn.model_selection import StratifiedShuffleSplitsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)for train_index, test_index in split.split(housing, housing[\"income_cat\"]):    strat_train_set = housing.loc[train_index]    strat_test_set = housing.loc[test_index]Let‡s see if this worked as expected. You can start by looking at the income category\\nproportions in the full housing dataset:\\n>>> housing[\"income_cat\"].value_counts() / len(housing)3.0    0.3505812.0    0.3188474.0    0.1763085.0    0.1144381.0    0.039826Name: income_cat, dtype: float64With similar code you can measure the income category proportions in the test set.\\nFigure 2-10 compares the income category proportions in the overall dataset, in the\\ntest set generated with stratified sampling, and in a test set generated using purely\\nrandom sampling. As you can see, the test set generated using stratified sampling has\\nincome category proportions almost identical to those in the full dataset, whereas the\\ntest set generated using purely random sampling is quite skewed.\\nFigure 2-10. Sampling bias comparison of \\nstrati†ed versus purely random sampling\\n52 | Chapter 2: End-to-End Machine Learning Project\\nNow you should remove the \\nincome_cat attribute so the data is back to its original\\nstate:\\nfor set in (strat_train_set, strat_test_set):    set.drop([\"income_cat\"], axis=1, inplace=True)We spent quite a bit of time on test set generation for a good reason: this is an often\\nneglected but critical part of a Machine Learning project. Moreover, many of these\\nideas will be useful later when we discuss cross-validation. Now it‡s time to move on\\nto the next stage: exploring the data.\\nDiscover and Visualize the Data to Gain InsightsSo far you have only taken a quick glance at the data to get a general understanding of\\nthe kind of data you are manipulating. Now the goal is to go a little bit more in depth.\\nFirst, make sure you have put the test set aside and you are only exploring the train…\\ning set. Also, if the training set is very large, you may want to sample an exploration\\nset, to make manipulations easy and fast. In our case, the set is quite small so you can\\njust work directly on the full set. Let‡s create a copy so you can play with it without\\nharming the training set:housing = strat_train_set.copy()Visualizing Geographical DataSince there is geographical information (latitude and longitude), it is a good idea to\\ncreate a scatterplot of all districts to visualize the data (\\nFigure 2-11):housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")Figure 2-11. A geographical scatterplot of the data\\nDiscover and Visualize the Data to Gain Insights | 53\\n15If you are reading this in grayscale, grab a red pen and scribble over most of the coastline from the Bay Area\\ndown to San Diego (as you might expect). You can add a patch of yellow around Sacramento as well.\\nThis looks like California all right, but other than that it is hard to see any particular\\npattern. Setting the \\nalpha option to 0.1 makes it much easier to visualize the places\\nwhere there is a high density of data points (\\nFigure 2-12):housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)Figure 2-12. A better visualization highlighting high-density areas\\nNow that‡s much better: you can clearly see the high-density areas, namely the Bay\\nArea and around Los Angeles and San Diego, plus a long line of fairly high density in\\nthe Central Valley, in particular around Sacramento and Fresno.\\nMore generally, our brains are very good at spotting patterns on pictures, but you\\nmay need to play around with visualization parameters to make the patterns stand\\nout.Now let‡s look at the housing prices (\\nFigure 2-13). The radius of each circle represents\\nthe district‡s population (option \\ns), and the color represents the price (option \\nc). We\\nwill use a predefined color map (option \\ncmap) called jet, which ranges from blue(low values) to red (high prices):15housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,    s=housing[\"population\"]/100, label=\"population\",    c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,)plt.legend()54 | Chapter 2: End-to-End Machine Learning Project\\nFigure 2-13. California housing prices\\nThis image tells you that the housing prices are very much related to the location\\n(e.g., close to the ocean) and to the population density, as you probably knew already.\\nIt will probably be useful to use a clustering algorithm to detect the main clusters, and\\nadd new features that measure the proximity to the cluster centers. The ocean prox…\\nimity attribute may be useful as well, although in Northern California the housing\\nprices in coastal districts are not too high, so it is not a simple rule.\\nLooking for CorrelationsSince the dataset is not too large, you can easily compute the \\nstandard correlation\\ncoe⁄cient (also called Pearson‹s r\\n) between every pair of attributes using the \\ncorr()method:corr_matrix = housing.corr()Now let‡s look at how much each attribute correlates with the median house value:\\n>>> corr_matrix[\"median_house_value\"].sort_values(ascending=False)median_house_value    1.000000median_income         0.687170total_rooms           0.135231housing_median_age    0.114220households            0.064702Discover and Visualize the Data to Gain Insights | 55\\ntotal_bedrooms        0.047865population           -0.026699longitude            -0.047279latitude             -0.142826Name: median_house_value, dtype: float64The correlation coefficient ranges from –1 to 1. When it is close to 1, it means that\\nthere is a strong positive correlation; for example, the median house value tends to go\\nup when the median income goes up. When the coefficient is close to –1, it means\\nthat there is a strong negative correlation; you can see a small negative correlation\\nbetween the latitude and the median house value (i.e., prices have a slight tendency to\\ngo down when you go north). Finally, coefficients close to zero mean that there is no\\nlinear correlation. \\nFigure 2-14 shows various plots along with the correlation coeffi…\\ncient between their horizontal and vertical axes.\\nFigure 2-14. Standard correlation \\ncoe⁄cient of various datasets (source: Wikipedia;\\npublic domain image)\\nThe correlation coefficient only measures linear correlations (ƒif \\nxgoes up, then \\ny generally goes up/down⁄). It may completely miss\\nout on nonlinear relationships (e.g., ƒif \\nx is close to zero then \\ny gen…\\nerally goes up⁄). Note how all the plots of the bottom row have a\\ncorrelation coefficient equal to zero despite the fact that their axes\\nare clearly not independent: these are examples of nonlinear rela…\\ntionships. Also, the second row shows examples where the correla…\\ntion coefficient is equal to 1 or –1; notice that this has nothing to\\ndo with the slope. For example, your height in inches has a correla…\\ntion coefficient of 1 with your height in feet or in nanometers.\\nAnother way to check for correlation between attributes is to use \\nPandas‡\\nscatter_matrix function, which plots every numerical attribute against every other\\nnumerical attribute. Since there are now 11 numerical attributes, you would get 11\\n2 =\\n56 | Chapter 2: End-to-End Machine Learning Project\\n121 plots, which would not fit on a page, so let‡s just focus on a few promising\\nattributes that seem most correlated with the median housing value (\\nFigure 2-15):from pandas.tools.plotting import scatter_matrixattributes = [\"median_house_value\", \"median_income\", \"total_rooms\",              \"housing_median_age\"]scatter_matrix(housing[attributes], figsize=(12, 8))Figure 2-15. Scatter matrix\\nThe main diagonal (top left to bottom right) would be full of straight lines if Pandas\\nplotted each variable against itself, which would not be very useful. So instead Pandas\\ndisplays a histogram of each attribute (other options are available; see \\nPandas‡ docu…\\nmentation for more details).\\nThe most promising attribute to predict the median house value is the median\\nincome, so let‡s zoom in on their correlation scatterplot (\\nFigure 2-16):housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",             alpha=0.1)Discover and Visualize the Data to Gain Insights | 57\\nFigure 2-16. Median income versus median house value\\nThis plot reveals a few things. First, the correlation is indeed very strong; you can\\nclearly see the upward trend and the points are not too dispersed. Second, the price\\ncap that we noticed earlier is clearly visible as a horizontal line at $500,000. But this\\nplot reveals other less obvious straight lines: a horizontal line around $450,000,\\nanother around $350,000, perhaps one around $280,000, and a few more below that.\\nYou may want to try removing the corresponding districts to prevent your algorithms\\nfrom learning to reproduce these data quirks.\\nExperimenting with Attribute CombinationsHopefully the previous sections gave you an idea of a few ways you can explore the\\ndata and gain insights. You identified a few data quirks that you may want to clean up\\nbefore feeding the data to a Machine Learning algorithm, and you found interesting\\ncorrelations between attributes, in particular with the target attribute. You also\\nnoticed that some attributes have a tail-heavy distribution, so you may want to trans…\\nform them (e.g., by computing their logarithm). Of course, your mileage will vary\\nconsiderably with each project, but the general ideas are similar.\\nOne last thing you may want to do before actually preparing the data for Machine\\nLearning algorithms is to try out various attribute combinations. For example, the\\ntotal number of rooms in a district is not very useful if you don‡t know how many\\nhouseholds there are. What you really want is the number of rooms per household.\\nSimilarly, the total number of bedrooms by itself is not very useful: you probably\\nwant to compare it to the number of rooms. And the population per household also\\n58 | Chapter 2: End-to-End Machine Learning Project\\nseems like an interesting attribute combination to look at. Let‡s create these new\\nattributes:\\nhousing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]And now let‡s look at the correlation matrix again:\\n>>> corr_matrix = housing.corr()>>> corr_matrix[\"median_house_value\"].sort_values(ascending=False)median_house_value          1.000000median_income               0.687170rooms_per_household         0.199343total_rooms                 0.135231housing_median_age          0.114220households                  0.064702total_bedrooms              0.047865population_per_household   -0.021984population                 -0.026699longitude                  -0.047279latitude                   -0.142826bedrooms_per_room          -0.260070Name: median_house_value, dtype: float64Hey, not bad! The new \\nbedrooms_per_room attribute is much more correlated with\\nthe median house value than the total number of rooms or bedrooms. Apparently\\nhouses with a lower bedroom/room ratio tend to be more expensive. The number of\\nrooms per household is also more informative than the total number of rooms in a\\ndistrict›obviously the larger the houses, the more expensive they are.This round of exploration does not have to be absolutely thorough; the point is to\\nstart off on the right foot and quickly gain insights that will help you get a first rea…\\nsonably good prototype. But this is an iterative process: once you get a prototype up\\nand running, you can analyze its output to gain more insights and come back to this\\nexploration step.\\nPrepare the Data for Machine Learning AlgorithmsIt‡s time to prepare the data for your Machine Learning algorithms. Instead of just\\ndoing this manually, you should write functions to do that, for several good reasons:\\n‹This will allow you to reproduce these transformations easily on any dataset (e.g.,\\nthe next time you get a fresh dataset).\\n‹You will gradually build a library of transformation functions that you can reuse\\nin future projects.‹You can use these functions in your live system to transform the new data before\\nfeeding it to your algorithms.Prepare the Data for Machine Learning Algorithms | 59\\n‹This will make it possible for you to easily try various transformations and see\\nwhich combination of transformations works best.\\nBut first let‡s revert to a clean training set (by copying \\nstrat_train_set once again),and let‡s separate the predictors and the labels since we don‡t necessarily want to apply\\nthe same transformations to the predictors and the target values (note that \\ndrop() creates a copy of the data and does not affect \\nstrat_train_set):housing = strat_train_set.drop(\"median_house_value\", axis=1)housing_labels = strat_train_set[\"median_house_value\"].copy()Data CleaningMost Machine Learning algorithms cannot work with missing features, so let‡s create\\na few functions to take care of them. You noticed earlier that the \\ntotal_bedroomsattribute has some missing values, so let‡s fix this. You have three options:\\n‹Get rid of the corresponding districts.‹Get rid of the whole attribute.\\n‹Set the values to some value (zero, the mean, the median, etc.).\\nYou can accomplish these easily using DataFrame‡s \\ndropna(), drop(), and fillna()methods:housing.dropna(subset=[\"total_bedrooms\"])    # option 1housing.drop(\"total_bedrooms\", axis=1)       # option 2median = housing[\"total_bedrooms\"].median()housing[\"total_bedrooms\"].fillna(median)     # option 3If you choose option 3, you should compute the median value on the training set, and\\nuse it to fill the missing values in the training set, but also don‡t forget to save the\\nmedian value that you have computed. You will need it later to replace missing values\\nin the test set when you want to evaluate your system, and also once the system goes\\nlive to replace missing values in new data.\\nScikit-Learn provides a handy class to take care of missing values: Imputer. Here is\\nhow to use it. First, you need to create an \\nImputer instance, specifying that you want\\nto replace each attribute‡s missing values with the median of that attribute:\\nfrom sklearn.preprocessing import Imputerimputer = Imputer(strategy=\"median\")Since the median can only be computed on numerical attributes, we need to create a\\ncopy of the data without the text attribute \\nocean_proximity:housing_num = housing.drop(\"ocean_proximity\", axis=1)60 | Chapter 2: End-to-End Machine Learning Project\\n16For more details on the design principles, see ƒAPI design for machine learning software: experiences from\\nthe scikit-learn project,⁄ L. Buitinck, G. Louppe, M. Blondel, F. Pedregosa, A. M™ller, et al. (2013).\\nNow you can fit the \\nimputer instance to the training data using the \\nfit() method:imputer.fit(housing_num)The imputer has simply computed the median of each attribute and stored the result\\nin its statistics_ instance variable. Only the total_bedrooms attribute had missing\\nvalues, but we cannot be sure that there won‡t be any missing values in new data after\\nthe system goes live, so it is safer to apply the \\nimputer to all the numerical attributes:\\n>>> imputer.statistics_array([ -118.51 , 34.26 , 29. , 2119. , 433. , 1164. , 408. , 3.5414])>>> housing_num.median().valuesarray([ -118.51 , 34.26 , 29. , 2119. , 433. , 1164. , 408. , 3.5414])Now you can use this ƒtrained⁄ \\nimputer to transform the training set by replacingmissing values by the learned medians:X = imputer.transform(housing_num)The result is a plain Numpy array containing the transformed features. If you want to\\nput it back into a Pandas DataFrame, it‡s simple:\\nhousing_tr = pd.DataFrame(X, columns=housing_num.columns)Scikit-Learn DesignScikit-Learn‡s API is remarkably well designed. The \\nmain design principles are:16‹Consistency\\n. All objects share a consistent and simple interface:\\n›Estimators\\n. Any object that can estimate some parameters based on a dataset\\nis called an estimator\\n (e.g., an \\nimputer is an estimator). The estimation itself is\\nperformed by the fit() method, and it takes only a dataset as a parameter (or\\ntwo for supervised learning algorithms; the second dataset contains the\\nlabels). Any other parameter needed to guide the estimation process is con…\\nsidered a hyperparameter (such as an \\nimputer‡s \\nstrategy), and it must be set\\nas an instance variable (generally via a constructor parameter).›Transformers\\n. Some estimators (such as an \\nimputer) can also transform adataset; these are called \\ntransformers\\n. Once again, the API is quite simple: the\\ntransformation is performed by the \\ntransform() method with the dataset to\\ntransform as a parameter. It returns the transformed dataset. This transforma…\\ntion generally relies on the learned parameters, as is the case for an imputer.All transformers also have a convenience method called \\nfit_transform() Prepare the Data for Machine Learning Algorithms | 61\\n17Some predictors also provide methods to measure the confidence of their predictions.that is equivalent to calling \\nfit() and then transform() (but sometimesfit_transform() is optimized and runs much faster).\\n›Predictors\\n. Finally, some estimators are capable of making predictions given a\\ndataset; they are called \\npredictors\\n. For example, the \\nLinearRegression model \\nin the previous chapter was a predictor: it predicted life satisfaction given a\\ncountry‡s GDP per capita. A predictor has a \\npredict() method that takes a\\ndataset of new instances and returns a dataset of corresponding predictions. It\\nalso has a score() method that measures the quality of the predictions given\\na test set (and the corresponding labels in the case of supervised learning\\nalgorithms).17‹Inspection\\n. All the estimator‡s hyperparameters are accessible directly via public\\ninstance variables (e.g., imputer.strategy), and all the estimator‡s learned\\nparameters are also accessible via public instance variables with an underscoresuffix (e.g., imputer.statistics_).‹Nonproliferation of classes\\n. Datasets are represented as NumPy arrays or SciPy\\nsparse matrices, instead of homemade classes. Hyperparameters are just regular\\nPython strings or numbers.\\n‹Composition\\n. Existing building blocks are reused as much as possible. For\\nexample, it is easy to create a \\nPipeline estimator from an arbitrary sequence of\\ntransformers followed by a final estimator, as we will see.\\n‹Sensible defaults\\n. Scikit-Learn provides reasonable default values for most\\nparameters, making it easy to create a baseline working system quickly.\\nHandling Text and Categorical AttributesEarlier we left out the categorical attribute \\nocean_proximity because it is a text\\nattribute so we cannot compute its median. Most Machine Learning algorithms pre…\\nfer to work with numbers anyway, so let‡s convert these text labels to numbers.\\nScikit-Learn provides a transformer for this task called LabelEncoder:>>> from sklearn.preprocessing import LabelEncoder>>> encoder = LabelEncoder()>>> housing_cat = housing[\"ocean_proximity\"]>>> housing_cat_encoded = encoder.fit_transform(housing_cat)>>> housing_cat_encodedarray([1, 1, 4, ..., 1, 0, 3])62 | Chapter 2: End-to-End Machine Learning Project\\n18NumPy‡s \\nreshape() function allows one dimension to be –1, which means ƒunspecified⁄: the value is inferred\\nfrom the length of the array and the remaining dimensions.\\n19See SciPy‡s documentation for more details.\\nThis is better: now we can use this numerical data in any ML algorithm. You can look\\nat the mapping that this encoder has learned using the \\nclasses_ attribute (ƒ<1H\\nOCEAN⁄ is mapped to 0, ƒINLAND⁄ is mapped to 1, etc.):\\n>>> print(encoder.classes_)[•<1H OCEAN• •INLAND• •ISLAND• •NEAR BAY• •NEAR OCEAN•]One issue with this representation is that ML algorithms will assume that two nearby\\nvalues are more similar than two distant values. Obviously this is not the case (for\\nexample, categories 0 and 4 are more similar than categories 0 and 1). To fix this\\nissue, a common solution is to create one binary attribute per category: one attribute\\nequal to 1 when the category is ƒ<1H OCEAN⁄ (and 0 otherwise), another attribute\\nequal to 1 when the category is ƒINLAND⁄ (and 0 otherwise), and so on. This is\\ncalled one-hot encoding\\n, because only one attribute will be equal to 1 (hot), while the\\nothers will be 0 (cold).Scikit-Learn provides a OneHotEncoder encoder to convert integer categorical values\\ninto one-hot vectors. Let‡s encode the categories as one-hot vectors. Note that\\nfit_transform() expects a 2D array, but \\nhousing_cat_encoded is a 1D array, so we\\nneed to reshape it:\\n18>>> from sklearn.preprocessing import OneHotEncoder>>> encoder = OneHotEncoder()>>> housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1,1))>>> housing_cat_1hot<16513x5 sparse matrix of type •<class •numpy.float64•>• with 16513 stored elements in Compressed Sparse Row format>\\nNotice that the output is a SciPy \\nsparse matrix\\n, instead of a NumPy array. This is very\\nuseful when you have categorical attributes with thousands of categories. After one-\\nhot encoding we get a matrix with thousands of columns, and the matrix is full of\\nzeros except for one 1 per row. Using up tons of memory mostly to store zeros would\\nbe very wasteful, so instead a sparse matrix only stores the location of the nonzero\\nelements. You can use it mostly like a normal 2D array,\\n19 but if you really want to con…\\nvert it to a (dense) NumPy array, just call the \\ntoarray() method:>>> housing_cat_1hot.toarray()array([[ 0.,  1.,  0.,  0.,  0.],       [ 0.,  1.,  0.,  0.,  0.],       [ 0.,  0.,  0.,  0.,  1.],       ...,       [ 0.,  1.,  0.,  0.,  0.],Prepare the Data for Machine Learning Algorithms | 63\\n       [ 1.,  0.,  0.,  0.,  0.],       [ 0.,  0.,  0.,  1.,  0.]])We can apply both transformations (from text categories to integer categories, then\\nfrom integer categories to one-hot vectors) in one shot using the \\nLabelBinarizer class:>>> from sklearn.preprocessing import LabelBinarizer>>> encoder = LabelBinarizer()>>> housing_cat_1hot = encoder.fit_transform(housing_cat)>>> housing_cat_1hotarray([[0, 1, 0, 0, 0],       [0, 1, 0, 0, 0],       [0, 0, 0, 0, 1],       ...,       [0, 1, 0, 0, 0],       [1, 0, 0, 0, 0],       [0, 0, 0, 1, 0]])Note that this returns a dense NumPy array by default. You can get a sparse matrix\\ninstead by passing sparse_output=True to the LabelBinarizer constructor.\\nCustom TransformersAlthough Scikit-Learn provides many useful transformers, you will need to write\\nyour own for tasks such as custom cleanup operations or combining specific\\nattributes. You will want your transformer to work seamlessly with Scikit-Learn func…\\ntionalities (such as pipelines), and since Scikit-Learn relies on duck typing (not inher…itance), all you need is to create a class and implement three methods: \\nfit()(returning self), transform(), and fit_transform(). You can get the last one for\\nfree by simply adding \\nTransformerMixin as a base class. Also, if you add \\nBaseEstimator as a base class (and avoid \\n*args and **kargs in your constructor) you will gettwo extra methods (get_params() and set_params()) that will be useful for auto…\\nmatic hyperparameter tuning. For example, here is a small transformer class that adds\\nthe combined attributes we discussed earlier:\\nfrom sklearn.base import BaseEstimator, TransformerMixinrooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6class CombinedAttributesAdder(BaseEstimator, TransformerMixin):    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs        self.add_bedrooms_per_room = add_bedrooms_per_room    def fit(self, X, y=None):        return self  # nothing else to do    def transform(self, X, y=None):        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]        population_per_household = X[:, population_ix] / X[:, household_ix]        if self.add_bedrooms_per_room:            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]64 | Chapter 2: End-to-End Machine Learning Project\\n            return np.c_[X, rooms_per_household, population_per_household,                         bedrooms_per_room]        else:            return np.c_[X, rooms_per_household, population_per_household]attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)housing_extra_attribs = attr_adder.transform(housing.values)In this example the transformer has one hyperparameter, \\nadd_bedrooms_per_room,set to True by default (it is often helpful to provide sensible defaults). This \\nhyperpara…\\nmeter will allow you to easily find out whether adding this attribute helps the\\nMachine Learning algorithms or not. More generally, you can add a hyperparameter\\nto gate any data preparation step that you are not 100% sure about. The more you\\nautomate these data preparation steps, the more combinations you can automatically\\ntry out, making it much more likely that you will find a great combination (and sav…\\ning you a lot of time).Feature ScalingOne of the most important transformations you need to apply to your data is \\nfeature\\nscaling\\n. With few exceptions, Machine Learning algorithms don‡t perform well when\\nthe input numerical attributes have very different scales. This is the case for the hous…\\ning data: the total number of rooms ranges from about 6 to 39,320, while the median\\nincomes only range from 0 to 15. Note that scaling the target values is generally not\\nrequired.There are two common ways to get all attributes to have the same scale: \\nmin-max\\nscaling\\n and standardization\\n.Min-max scaling (many people call this \\nnormalization\\n) is quite simple: values are\\nshifted and rescaled so that they end up ranging from 0 to 1. We do this by subtract…\\ning the min value and dividing by the max minus the min. Scikit-Learn provides a\\ntransformer called MinMaxScaler for this. It has a \\nfeature_range hyperparameter\\nthat lets you change the range if you don‡t want 0–1 for some reason.\\nStandardization is quite different: first it subtracts the mean value (so standardized\\nvalues always have a zero mean), and then it divides by the variance so that the result…\\ning distribution has unit variance. Unlike min-max scaling, standardization does not\\nbound values to a specific range, which may be a problem for some algorithms (e.g.,\\nneural networks often expect an input value ranging from 0 to 1). However, standard…\\nization is much less affected by outliers. For example, suppose a district had a median\\nincome equal to 100 (by mistake). Min-max scaling would then crush all the othervalues from 0–15 down to 0–0.15, whereas standardization would not be much affec…\\nted. Scikit-Learn provides a transformer called StandardScaler for standardization.\\nPrepare the Data for Machine Learning Algorithms | 65\\nAs with all the transformations, it is important to fit the scalers to\\nthe training data only, not to the full dataset (including the test set).\\nOnly then can you use them to transform the training set and thetest set (and new data).\\nTransformation PipelinesAs you can see, there are many data transformation steps that need to be executed in\\nthe right order. Fortunately, Scikit-Learn provides the \\nPipeline class to help withsuch sequences of transformations. Here is a small pipeline for the \\nnumerical\\nattributes:\\nfrom sklearn.pipeline import Pipelinefrom sklearn.preprocessing import StandardScalernum_pipeline = Pipeline([        (•imputer•, Imputer(strategy=\"median\")),        (•attribs_adder•, CombinedAttributesAdder()),        (•std_scaler•, StandardScaler()),    ])housing_num_tr = num_pipeline.fit_transform(housing_num)The Pipeline constructor takes a list of name/estimator pairs defining a sequence of\\nsteps. All but the last estimator must be transformers (i.e., they must have a\\nfit_transform() method). The names can be anything you like.\\nWhen you call the pipeline‡s \\nfit() method, it calls fit_transform() sequentially on\\nall transformers, passing the output of each call as the parameter to the next call, until\\nit reaches the final estimator, for which it just calls the \\nfit() method.The pipeline exposes the same methods as the final estimator. In this example, the last\\nestimator is a \\nStandardScaler, which is a transformer, so the pipeline has \\na transform() method that applies all the transforms to the data in sequence (it also has a\\nfit_transform method that we could have used instead of calling \\nfit() and thentransform()).You now have a pipeline for numerical values, and you also need to apply the \\nLabelBinarizer on the categorical values: how can you join these transformations into a sin…\\ngle pipeline? Scikit-Learn provides a FeatureUnion class for this. You give it a list of\\ntransformers (which can be entire transformer pipelines), and when its \\ntransform()method is called it runs each transformer‡s \\ntransform() method in parallel, waits fortheir output, and then concatenates them and returns the result (and of course calling\\nits fit() method calls all each transformer‡s \\nfit() method). A full pipeline handlingboth numerical and categorical attributes may look like this:\\n66 | Chapter 2: End-to-End Machine Learning Project\\n20But check out Pull Request #3886, which may introduce a \\nColumnTransformer class making attribute-specific\\ntransformations easy. You could also run \\npip3 install sklearn-pandas to get a DataFrameMapper class witha similar objective.from sklearn.pipeline import FeatureUnionnum_attribs = list(housing_num)cat_attribs = [\"ocean_proximity\"]num_pipeline = Pipeline([        (•selector•, DataFrameSelector(num_attribs)),        (•imputer•, Imputer(strategy=\"median\")),        (•attribs_adder•, CombinedAttributesAdder()),        (•std_scaler•, StandardScaler()),    ])cat_pipeline = Pipeline([        (•selector•, DataFrameSelector(cat_attribs)),        (•label_binarizer•, LabelBinarizer()),    ])full_pipeline = FeatureUnion(transformer_list=[        (\"num_pipeline\", num_pipeline),        (\"cat_pipeline\", cat_pipeline),    ])And you can run the whole pipeline simply:\\n>>> housing_prepared = full_pipeline.fit_transform(housing)>>> housing_preparedarray([[ 0.73225807, -0.67331551,  0.58426443, ...,  0.        ,         0.        ,  0.        ],       [-0.99102923,  1.63234656, -0.92655887, ...,  0.        ,         0.        ,  0.        ],       [...]>>> housing_prepared.shape(16513, 17)Each subpipeline starts with a selector transformer: it simply transforms the data by\\nselecting the desired attributes (numerical or categorical), dropping the rest, and con…\\nverting the resulting DataFrame to a NumPy array. There is nothing in Scikit-Learn\\nto handle Pandas DataFrames,\\n20 so we need to write a simple custom transformer \\nforthis task:from sklearn.base import BaseEstimator, TransformerMixinclass DataFrameSelector(BaseEstimator, TransformerMixin):    def __init__(self, attribute_names):        self.attribute_names = attribute_names    def fit(self, X, y=None):        return selfPrepare the Data for Machine Learning Algorithms | 67\\n    def transform(self, X):        return X[self.attribute_names].valuesSelect and Train a ModelAt last! You framed the problem, you got the data and explored it, you sampled a\\ntraining set and a test set, and you wrote transformation pipelines to clean up and\\nprepare your data for Machine Learning algorithms automatically. You are now ready\\nto select and train a Machine Learning model.\\nTraining and Evaluating on the Training SetThe good news is that thanks to all these previous steps, things are now going to be\\nmuch simpler than you might think. Let‡s first train a Linear Regression model, like\\nwe did in the previous chapter:\\nfrom sklearn.linear_model import LinearRegressionlin_reg = LinearRegression()lin_reg.fit(housing_prepared, housing_labels)Done! You now have a working Linear Regression model. Let‡s try it out on a few\\ninstances from the training set:>>> some_data = housing.iloc[:5]>>> some_labels = housing_labels.iloc[:5]>>> some_data_prepared = full_pipeline.transform(some_data)>>> print(\"Predictions:\\\\t\", lin_reg.predict(some_data_prepared))Predictions:  [ 303104.   44800.  308928.  294208.  368704.]\\n>>> print(\"Labels:\\\\t\\\\t\", list(some_labels))Labels:   [359400.0, 69700.0, 302100.0, 301300.0, 351900.0]\\nIt works, although the predictions are not exactly accurate (e.g., the second prediction\\nis off by more than 50%!). Let‡s measure this regression model‡s RMSE on the whole\\ntraining set using Scikit-Learn‡s \\nmean_squared_error function:>>> from sklearn.metrics import mean_squared_error>>> housing_predictions = lin_reg.predict(housing_prepared)>>> lin_mse = mean_squared_error(housing_labels, housing_predictions)>>> lin_rmse = np.sqrt(lin_mse)>>> lin_rmse68628.413493824875Okay, this is better than nothing but clearly not a great score: most districts‡\\nmedian_housing_values range between $120,000 and $265,000, so a typical predic…tion error of $68,628 is not very satisfying. This is an example of a model \\nunderfittingthe training data. When this happens it can mean that the features do not provide\\nenough information to make good predictions, or that the model is not powerful\\nenough. As we saw in the previous chapter, the main ways to fix underfitting are to\\n68 | Chapter 2: End-to-End Machine Learning Project\\nselect a more powerful model, to feed the training algorithm with better features, or\\nto reduce the constraints on the model. This model is not regularized, so this rules\\nout the last option. You could try to add more features (e.g., the log of the popula…\\ntion), but first let‡s try a more complex model to see how it does.\\nLet‡s train a \\nDecisionTreeRegressor. This is a powerful model, capable of finding\\ncomplex nonlinear relationships in the data (Decision Trees are presented in more\\ndetail in Chapter 6\\n). The code should look familiar by now:\\nfrom sklearn.tree import DecisionTreeRegressortree_reg = DecisionTreeRegressor()tree_reg.fit(housing_prepared, housing_labels)Now that the model is trained, let‡s evaluate it on the training set:\\n>>> housing_predictions = tree_reg.predict(housing_prepared)>>> tree_mse = mean_squared_error(housing_labels, housing_predictions)>>> tree_rmse = np.sqrt(tree_mse)>>> tree_rmse0.0Wait, what!? No error at all? Could this model really be absolutely perfect? Of course,\\nit is much more likely that the model has badly overfit the data. How can you be sure?\\nAs we saw earlier, you don‡t want to touch the test set until you are ready to launch a\\nmodel you are confident about, so you need to use part of the training set for train…\\ning, and part for model validation.\\nBetter Evaluation Using Cross-ValidationOne way to evaluate the Decision Tree model would be to use the \\ntrain_test_splitfunction to split the training set into a smaller training set and a validation set, then\\ntrain your models against the smaller training set and evaluate them against the vali…\\ndation set. It‡s a bit of work, but nothing too difficult and it would work fairly well.\\nA great alternative is to use Scikit-Learn‡s \\ncross-validation\\n feature. The following code\\nperforms K-fold cross-validation\\n: it randomly splits the training set into 10 distinct\\nsubsets called folds\\n, then it trains and evaluates the Decision Tree model 10 times,\\npicking a different fold for evaluation every time and training on the other 9 folds.\\nThe result is an array containing the 10 evaluation scores:\\nfrom sklearn.model_selection import cross_val_scorescores = cross_val_score(tree_reg, housing_prepared, housing_labels,                         scoring=\"neg_mean_squared_error\", cv=10)rmse_scores = np.sqrt(-scores)Select and Train a Model | 69\\nScikit-Learn cross-validation features expect a utility function\\n(greater is better) rather than a cost function (lower is better), so\\nthe scoring function is actually the opposite of the MSE (i.e., a neg…ative value), which is why the preceding code computes \\n-scoresbefore calculating the square root.\\nLet‡s look at the results:\\n>>> def display_scores(scores):...     print(\"Scores:\", scores)...     print(\"Mean:\", scores.mean())...     print(\"Standard deviation:\", scores.std())...>>> display_scores(tree_rmse_scores)Scores: [ 74678.4916885   64766.2398337   69632.86942005  69166.67693232          71486.76507766  73321.65695983  71860.04741226  71086.32691692          76934.2726093   69060.93319262]Mean: 71199.4280043Standard deviation: 3202.70522793Now the Decision Tree doesn‡t look as good as it did earlier. In fact, it seems to per…\\nform worse than the Linear Regression model! Notice that cross-validation allows\\nyou to get not only an estimate of the performance of your model, but also a measure\\nof how precise this estimate is (i.e., its standard deviation). The Decision Tree has a\\nscore of approximately 71,200, generally ﬁ3,200. You would not have this information\\nif you just used one validation set. But cross-validation comes at the cost of training\\nthe model several times, so it is not always possible.\\nLet‡s compute the same scores for the Linear Regression model just to be sure:\\n>>> lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,...                              scoring=\"neg_mean_squared_error\", cv=10)...>>> lin_rmse_scores = np.sqrt(-lin_scores)>>> display_scores(lin_rmse_scores)Scores: [ 70423.5893262   65804.84913139  66620.84314068  72510.11362141          66414.74423281  71958.89083606  67624.90198297  67825.36117664          72512.36533141  68028.11688067]Mean: 68972.377566Standard deviation: 2493.98819069That‡s right: the Decision Tree model is overfitting so badly that it performs worse\\nthan the Linear Regression model.Let‡s try one last model now: the \\nRandomForestRegressor. As we will see in Chap…\\nter 7, Random Forests work by training many Decision Trees on random subsets of\\nthe features, then averaging out their predictions. Building a model on top of many\\nother models is called Ensemble Learning\\n, and it is often a great way to push ML algo…\\nrithms even further. We will skip most of the code since it is essentially the same as\\nfor the other models:70 | Chapter 2: End-to-End Machine Learning Project\\n>>> from sklearn.ensemble import RandomForestRegressor>>> forest_reg = RandomForestRegressor()>>> forest_reg.fit(housing_prepared, housing_labels)>>> [...]>>> forest_rmse22542.396440343684>>> display_scores(forest_rmse_scores)Scores: [ 53789.2879722   50256.19806622  52521.55342602  53237.44937943          52428.82176158  55854.61222549  52158.02291609  50093.66125649          53240.80406125  52761.50852822]Mean: 52634.1919593Standard deviation: 1576.20472269Wow, this is much better: Random Forests look very promising. However, note that\\nthe score on the training set is still much lower than on the validation sets, meaning\\nthat the model is still overfitting the training set. Possible solutions for overfitting are\\nto simplify the model, constrain it (i.e., regularize it), or get a lot more training data.\\nHowever, before you dive much deeper in Random Forests, you should try out many\\nother models from various categories of Machine Learning algorithms (several Sup…\\nport Vector Machines with different kernels, possibly a neural network, etc.), without\\nspending too much time tweaking the hyperparameters. The goal is to shortlist a few\\n(two to five) promising models.You should save every model you experiment with, so you can\\ncome back easily to any model you want. Make sure you save both\\nthe hyperparameters and the trained parameters, as well as the\\ncross-validation scores and perhaps the actual predictions as well.\\nThis will allow you to easily compare scores across model types,\\nand compare the types of errors they make. You can easily save\\nScikit-Learn models by using Python‡s \\npickle module, or usingsklearn.externals.joblib, which is more efficient at serializing \\nlarge NumPy arrays:\\nfrom sklearn.externals import joblibjoblib.dump(my_model, \"my_model.pkl\")# and later...my_model_loaded = joblib.load(\"my_model.pkl\")Fine-Tune Your ModelLet‡s assume that you now have a shortlist of promising models. You now need to\\nfine-tune them. Let‡s look at a few ways you can do that.\\nFine-Tune Your Model | 71\\nGrid SearchOne way to do that would be to fiddle with the hyperparameters manually, until you\\nfind a great combination of hyperparameter values. This would be very tedious work,\\nand you may not have time to explore many combinations.\\nInstead you should get Scikit-Learn‡s \\nGridSearchCV to search for you. All you need todo is tell it which hyperparameters you want it to experiment with, and what values to\\ntry out, and it will evaluate all the possible combinations of hyperparameter values,\\nusing cross-validation. For example, the following code searches for the best combi…\\nnation of hyperparameter values for the \\nRandomForestRegressor:from sklearn.model_selection import GridSearchCVparam_grid = [    {•n_estimators•: [3, 10, 30], •max_features•: [2, 4, 6, 8]},    {•bootstrap•: [False], •n_estimators•: [3, 10], •max_features•: [2, 3, 4]},  ]forest_reg = RandomForestRegressor()grid_search = GridSearchCV(forest_reg, param_grid, cv=5,                           scoring=•neg_mean_squared_error•)grid_search.fit(housing_prepared, housing_labels)When you have no idea what value a hyperparameter should have,\\na simple approach is to try out consecutive powers of 10 (or a\\nsmaller number if you want a more fine-grained search, as shown\\nin this example with the \\nn_estimators hyperparameter).\\nThis param_grid tells Scikit-Learn to first evaluate all 3 ‰ 4 = 12 combinations of\\nn_estimators and max_features hyperparameter values specified in the first \\ndict(don‡t worry about what these hyperparameters mean for now; they will be explained\\nin Chapter 7\\n), then try all 2 ‰ 3 = 6 combinations of hyperparameter values in the\\nsecond dict, but this time with the bootstrap hyperparameter set to \\nFalse instead of\\nTrue (which is the default value for this hyperparameter).\\nAll in all, the grid search will explore 12 + 6 = 18 combinations of \\nRandomForestRegressor hyperparameter values, and it will train each model five times (since we are\\nusing five-fold cross validation). In other words, all in all, there will be 18 ‰ 5 = 90\\nrounds of training! It may take quite a long time, but when it is done you can get the\\nbest combination of parameters like this:\\n>>> grid_search.best_params_{•max_features•: 6, •n_estimators•: 30}72 | Chapter 2: End-to-End Machine Learning Project\\nSince 30 is the maximum value of \\nn_estimators that was evalu…\\nated, you should probably evaluate higher values as well, since the\\nscore may continue to improve.\\nYou can also get the best estimator directly:\\n>>> grid_search.best_estimator_RandomForestRegressor(bootstrap=True, criterion=•mse•, max_depth=None,           max_features=6, max_leaf_nodes=None, min_samples_leaf=1,           min_samples_split=2, min_weight_fraction_leaf=0.0,           n_estimators=30, n_jobs=1, oob_score=False, random_state=None,           verbose=0, warm_start=False)If GridSearchCV is initialized with refit=True (which is thedefault), then once it finds the best estimator using cross-\\nvalidation, it retrains it on the whole training set. This is usually a\\ngood idea since feeding it more data will likely improve its perfor…\\nmance.And of course the evaluation scores are also available:\\n>>> cvres = grid_search.cv_results_... for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):...     print(np.sqrt(-mean_score), params)...64912.0351358 {•max_features•: 2, •n_estimators•: 3}55535.2786524 {•max_features•: 2, •n_estimators•: 10}52940.2696165 {•max_features•: 2, •n_estimators•: 30}60384.0908354 {•max_features•: 4, •n_estimators•: 3}52709.9199934 {•max_features•: 4, •n_estimators•: 10}50503.5985321 {•max_features•: 4, •n_estimators•: 30}59058.1153485 {•max_features•: 6, •n_estimators•: 3}52172.0292957 {•max_features•: 6, •n_estimators•: 10}49958.9555932 {•max_features•: 6, •n_estimators•: 30}59122.260006 {•max_features•: 8, •n_estimators•: 3}52441.5896087 {•max_features•: 8, •n_estimators•: 10}50041.4899416 {•max_features•: 8, •n_estimators•: 30}62371.1221202 {•bootstrap•: False, •max_features•: 2, •n_estimators•: 3}54572.2557534 {•bootstrap•: False, •max_features•: 2, •n_estimators•: 10}59634.0533132 {•bootstrap•: False, •max_features•: 3, •n_estimators•: 3}52456.0883904 {•bootstrap•: False, •max_features•: 3, •n_estimators•: 10}58825.665239 {•bootstrap•: False, •max_features•: 4, •n_estimators•: 3}52012.9945396 {•bootstrap•: False, •max_features•: 4, •n_estimators•: 10}In this example, we obtain the best solution by setting the \\nmax_features hyperpara…\\nmeter to 6, and the n_estimators hyperparameter to \\n30. The RMSE score for thiscombination is 49,959, which is slightly better than the score you got earlier using the\\nFine-Tune Your Model | 73\\ndefault hyperparameter values (which was 52,634). Congratulations, you have suc…\\ncessfully fine-tuned your best model!Don‡t forget that you can treat some of the data preparation steps as\\nhyperparameters. For example, the grid search will automatically\\nfind out whether or not to add a feature you were not sure about\\n(e.g., using the add_bedrooms_per_room hyperparameter of your\\nCombinedAttributesAdder transformer). It may similarly be used\\nto automatically find the best way to handle outliers, missing fea…\\ntures, feature selection, and more.\\nRandomized SearchThe grid search approach is fine when you are exploring relatively few combinations,\\nlike in the previous example, but when the hyperparameter \\nsearch space\\n is large, it isoften preferable to use RandomizedSearchCV instead. This class can be used in much\\nthe same way as the \\nGridSearchCV class, but instead of trying out all possible combi…\\nnations, it evaluates a given number of random combinations by selecting a random\\nvalue for each hyperparameter at every iteration. This approach has two main bene…\\nfits:‹If you let the randomized search run for, say, 1,000 iterations, this approach will\\nexplore 1,000 different values for each hyperparameter (instead of just a few val…\\nues per hyperparameter with the grid search approach).\\n‹You have more control over the computing budget you want to allocate to hyper…\\nparameter search, simply by setting the number of iterations.\\nEnsemble MethodsAnother way to fine-tune your system is to try to combine the models that perform\\nbest. The group (or ƒensemble⁄) will often perform better than the best individual\\nmodel (just like Random Forests perform better than the individual Decision Trees\\nthey rely on), especially if the individual models make very different types of errors.\\nWe will cover this topic in more detail in \\nChapter 7\\n.Analyze the Best Models and Their ErrorsYou will often gain good insights on the problem by inspecting the best models. For\\nexample, the \\nRandomForestRegressor can indicate the relative importance of each\\nattribute for making accurate predictions:\\n>>> feature_importances = grid_search.best_estimator_.feature_importances_>>> feature_importancesarray([  7.14156423e-02,   6.76139189e-02,   4.44260894e-02,74 | Chapter 2: End-to-End Machine Learning Project\\n         1.66308583e-02,   1.66076861e-02,   1.82402545e-02,         1.63458761e-02,   3.26497987e-01,   6.04365775e-02,         1.13055290e-01,   7.79324766e-02,   1.12166442e-02,         1.53344918e-01,   8.41308969e-05,   2.68483884e-03,         3.46681181e-03])Let‡s display these importance scores next to their corresponding attribute names:\\n>>> extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]>>> cat_one_hot_attribs = list(encoder.classes_)>>> attributes = num_attribs + extra_attribs + cat_one_hot_attribs>>> sorted(zip(feature_importances, attributes), reverse=True)[(0.32649798665134971, •median_income•), (0.15334491760305854, •INLAND•), (0.11305529021187399, •pop_per_hhold•), (0.07793247662544775, •bedrooms_per_room•), (0.071415642259275158, •longitude•), (0.067613918945568688, •latitude•), (0.060436577499703222, •rooms_per_hhold•), (0.04442608939578685, •housing_median_age•), (0.018240254462909437, •population•), (0.01663085833886218, •total_rooms•), (0.016607686091288865, •total_bedrooms•), (0.016345876147580776, •households•), (0.011216644219017424, •<1H OCEAN•), (0.0034668118081117387, •NEAR OCEAN•), (0.0026848388432755429, •NEAR BAY•), (8.4130896890070617e-05, •ISLAND•)]With this information, you may want to try dropping some of the less useful features\\n(e.g., apparently only one \\nocean_proximity category is really useful, so you could try\\ndropping the others).You should also look at the specific errors that your system makes, then try to under…\\nstand why it makes them and what could fix the problem (adding extra features or, on\\nthe contrary, getting rid of uninformative ones, cleaning up outliers, etc.).\\nEvaluate Your System on the Test SetAfter tweaking your models for a while, you eventually have a system that performs\\nsufficiently well. Now is the time to evaluate the final model on the test set. There is\\nnothing special about this process; just get the predictors and the labels from yourtest set, run your full_pipeline to transform the data (call \\ntransform(), not\\nfit_transform()!), and evaluate the final model on the test set:\\nfinal_model = grid_search.best_estimator_X_test = strat_test_set.drop(\"median_house_value\", axis=1)y_test = strat_test_set[\"median_house_value\"].copy()X_test_prepared = full_pipeline.transform(X_test)Fine-Tune Your Model | 75\\nfinal_predictions = final_model.predict(X_test_prepared)final_mse = mean_squared_error(y_test, final_predictions)final_rmse = np.sqrt(final_mse)   # => evaluates to 48,209.6The performance will usually be slightly worse than what you measured using cross-\\nvalidation if you did a lot of hyperparameter tuning (because your system ends up\\nfine-tuned to perform well on the validation data, and will likely not perform as well\\non unknown datasets). It is not the case in this example, but when this happens you\\nmust resist the temptation to tweak the hyperparameters to make the numbers look\\ngood on the test set; the improvements would be unlikely to generalize to new data.\\nNow comes the project prelaunch phase: you need to present your solution (high…\\nlighting what you have learned, what worked and what did not, what assumptions\\nwere made, and what your system‡s limitations are), document everything, and create\\nnice presentations with clear visualizations and easy-to-remember statements (e.g.,\\nƒthe median income is the number one predictor of housing prices⁄).\\nLaunch, Monitor, and Maintain Your SystemPerfect, you got approval to launch! You need to get your solution ready for produc…\\ntion, in particular by plugging the production input data sources into your system\\nand writing tests.You also need to write monitoring code to check your system‡s live performance at\\nregular intervals and trigger alerts when it drops. This is important to catch not only\\nsudden breakage, but also performance degradation. This is quite common because\\nmodels tend to ƒrot⁄ as data evolves over time, unless the models are regularly trained\\non fresh data.\\nEvaluating your system‡s performance will require sampling the system‡s predictions\\nand evaluating them. This will generally require a human analysis. These analysts\\nmay be field experts, or workers on a crowdsourcing platform (such as Amazon\\nMechanical Turk or CrowdFlower). Either way, you need to plug the human evalua…\\ntion pipeline into your system.\\nYou should also make sure you evaluate the system‡s input data quality. Sometimes\\nperformance will degrade slightly because of a poor quality signal (e.g., a malfunc…\\ntioning sensor sending random values, or another team‡s output becoming stale), but\\nit may take a while before your system‡s performance degrades enough to trigger an\\nalert. If you monitor your system‡s inputs, you may catch this earlier. Monitoring the\\ninputs is particularly important for online learning systems.\\nFinally, you will generally want to train your models on a regular basis using fresh\\ndata. You should automate this process as much as possible. If you don‡t, you are very\\n76 | Chapter 2: End-to-End Machine Learning Project\\nlikely to refresh your model only every six months (at best), and your system‡s perfor…\\nmance may fluctuate severely over time. If your system is an online learning system,\\nyou should make sure you save snapshots of its state at regular intervals so you can\\neasily roll back to a previously working state.\\nTry It Out!Hopefully this chapter gave you a good idea of what a Machine Learning project\\nlooks like, and showed you some of the tools you can use to train a great system. As\\nyou can see, much of the work is in the data preparation step, building monitoring\\ntools, setting up human evaluation pipelines, and automating regular model training.\\nThe Machine Learning algorithms are also important, of course, but it is probably\\npreferable to be comfortable with the overall process and know three or four algo…rithms well rather than to spend all your time exploring advanced algorithms and not\\nenough time on the overall process.So, if you have not already done so, now is a good time to pick up a laptop, select a\\ndataset that you are interested in, and try to go through the whole process from A to\\nZ. A good place to start is on a competition website such as \\nhttp://kaggle.com/\\n: youwill have a dataset to play with, a clear goal, and people to share the experience with.\\nExercisesUsing this chapter‡s housing dataset:\\n1.Try a Support Vector Machine regressor (\\nsklearn.svm.SVR), with various hyper…\\nparameters such as kernel=\"linear\" (with various values for the C hyperpara…\\nmeter) or kernel=\"rbf\" (with various values for the C and gammahyperparameters). Don‡t worry about what these hyperparameters mean for now.\\nHow does the best \\nSVR predictor perform?2.Try replacing \\nGridSearchCV with RandomizedSearchCV.3.Try adding a transformer in the preparation pipeline to select only the most\\nimportant attributes.\\n4.Try creating a single pipeline that does the full data preparation plus the final\\nprediction.5.Automatically explore some preparation options using \\nGridSearchCV.Solutions to these exercises are available in the online Jupyter notebooks at \\nhttps://\\ngithub.com/ageron/handson-ml\\n.Try It Out! | 77\\n1By default Scikit-Learn caches downloaded datasets in a directory called \\n$HOME/scikit_learn_data\\n.CHAPTER 3Classi•cationIn Chapter 1\\n we mentioned that the most common supervised learning tasks are\\nregression (predicting values) and classification (predicting classes). In \\nChapter 2\\n weexplored a regression task, predicting housing values, using various algorithms suchas Linear Regression, Decision Trees, and Random Forests (which will be explained\\nin further detail in later chapters). Now we will turn our attention to classification\\nsystems.MNISTIn this chapter, we will be using the MNIST dataset, which is a set of 70,000 small\\nimages of digits handwritten by high school students and employees of the US Cen…\\nsus Bureau. Each image is labeled with the digit it represents. This set has been stud…\\nied so much that it is often called the ƒHello World⁄ of Machine Learning: whenever\\npeople come up with a new classification algorithm, they are curious to see how it\\nwill perform on MNIST. Whenever someone learns Machine Learning, sooner or\\nlater they tackle MNIST.\\nScikit-Learn provides many helper functions to download popular datasets. MNIST is\\none of them. The following code fetches the MNIST dataset:\\n1>>> from sklearn.datasets import fetch_mldata>>> mnist = fetch_mldata(•MNIST original•)>>> mnist{•COL_NAMES•: [•label•, •data•], •DESCR•: •mldata.org dataset: mnist-original•, •data•: array([[0, 0, 0, ..., 0, 0, 0],        [0, 0, 0, ..., 0, 0, 0],79        [0, 0, 0, ..., 0, 0, 0],        ...,        [0, 0, 0, ..., 0, 0, 0],        [0, 0, 0, ..., 0, 0, 0],        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8), •target•: array([ 0.,  0.,  0., ...,  9.,  9.,  9.])}Datasets loaded by Scikit-Learn generally have a similar dictionary structure includ…\\ning:‹A DESCR key describing the dataset\\n‹A data key containing an array with one row per instance and one column per\\nfeature\\n‹A target key containing an array with the labels\\nLet‡s look at these arrays:\\n>>> X, y = mnist[\"data\"], mnist[\"target\"]>>> X.shape(70000, 784)>>> y.shape(70000,)There are 70,000 images, and each image has 784 features. This is because each image\\nis 28‰28 pixels, and each feature simply represents one pixel‡s intensity, from 0\\n(white) to 255 (black). Let‡s take a peek at one digit from the dataset. All you need to\\ndo is grab an instance‡s feature vector, reshape it to a 28‰28 array, and display it using\\nMatplotlib‡s \\nimshow() function:%matplotlib inlineimport matplotlibimport matplotlib.pyplot as pltsome_digit = X[36000]some_digit_image = some_digit.reshape(28, 28)plt.imshow(some_digit_image, cmap = matplotlib.cm.binary,           interpolation=\"nearest\")plt.axis(\"off\")plt.show()This looks like a 5, and indeed that‡s what the label tells us:\\n80 | Chapter 3: \\nClassi•cation2Shuffling may be a bad idea in some contexts›for example, if you are working on time series data (such as\\nstock market prices or weather conditions). We will explore this in the next chapters.\\n>>> y[36000]5.0Figure 3-1 shows a few more images from the MNIST dataset to give you a feel for\\nthe complexity of the classification task.\\nFigure 3-1. A few digits from the MNIST dataset\\nBut wait! You should always create a test set and set it aside before inspecting the data\\nclosely. The MNIST dataset is actually already split into a training set (the first 60,000\\nimages) and a test set (the last 10,000 images):X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]Let‡s also shuffle the training set; this will guarantee that all cross-validation \\nfolds willbe similar (you don‡t want one fold to be missing some digits). Moreover, some learn…\\ning algorithms are sensitive to the order of the training instances, and they performpoorly if they get many similar instances in a row. Shuffling the dataset ensures that\\nthis won‡t happen:\\n2MNIST | 81\\nimport numpy as npshuffle_index = np.random.permutation(60000)X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]Training a Binary Classi•erLet‡s simplify the problem for now and only try to identify one digit›for example,\\nthe number 5. This ƒ5-detector⁄ will be an example of a \\nbinary \\nclassi†er, capable of\\ndistinguishing between just two classes, 5 and not-5. Let‡s create the target vectors for\\nthis classification task:\\ny_train_5 = (y_train == 5)  # True for all 5s, False for all other digits.y_test_5 = (y_test == 5)Okay, now let‡s pick a classifier and train it. A good place to start is with a \\nStochastic\\nGradient Descent\\n (SGD) classifier, using Scikit-Learn‡s \\nSGDClassifier class. This clas…\\nsifier has the advantage of being capable of handling very large datasets efficiently.\\nThis is in part because SGD deals with training instances independently, one at a time\\n(which also makes SGD well suited for online learning\\n), as we will see later. Let‡s create\\nan SGDClassifier and train it on the whole training set:from sklearn.linear_model import SGDClassifiersgd_clf = SGDClassifier(random_state=42)sgd_clf.fit(X_train, y_train_5)The SGDClassifier relies on randomness during training (hencethe name ƒstochastic⁄). If you want reproducible results, you\\nshould set the random_state parameter.\\nNow you can use it to detect images of the number 5:\\n>>> sgd_clf.predict([some_digit])array([ True], dtype=bool)The classifier guesses that this image represents a 5 (\\nTrue). Looks like it guessed right\\nin this particular case! Now, let‡s evaluate this model‡s performance.\\nPerformance MeasuresEvaluating a classifier is often significantly trickier than evaluating a regressor, so we\\nwill spend a large part of this chapter on this topic. There are many performance\\nmeasures available, so grab another coffee and get ready to learn many new concepts\\nand acronyms!\\n82 | Chapter 3: \\nClassi•cationMeasuring Accuracy Using Cross-ValidationA good way to evaluate a model is to use cross-validation, just as you did in \\nChap…\\nter 2.Implementing Cross-ValidationOccasionally you will need more control over the cross-validation process than what\\ncross_val_score() and similar functions provide. In these cases, you can implement\\ncross-validation yourself; it is actually fairly straightforward. The following code does\\nroughly the same thing as the preceding cross_val_score() code, and prints the \\nsame result:from sklearn.model_selection import StratifiedKFoldfrom sklearn.base import cloneskfolds = StratifiedKFold(n_splits=3, random_state=42)for train_index, test_index in skfolds.split(X_train, y_train_5):    clone_clf = clone(sgd_clf)    X_train_folds = X_train[train_index]    y_train_folds = (y_train_5[train_index])    X_test_fold = X_train[test_index]    y_test_fold = (y_train_5[test_index])    clone_clf.fit(X_train_folds, y_train_folds)    y_pred = clone_clf.predict(X_test_fold)    n_correct = sum(y_pred == y_test_fold)    print(n_correct / len(y_pred))  # prints 0.9502, 0.96565 and 0.96495The StratifiedKFold class performs stratified sampling (as explained in \\nChapter 2\\n)to produce folds that contain a representative ratio of each class. At each iteration the\\ncode creates a clone of the classifier, trains that clone on the training folds, and makes\\npredictions on the test fold. Then it counts the number of correct predictions and\\noutputs the ratio of correct predictions.\\nLet‡s use the \\ncross_val_score() function to evaluate your \\nSGDClassifier modelusing K-fold cross-validation, with three folds. Remember that K-fold cross-\\nvalidation means splitting the training set into K-folds (in this case, three), then mak…\\ning predictions and evaluating them on each fold using a model trained on the\\nremaining folds (see Chapter 2\\n):>>> from sklearn.model_selection import cross_val_score>>> cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")array([ 0.9502 ,  0.96565,  0.96495])Performance Measures | 83\\nWow! Above 95% \\naccuracy\\n (ratio of correct predictions) on all cross-validation folds? \\nThis looks amazing, doesn‡t it? Well, before you get too excited, let‡s look at a very\\ndumb classifier that just classifies every single image in the ƒnot-5⁄ class:\\nfrom sklearn.base import BaseEstimatorclass Never5Classifier(BaseEstimator):    def fit(self, X, y=None):        pass    def predict(self, X):        return np.zeros((len(X), 1), dtype=bool)Can you guess this model‡s accuracy? Let‡s find out:\\n>>> never_5_clf = Never5Classifier()>>> cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")array([ 0.909  ,  0.90715,  0.9128 ])That‡s right, it has over 90% accuracy! This is simply because only about 10% of the\\nimages are 5s, so if you always guess that an image is \\nnot\\n a 5, you will be right about\\n90% of the time. Beats Nostradamus.\\nThis demonstrates why accuracy is generally not the preferred performance measure\\nfor classifiers, especially when you are dealing with skewed datasets\\n (i.e., when someclasses are much more frequent than others).\\nConfusion MatrixA much better way to evaluate the performance of a classifier is to look at the \\nconfu…\\nsion matrix\\n. The general idea is to count the number of times instances of class A are\\nclassified as class B. For example, to know the number of times the classifier confused\\nimages of 5s with 3s, you would look in the 5th row and 3rd column of the confusionmatrix.\\nTo compute the confusion matrix, you first need to have a set of predictions, so they\\ncan be compared to the actual targets. You could make predictions on the test set, but\\nlet‡s keep it untouched for now (remember that you want to use the test set only at the\\nvery end of your project, once you have a classifier that you are ready to launch).\\nInstead, you can use the cross_val_predict() function:from sklearn.model_selection import cross_val_predicty_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)Just like the \\ncross_val_score() function, cross_val_predict() performs K-foldcross-validation, but instead of returning the evaluation scores, it returns the predic…\\ntions made on each test fold. This means that you get a clean prediction for each\\ninstance in the training set (ƒclean⁄ meaning that the prediction is made by a model\\nthat never saw the data during training).\\n84 | Chapter 3: \\nClassi•cationNow you are ready to get the confusion matrix using the \\nconfusion_matrix() func…tion. Just pass it the target classes (\\ny_train_5) and the predicted classes(y_train_pred):>>> from sklearn.metrics import confusion_matrix>>> confusion_matrix(y_train_5, y_train_pred)array([[53272,  1307],       [ 1077,  4344]])Each row in a confusion matrix represents an \\nactual class\\n, while each column repre…sents a \\npredicted class\\n. The first row of this matrix considers non-5 images (the \\nnega…\\ntive class\\n): 53,272 of them were correctly classified as non-5s (they are called true\\nnegatives\\n), while the remaining 1,307 were wrongly classified as 5s (false positives\\n).The second row considers the images of 5s (the positive class\\n): 1,077 were wronglyclassified as non-5s (false negatives\\n), while the remaining 4,344 were correctly classi…fied as 5s (true positives\\n). A perfect classifier would have only true positives and true\\nnegatives, so its confusion matrix would have nonzero values only on its main diago…\\nnal (top left to bottom right):\\n>>> confusion_matrix(y_train_5, y_train_perfect_predictions)array([[54579,    0],       [    0, 5421]])The confusion matrix gives you a lot of information, but sometimes you may prefer a\\nmore concise metric. An interesting one to look at is the accuracy of the positive pre…\\ndictions; this is called the precision\\n of the classifier (Equation 3-1\\n).Equation 3-1. Precision\\nprecision=\\nTP\\nTP\\n+FP\\nTP is the number of true positives, and FP is the number of false positives.\\nA trivial way to have perfect precision is to make one single positive prediction and\\nensure it is correct (precision = 1/1 = 100%). This would not be very useful since the\\nclassifier would ignore all but one positive instance. So precision is typically usedalong with another metric named recall\\n, also called sensitivity\\n or true positive rate\\n(TPR\\n): this is the ratio of positive instances that are correctly detected by the classifier\\n(Equation 3-2\\n).Equation 3-2. Recall\\nrecall=\\nTP\\nTP\\n+FN\\nFN is of course the number of false negatives.\\nPerformance Measures | 85\\nIf you are confused about the confusion matrix, \\nFigure 3-2 may help.\\nFigure 3-2. An illustrated confusion matrix\\nPrecision and RecallScikit-Learn provides several functions to compute classifier metrics, including \\npreci…sion and recall:>>> from sklearn.metrics import precision_score, recall_score>>> precision_score(y_train_5, y_pred)     # == 4344 / (4344 + 1307)0.76871350203503808>>> recall_score(y_train_5, y_train_pred)  # == 4344 / (4344 + 1077)0.79136690647482011Now your 5-detector does not look as shiny as it did when you looked at its accuracy.\\nWhen it claims an image represents a 5, it is correct only 77% of the time. Moreover,\\nit only detects 79% of the 5s.It is often convenient to combine precision and recall into a single metric called the \\nF1score\\n, in particular if you need a simple way to compare two classifiers. The F\\n1 score is the harmonic mean\\n of precision and recall (Equation 3-3\\n). Whereas the regular meantreats all values equally, the harmonic mean gives much more weight to low values.\\nAs a result, the classifier will only get a high F1 score if both recall and precision arehigh.Equation 3-3. F\\n1 score\\nF1=21precision+1recall=2‰\\nprecision‰recall\\nprecision+recall\\n=TP\\nTP\\n+FN\\n+FP\\n286 | Chapter 3: \\nClassi•cationTo compute the F\\n1 score, simply call the \\nf1_score() function:>>> from sklearn.metrics import f1_score>>> f1_score(y_train_5, y_pred)0.78468208092485547The F1 score favors classifiers that have similar precision and recall. This is not always\\nwhat you want: in some contexts you mostly care about precision, and in other con…\\ntexts you really care about recall. For example, if you trained a classifier to detect vid…\\neos that are safe for kids, you would probably prefer a classifier that rejects many\\ngood videos (low recall) but keeps only safe ones (high precision), rather than a clas…\\nsifier that has a much higher recall but lets a few really bad videos show up in your\\nproduct (in such cases, you may even want to add a human pipeline to check the clas…\\nsifier‡s video selection). On the other hand, suppose you train a classifier to detect\\nshoplifters on surveillance images: it is probably fine if your classifier has only 30%\\nprecision as long as it has 99% recall (sure, the security guards will get a few falsealerts, but almost all shoplifters will get caught).\\nUnfortunately, you can‡t have it both ways: increasing precision reduces recall, and\\nvice versa. This is called the precision/recall \\ntradeo›.Precision/Recall Tradeo†To understand this tradeoff, let‡s look at how the \\nSGDClassifier makes its classifica…tion decisions. For each instance, it computes a score based on a \\ndecision function\\n, and if that score is greater than a threshold, it assigns the instance to the positive\\nclass, or else it assigns it to the negative class. \\nFigure 3-3 shows a few digits positioned\\nfrom the lowest score on the left to the highest score on the right. Suppose the \\ndeci…\\nsion threshold\\n is positioned at the central arrow (between the two 5s): you will find 4\\ntrue positives (actual 5s) on the right of that threshold, and one false positive (actually\\na 6). Therefore, with that threshold, the precision is 80% (4 out of 5). But out of 6\\nactual 5s, the classifier only detects 4, so the recall is 67% (4 out of 6). Now if you\\nraise the threshold (move it to the arrow on the right), the false positive (the 6)\\nbecomes a true negative, thereby increasing precision (up to 100% in this case), but\\none true positive becomes a false negative, decreasing recall down to 50%. Conversely,\\nlowering the threshold increases recall and reduces precision.Performance Measures | 87\\nFigure 3-3. Decision threshold and precision/recall \\ntradeo›Scikit-Learn does not let you set the threshold directly, but it does give you access to\\nthe decision scores that it uses to make predictions. Instead of calling the classifier‡s\\npredict() method, you can call its decision_function() method, which returns ascore for each instance, and then make predictions based on those scores using any\\nthreshold you want:\\n>>> y_scores = sgd_clf.decision_function([some_digit])>>> y_scoresarray([ 161855.74572176])>>> threshold = 0>>> y_some_digit_pred = (y_scores > threshold)array([ True], dtype=bool)The SGDClassifier uses a threshold equal to 0, so the previous code returns the same\\nresult as the predict() method (i.e., True). Let‡s raise the threshold:\\n>>> threshold = 200000>>> y_some_digit_pred = (y_scores > threshold)>>> y_some_digit_predarray([False], dtype=bool)This confirms that raising the threshold decreases recall. The image actually repre…\\nsents a 5, and the classifier detects it when the threshold is 0, but it misses it when the\\nthreshold is increased to 200,000.So how can you decide which threshold to use? For this you will first need to get the\\nscores of all instances in the training set using the cross_val_predict() functionagain, but this time specifying that you want it to return decision scores instead of\\npredictions:y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,                             method=\"decision_function\")Now with these scores you can compute precision and recall for all possible thresh…\\nolds using the precision_recall_curve() function:88 | Chapter 3: \\nClassi•cationfrom sklearn.metrics import precision_recall_curveprecisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)Finally, you can plot precision and recall as functions of the threshold value using\\nMatplotlib (\\nFigure 3-4):def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")    plt.xlabel(\"Threshold\")    plt.legend(loc=\"upper left\")    plt.ylim([0, 1])plot_precision_recall_vs_threshold(precisions, recalls, thresholds)plt.show()Figure 3-4. Precision and recall versus the decision threshold\\nYou may wonder why the precision curve is bumpier than the recall\\ncurve in \\nFigure 3-4. The reason is that precision may sometimes go\\ndown when you raise the threshold (although in general it will goup). To understand why, look back at \\nFigure 3-3 and notice what\\nhappens when you start from the central threshold and move it just\\none digit to the right: precision goes from 4/5 (80%) down to 3/4\\n(75%). On the other hand, recall can only go down when the thres…hold is increased, which explains why its curve looks smooth.\\nNow you can simply select the threshold value that gives you the best precision/recall\\ntradeoff for your task. Another way to select a good precision/recall tradeoff is to plot\\nprecision directly against recall, as shown in Figure 3-5.Performance Measures | 89\\nFigure 3-5. Precision versus recall\\nYou can see that precision really starts to fall sharply around 80% recall. You will\\nprobably want to select a precision/recall tradeoff just before that drop›for example,\\nat around 60% recall. But of course the choice depends on your project.\\nSo let‡s suppose you decide to aim for 90% precision. You look up the first plot\\n(zooming in a bit) and find that you need to use a threshold of about 70,000. To make\\npredictions (on the training set for now), instead of calling the classifier‡s \\npredict()method, you can just run this code:y_train_pred_90 = (y_scores > 70000)Let‡s check these predictions‡ precision and recall:\\n>>> precision_score(y_train_5, y_train_pred_90)0.8998702983138781>>> recall_score(y_train_5, y_train_pred_90)0.63991883416343853Great, you have a 90% precision classifier (or close enough)! As you can see, it is\\nfairly easy to create a classifier with virtually any precision you want: just set a high\\nenough threshold, and you‡re done. Hmm, not so fast. A high-precision classifier is\\nnot very useful if its recall is too low!\\nIf someone says ƒlet‡s reach 99% precision,⁄ you should ask, ƒat\\nwhat recall?⁄\\n90 | Chapter 3: \\nClassi•cationThe ROC CurveThe receiver operating characteristic\\n (ROC) curve is another common tool used with\\nbinary classifiers. It is very similar to the precision/recall curve, but instead of plot…\\nting precision versus recall, the ROC curve plots the \\ntrue positive rate\\n (another namefor recall) against the false positive rate\\n. The FPR is the ratio of negative instances that\\nare incorrectly classified as positive. It is equal to one minus the \\ntrue negative rate\\n, which is the ratio of negative instances that are correctly classified as negative. The\\nTNR is also called speci†city. Hence the ROC curve plots \\nsensitivity\\n (recall) versus1 – speci†city.To plot the ROC curve, you first need to compute the TPR and FPR for various thres…\\nhold values, using the roc_curve() function:from sklearn.metrics import roc_curvefpr, tpr, thresholds = roc_curve(y_train_5, y_scores)Then you can plot the FPR against the TPR using Matplotlib. This code produces the\\nplot in Figure 3-6:def plot_roc_curve(fpr, tpr, label=None):    plt.plot(fpr, tpr, linewidth=2, label=label)    plt.plot([0, 1], [0, 1], •k--•)    plt.axis([0, 1, 0, 1])    plt.xlabel(•False Positive Rate•)    plt.ylabel(•True Positive Rate•)plot_roc_curve(fpr, tpr)plt.show()Figure 3-6. ROC curve\\nPerformance Measures | 91\\nOnce again there is a tradeoff: the higher the recall (TPR), the more false positives(FPR) the classifier produces. The dotted line represents the ROC curve of a purely\\nrandom classifier; a good classifier stays as far away from that line as possible (toward\\nthe top-left corner).One way to compare classifiers is to measure the \\narea under the curve\\n (AUC). \\nA per…fect classifier will have a \\nROC AUC\\n equal to 1, whereas a purely random classifier willhave a ROC AUC equal to 0.5. Scikit-Learn provides a function to compute the ROC\\nAUC:\\n>>> from sklearn.metrics import roc_auc_score>>> roc_auc_score(y_train_5, y_scores)0.97061072797174941Since the ROC curve is so similar to the precision/recall (or PR)\\ncurve, you may wonder how to decide which one to use. As a rule\\nof thumb, you should prefer the PR curve whenever the positive\\nclass is rare or when you care more about the false positives thanthe false negatives, and the ROC curve otherwise. For example,\\nlooking at the previous ROC curve (and the ROC AUC score), you\\nmay think that the classifier is really good. But this is mostly\\nbecause there are few positives (5s) compared to the negatives\\n(non-5s). In contrast, the PR curve makes it clear that the classifier\\nhas room for improvement (the curve could be closer to the top-\\nright corner).\\nLet‡s train a \\nRandomForestClassifier and compare its ROC curve and ROC AUC\\nscore to the SGDClassifier. First, you need to get scores for each instance in thetraining set. But due to the way it works (see \\nChapter 7\\n), the RandomForestClassifier class does not have a \\ndecision_function() method. Instead it has a predict_proba() method. Scikit-Learn classifiers generally have one or the other. The\\npredict_proba() method returns an array containing a row per instance and a col…\\numn per class, each containing the probability that the given instance belongs to the\\ngiven class (e.g., 70% chance that the image represents a 5):\\nfrom sklearn.ensemble import RandomForestClassifierforest_clf = RandomForestClassifier(random_state=42)y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3,                                    method=\"predict_proba\")But to plot a ROC curve, you need scores, not probabilities. A simple solution is to\\nuse the positive class‡s probability as the score:\\ny_scores_forest = y_probas_forest[:, 1]   # score = proba of positive classfpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5,y_scores_forest)92 | Chapter 3: \\nClassi•cationNow you are ready to plot the ROC curve. It is useful to plot the first ROC curve as\\nwell to see how they compare (\\nFigure 3-7):plt.plot(fpr, tpr, \"b:\", label=\"SGD\")plot_roc_curve(fpr_forest, tpr_forest, \"Random Forest\")plt.legend(loc=\"bottom right\")plt.show()Figure 3-7. Comparing ROC curves\\nAs you can see in Figure 3-7, the RandomForestClassifier‡s ROC curve looks much\\nbetter than the SGDClassifier‡s: it comes much closer to the top-left corner. As a\\nresult, its ROC AUC score is also significantly better:\\n>>> roc_auc_score(y_train_5, y_scores_forest)0.99312433660038291Try measuring the precision and recall scores: you should find 98.5% precision and\\n82.8% recall. Not too bad!\\nHopefully you now know how to train binary classifiers, choose the appropriate met…\\nric for your task, evaluate your classifiers using cross-validation, select the precision/\\nrecall tradeoff that fits your needs, and compare various models using ROC curves\\nand ROC AUC scores. Now let‡s try to detect more than just the 5s.\\nMulticlass Classi•cationWhereas binary classifiers distinguish between two classes, \\nmulticlass \\nclassi†ers (alsocalled multinomial \\nclassi†ers) can distinguish between more than two classes.Multiclass Classi•cation | 93\\nSome algorithms (such as Random Forest classifiers or naive Bayes classifiers) are\\ncapable of handling multiple classes directly. Others (such as Support Vector Machine\\nclassifiers or Linear classifiers) are strictly binary classifiers. However, there are vari…\\nous strategies that you can use to perform multiclass classification using multiple\\nbinary classifiers.\\nFor example, one way to create a system that can classify the digit images into 10\\nclasses (from 0 to 9) is to train 10 binary classifiers, one for each digit (a 0-detector, a\\n1-detector, a 2-detector, and so on). Then when you want to classify an image, you get\\nthe decision score from each classifier for that image and you select the class whose\\nclassifier outputs the highest score. This is called the one-versus-all\\n (OvA) strategy \\n(also called one-versus-the-rest\\n).Another strategy is to train a binary classifier for every pair of digits: one to distin…\\nguish 0s and 1s, another to distinguish 0s and 2s, another for 1s and 2s, and so on.This is called the one-versus-one\\n (OvO) strategy. If there are \\nN classes, you need totrain N ‰ (N – 1) / 2 classifiers. For the MNIST problem, this means training 45\\nbinary classifiers! When you want to classify an image, you have to run the image\\nthrough all 45 classifiers and see which class wins the most duels. The main advan…tage of OvO is that each classifier only needs to be trained on the part of the training\\nset for the two classes that it must distinguish.\\nSome algorithms (such as Support Vector Machine classifiers) scale poorly with the\\nsize of the training set, so for these algorithms OvO is preferred since it is faster to\\ntrain many classifiers on small training sets than training few classifiers on large\\ntraining sets. For most binary classification algorithms, however, OvA is preferred.\\nScikit-Learn detects when you try to use a binary classification algorithm for a multi…\\nclass classification task, and it automatically runs OvA (except for SVM classifiers for\\nwhich it uses OvO). Let‡s try this with the \\nSGDClassifier:>>> sgd_clf.fit(X_train, y_train)  # y_train, not y_train_5>>> sgd_clf.predict([some_digit])array([ 5.])That was easy! This code trains the \\nSGDClassifier on the training set using the origi…\\nnal target classes from 0 to 9 (y_train), instead of the 5-versus-all target classes(y_train_5). Then it makes a prediction (a correct one in this case). Under the hood,\\nScikit-Learn actually trained 10 binary classifiers, got their decision scores for the\\nimage, and selected the class with the highest score.To see that this is indeed the case, you can call the \\ndecision_function() method.Instead of returning just one score per instance, it now returns 10 scores, one perclass:>>> some_digit_scores = sgd_clf.decision_function([some_digit])>>> some_digit_scores94 | Chapter 3: \\nClassi•cationarray([[-311402.62954431, -363517.28355739, -446449.5306454 ,        -183226.61023518, -414337.15339485,  161855.74572176,        -452576.39616343, -471957.14962573, -518542.33997148,        -536774.63961222]])The highest score is indeed the one corresponding to class 5:>>> np.argmax(some_digit_scores)5>>> sgd_clf.classes_array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.])>>> sgd_clf.classes[5]5.0When a classifier is trained, it stores the list of target classes in itsclasses_ attribute, ordered by value. In this case, the index of each\\nclass in the classes_ array conveniently matches the class itself\\n(e.g., the class at index 5 happens to be class 5), but in general you\\nwon‡t be so lucky.\\nIf you want to force ScikitLearn to use one-versus-one or one-versus-all, you can use\\nthe OneVsOneClassifier or OneVsRestClassifier classes. Simply create an instance\\nand pass a binary classifier to its constructor. For example, this code creates a multi…\\nclass classifier using the OvO strategy, based on a \\nSGDClassifier:>>> from sklearn.multiclass import OneVsOneClassifier>>> ovo_clf = OneVsOneClassifier(SGDClassifier(random_state=42))>>> ovo_clf.fit(X_train, y_train)>>> ovo_clf.predict([some_digit])array([ 5.])>>> len(ovo_clf.estimators_)45Training a \\nRandomForestClassifier is just as easy:\\n>>> forest_clf.fit(X_train, y_train)>>> forest_clf.predict([some_digit])array([ 5.])This time Scikit-Learn did not have to run OvA or OvO because Random Forest\\nclassifiers can directly classify instances into multiple classes. You can call\\npredict_proba() to get the list of probabilities that the classifier assigned to each\\ninstance for each class:>>> forest_clf.predict_proba([some_digit])array([[ 0.1,  0. ,  0. ,  0.1,  0. ,  0.8,  0. ,  0. ,  0. ,  0. ]])You can see that the classifier is fairly confident about its prediction: the 0.8 at the 5\\nthindex in the array means that the model estimates an 80% probability that the image\\nMulticlass Classi•cation | 95\\nrepresents a 5. It also thinks that the image could instead be a 0 or a 3 (10% chance\\neach).Now of course you want to evaluate these classifiers. As usual, you want to use cross-\\nvalidation. Let‡s evaluate the \\nSGDClassifier‡s accuracy using the \\ncross_val_score()function:>>> cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")array([ 0.84063187,  0.84899245,  0.86652998])It gets over 84% on all test folds. If you used a random classifier, you would get 10%\\naccuracy, so this is not such a bad score, but you can still do much better. For exam…\\nple, simply scaling the inputs (as discussed in \\nChapter 2\\n) increases accuracy above90%:>>> from sklearn.preprocessing import StandardScaler>>> scaler = StandardScaler()>>> X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))>>> cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\")array([ 0.91011798,  0.90874544,  0.906636  ])Error AnalysisOf course, if this were a real project, you would follow the steps in your Machine\\nLearning project checklist (see Appendix B\\n): exploring data preparation options, try…\\ning out multiple models, shortlisting the best ones and fine-tuning their hyperpara…\\nmeters using GridSearchCV, and automating as much as possible, as you did in the\\nprevious chapter. Here, we will assume that you have found a promising model and\\nyou want to find ways to improve it. One way to do this is to analyze the types of\\nerrors it makes.First, you can look at the confusion matrix. You need to make predictions using the\\ncross_val_predict() function, \\nthen call the confusion_matrix() function, just likeyou did earlier:\\n>>> y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)>>> conf_mx = confusion_matrix(y_train, y_train_pred)>>> conf_mxarray([[5725,    3,   24,    9,   10,   49,   50,   10,   39,    4],       [   2, 6493,   43,   25,    7,   40,    5,   10,  109,    8],       [  51,   41, 5321,  104,   89,   26,   87,   60,  166,   13],       [  47,   46,  141, 5342,    1,  231,   40,   50,  141,   92],       [  19,   29,   41,   10, 5366,    9,   56,   37,   86,  189],       [  73,   45,   36,  193,   64, 4582,  111,   30,  193,   94],       [  29,   34,   44,    2,   42,   85, 5627,   10,   45,    0],       [  25,   24,   74,   32,   54,   12,    6, 5787,   15,  236],       [  52,  161,   73,  156,   10,  163,   61,   25, 5027,  123],       [  43,   35,   26,   92,  178,   28,    2,  223,   82, 5240]])96 | Chapter 3: \\nClassi•cationThat‡s a lot of numbers. It‡s often more convenient to look at an image representation\\nof the confusion matrix, using Matplotlib‡s \\nmatshow() function:plt.matshow(conf_mx, cmap=plt.cm.gray)plt.show()This confusion matrix looks fairly good, since most images are on the main diagonal,\\nwhich means that they were classified correctly. The 5s look slightly darker than the\\nother digits, which could mean that there are fewer images of 5s in the dataset or that\\nthe classifier does not perform as well on 5s as on other digits. In fact, you can verify\\nthat both are the case.\\nLet‡s focus the plot on the errors. First, you need to divide each value in the confusion\\nmatrix by the number of images in the corresponding class, so you can compare error\\nrates instead of absolute number of errors (which would make abundant classes look\\nunfairly bad):row_sums = conf_mx.sum(axis=1, keepdims=True)norm_conf_mx = conf_mx / row_sumsNow let‡s fill the diagonal with zeros to keep only the errors, and let‡s plot the result:\\nnp.fill_diagonal(norm_conf_mx, 0)plt.matshow(norm_conf_mx, cmap=plt.cm.gray)plt.show()Error Analysis | 97\\nNow you can clearly see the kinds of errors the classifier makes. Remember that rows\\nrepresent actual classes, while columns represent predicted classes. The columns for\\nclasses 8 and 9 are quite bright, which tells you that many images get misclassified as\\n8s or 9s. Similarly, the rows for classes 8 and 9 are also quite bright, telling you that 8s\\nand 9s are often confused with other digits. Conversely, some rows are pretty dark,\\nsuch as row 1: this means that most 1s are classified correctly (a few are confused\\nwith 8s, but that‡s about it). Notice that the errors are not perfectly symmetrical; for\\nexample, there are more 5s misclassified as 8s than the reverse.\\nAnalyzing the confusion matrix can often give you insights on ways to improve your\\nclassifier. Looking at this plot, it seems that your efforts should be spent on improving\\nclassification of 8s and 9s, as well as fixing the specific 3/5 confusion. For example,\\nyou could try to gather more training data for these digits. Or you could engineer\\nnew features that would help the classifier›for example, writing an algorithm to\\ncount the number of closed loops (e.g., 8 has two, 6 has one, 5 has none). Or you\\ncould preprocess the images (e.g., using Scikit-Image, Pillow, or OpenCV) to make\\nsome patterns stand out more, such as closed loops.\\nAnalyzing individual errors can also be a good way to gain insights on what your\\nclassifier is doing and why it is failing, but it is more difficult and time-consuming.\\nFor example, let‡s plot examples of 3s and 5s:\\ncl_a, cl_b = 3, 5X_aa = X_train[(y_train == cl_a) & (y_train_pred == cl_a)]X_ab = X_train[(y_train == cl_a) & (y_train_pred == cl_b)]X_ba = X_train[(y_train == cl_b) & (y_train_pred == cl_a)]X_bb = X_train[(y_train == cl_b) & (y_train_pred == cl_b)]plt.figure(figsize=(8,8))plt.subplot(221); plot_digits(X_aa[:25], images_per_row=5)plt.subplot(222); plot_digits(X_ab[:25], images_per_row=5)98 | Chapter 3: \\nClassi•cation3But remember that our brain is a fantastic pattern recognition system, and our visual system does a lot of\\ncomplex preprocessing before any information reaches our consciousness, so the fact that it feels simple does\\nnot mean that it is.\\nplt.subplot(223); plot_digits(X_ba[:25], images_per_row=5)plt.subplot(224); plot_digits(X_bb[:25], images_per_row=5)plt.show()The two 5‰5 blocks on the left show digits classified as 3s, and the two 5‰5 blocks onthe right show images classified as 5s. Some of the digits that the classifier gets wrong\\n(i.e., in the bottom-left and top-right blocks) are so badly written that even a human\\nwould have trouble classifying them (e.g., the 5 on the 8\\nth row and 1st column trulylooks like a 3). However, most misclassified images seem like obvious errors to us,\\nand it‡s hard to understand why the classifier made the mistakes it did.\\n3 The reason isthat we used a simple \\nSGDClassifier, which is a linear model. All it does is assign aweight per class to each pixel, and when it sees a new image it just sums up the weigh…\\nted pixel intensities to get a score for each class. So since 3s and 5s differ only by a few\\npixels, this model will easily confuse them.The main difference between 3s and 5s is the position of the small line that joins the\\ntop line to the bottom arc. If you draw a 3 with the junction slightly shifted to the left,\\nthe classifier might classify it as a 5, and vice versa. In other words, this classifier is\\nquite sensitive to image shifting and rotation. So one way to reduce the 3/5 confusion\\nwould be to preprocess the images to ensure that they are well centered and not too\\nrotated. This will probably help reduce other errors as well.\\nError Analysis | 99\\nMultilabel Classi•cationUntil \\nnow each instance has always been assigned to just one class. In some cases you\\nmay want your classifier to output multiple classes for each instance. For example,\\nconsider a face-recognition classifier: what should it do if it recognizes several people\\non the same picture? Of course it should attach one label per person it recognizes. Say\\nthe classifier has been trained to recognize three faces, Alice, Bob, and Charlie; then\\nwhen it is shown a picture of Alice and Charlie, it should output [1, 0, 1] (meaningƒAlice yes, Bob no, Charlie yes⁄). Such a classification system that outputs multiple\\nbinary labels is called a \\nmultilabel \\nclassi†cation system.We won‡t go into face recognition just yet, but let‡s look at a simpler example, just for\\nillustration purposes:\\nfrom sklearn.neighbors import KNeighborsClassifiery_train_large = (y_train >= 7)y_train_odd = (y_train % 2 == 1)y_multilabel = np.c_[y_train_large, y_train_odd]knn_clf = KNeighborsClassifier()knn_clf.fit(X_train, y_multilabel)This code creates a \\ny_multilabel array containing two target labels for each digit\\nimage: the first indicates whether or not the digit is large (7, 8, or 9) and the second\\nindicates whether or not it is odd. The next lines create a \\nKNeighborsClassifier instance (which supports multilabel classification, but not all classifiers do) and we\\ntrain it using the multiple targets array. Now you can make a prediction, and notice\\nthat it outputs two labels:\\n>>> knn_clf.predict([some_digit])array([[False,  True]], dtype=bool)And it gets it right! The digit 5 is indeed not large (\\nFalse) and odd (True).There are many ways to evaluate a multilabel classifier, and selecting the right metric\\nreally depends on your project. For example, one approach is to measure the F\\n1 score\\nfor each individual label (or any other binary classifier metric discussed earlier), then\\nsimply compute the average score. This code computes the average F\\n1 score across alllabels:>>> y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_train, cv=3)>>> f1_score(y_train, y_train_knn_pred, average=\"macro\")0.96845540180280221This assumes that all labels are equally important, which may not be the case. In par…\\nticular, if you have many more pictures of Alice than of Bob or Charlie, you may want\\nto give more weight to the classifier‡s score on pictures of Alice. One simple option is\\n100 | Chapter 3: \\nClassi•cation4Scikit-Learn offers a few other averaging options and multilabel classifier metrics; see the documentation for\\nmore details.to give each label a weight equal to its \\nsupport\\n (i.e., the number of instances with that\\ntarget label). To do this, simply set \\naverage=\"weighted\" in the preceding code.4Multioutput Classi•cationThe last type of classification task we are going to discuss here is called \\nmultioutput-\\nmulticlass \\nclassi†cation (or simply \\nmultioutput \\nclassi†cation). It is simply a generaliza…\\ntion of multilabel classification where each label can be multiclass (i.e., it can have\\nmore than two possible values).To illustrate this, let‡s build a system that removes noise from images. It will take as\\ninput a noisy digit image, and it will (hopefully) output a clean digit image, repre…\\nsented as an array of pixel intensities, just like the MNIST images. Notice that the\\nclassifier‡s output is multilabel (one label per pixel) and each label can have multiple\\nvalues (pixel intensity ranges from 0 to 255). It is thus an example of a multioutput\\nclassification system.\\nThe line between classification and regression is sometimes blurry,\\nsuch as in this example. Arguably, predicting pixel intensity is more\\nakin to regression than to classification. Moreover, multioutput\\nsystems are not limited to classification tasks; you could even have\\na system that outputs multiple labels per instance, including both\\nclass labels and value labels.Let‡s start by creating the training and test sets by taking the MNIST images and\\nadding noise to their pixel intensities using NumPy‡s \\nrandint() function. The targetimages will be the original images:noise = rnd.randint(0, 100, (len(X_train), 784))noise = rnd.randint(0, 100, (len(X_test), 784))X_train_mod = X_train + noiseX_test_mod = X_test + noisey_train_mod = X_trainy_test_mod = X_testLet‡s take a peek at an image from the test set (yes, we‡re snooping on the test data, so\\nyou should be frowning right now):\\nMultioutput Classi•cation | 101\\n5You can use the \\nshift() function from the scipy.ndimage.interpolation module. For example,\\nshift(image, [2, 1], cval=0) shifts the image 2 pixels down and 1 pixel to the right.\\nOn the left is the noisy input image, and on the right is the clean target image. Now\\nlet‡s train the classifier and make it clean this image:\\nknn_clf.fit(X_train_mod, y_train_mod)clean_digit = knn_clf.predict([X_test_mod[some_index]])plot_digit(clean_digit)Looks close enough to the target! This concludes our tour of classification. Hopefully\\nyou should now know how to select good metrics for classification tasks, pick the\\nappropriate precision/recall tradeoff, compare classifiers, and more generally build\\ngood classification systems for a variety of tasks.\\nExercises1.Try to build a classifier for the MNIST dataset that achieves over 97% accuracy\\non the test set. Hint: the \\nKNeighborsClassifier works quite well for this task;you just need to find good hyperparameter values (try a grid search on the\\nweights and n_neighbors hyperparameters).\\n2.Write a function that can shift an MNIST image in any direction (left, right, up,\\nor down) by one pixel.5 Then, for each image in the training set, create four shif…\\nted copies (one per direction) and add them to the training set. Finally, train your\\nbest model on this expanded training set and measure its accuracy on the test set.You should observe that your model performs even better now! This technique of\\n102 | Chapter 3: \\nClassi•cationartificially growing the training set is called data augmentation\\n or training set\\nexpansion\\n.3.Tackle the \\nTitanic\\n dataset. A great place to start is on \\nKaggle.4.Build a spam classifier (a more challenging exercise):‹Download examples of spam and ham from \\nApache SpamAssassin‡s public\\ndatasets\\n.‹Unzip the datasets and familiarize yourself with the data format.\\n‹Split the datasets into a training set and a test set.\\n‹Write a data preparation pipeline to convert each email into a feature vector.\\nYour preparation pipeline should transform an email into a (sparse) vector\\nindicating the presence or absence of each possible word. For example, if all\\nemails only ever contain four words, ƒHello,⁄ ƒhow,⁄ ƒare,⁄ ƒyou,⁄ then the email\\nƒHello you Hello Hello you⁄ would be converted into a vector [1, 0, 0, 1]\\n(meaning [ƒHello⁄ is present, ƒhow⁄ is absent, ƒare⁄ is absent, ƒyou⁄ is\\npresent]), or [3, 0, 0, 2] if you prefer to count the number of occurrences of\\neach word.‹You may want to add hyperparameters to your preparation pipeline to control\\nwhether or not to strip off email headers, convert each email to lowercase,\\nremove punctuation, replace all URLs with ƒURL,⁄ replace all numbers with\\nƒNUMBER,⁄ or even perform \\nstemming\\n (i.e., trim off word endings; there arePython libraries available to do this).\\n‹Then try out several classifiers and see if you can build a great spam classifier,\\nwith both high recall and high precision.Solutions to these exercises are available in the online Jupyter notebooks at \\nhttps://\\ngithub.com/ageron/handson-ml\\n.Exercises | 103\\nCHAPTER 4Training ModelsSo far we have treated Machine Learning models and their training algorithms mostly\\nlike black boxes. If you went through some of the exercises in the previous chapters,\\nyou may have been surprised by how much you can get done without knowing any…\\nthing about what‡s under the hood: you optimized a regression system, you improved\\na digit image classifier, and you even built a spam classifier from scratch›all this\\nwithout knowing how they actually work. Indeed, in many situations you don‡t really\\nneed to know the implementation details.\\nHowever, having a good understanding of how things work can help you quickly\\nhome in on the appropriate model, the right training algorithm to use, and a good set\\nof hyperparameters for your task. Understanding what‡s under the hood will also help\\nyou debug issues and perform error analysis more efficiently. Lastly, most of the top…\\nics discussed in this chapter will be essential in understanding, building, and training\\nneural networks (discussed in Part II\\n of this book).In this chapter, we will start by looking at the Linear Regression model, one of the\\nsimplest models there is. We will discuss two very different ways to train it:\\n‹Using a direct ƒclosed-form⁄ equation that directly computes the model parame…\\nters that best fit the model to the training set (i.e., the model parameters that\\nminimize the cost function over the training set).‹Using an iterative optimization approach, called Gradient Descent (GD), that\\ngradually tweaks the model parameters to minimize the cost function over thetraining set, eventually converging to the same set of parameters as the first\\nmethod. We will look at a few variants of Gradient Descent that we will use again\\nand again when we study neural networks in Part II\\n: Batch GD, Mini-batch GD,\\nand Stochastic GD.\\n105Next we will look at Polynomial Regression, a more complex model that can fit non…\\nlinear datasets. Since this model has more parameters than Linear Regression, it is\\nmore prone to overfitting the training data, so we will look at how to detect whether\\nor not this is the case, using learning curves, and then we will look at several regulari…\\nzation techniques that can reduce the risk of overfitting the training set.\\nFinally, we will look at two more models that are commonly used for classification\\ntasks: Logistic Regression and Softmax Regression.There will be quite a few math equations in this chapter, using basic\\nnotions of linear algebra and calculus. To understand these equa…\\ntions, you will need to know what vectors and matrices are, how to\\ntranspose them, what the dot product is, what matrix inverse is,\\nand what partial derivatives are. If you are unfamiliar with these\\nconcepts, please go through the linear algebra and calculus intro…\\nductory tutorials available as Jupyter notebooks in the online sup…\\nplemental material. For those who are truly allergic to\\nmathematics, you should still go through this chapter and simply\\nskip the equations; hopefully, the text will be sufficient to help you\\nunderstand most of the concepts.Linear RegressionIn Chapter 1\\n, we looked at a simple regression model of life satisfaction: \\nlife_satisfac…\\ntion\\n = –0 + –1 ‰ GDP_per_capita\\n.This model is just a linear function of the input feature \\nGDP_per_capita. –0 and \\n–1 are\\nthe model‡s parameters.\\nMore generally, a linear model makes a prediction by simply computing a weighted\\nsum of the input features, plus a constant called \\nthe bias term\\n (also called the \\nintercept\\nterm\\n), as shown in Equation 4-1\\n.Equation 4-1. Linear Regression model prediction\\ny=–0+–1x1+–2x2++–nxn‹ƒ is the predicted value.‹n is the number of features.\\n‹xi is the ith feature value.\\n‹–j is the jth model parameter (including the bias term –0 and the feature weights\\n–1, –2, , –n).106 | Chapter 4: Training Models\\n1It is often the case that a learning algorithm will try to optimize a different function than the performance\\nmeasure used to evaluate the final model. This is generally because that function is easier to compute, because\\nit has useful differentiation properties that the performance measure lacks, or because we want to constrain\\nthe model during training, as we will see when we discuss regularization.\\nThis can be written much more concisely using a vectorized form, as shown in \\nEqua…tion 4-2.Equation 4-2. Linear Regression model prediction (vectorized form)\\ny=h–=–T’‹– is the model‡s \\nparameter vector\\n, containing the bias term \\n–0 and the feature\\nweights \\n–1 to –n.‹–T is the transpose of – (a row vector instead of a column vector).‹x is the instance‡s \\nfeature vector\\n, containing \\nx0 to xn, with x0 always equal to 1.\\n‹–T ’ x is the dot product of –T and x.‹h– is the hypothesis function, using the model parameters \\n–.Okay, that‡s the Linear Regression model, so now how do we train it? Well, recall that\\ntraining a model means setting its parameters so that the model best fits the training\\nset. For this purpose, we first need a measure of how well (or poorly) the model fits\\nthe training data. In \\nChapter 2\\n we saw that the most common performance measure\\nof a regression model is the Root Mean Square Error (RMSE) (\\nEquation 2-1\\n). There…fore, to train a Linear Regression model, you need to find the value of – that minimi…\\nzes the RMSE. In practice, it is simpler to minimize the Mean Square Error (MSE)\\nthan the RMSE, and it leads to the same result (because the value that minimizes a\\nfunction also minimizes its square root).1The MSE of a Linear Regression hypothesis \\nh– on a training set X is calculated using\\nEquation 4-3\\n.Equation 4-3. MSE cost function for a Linear Regression model\\nMSE,h–=1m“i=1\\nm–T’i”yi2Most of these notations were presented in \\nChapter 2\\n (see ƒNotations⁄\\n on page 38).The only difference is that we write \\nh– instead of just h in order to make it clear that\\nthe model is parametrized by the vector –. To simplify notations, we will just write\\nMSE(–) instead of MSE(X, h–).Linear Regression | 107\\n2The demonstration that this returns the value of \\n– that minimizes the cost function is outside the scope of this\\nbook.The Normal EquationTo \\nfind the value of – that minimizes the cost function, there is a \\nclosed-form solution\\n›in other words, a mathematical equation that gives the result directly. This is called\\nthe Normal Equation\\n (Equation 4-4\\n).2Equation 4-4. Normal Equation\\n–=T’”1\\n’T’‹– is the value of – that minimizes the cost function.\\n‹y is the vector of target values containing \\ny(1) to y(m).Let‡s generate some linear-looking data to test this equation on (\\nFigure 4-1):import numpy as npX = 2 * np.random.rand(100, 1)y = 4 + 3 * X + np.random.randn(100, 1)Figure 4-1. Randomly generated linear dataset\\n108 | Chapter 4: Training Models\\nNow let‡s compute \\n– using the Normal Equation. We will use the \\ninv() function from\\nNumPy‡s Linear Algebra module (\\nnp.linalg) to compute the inverse of a matrix, and\\nthe dot() method for matrix multiplication:\\nX_b = np.c_[np.ones((100, 1)), X]  # add x0 = 1 to each instancetheta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)The actual function that we used to generate the data is \\ny = 4 + 3x0 + Gaussian noise.\\nLet‡s see what the equation found:\\n>>> theta_bestarray([[ 4.21509616],       [ 2.77011339]])We would have hoped for \\n–0 = 4 and –1 = 3 instead of –0 = 3.865 and –1 = 3.139. Closeenough, but the noise made it impossible to recover the exact parameters of the origi…\\nnal function.Now you can make predictions using \\n–:>>> X_new = np.array([[0], [2]])>>> X_new_b = np.c_[np.ones((2, 1)), X_new] # add x0 = 1 to each instance>>> y_predict = X_new_b.dot(theta_best)>>> y_predictarray([[ 4.21509616],       [ 9.75532293]])Let‡s plot this model‡s predictions (\\nFigure 4-2):plt.plot(X_new, y_predict, \"r-\")plt.plot(X, y, \"b.\")plt.axis([0, 2, 0, 15])plt.show()Figure 4-2. Linear Regression model predictions\\nLinear Regression | 109\\n3Note that Scikit-Learn separates the bias term (\\nintercept_) from the feature weights (\\ncoef_).The equivalent code using Scikit-Learn looks like this:\\n3>>> from sklearn.linear_model import LinearRegression>>> lin_reg = LinearRegression()>>> lin_reg.fit(X, y)>>> lin_reg.intercept_, lin_reg.coef_(array([ 4.21509616]), array([[ 2.77011339]]))>>> lin_reg.predict(X_new)array([[ 4.21509616],       [ 9.75532293]])Computational ComplexityThe Normal Equation computes the inverse of \\nXT ’ X, which is an n ‰ n matrix\\n(where n is the number of features). The \\ncomputational complexity\\n of inverting such a\\nmatrix is typically about \\nO(n2.4) to O(n3) (depending on the implementation). In\\nother words, if you double the number of features, you multiply the computation\\ntime by roughly 22.4 = 5.3 to 23 = 8.The Normal Equation gets very slow when the number of features\\ngrows large (e.g., 100,000).On the positive side, this equation is linear with regards to the number of instances in\\nthe training set (it is O(m)), so it handles large training sets efficiently, provided they\\ncan fit in memory.\\nAlso, once you have trained your Linear Regression model (using the Normal Equa…\\ntion or any other algorithm), predictions are very fast: the computational complexity\\nis linear with regards to both the number of instances you want to make predictions\\non and the number of features. In other words, making predictions on twice as many\\ninstances (or twice as many features) will just take roughly twice as much time.\\nNow we will look at very different ways to train a Linear Regression model, better\\nsuited for cases where there are a large number of features, or too many training\\ninstances to fit in memory.\\n110 | Chapter 4: Training Models\\nGradient DescentGradient Descent\\n is a very generic optimization algorithm capable of finding optimal\\nsolutions to a wide range of problems. The general idea of Gradient Descent is to\\ntweak parameters iteratively in order to minimize a cost function.\\nSuppose you are lost in the mountains in a dense fog; you can only feel the slope of\\nthe ground below your feet. A good strategy to get to the bottom of the valley quickly\\nis to go downhill in the direction of the steepest slope. This is exactly what Gradient\\nDescent does: it measures the local gradient of the error function with regards to the \\nparameter vector –, and it goes in the direction of descending gradient. Once the gra…\\ndient is zero, you have reached a minimum!\\nConcretely, you start by filling \\n– with random values (this is called random initializa…\\ntion\\n), and then you improve it gradually, taking one baby step at a time, each step\\nattempting to decrease the cost function (e.g., the MSE), until the algorithm \\nconverges\\nto a minimum (see \\nFigure 4-3).Figure 4-3. Gradient Descent\\nAn important parameter in Gradient Descent is the size of the steps, determined by \\nthe learning rate\\n hyperparameter. If the learning rate is too small, then the algorithm\\nwill have to go through many iterations to converge, which will take a long time (see\\nFigure 4-4).Gradient Descent | 111\\nFigure 4-4. Learning rate too small\\nOn the other hand, if the learning rate is too high, you might jump across the valley\\nand end up on the other side, possibly even higher up than you were before. Thismight make the algorithm diverge, with larger and larger values, failing to find a good\\nsolution (see Figure 4-5).Figure 4-5. Learning rate too large\\nFinally, not all cost functions look like nice regular bowls. There may be holes, ridges,\\nplateaus, and all sorts of irregular terrains, making convergence to the minimum very\\ndifficult. Figure 4-6 shows the two main challenges with Gradient Descent: if the ran…\\ndom initialization starts the algorithm on the left, then it will converge to a \\nlocal mini…\\nmum\\n, which is not as good as the global minimum\\n. If it starts on the right, then it will\\ntake a very long time to cross the plateau, and if you stop too early you will never\\nreach the global minimum.\\n112 | Chapter 4: Training Models\\n4Technically speaking, its derivative is \\nLipschitz continuous\\n.5Since feature 1 is smaller, it takes a larger change in \\n–1 to affect the cost function, which is why the bowl is\\nelongated along the \\n–1 axis.Figure 4-6. Gradient Descent pitfalls\\nFortunately, the MSE cost function for a Linear Regression model happens to be a\\nconvex function\\n, which means that if you pick any two points on the curve, the line\\nsegment joining them never crosses the curve. This implies that there are no local\\nminima, just one global minimum. It is also a continuous function with a slope that\\nnever changes abruptly.\\n4 These two facts have a great consequence: Gradient Descent\\nis guaranteed to approach arbitrarily close the global minimum (if you wait long\\nenough and if the learning rate is not too high).\\nIn fact, the cost function has the shape of a bowl, but it can be an elongated bowl if\\nthe features have very different scales. \\nFigure 4-7 shows Gradient Descent on a train…\\ning set where features 1 and 2 have the same scale (on the left), and on a training set\\nwhere feature 1 has much smaller values than feature 2 (on the right).\\n5Figure 4-7. Gradient Descent with and without feature scaling\\nGradient Descent | 113\\nAs you can see, on the left the Gradient Descent algorithm goes straight toward the\\nminimum, thereby reaching it quickly, whereas on the right it first goes in a direction\\nalmost orthogonal to the direction of the global minimum, and it ends with a long\\nmarch down an almost flat valley. It will eventually reach the minimum, but it will\\ntake a long time.When using Gradient Descent, you should ensure that all features\\nhave a similar scale (e.g., using Scikit-Learn‡s \\nStandardScalerclass), or else it will take much longer to converge.\\nThis diagram also illustrates the fact that training a model means searching for a\\ncombination of model parameters that minimizes a cost function (over the training\\nset). It is a search in the model‡s \\nparameter space\\n: the more parameters a model has,the more dimensions this space has, and the harder the search is: searching for a nee…dle in a 300-dimensional haystack is much trickier than in three dimensions. Fortu…\\nnately, since the cost function is convex in the case of Linear Regression, the needle is\\nsimply at the bottom of the bowl.\\nBatch Gradient DescentTo implement Gradient Descent, you need to compute the gradient of the cost func…\\ntion with regards to each model parameter –j. In other words, you need to calculate\\nhow much the cost function will change if you change \\n–j just a little bit. This is called a partial derivative\\n. It is like asking ƒwhat is the slope of the mountain under my feet\\nif I face east?⁄ and then asking the same question facing north (and so on for all other\\ndimensions, if you can imagine a universe with more than three dimensions). Equa…tion 4-5 computes the partial derivative of the cost function with regards to parame…\\nter –j, noted ﬂﬂ–jMSE–.Equation 4-5. Partial derivatives of the cost function\\nﬂﬂ–jMSE–=2m“i=1\\nm–T’i”yixjiInstead of computing these gradients individually, you can use \\nEquation 4-6\\n to com…pute them all in one go. The gradient vector, noted \\n–MSE(–), contains all the partial\\nderivatives of the cost function (one for each model parameter).\\n114 | Chapter 4: Training Models\\n6Eta (\\n−) is the 7th letter of the Greek alphabet.Equation 4-6. Gradient vector of the cost function\\n–MSE–=ﬂﬂ–0MSE–ﬂﬂ–1MSE–ﬂﬂ–nMSE–=2mT’’–”Notice that this formula involves calculations over the full training\\nset X, at each Gradient Descent step! This is why the algorithm is\\ncalled Batch Gradient Descent\\n: it uses the whole batch of training\\ndata at every step. As a result it is terribly slow on very large train…\\ning sets (but we will see much faster Gradient Descent algorithms\\nshortly). However, Gradient Descent scales well with the number of\\nfeatures; training a Linear Regression model when there are hun…\\ndreds of thousands of features is much faster using Gradient\\nDescent than using the Normal Equation.\\nOnce you have the gradient vector, which points uphill, just go in the opposite direc…\\ntion to go downhill. This means subtracting –MSE(–) from –. This is where the learning rate \\n− comes into play:\\n6 multiply the gradient vector by \\n− to determine thesize of the downhill step (Equation 4-7\\n).Equation 4-7. Gradient Descent step\\n–nextstep\\n=–”−–MSE–Let‡s look at a quick implementation of this algorithm:\\neta = 0.1  # learning raten_iterations = 1000m = 100theta = np.random.randn(2,1)  # random initializationfor iteration in range(n_iterations):    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)    theta = theta - eta * gradientsGradient Descent | 115\\nThat wasn‡t too hard! Let‡s look at the resulting \\ntheta:>>> thetaarray([[ 4.21509616],       [ 2.77011339]])Hey, that‡s exactly what the Normal Equation found! Gradient Descent worked per…\\nfectly. But what if you had used a different learning rate \\neta? Figure 4-8 shows thefirst 10 steps of Gradient Descent using three different learning rates (the dashed line\\nrepresents the starting point).\\nFigure 4-8. Gradient Descent with various learning rates\\nOn the left, the learning rate is too low: the algorithm will eventually reach the solu…\\ntion, but it will take a long time. In the middle, the learning rate looks pretty good: in\\njust a few iterations, it has already converged to the solution. On the right, the learn…\\ning rate is too high: the algorithm diverges, jumping all over the place and actually\\ngetting further and further away from the solution at every step.\\nTo find a good learning rate, you can use grid search (see \\nChapter 2\\n). However, you\\nmay want to limit the number of iterations so that grid search can eliminate models\\nthat take too long to converge.\\nYou may wonder how to set the number of iterations. If it is too low, you will still be\\nfar away from the optimal solution when the algorithm stops, but if it is too high, you\\nwill waste time while the model parameters do not change anymore. A simple solu…\\ntion is to set a very large number of iterations but to interrupt the algorithm when the\\ngradient vector becomes tiny›that is, when its norm becomes smaller than a tiny\\nnumber \\n (called the tolerance\\n)›because this happens when Gradient Descent has\\n(almost) reached the minimum.\\n116 | Chapter 4: Training Models\\n7Out-of-core algorithms are discussed in Chapter 1\\n.Convergence RateWhen the cost function is convex and its slope does not change abruptly (as is the\\ncase for the MSE cost function), it can be shown that Batch Gradient Descent with a\\nfixed learning rate has a \\nconvergence rate\\n of \\nO1iterations. In other words, if you dividethe tolerance  by 10 (to have a more precise solution), then the algorithm will have\\nto run about 10 times more iterations.\\nStochastic Gradient DescentThe main problem with Batch Gradient Descent is the fact that it uses the whole\\ntraining set to compute the gradients at every step, which makes it very slow when\\nthe training set is large. At the opposite extreme, \\nStochastic Gradient Descent\\n justpicks a random instance in the training set at every step and computes the gradients\\nbased only on that single instance. Obviously this makes the algorithm much faster\\nsince it has very little data to manipulate at every iteration. It also makes it possible to\\ntrain on huge training sets, since only one instance needs to be in memory at each\\niteration (SGD can be implemented as an out-of-core algorithm.\\n7)On the other hand, due to its stochastic (i.e., random) nature, this algorithm is much\\nless regular than Batch Gradient Descent: instead of gently decreasing until it reaches\\nthe minimum, the cost function will bounce up and down, decreasing only on aver…\\nage. Over time it will end up very close to the minimum, but once it gets there it will\\ncontinue to bounce around, never settling down (see \\nFigure 4-9). So once the algo…rithm stops, the final parameter values are good, but not optimal.Figure 4-9. Stochastic Gradient Descent\\nGradient Descent | 117\\nWhen the cost function is very irregular (as in \\nFigure 4-6), this can actually help thealgorithm jump out of local minima, so Stochastic Gradient Descent has a better\\nchance of finding the global minimum than Batch Gradient Descent does.\\nTherefore randomness is good to escape from local optima, but bad because it means\\nthat the algorithm can never settle at the minimum. One solution to this dilemma is\\nto gradually reduce the learning rate. The steps start out large (which helps make\\nquick progress and escape local minima), then get smaller and smaller, allowing the\\nalgorithm to settle at the global minimum. This process is called \\nsimulated annealing\\n,because it resembles the process of annealing in metallurgy where molten metal is\\nslowly cooled down. The function that determines the learning rate at each iteration\\nis called the learning schedule\\n. If the learning rate is reduced too quickly, you may get\\nstuck in a local minimum, or even end up frozen halfway to the minimum. If the\\nlearning rate is reduced too slowly, you may jump around the minimum for a long\\ntime and end up with a suboptimal solution if you halt training too early.\\nThis code implements Stochastic Gradient Descent using a simple learning schedule:\\nn_epochs = 50t0, t1 = 5, 50  # learning schedule hyperparametersdef learning_schedule(t):    return t0 / (t + t1)theta = np.random.randn(2,1)  # random initializationfor epoch in range(n_epochs):    for i in range(m):        random_index = np.random.randint(m)        xi = X_b[random_index:random_index+1]        yi = y[random_index:random_index+1]        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)        eta = learning_schedule(epoch * m + i)        theta = theta - eta * gradientsBy convention we iterate by rounds of \\nm iterations; each round is called an \\nepoch\\n. While the Batch Gradient Descent code iterated 1,000 times through the whole train…\\ning set, this code goes through the training set only 50 times and reaches a fairly goodsolution:>>> thetaarray([[ 4.21076011],      [ 2.74856079]])Figure 4-10 shows the first 10 steps of training (notice how irregular the steps are).118 | Chapter 4: Training Models\\nFigure 4-10. Stochastic Gradient Descent \\n†rst 10 steps\\nNote that since instances are picked randomly, some instances may be picked several\\ntimes per epoch while others may not be picked at all. If you want to be sure that the\\nalgorithm goes through every instance at each epoch, another approach is to shuffle\\nthe training set, then go through it instance by instance, then shuffle it again, and so\\non. However, this generally converges more slowly.\\nTo perform Linear Regression using SGD with Scikit-Learn, you can use the \\nSGDRegressor class, which defaults to optimizing the squared error cost function. The fol…\\nlowing code runs 50 epochs, starting with a learning rate of 0.1 (\\neta0=0.1), using thedefault learning schedule (different from the preceding one), and it does not use any \\nregularization (\\npenalty=None; more details on this shortly):from sklearn.linear_model import SGDRegressorsgd_reg = SGDRegressor(n_iter=50, penalty=None, eta0=0.1)sgd_reg.fit(X, y.ravel())Once again, you find a solution very close to the one returned by the Normal Equa…\\ntion:>>> sgd_reg.intercept_, sgd_reg.coef_(array([ 4.18380366]), array([ 2.74205299]))Mini-batch Gradient DescentThe last Gradient Descent algorithm we will look at is called \\nMini-batch Gradient\\nDescent\\n. It is quite simple to understand once you know Batch and Stochastic Gradi…\\nent Descent: at each step, instead of computing the gradients based on the full train…\\ning set (as in Batch GD) or based on just one instance (as in Stochastic GD), Mini-\\nGradient Descent | 119\\n8While the Normal Equation can only perform Linear Regression, the Gradient Descent algorithms can be\\nused to train many other models, as we will see.\\nbatch GD computes the gradients on small random sets of instances called \\nmini-\\nbatches\\n. The main advantage of Mini-batch GD over Stochastic GD is that you can\\nget a performance boost from hardware optimization of matrix operations, especially\\nwhen using GPUs.\\nThe algorithm‡s progress in parameter space is less erratic than with SGD, especially\\nwith fairly large mini-batches. As a result, Mini-batch GD will end up walking\\naround a bit closer to the minimum than SGD. But, on the other hand, it may be\\nharder for it to escape from local minima (in the case of problems that suffer from\\nlocal minima, unlike Linear Regression as we saw earlier). \\nFigure 4-11 shows thepaths taken by the three Gradient Descent algorithms in parameter space during\\ntraining. They all end up near the minimum, but Batch GD‡s path actually stops at the\\nminimum, while both Stochastic GD and Mini-batch GD continue to walk around.\\nHowever, don‡t forget that Batch GD takes a lot of time to take each step, and Stochas…\\ntic GD and Mini-batch GD would also reach the minimum if you used a good learn…\\ning schedule.Figure 4-11. Gradient Descent paths in parameter space\\nLet‡s compare the algorithms we‡ve discussed so far for Linear Regression\\n8 (recall that\\nm is the number of training instances and \\nn is the number of features); see \\nTable 4-1\\n.Table 4-1. Comparison of algorithms for Linear Regression\\nAlgorithmLarge \\nmOut-of-core support\\nLarge \\nnHyperparamsScaling required\\nScikit-LearnNormal Equation\\nFastNoSlow0NoLinearRegressionBatch GD\\nSlowNoFast2Yesn/a120 | Chapter 4: Training Models\\n9A quadratic equation is of the form \\ny = ax2 + bx\\n + c.AlgorithmLarge \\nmOut-of-core support\\nLarge \\nnHyperparamsScaling required\\nScikit-LearnStochastic GD\\nFastYesFast•2YesSGDRegressorMini-batch GD\\nFastYesFast•2Yesn/aThere is almost no difference after training: all these algorithmsend up with very similar models and make predictions in exactly \\nthe same way.\\nPolynomial RegressionWhat if your data is actually more complex than a simple straight line? Surprisingly,\\nyou can actually use a linear model to fit nonlinear data. A simple way to do this is to\\nadd powers of each feature as new features, then train a linear model on this extended\\nset of features. This technique is called \\nPolynomial Regression\\n.Let‡s look at an example. First, let‡s generate some nonlinear data, based on a simple\\nquadratic equation\\n9 (plus some noise; see Figure 4-12):m = 100X = 6 * np.random.rand(m, 1) - 3y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)Figure 4-12. Generated nonlinear and noisy dataset\\nPolynomial Regression | 121\\nClearly, a straight line will never fit this data properly. So let‡s use Scikit-Learn‡s \\nPolynomialFeatures class to transform our training data, adding the square (2\\nnd-degreepolynomial) of each feature in the training set as new features (in this case there is\\njust one feature):\\n>>> from sklearn.preprocessing import PolynomialFeatures>>> poly_features = PolynomialFeatures(degree=2, include_bias=False)>>> X_poly = poly_features.fit_transform(X)>>> X[0]array([-0.75275929])>>> X_poly[0]array([-0.75275929,  0.56664654])X_poly now contains the original feature of \\nX plus the square of this feature. Now you\\ncan fit a LinearRegression model to this extended training data (\\nFigure 4-13):>>> lin_reg = LinearRegression()>>> lin_reg.fit(X_poly, y)>>> lin_reg.intercept_, lin_reg.coef_(array([ 1.78134581]), array([[ 0.93366893,  0.56456263]]))Figure 4-13. Polynomial Regression model predictions\\nNot bad: the model estimates \\ny=0.56\\nx12+0.93\\nx1+1.78\\n when in fact the originalfunction was y=0.5\\nx12+1.0\\nx1+2.0+Gaussiannoise\\n.Note that when there are multiple features, Polynomial Regression is capable of find…\\ning relationships between features (which is something a plain Linear Regression\\nmodel cannot do). This is made possible by the fact that \\nPolynomialFeatures alsoadds all combinations of features up to the given degree. For example, if there were\\n122 | Chapter 4: Training Models\\ntwo features \\na and b, PolynomialFeatures with degree=3 would not only add thefeatures \\na2, a3, b2, and b3, but also the combinations \\nab\\n, a2b, and ab\\n2.PolynomialFeatures(degree=d) transforms an array containing \\nnfeatures into an array containing \\nn+d!d!n! features, where \\nn! is thefactorial\\n of \\nn, equal to 1 ‰ 2 ‰ 3 ‰  ‰ n. Beware of the combinato…\\nrial explosion of the number of features!\\nLearning CurvesIf you perform high-degree Polynomial Regression, you will likely fit the training\\ndata much better than with plain Linear Regression. For example, \\nFigure 4-14 applies\\na 300-degree polynomial model to the preceding training data, and compares the\\nresult with a pure linear model and a quadratic model (2\\nnd-degree polynomial).Notice how the 300-degree polynomial model wiggles around to get as close as possi…\\nble to the training instances.Figure 4-14. High-degree Polynomial Regression\\nOf course, this high-degree Polynomial Regression model is severely overfitting the\\ntraining data, while the linear model is underfitting it. The model that will generalize\\nbest in this case is the quadratic model. It makes sense since the data was generated\\nusing a quadratic model, but in general you won‡t know what function generated the\\ndata, so how can you decide how complex your model should be? How can you tell\\nthat your model is overfitting or underfitting the data?\\nLearning Curves | 123\\nIn Chapter 2\\n you used cross-validation to get an estimate of a model‡s generalization\\nperformance. If a model performs well on the training data but generalizes poorly\\naccording to the cross-validation metrics, then your model is overfitting. If it per…\\nforms poorly on both, then it is underfitting. This is one way to tell when a model is\\ntoo simple or too complex.\\nAnother way is to look at the \\nlearning curves\\n: these are plots of the model‡s perfor…\\nmance on the training set and the validation set as a function of the training set size.\\nTo generate the plots, simply train the model several times on different sized subsets\\nof the training set. The following code defines a function that plots the learning\\ncurves of a model given some training data:\\nfrom sklearn.metrics import mean_squared_errorfrom sklearn.model_selection import train_test_splitdef plot_learning_curves(model, X, y):    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)    train_errors, val_errors = [], []    for m in range(1, len(X_train)):        model.fit(X_train[:m], y_train[:m])        y_train_predict = model.predict(X_train[:m])        y_val_predict = model.predict(X_val)        train_errors.append(mean_squared_error(y_train_predict, y_train[:m]))        val_errors.append(mean_squared_error(y_val_predict, y_val))    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")Let‡s look at the learning curves of the plain Linear Regression model (a straight line;\\nFigure 4-15):lin_reg = LinearRegression()plot_learning_curves(lin_reg, X, y)Figure 4-15. Learning curves\\n124 | Chapter 4: Training Models\\nThis deserves a bit of explanation. First, let‡s look at the performance on the training\\ndata: when there are just one or two instances in the training set, the model can fit\\nthem perfectly, which is why the curve starts at zero. But as new instances are added\\nto the training set, it becomes impossible for the model to fit the training data per…\\nfectly, both because the data is noisy and because it is not linear at all. So the error on\\nthe training data goes up until it reaches a plateau, at which point adding new instan…\\nces to the training set doesn‡t make the average error much better or worse. Now let‡s\\nlook at the performance of the model on the validation data. When the model is\\ntrained on very few training instances, it is incapable of generalizing properly, which\\nis why the validation error is initially quite big. Then as the model is shown more\\ntraining examples, it learns and thus the validation error slowly goes down. However,\\nonce again a straight line cannot do a good job modeling the data, so the error ends\\nup at a plateau, very close to the other curve.\\nThese learning curves are typical of an underfitting model. Both curves have reached\\na plateau; they are close and fairly high.\\nIf your model is underfitting the training data, adding more train…\\ning examples will not help. You need to use a more complex model\\nor come up with better features.\\nNow let‡s look at the learning curves of a 10\\nth-degree polynomial model on the samedata (\\nFigure 4-16):from sklearn.pipeline import Pipelinepolynomial_regression = Pipeline((        (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),        (\"sgd_reg\", LinearRegression()),    ))plot_learning_curves(polynomial_regression, X, y)These learning curves look a bit like the previous ones, but there are two very impor…\\ntant differences:\\n‹The error on the training data is much lower than with the Linear Regression\\nmodel.‹There is a gap between the curves. This means that the model performs signifi…\\ncantly better on the training data than on the validation data, which is the hall…\\nmark of an overfitting model. However, if you used a much larger training set,\\nthe two curves would continue to get closer.\\nLearning Curves | 125\\n10This notion of bias is not to be confused with the bias term of linear models.Figure 4-16. Learning curves for the polynomial model\\nOne way to improve an overfitting model is to feed it more training\\ndata until the validation error reaches the training error.\\nThe Bias/Variance Tradeo†An important theoretical result of statistics and Machine Learning is the fact that a\\nmodel‡s generalization error can be expressed as the sum of three very different\\nerrors:Bias\\nThis part of the generalization error is due to wrong assumptions, such as assum…\\ning that the data is linear when it is actually quadratic. A high-bias model is most\\nlikely to underfit the training data.\\n10Variance\\nThis part is due to the model‡s excessive sensitivity to small variations in the\\ntraining data. A model with many degrees of freedom (such as a high-degree pol…\\nynomial model) is likely to have high variance, and thus to overfit the training\\ndata.\\n126 | Chapter 4: Training Models\\nIrreducible error\\nThis part is due to the noisiness of the data itself. The only way to reduce this\\npart of the error is to clean up the data (e.g., fix the data sources, such as broken\\nsensors, or detect and remove outliers).Increasing a model‡s complexity will typically increase its variance and reduce its bias.\\nConversely, reducing a model‡s complexity increases its bias and reduces its variance. \\nThis is why it is called a tradeoff.\\nRegularized Linear ModelsAs we saw in Chapters \\n1 and 2, a good way to reduce overfitting is to regularize the\\nmodel (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will befor it to overfit the data. For example, a simple way to regularize a polynomial model\\nis to reduce the number of polynomial degrees.\\nFor a linear model, regularization is typically achieved by constraining the weights of\\nthe model. We will now look at Ridge Regression, Lasso Regression, and Elastic Net,\\nwhich implement three different ways to constrain the weights.\\nRidge RegressionRidge Regression\\n (also called Tikhonov regularization\\n) is a regularized version of Lin…ear Regression: a regularization term\\n equal to ‰“i=1\\nn–i2 is added to the cost function. This forces the learning algorithm to not only fit the data but also keep the model\\nweights as small as possible. Note that the regularization term should only be added\\nto the cost function during training. Once the model is trained, you want to evaluate\\nthe model‡s performance using the unregularized performance measure.\\nIt is quite common for the cost function used during training to be\\ndifferent from the performance measure used for testing. Apart\\nfrom regularization, another reason why they might be different is\\nthat a good training cost function should have optimization-\\nfriendly derivatives, while the performance measure used for test…\\ning should be as close as possible to the final objective. A goodexample of this is a classifier trained using a cost function such as\\nthe log loss (discussed in a moment) but evaluated using precision/\\nrecall.The hyperparameter \\n‰ controls how much you want to regularize the model. If \\n‰ = 0then Ridge Regression is just Linear Regression. If ‰ is very large, then all weights end\\nRegularized Linear Models | 127\\n11It is common to use the notation \\nJ(–) for cost functions that don‡t have a short name; we will often use this\\nnotation throughout the rest of this book. The context will make it clear which cost function is being dis…\\ncussed.12Norms are discussed in \\nChapter 2\\n.13A square matrix full of 0s except for 1s on the main diagonal (top-left to bottom-right).\\nup very close to zero and the result is a flat line going through the data‡s mean. \\nEqua…tion 4-8 presents the Ridge Regression cost function.\\n11Equation 4-8. Ridge Regression cost function\\nJ–=MSE\\n–+‰12“i=1\\nn–i2Note that the bias term \\n–0 is not regularized (the sum starts at \\ni = 1, not 0). If wedefine w as the vector of feature weights (\\n–1 to –n), then the regularization term is\\nsimply equal to Ł(\\n w 2)2, where  ’ 2 represents the —\\n2 norm of the weight vector.\\n12For Gradient Descent, just add \\n‰w to the MSE gradient vector (\\nEquation 4-6\\n).It is important to scale the data (e.g., using a \\nStandardScaler) before performing Ridge Regression, as it is sensitive to the scale ofthe input features. This is true of most regularized models.\\nFigure 4-17 shows several Ridge models trained on some linear data using different \\n‰value. On the left, plain Ridge models are used, leading to linear predictions. On theright, the data is first expanded using \\nPolynomialFeatures(degree=10), then it isscaled using a StandardScaler, and finally the Ridge models are applied to the result…\\ning features: this is Polynomial Regression with Ridge regularization. Note how\\nincreasing ‰ leads to flatter (i.e., less extreme, more reasonable) predictions; this\\nreduces the model‡s variance but increases its bias.\\nAs with Linear Regression, we can perform Ridge Regression either by computing a \\nclosed-form equation or by performing Gradient Descent. The pros and cons are the\\nsame. Equation 4-9\\n shows the closed-form solution (where A is the n ‰ n identity\\nmatrix\\n13 except with a 0 in the top-left cell, corresponding to the bias term).128 | Chapter 4: Training Models\\n14Alternatively you can use the \\nRidge class with the \"sag\" solver. Stochastic Average GD is a variant of SGD.\\nFor more details, see the presentation \\nƒMinimizing Finite Sums with the Stochastic Average Gradient Algo…\\nrithm⁄\\n by Mark Schmidt et al. from the University of British Columbia.\\nFigure 4-17. Ridge Regression\\nEquation 4-9. Ridge Regression closed-form solution\\n–=T’+‰”1\\n’T’Here is how to perform Ridge Regression with Scikit-Learn using a closed-form solu…\\ntion (a variant of \\nEquation 4-9\\n using a matrix factorization technique by Andr•-Louis\\nCholesky):>>> from sklearn.linear_model import Ridge>>> ridge_reg = Ridge(alpha=1, solver=\"cholesky\")>>> ridge_reg.fit(X, y)>>> ridge_reg.predict([[1.5]])array([[ 1.55071465]])And using Stochastic Gradient Descent:\\n14>>> sgd_reg = SGDRegressor(penalty=\"l2\")>>> sgd_reg.fit(X, y.ravel())>>> sgd_reg.predict([[1.5]])array([[ 1.13500145]])The penalty hyperparameter sets the type of regularization term to use. Specifying\\n\"l2\" indicates that you want SGD to add a regularization term to the cost function \\nequal to half the square of the —2 norm of the weight vector: this is simply \\nRidgeRegression.Regularized Linear Models | 129\\nLasso RegressionLeast Absolute Shrinkage and Selection Operator Regression\\n (simply called \\nLasso\\nRegression\\n) is another regularized version of Linear Regression: just like RidgeRegression, it adds a regularization term to the cost function, but it uses the —\\n1 normof the weight vector instead of half the square of the —\\n2 norm (see Equation 4-10\\n).Equation 4-10. Lasso Regression cost function\\nJ–=MSE\\n–+‰“i=1\\nn–iFigure 4-18 shows the same thing as Figure 4-17 but replaces Ridge models withLasso models and uses smaller ‰ values.Figure 4-18. Lasso Regression\\nAn important characteristic of Lasso Regression is that it tends to completely elimi…\\nnate the weights of the least important features (i.e., set them to zero). For example,\\nthe dashed line in the right plot on \\nFigure 4-18 (with \\n‰ = 10\\n-7) looks quadratic, almost\\nlinear: all the weights for the high-degree polynomial features are equal to zero. In\\nother words, Lasso Regression automatically performs feature selection and outputs \\nasparse model\\n (i.e., with few nonzero feature weights).\\nYou can get a sense of why this is the case by looking at \\nFigure 4-19: on the top-leftplot, the background contours (ellipses) represent an unregularized MSE cost func…\\ntion (‰ = 0), and the white circles show the Batch Gradient Descent path with that\\ncost function. The foreground contours (diamonds) represent the —\\n1 penalty, and the\\ntriangles show the BGD path for this penalty only (\\n‰ Œ ‚\\n). Notice how the path first\\n130 | Chapter 4: Training Models\\n15You can think of a subgradient vector at a nondifferentiable point as an intermediate vector between the gra…\\ndient vectors around that point.\\nreaches –1 = 0, then rolls down a gutter until it reaches \\n–2 = 0. On the top-right plot,\\nthe contours represent the same cost function plus an —\\n1 penalty with ‰ = 0.5. Theglobal minimum is on the \\n–2 = 0 axis. BGD first reaches \\n–2 = 0, then rolls down thegutter until it reaches the global minimum. The two bottom plots show the same\\nthing but uses an —2 penalty instead. The regularized minimum is closer to \\n– = 0 thanthe unregularized minimum, but the weights do not get fully eliminated.\\nFigure 4-19. Lasso versus Ridge regularization\\nOn the Lasso cost function, the BGD path tends to bounce across\\nthe gutter toward the end. This is because the slope changes\\nabruptly at \\n–2 = 0. You need to gradually reduce the learning rate in\\norder to actually converge to the global minimum.\\nThe Lasso cost function is not differentiable at \\n–i = 0 (for i = 1, 2, , n), but Gradient\\nDescent still works fine if you use a \\nsubgradient vector\\n g15 instead when any \\n–i = 0.Equation 4-11\\n shows a subgradient vector equation you can use for Gradient Descent\\nwith the Lasso cost function.Regularized Linear Models | 131\\nEquation 4-11. Lasso Regression subgradient vector\\ng–,J=–MSE–+‰sign–1sign–2sign–nwheresign\\n–i=”1if\\n–i<0\\n0if\\n–i=0\\n+1if\\n–i>0\\nHere is a small Scikit-Learn example using the \\nLasso class. Note that you could\\ninstead use an SGDRegressor(penalty=\"l1\").>>> from sklearn.linear_model import Lasso>>> lasso_reg = Lasso(alpha=0.1)>>> lasso_reg.fit(X, y)>>> lasso_reg.predict([[1.5]])array([ 1.53788174])Elastic NetElastic Net is a middle ground between Ridge Regression and Lasso Regression. The\\nregularization term is a simple mix of both Ridge and Lasso‡s regularization terms,\\nand you can control the mix ratio \\nr. When r = 0, Elastic Net is equivalent to Ridge\\nRegression, and when r = 1, it is equivalent to Lasso Regression (see \\nEquation 4-12\\n).Equation 4-12. Elastic Net cost function\\nJ–=MSE\\n–+r‰\\n“i=1\\nn–i+1”\\nr2‰“i=1\\nn–i2So when should you use Linear Regression, Ridge, Lasso, or Elastic Net? It is almost\\nalways preferable to have at least a little bit of regularization, so generally you should\\navoid plain Linear Regression. Ridge is a good default, but if you suspect that only a\\nfew features are actually useful, you should prefer Lasso or Elastic Net since they tend\\nto reduce the useless features‡ weights down to zero as we have discussed. In general, \\nElastic Net is preferred over Lasso since Lasso may behave erratically when the num…\\nber of features is greater than the number of training instances or when several fea…\\ntures are strongly correlated.\\nHere is a short example using Scikit-Learn‡s \\nElasticNet (l1_ratio corresponds tothe mix ratio \\nr):>>> from sklearn.linear_model import ElasticNet>>> elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)>>> elastic_net.fit(X, y)>>> elastic_net.predict([[1.5]])array([ 1.54333232])132 | Chapter 4: Training Models\\nEarly StoppingA very different way to regularize iterative learning algorithms such as Gradient\\nDescent is to stop training as soon as the validation error reaches a minimum. This is\\ncalled early stopping\\n. Figure 4-20 shows a complex model (in this case a high-degree\\nPolynomial Regression model) being trained using Batch Gradient Descent. As the\\nepochs go by, the algorithm learns and its prediction error (RMSE) on the training set\\nnaturally goes down, and so does its prediction error on the validation set. However,\\nafter a while the validation error stops decreasing and actually starts to go back up.\\nThis indicates that the model has started to overfit the training data. With early stop…\\nping you just stop training as soon as the validation error reaches the minimum. It is\\nsuch a simple and efficient regularization technique that Geoffrey Hinton called it a\\nƒbeautiful free lunch.⁄\\nFigure 4-20. Early stopping regularization\\nWith Stochastic and Mini-batch Gradient Descent, the curves are\\nnot so smooth, and it may be hard to know whether you have\\nreached the minimum or not. One solution is to stop only after the\\nvalidation error has been above the minimum for some time (when\\nyou are confident that the model will not do any better), then roll\\nback the model parameters to the point where the validation error\\nwas at a minimum.\\nHere is a basic implementation of early stopping:\\nfrom sklearn.base import cloneRegularized Linear Models | 133\\nsgd_reg = SGDRegressor(n_iter=1, warm_start=True, penalty=None,                       learning_rate=\"constant\", eta0=0.0005)minimum_val_error = float(\"inf\")best_epoch = Nonebest_model = Nonefor epoch in range(1000):    sgd_reg.fit(X_train_poly_scaled, y_train)  # continues where it left off    y_val_predict = sgd_reg.predict(X_val_poly_scaled)    val_error = mean_squared_error(y_val_predict, y_val)    if val_error < minimum_val_error:        minimum_val_error = val_error        best_epoch = epoch        best_model = clone(sgd_reg)Note that with \\nwarm_start=True, when the fit() method is called, it just continues\\ntraining where it left off instead of restarting from scratch.\\nLogistic RegressionAs we discussed in Chapter 1\\n, some regression algorithms can be used for classifica…tion as well (and vice versa). Logistic Regression\\n (also called Logit Regression\\n) is com…monly used to estimate the probability that an instance belongs to a particular class\\n(e.g., what is the probability that this email is spam?). If the estimated probability is\\ngreater than 50%, then the model predicts that the instance belongs to that class\\n(called the positive class, labeled ƒ1⁄), or else it predicts that it does not (i.e., it\\nbelongs to the negative class, labeled ƒ0⁄). This makes it a binary classifier.\\nEstimating ProbabilitiesSo how does it work? Just like a Linear Regression model, a Logistic Regression\\nmodel computes a weighted sum of the input features (plus a bias term), but instead\\nof outputting the result directly like the Linear Regression model does, it outputs thelogistic\\n of this result (see Equation 4-13\\n).Equation 4-13. Logistic Regression model estimated probability (vectorized form)\\np=h–=„–T’The logistic›also called the logit\\n, noted „(’)›is a sigmoid function\\n (i.e., S-shaped)\\nthat outputs a number between 0 and 1. It is defined as shown in \\nEquation 4-14\\n andFigure 4-21.134 | Chapter 4: Training Models\\nEquation 4-14. Logistic function\\n„t=11+exp\\n”tFigure 4-21. Logistic function\\nOnce the Logistic Regression model has estimated the probability \\np = h–(x) that an\\ninstance x belongs to the positive class, it can make its prediction ƒ easily (see Equa…tion 4-15).Equation 4-15. Logistic Regression model prediction\\ny=0if\\np<0.5,\\n1if\\npŠ0.5.\\nNotice that \\n„(t) < 0.5 when t < 0, and „(t) Š 0.5 when t Š 0, so a Logistic Regressionmodel predicts 1 if –T ’ x is positive, and 0 if it is negative.\\nTraining and Cost FunctionGood, now you know how a Logistic Regression model estimates probabilities and\\nmakes predictions. But how is it trained? The objective of training is to set the param…\\neter vector – so that the model estimates high probabilities for positive instances (\\ny =1) and low probabilities for negative instances (\\ny = 0). This idea is captured by the\\ncost function shown in Equation 4-16\\n for a single training instance x.Equation 4-16. Cost function of a single training instance\\nc–=”log\\npify=1,\\n”log\\n1”\\npify=0.\\nThis cost function makes sense because – log(\\nt) grows very large when \\nt approaches\\n0, so the cost will be large if the model estimates a probability close to 0 for a positive\\nLogistic Regression | 135\\ninstance, and it will also be very large if the model estimates a probability close to 1\\nfor a negative instance. On the other hand, – log(\\nt) is close to 0 when t is close to 1, so\\nthe cost will be close to 0 if the estimated probability is close to 0 for a negative\\ninstance or close to 1 for a positive instance, which is precisely what we want.\\nThe cost function over the whole training set is simply the average cost over all train…\\ning instances. It can be written in a single expression (as you can verify easily), called \\nthe log loss\\n, shown in Equation 4-17\\n.Equation 4-17. Logistic Regression cost function (log loss)\\nJ–=”\\n1m“i=1\\nmyilog\\npi+1”\\nyilog\\n1”\\npiThe bad news is that there is no known closed-form equation to compute the value of\\n– that minimizes this cost function (there is no equivalent of the Normal Equation).\\nBut the good news is that this cost function is convex, so Gradient Descent (or any\\nother optimization algorithm) is guaranteed to find the global minimum (if the learn…\\ning rate is not too large and you wait long enough). The partial derivatives of the cost\\nfunction with regards to the jth model parameter –j is given by Equation 4-18\\n.Equation 4-18. Logistic cost function partial derivatives\\nﬂﬂ–jJ–=1m“i=1\\nm„–T’i”yixjiThis equation looks very much like \\nEquation 4-5\\n: for each instance it computes the\\nprediction error and multiplies it by the j\\nth feature value, and then it computes the\\naverage over all training instances. Once you have the gradient vector containing all\\nthe partial derivatives you can use it in the Batch Gradient Descent algorithm. That‡s\\nit: you now know how to train a Logistic Regression model. For Stochastic GD you\\nwould of course just take one instance at a time, and for Mini-batch GD you would\\nuse a mini-batch at a time.\\nDecision BoundariesLet‡s use the iris dataset to illustrate Logistic Regression. This is a famous dataset that\\ncontains the sepal and petal length and width of 150 iris flowers of three different\\nspecies: Iris-Setosa, Iris-Versicolor, and Iris-Virginica (see \\nFigure 4-22).136 | Chapter 4: Training Models\\n16Photos reproduced from the corresponding Wikipedia pages. Iris-Virginica photo by Frank Mayfield (\\nCrea…tive Commons BY-SA 2.0\\n), Iris-Versicolor photo by D. Gordon E. Robertson (\\nCreative Commons BY-SA 3.0\\n),and Iris-Setosa photo is public domain.Figure 4-22. Flowers of three iris plant species\\n16Let‡s try to build a classifier to detect the Iris-Virginica type based only on the petal\\nwidth feature. First let‡s load the data:\\n>>> from sklearn import datasets>>> iris = datasets.load_iris()>>> list(iris.keys())[•data•, •target_names•, •feature_names•, •target•, •DESCR•]>>> X = iris[\"data\"][:, 3:]  # petal width>>> y = (iris[\"target\"] == 2).astype(np.int)  # 1 if Iris-Virginica, else 0Now let‡s train a Logistic Regression model:\\nfrom sklearn.linear_model import LogisticRegressionlog_reg = LogisticRegression()log_reg.fit(X, y)Let‡s look at the model‡s estimated probabilities for flowers with petal widths varying\\nfrom 0 to 3 cm (Figure 4-23):X_new = np.linspace(0, 3, 1000).reshape(-1, 1)y_proba = log_reg.predict_proba(X_new)plt.plot(X_new, y_proba[:, 1], \"g-\", label=\"Iris-Virginica\")plt.plot(X_new, y_proba[:, 0], \"b--\", label=\"Not Iris-Virginica\")# + more Matplotlib code to make the image look prettyLogistic Regression | 137\\n17It is the the set of points \\nx such that \\n–0 + –1x1 + –2x2 = 0, which defines a straight line.\\nFigure 4-23. Estimated probabilities and decision boundary\\nThe petal width of Iris-Virginica flowers (represented by triangles) ranges from 1.4\\ncm to 2.5 cm, while the other iris flowers (represented by squares) generally have a\\nsmaller petal width, ranging from 0.1 cm to 1.8 cm. Notice that there is a bit of over…\\nlap. Above about 2 cm the classifier is highly confident that the flower is an Iris-\\nVirginica (it outputs a high probability to that class), while below 1 cm it is highly\\nconfident that it is not an Iris-Virginica (high probability for the ƒNot Iris-Virginica⁄\\nclass). In between these extremes, the classifier is unsure. However, if you ask it to\\npredict the class (using the predict() method rather than the \\npredict_proba()method), it will return whichever class is the most likely. Therefore, there is a \\ndecision\\nboundary\\n at around 1.6 cm where both probabilities are equal to 50%: if the petal\\nwidth is higher than 1.6 cm, the classifier will predict that the flower is an Iris-\\nVirginica, or else it will predict that it is not (even if it is not very confident):\\n>>> log_reg.predict([[1.7], [1.5]])array([1, 0])Figure 4-24 shows the same dataset but this time displaying two features: petal width\\nand length. Once trained, the Logistic Regression classifier can estimate the probabil…\\nity that a new flower is an Iris-Virginica based on these two features. The dashed line\\nrepresents the points where the model estimates a 50% probability: this is the model‡s\\ndecision boundary. Note that it is a linear boundary.\\n17 Each parallel line represents the\\npoints where the model outputs a specific probability, from 15% (bottom left) to 90%\\n(top right). All the flowers beyond the top-right line have an over 90% chance of\\nbeing Iris-Virginica according to the model.\\n138 | Chapter 4: Training Models\\nFigure 4-24. Linear decision boundary\\nJust like the other linear models, Logistic Regression models can be regularized using \\n—1 or —2 penalties. Scitkit-Learn actually adds an —2 penalty by default.\\nThe hyperparameter controlling the regularization strength of a\\nScikit-Learn LogisticRegression model is not alpha (as in otherlinear models), but its inverse: \\nC. The higher the value of C, the less\\nthe model is regularized.Softmax RegressionThe Logistic Regression model can be generalized to support multiple classes directly,\\nwithout having to train and combine multiple binary classifiers (as discussed in\\nChapter 3\\n). This is called So“max Regression\\n, or Multinomial Logistic Regression\\n.The idea is quite simple: when given an instance \\nx, the Softmax Regression modelfirst computes a score \\nsk(x) for each class k, then estimates the probability of each\\nclass by applying the \\nso“max function\\n (also called the normalized exponential\\n) to thescores. The equation to compute \\nsk(x) should look familiar, as it is just like the equa…\\ntion for Linear Regression prediction (see Equation 4-19\\n).Equation 4-19. \\nSo“max score for class k\\nsk=–kT’Note that each class has its own dedicated parameter vector \\n–k. All these vectors aretypically stored as rows in a parameter matrix\\n ”.Once you have computed the score of every class for the instance \\nx, you can estimate\\nthe probability pk that the instance belongs to class \\nk by running the scores throughLogistic Regression | 139\\nthe softmax function (Equation 4-20\\n): it computes the exponential of every score,\\nthen normalizes them (dividing by the sum of all the exponentials).\\nEquation 4-20. \\nSo“max function\\npk=„k=expsk“j=1\\nKexpsj‹K is the number of classes.\\n‹s(x) is a vector containing the scores of each class for the instance \\nx.‹„(s(x))k is the estimated probability that the instance \\nx belongs to class k giventhe scores of each class for that instance.\\nJust like the Logistic Regression classifier, the Softmax Regression classifier predicts\\nthe class with the highest estimated probability (which is simply the class with the\\nhighest score), as shown in Equation 4-21\\n.Equation 4-21. \\nSo“max Regression \\nclassi†er prediction\\ny=argmax\\nk„k=argmax\\nksk=argmax\\nk–kT’‹The argmax\\n operator returns the value of a variable that maximizes a function. In\\nthis equation, it returns the value of \\nk that maximizes the estimated probability\\n„(s(x))k.The Softmax Regression classifier predicts only one class at a time\\n(i.e., it is multiclass, not multioutput) so it should be used only with\\nmutually exclusive classes such as different types of plants. You\\ncannot use it to recognize multiple people in one picture.\\nNow that you know how the model estimates probabilities and makes predictions,\\nlet‡s take a look at training. The objective is to have a model that estimates a high\\nprobability for the target class (and consequently a low probability for the other\\nclasses). Minimizing the cost function shown in Equation 4-22\\n, called the cross\\nentropy\\n, should lead to this objective because it penalizes the model when it estimates\\na low probability for a target class. Cross entropy is frequently used to measure how\\nwell a set of estimated class probabilities match the target classes (we will use it again\\nseveral times in the following chapters).\\n140 | Chapter 4: Training Models\\nEquation 4-22. Cross entropy cost function\\nJ”=”\\n1m“i=1\\nm“k=1\\nKykilogpki‹yki is equal to 1 if the target class for the ith instance is k; otherwise, it is equal to\\n0.Notice that when there are just two classes (\\nK = 2), this cost function is equivalent to\\nthe Logistic Regression‡s cost function (log loss; see \\nEquation 4-17\\n).Cross EntropyCross entropy originated from information theory. Suppose you want to efficiently\\ntransmit information about the weather every day. If there are eight options (sunny,\\nrainy, etc.), you could encode each option using 3 bits since 2\\n3 = 8. However, if you\\nthink it will be sunny almost every day, it would be much more efficient to code\\nƒsunny⁄ on just one bit (0) and the other seven options on 4 bits (starting with a 1).\\nCross entropy measures the average number of bits you actually send per option. If\\nyour assumption about the weather is perfect, cross entropy will just be equal to the\\nentropy of the weather itself (i.e., its intrinsic unpredictability). But if your assump…\\ntions are wrong (e.g., if it rains often), cross entropy will be greater by an amount \\ncalled the Kullback‘Leibler divergence\\n.The cross entropy between two probability distributions \\np and q is defined asHp,q=”“\\nxpxlogqx (at least when the distributions are discrete).\\nThe gradient vector of this cost function with regards to \\n–k is given by Equation 4-23\\n:Equation 4-23. Cross entropy gradient vector for class k\\n–kJ”=1m“i=1\\nmpki”ykiiNow you can compute the gradient vector for every class, then use Gradient Descent\\n(or any other optimization algorithm) to find the parameter matrix \\n” that minimizes\\nthe cost function.Let‡s use Softmax Regression to classify the iris flowers into all three classes. Scikit-\\nLearn‡s \\nLogisticRegression uses \\none-versus-all by default when you train it on more\\nthan two classes, but you can set the multi_class hyperparameter to \\n\"multinomial\"to switch it to Softmax Regression instead. You must also specify a solver that sup…\\nports Softmax Regression, such as the \"lbfgs\" solver (see Scikit-Learn‡s documenta…\\nLogistic Regression | 141\\ntion for more details). It also applies —\\n2 regularization by default, which you can\\ncontrol using the hyperparameter \\nC.X = iris[\"data\"][:, (2, 3)]  # petal length, petal widthy = iris[\"target\"]softmax_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10)softmax_reg.fit(X, y)So the next time you find an iris with 5 cm long and 2 cm wide petals, you can askyour model to tell you what type of iris it is, and it will answer Iris-Virginica (class 2)\\nwith 94.2% probability (or Iris-Versicolor with 5.8% probability):\\n>>> softmax_reg.predict([[5, 2]])array([2])>>> softmax_reg.predict_proba([[5, 2]])array([[  6.33134078e-07,   5.75276067e-02,   9.42471760e-01]])Figure 4-25 shows the resulting decision boundaries, represented by the background\\ncolors. Notice that the decision boundaries between any two classes are linear. The\\nfigure also shows the probabilities for the Iris-Versicolor class, represented by the\\ncurved lines (e.g., the line labeled with 0.450 represents the 45% probability bound…\\nary). Notice that the model can predict a class that has an estimated probability below\\n50%. For example, at the point where all decision boundaries meet, all classes have an\\nequal estimated probability of 33%.\\nFigure 4-25. \\nSo“max Regression decision boundaries\\nExercises1.What Linear Regression training algorithm can you use if you have a training set\\nwith millions of features?\\n2.Suppose the features in your training set have very different scales. What algo…\\nrithms might suffer from this, and how? What can you do about it?\\n142 | Chapter 4: Training Models\\n3.Can Gradient Descent get stuck in a local minimum when training a Logistic\\nRegression model?4.Do all Gradient Descent algorithms lead to the same model provided you let\\nthem run long enough?5.Suppose you use Batch Gradient Descent and you plot the validation error at\\nevery epoch. If you notice that the validation error consistently goes up, what is\\nlikely going on? How can you fix this?\\n6.Is it a good idea to stop Mini-batch Gradient Descent immediately when the vali…\\ndation error goes up?\\n7.Which Gradient Descent algorithm (among those we discussed) will reach the\\nvicinity of the optimal solution the fastest? Which will actually converge? How\\ncan you make the others converge as well?\\n8.Suppose you are using Polynomial Regression. You plot the learning curves and\\nyou notice that there is a large gap between the training error and the validation\\nerror. What is happening? What are three ways to solve this?\\n9.Suppose you are using Ridge Regression and you notice that the training error\\nand the validation error are almost equal and fairly high. Would you say that the\\nmodel suffers from high bias or high variance? Should you increase the regulari…zation hyperparameter \\n‰ or reduce it?10.Why would you want to use:\\n‹Ridge Regression instead of Linear Regression?‹Lasso instead of Ridge Regression?‹Elastic Net instead of Lasso?\\n11.Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime.\\nShould you implement two Logistic Regression classifiers or one Softmax Regres…\\nsion classifier?12.Implement Batch Gradient Descent with early stopping for Softmax Regression \\n(without using Scikit-Learn).Solutions to these exercises are available in \\nAppendix A\\n.Exercises | 143\\nCHAPTER 5Support Vector MachinesA Support Vector Machine\\n (SVM) is a very powerful and versatile Machine Learning\\nmodel, capable of performing linear or nonlinear classification, regression, and even\\noutlier detection. It is one of the most popular models in Machine Learning, and any…\\none interested in Machine Learning should have it in their toolbox. SVMs are partic…\\nularly well suited for classification of complex but small- or medium-sized datasets.\\nThis chapter will explain the core concepts of SVMs, how to use them, and how they\\nwork.Linear SVM Classi•cationThe fundamental idea behind SVMs is best explained with some pictures. \\nFigure 5-1shows part of the iris dataset that was introduced at the end of \\nChapter 4\\n. The twoclasses can clearly be separated easily with a straight line (they are \\nlinearly separable\\n).The left plot shows the decision boundaries of three possible linear classifiers. Themodel whose decision boundary is represented by the dashed line is so bad that it\\ndoes not even separate the classes properly. The other two models work perfectly on\\nthis training set, but their decision boundaries come so close to the instances that\\nthese models will probably not perform as well on new instances. In contrast, the\\nsolid line in the plot on the right represents the decision boundary of an SVM classi…\\nfier; this line not only separates the two classes but also stays as far away from the\\nclosest training instances as possible. You can think of an SVM classifier as fitting the\\nwidest possible street (represented by the parallel dashed lines) between the classes.\\nThis is called large margin \\nclassi†cation.145Figure 5-1. Large margin \\nclassi†cationNotice that adding more training instances ƒoff the street⁄ will not affect the decision\\nboundary at all: it is fully determined (or ƒsupported⁄) by the instances located on the\\nedge of the street. These instances are called the support vectors\\n (they are circled inFigure 5-1).SVMs are sensitive to the feature scales, as you can see in\\nFigure 5-2: on the left plot, the vertical scale is much larger than the\\nhorizontal scale, so the widest possible street is close to horizontal.\\nAfter feature scaling (e.g., using Scikit-Learn‡s \\nStandardScaler), the decision boundary looks much better (on the right plot).\\nFigure 5-2. Sensitivity to feature scales\\nSoft Margin Classi•cationIf we strictly impose that all instances be off the street and on the right side, this is\\ncalled hard margin \\nclassi†cation. There are two main issues with hard margin classifi…cation. First, it only works if the data is linearly separable, and second it is quite sensi…\\ntive to outliers. Figure 5-3 shows the iris dataset with just one additional outlier: on\\nthe left, it is impossible to find a hard margin, and on the right the decision boundary\\nends up very different from the one we saw in \\nFigure 5-1 without the outlier, and it\\nwill probably not generalize as well.146 | Chapter 5: Support Vector Machines\\nFigure 5-3. Hard margin sensitivity to outliers\\nTo avoid these issues it is preferable to use a more flexible model. The objective is to\\nfind a good balance between keeping the street as large as possible and limiting themargin violations\\n (i.e., instances that end up in the middle of the street or even on the\\nwrong side). This is called so“ margin \\nclassi†cation.In Scikit-Learn‡s SVM classes, you can control this balance using the \\nC hyperparame…\\nter: a smaller \\nC value leads to a wider street but more margin violations. \\nFigure 5-4shows the decision boundaries and margins of two soft margin SVM classifiers on anonlinearly separable dataset. On the left, using a high \\nC value the classifier makesfewer margin violations but ends up with a smaller margin. On the right, using a low\\nC value the margin is much larger, but many instances end up on the street. However,\\nit seems likely that the second classifier will generalize better: in fact even on this\\ntraining set it makes fewer prediction errors, since most of the margin violations are\\nactually on the correct side of the decision boundary.\\nFigure 5-4. Fewer margin violations versus large margin\\nIf your SVM model is overfitting, you can try regularizing it by\\nreducing C.The following Scikit-Learn code loads the iris dataset, scales the features, and then\\ntrains a linear SVM model (using the LinearSVC class with C = 0.1 and the hinge loss\\nLinear SVM Classi•cation | 147\\nfunction, described shortly) to detect Iris-Virginica flowers. The resulting model is\\nrepresented on the right of \\nFigure 5-4.import numpy as npfrom sklearn import datasetsfrom sklearn.pipeline import Pipelinefrom sklearn.preprocessing import StandardScalerfrom sklearn.svm import LinearSVCiris = datasets.load_iris()X = iris[\"data\"][:, (2, 3)]  # petal length, petal widthy = (iris[\"target\"] == 2).astype(np.float64)  # Iris-Virginicasvm_clf = Pipeline((        (\"scaler\", StandardScaler()),        (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")),    ))svm_clf.fit(X_scaled, y)Then, as usual, you can use the model to make predictions:>>> svm_clf.predict([[5.5, 1.7]])array([ 1.])Unlike Logistic Regression classifiers, SVM classifiers do not out…\\nput probabilities for each class.Alternatively, you could use the \\nSVC class, using SVC(kernel=\"linear\", C=1), but itis much slower, especially with large training sets, so it is not recommended. Another\\noption is to use the SGDClassifier class, with SGDClassifier(loss=\"hinge\",alpha=1/(m*C)). This applies regular Stochastic Gradient Descent (see \\nChapter 4\\n) totrain a linear SVM classifier. It does not converge as fast as the \\nLinearSVC class, but itcan be useful to handle huge datasets that do not fit in memory (out-of-core train…\\ning), or to handle online classification tasks.\\nThe LinearSVC class regularizes the bias term, so you should center\\nthe training set first by subtracting its mean. This is automatic if\\nyou scale the data using the \\nStandardScaler. Moreover, make sure\\nyou set the loss hyperparameter to \\n\"hinge\", as it is not the default\\nvalue. Finally, for better performance you should set the \\ndualhyperparameter to \\nFalse, unless there are more features than\\ntraining instances (we will discuss duality later in the chapter).\\n148 | Chapter 5: Support Vector Machines\\nNonlinear SVM Classi•cationAlthough linear SVM classifiers are efficient and work surprisingly well in many\\ncases, many datasets are not even close to being linearly separable. One approach to\\nhandling nonlinear datasets is to add more features, such as \\npolynomial features (as\\nyou did in Chapter 4\\n); in some cases this can result in a linearly separable dataset.\\nConsider the left plot in Figure 5-5: it represents a simple dataset with just one feature\\nx1. This dataset is not linearly separable, as you can see. But if you add a second fea…\\nture x2 = (x1)2, the resulting 2D dataset is perfectly linearly separable.\\nFigure 5-5. Adding features to make a dataset linearly separable\\nTo implement this idea using Scikit-Learn, you can create a \\nPipeline containing a\\nPolynomialFeatures transformer (discussed in ƒPolynomial Regression⁄ on page\\n121), followed by a StandardScaler and a LinearSVC. Let‡s test this on the moons\\ndataset (see \\nFigure 5-6):from sklearn.datasets import make_moonsfrom sklearn.pipeline import Pipelinefrom sklearn.preprocessing import PolynomialFeaturespolynomial_svm_clf = Pipeline((        (\"poly_features\", PolynomialFeatures(degree=3)),        (\"scaler\", StandardScaler()),        (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))    ))polynomial_svm_clf.fit(X, y)Nonlinear SVM Classi•cation | 149\\nFigure 5-6. Linear SVM \\nclassi†er using polynomial features\\nPolynomial KernelAdding polynomial features is simple to implement and can work great with all sorts\\nof Machine Learning algorithms (not just SVMs), but at a low polynomial degree it\\ncannot deal with very complex datasets, and with a high polynomial degree it creates\\na huge number of features, making the model too slow.\\nFortunately, when using SVMs you can apply an almost miraculous mathematical\\ntechnique called the kernel trick\\n (it is explained in a moment). It makes it possible to\\nget the same result as if you added many polynomial features, even with very high-\\ndegree polynomials, without actually having to add them. So there is no combinato…\\nrial explosion of the number of features since you don‡t actually add any features. This\\ntrick is implemented by the \\nSVC class. Let‡s test it on the moons dataset:\\nfrom sklearn.svm import SVCpoly_kernel_svm_clf = Pipeline((        (\"scaler\", StandardScaler()),        (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))    ))poly_kernel_svm_clf.fit(X, y)This code trains an SVM classifier using a 3rd-degree polynomial kernel. It is repre…\\nsented on the left of \\nFigure 5-7. On the right is another SVM classifier using a 10\\nth-degree polynomial kernel. Obviously, if your model is overfitting, you might want to\\n150 | Chapter 5: Support Vector Machines\\nreduce the polynomial degree. Conversely, if it is underfitting, you can try increasing\\nit. The hyperparameter \\ncoef0 controls how much the model is influenced by high-\\ndegree polynomials versus low-degree polynomials.Figure 5-7. SVM \\nclassi†ers with a polynomial kernel\\nA common approach to find the right hyperparameter values is to\\nuse grid search (see Chapter 2\\n). It is often faster to first do a very\\ncoarse grid search, then a finer grid search around the best valuesfound. Having a good sense of what each hyperparameter actually\\ndoes can also help you search in the right part of the \\nhyperparame…\\nter space.Adding Similarity FeaturesAnother technique to tackle nonlinear problems is to add features computed using a\\nsimilarity function\\n that measures how much each instance resembles a particular\\nlandmark\\n. For example, let‡s take the one-dimensional dataset discussed earlier and\\nadd two landmarks to it at \\nx1 = –2 and x1 = 1 (see the left plot in Figure 5-8). Next,\\nlet‡s define the similarity function to be the Gaussian \\nRadial Basis Function\\n (RBF)with ’ = 0.3 (see Equation 5-1\\n).Equation 5-1. Gaussian RBF\\n‚’,—\\n=exp\\n”’\\n”—\\n2It is a bell-shaped function varying from 0 (very far away from the landmark) to 1 (at\\nthe landmark). Now we are ready to compute the new features. For example, let‡s look\\nat the instance \\nx1 = –1: it is located at a distance of 1 from the first landmark, and 2\\nfrom the second landmark. Therefore its new features are \\nx2 = exp (–0.3 ‰ 12) Ÿ 0.74and x3 = exp (–0.3 ‰ 22) Ÿ 0.30. The plot on the right of \\nFigure 5-8 shows the trans…formed dataset (dropping the original features). As you can see, it is now linearly\\nseparable.Nonlinear SVM Classi•cation | 151\\nFigure 5-8. Similarity features using the Gaussian RBF\\nYou may wonder how to select the landmarks. The simplest approach is to create a\\nlandmark at the location of each and every instance in the dataset. This creates many\\ndimensions and thus increases the chances that the transformed training set will be\\nlinearly separable. The downside is that a training set with \\nm instances and n features\\ngets transformed into a training set with \\nm instances and m features (assuming you\\ndrop the original features). If your training set is very large, you end up with an\\nequally large number of features.\\nGaussian RBF KernelJust \\nlike the polynomial features method, the similarity features method can be useful\\nwith any Machine Learning algorithm, but it may be computationally expensive to\\ncompute all the additional features, especially on large training sets. However, once\\nagain the kernel trick does its SVM magic: it makes it possible to obtain a similarresult as if you had added many similarity features, without actually having to add\\nthem. Let‡s try the Gaussian RBF kernel using the \\nSVC class:rbf_kernel_svm_clf = Pipeline((        (\"scaler\", StandardScaler()),        (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))    ))rbf_kernel_svm_clf.fit(X, y)This model is represented on the bottom left of \\nFigure 5-9. The other plots showmodels trained with different values of hyperparameters \\ngamma (’) and C. Increasinggamma makes the bell-shape curve narrower (see the left plot of \\nFigure 5-8), and as aresult each instance‡s range of influence is smaller: the decision boundary ends up\\nbeing more irregular, wiggling around individual instances. Conversely, a small \\ngamma value makes the bell-shaped curve wider, so instances have a larger range of influ…\\nence, and the decision boundary ends up smoother. So \\n’ acts like a regularization\\nhyperparameter: if your model is overfitting, you should reduce it, and if it is \\nunder…fitting, you should increase it (similar to the C hyperparameter).\\n152 | Chapter 5: Support Vector Machines\\n1ƒA Dual Coordinate Descent Method for Large-scale Linear SVM,⁄ Lin et al. (2008).\\nFigure 5-9. SVM \\nclassi†ers using an RBF kernel\\nOther kernels exist but are used much more rarely. For example, some kernels are\\nspecialized for specific data structures. \\nString kernels\\n are sometimes used when classi…fying text documents or DNA sequences (e.g., using the \\nstring subsequence kernel\\n orkernels based on the Levenshtein distance\\n).With so many kernels to choose from, how can you decide which\\none to use? As a rule of thumb, you should always try the linear\\nkernel first (remember that \\nLinearSVC is much faster than \\nSVC(kernel=\"linear\")), especially if the training set is very large or if it\\nhas plenty of features. If the training set is not too large, you should\\ntry the Gaussian RBF kernel as well; it works well in most cases.\\nThen if you have spare time and computing power, you can also\\nexperiment with a few other kernels using cross-validation and grid\\nsearch, especially if there are kernels specialized for your trainingset‡s data structure.\\nComputational ComplexityThe LinearSVC class is based on the liblinear\\n library, \\nwhich implements an \\noptimizedalgorithm for linear SVMs.1 It does not support the kernel trick, but it scales almost\\nNonlinear SVM Classi•cation | 153\\n2ƒSequential Minimal Optimization (SMO),⁄ J. Platt (1998).\\nlinearly with the number of training instances and the number of features: its training\\ntime complexity is roughly \\nO(m ‰ n).The algorithm takes longer if you require a very high precision. This is controlled by\\nthe tolerance hyperparameter \\n (called tol in Scikit-Learn). In most classification\\ntasks, the default tolerance is fine.\\nThe SVC class is based on the libsvm\\n library, which implements \\nan algorithm that sup…\\nports the kernel trick.2 The training time complexity is usually between \\nO(m2 ‰ n)and O(m3 ‰ n). Unfortunately, this means that it gets dreadfully slow when the num…\\nber of training instances gets large (e.g., hundreds of thousands of instances). This\\nalgorithm is perfect for complex but small or medium training sets. However, it scales\\nwell with the number of features, especially with \\nsparse features\\n (i.e., when eachinstance has few nonzero features). In this case, the algorithm scales roughly with the\\naverage number of nonzero features per instance. \\nTable 5-1\\n compares \\nScikit-Learn‡s\\nSVM classification classes.\\nTable 5-1. Comparison of Scikit-Learn classes for SVM \\nclassi†cationClassTime complexity\\nOut-of-core support\\nScaling required\\nKernel trick\\nLinearSVCO(m † \\nn)NoYesNoSGDClassifierO(m † \\nn)YesYesNoSVCO(m‡ † \\nn) to O(\\nm… † \\nn)NoYesYesSVM RegressionAs we mentioned earlier, the SVM algorithm is quite versatile: not only does it sup…\\nport linear and nonlinear classification, but it also supports linear and nonlinear\\nregression. The trick is to reverse the objective: instead of trying to fit the largest pos…\\nsible street between two classes while limiting margin violations, SVM Regression\\ntries to fit as many instances as possible \\non\\n the street while limiting margin violations\\n(i.e., instances o› the street). The width of the street is controlled by a hyperparame…\\nter . Figure 5-10 shows two linear SVM Regression models trained on some randomlinear data, one with a large margin (\\n = 1.5) and the other with a small margin ( =0.5).154 | Chapter 5: Support Vector Machines\\nFigure 5-10. SVM Regression\\nAdding more training instances within the margin does not affect the model‡s predic…\\ntions; thus, the model is said to be \\n-insensitive\\n.You can use Scikit-Learn‡s \\nLinearSVR class to perform linear SVM Regression. Thefollowing code produces the model represented on the left of \\nFigure 5-10 (the train…ing data should be scaled and centered first):\\nfrom sklearn.svm import LinearSVRsvm_reg = LinearSVR(epsilon=1.5)svm_reg.fit(X, y)To tackle nonlinear regression tasks, you can use a kernelized SVM model. For exam…\\nple, Figure 5-11 shows SVM Regression on a random quadratic training set, using a\\n2nd-degree polynomial kernel. There is little regularization on the left plot (i.e., a large\\nC value), and much more regularization on the right plot (i.e., a small \\nC value).Figure 5-11. SVM regression using a 2\\nnd\\n-degree polynomial kernel\\nSVM Regression | 155\\nThe following code produces the model represented on the left of \\nFigure 5-11 usingScikit-Learn‡s \\nSVR class (which supports the kernel trick). The SVR class is the regres…sion equivalent of the \\nSVC class, and the LinearSVR class is the regression equivalent\\nof the LinearSVC class. The LinearSVR class scales linearly with the size of the train…ing set (just like the LinearSVC class), while the SVR class gets much too slow when\\nthe training set grows large (just like the SVC class).from sklearn.svm import SVRsvm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)svm_poly_reg.fit(X, y)SVMs can also be used for outlier detection; see Scikit-Learn‡s doc…\\numentation for more details.\\nUnder the HoodThis section explains how SVMs make predictions and how their training algorithmswork, starting with linear SVM classifiers. You can safely skip it and go straight to the\\nexercises at the end of this chapter if you are just getting started with Machine Learn…\\ning, and come back later when you want to get a deeper understanding of SVMs.\\nFirst, a word about notations: in \\nChapter 4\\n we used the convention of putting all the \\nmodel parameters in one vector –, including the bias term –0 and the input feature\\nweights \\n–1 to \\n–n, and adding a bias input \\nx0 = 1 to all instances. In this chapter, we will\\nuse a different convention, which is more convenient (and more common) when you\\nare dealing with SVMs: the bias term will be called b and the feature weights vector\\nwill be called w. No bias feature will be added to the input feature vectors.\\nDecision Function and PredictionsThe linear SVM classifier model predicts the class of a new instance x by simply com…\\nputing the decision function wT ’ x + b = w1 x1 +  + wn xn + b: if the result is posi…tive, the predicted class ƒ is the positive class (1), or else it is the negative class (0); see\\nEquation 5-2\\n.Equation 5-2. Linear SVM \\nclassi†er prediction\\ny=0if\\nT’+b<0,\\n1if\\nT’+bŠ0\\n156 | Chapter 5: Support Vector Machines\\n3More generally, when there are \\nn features, the decision function is an \\nn-dimensional hyperplane\\n, and the deci…sion boundary is an (\\nn – 1)-dimensional hyperplane.\\nFigure 5-12 shows the decision function that corresponds to the model on the right of\\nFigure 5-4: it is a two-dimensional plane since this dataset has two features (petal\\nwidth and petal length). The decision boundary is the set of points where the decision\\nfunction is equal to 0: it is the intersection of two planes, which is a straight line (rep…\\nresented by the thick solid line).\\n3Figure 5-12. Decision function for the iris dataset\\nThe dashed lines represent the points where the decision function is equal to 1 or –1:\\nthey are parallel and at equal distance to the decision boundary, forming a margin\\naround it. Training a linear SVM classifier means finding the value of \\nw and b that\\nmake this margin as wide as possible while avoiding margin violations (hard margin)\\nor limiting them (soft margin).Training ObjectiveConsider the slope of the decision function: it is equal to the norm of the weight vec…\\ntor, \\n w . If we divide this slope by 2, the points where the decision function is equal\\nto ﬁ1 are going to be twice as far away from the decision boundary. In other words,\\ndividing the slope by 2 will multiply the margin by 2. Perhaps this is easier to visual…\\nize in 2D in Figure 5-13. The smaller the weight vector \\nw, the larger the margin.Under the Hood | 157\\n4Zeta (™) is the 8th letter of the Greek alphabet.Figure 5-13. A smaller weight vector results in a larger margin\\nSo we want to minimize \\n w  to get a large margin. However, if we also want to avoid\\nany margin violation (hard margin), then we need the decision function to be greater\\nthan 1 for all positive training instances, and lower than –1 for negative training\\ninstances. If we define t(i) = –1 for negative instances (if \\ny(i) = 0) and \\nt(i) = 1 for positive\\ninstances (if y(i) = 1), then we can express this constraint as \\nt(i)(wT ’ x(i) + b) Š 1 for allinstances.We can therefore express the hard margin linear SVM classifier objective as the \\ncon…\\nstrained optimization\\n problem in Equation 5-3\\n.Equation 5-3. Hard margin linear SVM \\nclassi†er objective\\nminimize,b12T’subjectto\\ntiT’i+bŠ1for\\ni=1,2,\\n,mWe are minimizing \\n12wT ’ w, which is equal to 12 w 2, rather than\\nminimizing  w . This is because it will give the same result (since\\nthe values of w and b that minimize a value also minimize half of\\nits square), but 12 w 2 has a nice and simple derivative (it is just\\nw) while  w  is not differentiable at \\nw = 0. Optimization algo…\\nrithms work much better on differentiable functions.\\nTo get the soft margin objective, we need to introduce \\na slack variable\\n ™(i) Š\\n 0 for eachinstance:4 ™(i) measures how much the i\\nth instance is allowed to violate the margin. We\\nnow have two conflicting objectives: making the slack variables as small as possible to\\nreduce the margin violations, and making \\n12wT ’ \\nw as small as possible to increase themargin. This is where the C hyperparameter comes in: it allows us to define the trade…\\n158 | Chapter 5: Support Vector Machines\\n5To learn more about Quadratic Programming, you can start by reading Stephen Boyd and Lieven Vanden…\\nberghe, Convex Optimization\\n (Cambridge, UK: Cambridge University Press, 2004) or watch Richard Brown‡s\\nseries of video lectures.off between these two objectives. This gives us the constrained optimization problem\\nin Equation 5-4\\n.Equation 5-4. \\nSo“ margin linear SVM \\nclassi†er objective\\nminimize,b,™12T’+C“i=1\\nm™isubjectto\\ntiT’i+bŠ1”\\n™iand™iŠ0for\\ni=1,2,\\n,mQuadratic ProgrammingThe hard margin and soft margin problems are both convex quadratic optimization\\nproblems with linear constraints. Such problems are known as \\nQuadratic Program…\\nming\\n (QP) problems. Many off-the-shelf solvers are available to solve QP problems\\nusing a variety of techniques that are outside the scope of this book.\\n5 The generalproblem formulation is given by \\nEquation 5-5\\n.Equation 5-5. Quadratic Programming problem\\nMinimize12T’’+T’subjectto\\n’Žwhereisan\\nnp…dimensionalvector(\\nnp=numberofparameters),\\nisan\\nnp‰npmatrix,isan\\nnp…dimensionalvector,\\nisan\\nnc‰npmatrix(\\nnc=numberofconstraints),\\nisan\\nnc…dimensionalvector.\\nNote that the expression \\nA ’ p Ž b actually defines nc constraints: \\npT ’ a(i) Ž b(i) for i =1, 2, , nc, where a(i) is the vector containing the elements of the i\\nth row of A and \\nb(i) is\\nthe ith element of \\nb.You can easily verify that if you set the QP parameters in the following way, you get\\nthe hard margin linear SVM classifier objective:‹np = n + 1, where n is the number of features (the +1 is for the bias term).\\nUnder the Hood | 159\\n6The objective function is convex, and the inequality constraints are continuously differentiable and convex\\nfunctions.‹nc = m, where m is the number of training instances.\\n‹H is the np ‰ np identity matrix, except with a zero in the top-left cell (to ignore\\nthe bias term).‹f = 0, an np-dimensional vector full of 0s.‹b = 1, an nc-dimensional vector full of 1s.‹a(i) = –t(i) ı(i), where ı(i) is equal to x(i) with an extra bias feature \\nı0 = 1.So one way to train a hard margin linear SVM classifier is just to use an off-the-shelf\\nQP solver by passing it the preceding parameters. The resulting vector p will contain\\nthe bias term b = p0 and the feature weights \\nwi = pi for i = 1, 2, , m. Similarly, you\\ncan use a QP solver to solve the soft margin problem (see the exercises at the end of\\nthe chapter).\\nHowever, to use the kernel trick we are going to look at a different constrained opti…\\nmization problem.\\nThe Dual ProblemGiven a constrained optimization problem, known as the \\nprimal problem\\n, it is possi…ble to express a different but closely related problem, called \\nits dual problem\\n. The sol…ution to the dual problem typically gives a lower bound to the solution of the primalproblem, but under some conditions it can even have the same solutions as the pri…\\nmal problem. Luckily, the SVM problem happens to meet these conditions,\\n6 so youcan choose to solve the primal problem or the dual problem; both will have the same\\nsolution. Equation 5-6\\n shows the dual form of the linear SVM objective (if you areinterested in knowing how to derive the dual problem from the primal problem, see\\nAppendix C\\n).Equation 5-6. Dual form of the linear SVM objective\\nminimize‰12“i=1\\nm“j=1\\nm‰i‰jtitjiT’j”“i=1\\nm‰isubjectto\\n‰iŠ0for\\ni=1,2,\\n,m160 | Chapter 5: Support Vector Machines\\nOnce you find the vector ‰ that minimizes this equation (using a QP solver), you can\\ncompute \\n and b that minimize the primal problem by using \\nEquation 5-7\\n.Equation 5-7. From the dual solution to the primal solution\\n=“i=1\\nm‰itiib=1ns“i=1\\n‰i>0\\nm1”\\ntiT’iThe dual problem is faster to solve than the primal when the number of training\\ninstances is smaller than the number of features. More importantly, it makes the ker…\\nnel trick possible, while the primal does not. So what is this kernel trick anyway?\\nKernelized SVMSuppose you want to apply a 2\\nnd-degree polynomial transformation to a two-\\ndimensional training set (such as the moons training set), then train a linear SVMclassifier on the transformed training set. Equation 5-8\\n shows the 2nd-degree polyno…mial mapping function \\n‚ that you want to apply.\\nEquation 5-8. Second-degree polynomial mapping\\n‚=‚x1x2=x122x1x2x22Notice that the transformed vector is three-dimensional instead of two-dimensional.\\nNow let‡s look at what happens to a couple of two-dimensional vectors, \\na and \\nb, if weapply this 2\\nnd-degree polynomial mapping and then compute the dot product of the\\ntransformed vectors (See Equation 5-9\\n).Under the Hood | 161\\nEquation 5-9. Kernel trick for a 2\\nnd\\n-degree polynomial mapping\\n‚T’‚=a122a1a2a22T’b122b1b2b22=a12b12+2\\na1b1a2b2+a22b22=a1b1+a2b22=a1a2T’b1b22=T’2How about that? The dot product of the transformed vectors is equal to the square of\\nthe dot product of the original vectors: ‚(a)T ’ ‚(b) = (aT ’ b)2.Now here is the key insight: if you apply the transformation \\n‚ to all training instan…ces, then the dual problem (see Equation 5-6\\n) will contain the dot product \\n‚(x(i))T ’‚(x(j)). But if ‚ is the 2nd-degree polynomial transformation defined in \\nEquation 5-8\\n,then you can replace this dot product of transformed vectors simply by \\niT’j2.So you don‡t actually need to transform the training instances at all: just replace the\\ndot product by its square in Equation 5-6\\n. The result will be strictly the same as if youwent through the trouble of actually transforming the training set then fitting a linear\\nSVM algorithm, but this trick makes the whole process much more computationally\\nefficient. This is the essence of the kernel trick.\\nThe function K(a, b) = (aT ’ b)2 is called a 2nd-degree polynomial kernel\\n. In Machine\\nLearning, a kernel\\n is a function capable of computing the dot product \\n‚(a)T ’ ‚(b)based only on the original vectors a and b, without having to compute (or even to\\nknow about) the transformation \\n‚. Equation 5-10\\n lists some of the most commonlyused kernels.Equation 5-10. Common kernels\\nLinear:K,=T’Polynomial:K,=’T’+rdGaussianRBF:\\nK,=exp\\n”’\\n”\\n2Sigmoid:K,=tanh\\n’T’+r162 | Chapter 5: Support Vector Machines\\nMercer‡s TheoremAccording to \\nMercer‹s theorem\\n, if a function K(a, b) respects a few mathematical con…\\nditions called Mercer‹s conditions\\n (K must be continuous, symmetric in its arguments\\nso K(a, b) = K(b, a), etc.), then there exists a function ‚ that maps \\na and b into\\nanother space (possibly with much higher dimensions) such that \\nK(a, b) = ‚(a)T ’‚(b). So you can use K as a kernel since you know ‚ exists, even if you don‡t know\\nwhat \\n‚ is. In the case of the Gaussian RBF kernel, it can be shown that \\n‚ actuallymaps each training instance to an infinite-dimensional space, so it‡s a good thing you\\ndon‡t need to actually perform the mapping!\\nNote that some frequently used kernels (such as the Sigmoid kernel) don‡t respect all\\nof Mercer‡s conditions, yet they generally work well in practice.\\nThere is still one loose end we must tie. \\nEquation 5-7\\n shows how to go from the dualsolution to the primal solution in the case of a linear SVM classifier, but if you apply\\nthe kernel trick you end up with equations that include \\n‚(x(i)). In fact,  must have\\nthe same number of dimensions as \\n‚(x(i)), which may be huge or even infinite, so you\\ncan‡t compute it. But how can you make predictions without knowing \\n? Well, the\\ngood news is that you can plug in the formula for \\n from \\nEquation 5-7\\n into the deci…\\nsion function for a new instance x(n), and you get an equation with only dot products\\nbetween input vectors. This makes it possible to use the kernel trick, once again\\n(Equation 5-11\\n).Equation 5-11. Making predictions with a kernelized SVM\\nh,b‚n=T’‚n+b=“i=1\\nm‰iti‚iT’‚n+b=“i=1\\nm‰iti‚iT’‚n+b=“i=1\\n‰i>0\\nm‰itiKi,n+bNote that since \\n‰(i) ł\\n 0 only for support vectors, making predictions involves comput…\\ning the dot product of the new input vector \\nx(n) with only the support vectors, not allthe training instances. Of course, you also need to compute the bias term \\nb, using thesame trick (Equation 5-12\\n).Under the Hood | 163\\nEquation 5-12. Computing the bias term using the kernel trick\\nb=1ns“i=1\\n‰i>0\\nm1”\\ntiT’‚i=1ns“i=1\\n‰i>0\\nm1”\\nti“j=1\\nm‰jtj‚jT’‚i=1ns“i=1\\n‰i>0\\nm1”\\nti“j=1\\n‰j>0\\nm‰jtjKi,jIf you are starting to get a headache, it‡s perfectly normal: it‡s an unfortunate side\\neffects of the kernel trick.Online SVMsBefore \\nconcluding this chapter, let‡s take a quick look at online SVM classifiers (recall\\nthat online learning means learning incrementally, typically as new instances arrive).\\nFor linear SVM classifiers, one method is to use Gradient Descent (e.g., using\\nSGDClassifier) to minimize the cost function in Equation 5-13\\n, which is derivedfrom the primal problem. Unfortunately it converges much more slowly than the\\nmethods based on QP.\\nEquation 5-13. Linear SVM \\nclassi†er cost function\\nJ,b=12T’+C“i=1\\nmmax0,1”\\ntiT’i+bThe first sum in the cost function will push the model to have a small weight vector\\nw, leading to a larger margin. The second sum computes the total of all margin viola…\\ntions. An instance‡s margin violation is equal to 0 if it is located off the street and on\\nthe correct side, or else it is proportional to the distance to the correct side of thestreet. Minimizing this term ensures that the model makes the margin violations as\\nsmall and as few as possibleHinge LossThe function max\\n(0, 1 – t) is called the hinge loss\\n function (represented below). It is\\nequal to 0 when t Š\\n 1. Its derivative (slope) is equal to –1 if \\nt < 1 and 0 if t > 1. It is not\\ndifferentiable at \\nt = 1, but just like for Lasso Regression (see ƒLasso Regression⁄\\n onpage 130) you can still use Gradient Descent using any \\nsubderivative\\n at \\nt = 0 (i.e., any\\nvalue between –1 and 0).164 | Chapter 5: Support Vector Machines\\n7ƒIncremental and Decremental Support Vector Machine Learning,⁄ G. Cauwenberghs, T. Poggio (2001).\\n8ƒFast Kernel Classifiers with Online and Active Learning,ƒ A. Bordes, S. Ertekin, J. Weston, L. Bottou (2005).\\nIt is also possible to implement online kernelized SVMs›for example, using \\nƒIncre…mental and Decremental SVM Learning⁄\\n7 or \\nƒFast Kernel Classifiers with Online and\\nActive Learning.⁄\\n8 However, these are implemented in Matlab and C++. For large-\\nscale nonlinear problems, you may want to consider using neural networks instead \\n(see Part II\\n).Exercises1.What is the fundamental idea behind Support Vector Machines?\\n2.What is a support vector?\\n3.Why is it important to scale the inputs when using SVMs?\\n4.Can an SVM classifier output a confidence score when it classifies an instance?What about a probability?\\n5.Should you use the primal or the dual form of the SVM problem to train a modelon a training set with millions of instances and hundreds of features?\\n6.Say you trained an SVM classifier with an RBF kernel. It seems to underfit the\\ntraining set: should you increase or decrease ’ (gamma)? What about \\nC?7.How should you set the QP parameters (\\nH, f, A, and b) to solve the soft marginlinear SVM classifier problem using an off-the-shelf QP solver?8.Train a \\nLinearSVC on a linearly separable dataset. Then train an \\nSVC and aSGDClassifier on the same dataset. See if you can get them to produce roughly\\nthe same model.9.Train an SVM classifier on the MNIST dataset. Since SVM classifiers are binary\\nclassifiers, you will need to use one-versus-all to classify all 10 digits. You may\\nExercises | 165\\nwant to tune the hyperparameters using small validation sets to speed up the pro…\\ncess. What accuracy can you reach?\\n10.Train an SVM regressor on the California housing dataset.\\nSolutions to these exercises are available in \\nAppendix A\\n.166 | Chapter 5: Support Vector Machines\\nCHAPTER 6Decision TreesLike SVMs, Decision Trees\\n are versatile Machine Learning algorithms that can per…\\nform both classification and regression tasks, and even multioutput tasks. They are\\nvery powerful algorithms, capable of fitting complex datasets. For example, in \\nChap…\\nter 2 you trained a DecisionTreeRegressor model on the California housing dataset,\\nfitting it perfectly (actually overfitting it).Decision Trees are also the fundamental components of Random Forests (see \\nChap…\\nter 7), which are among the most powerful Machine Learning algorithms available\\ntoday.\\nIn this chapter we will start by discussing how to train, visualize, and make predic…\\ntions with Decision Trees. Then we will go through the CART training algorithm\\nused by Scikit-Learn, and we will discuss how to regularize trees and use them forregression tasks. Finally, we will discuss some of the limitations of Decision Trees.\\nTraining and Visualizing a Decision TreeTo understand Decision Trees, let‡s just build one and take a look at how it makes pre…\\ndictions. The following code trains a DecisionTreeClassifier on the iris dataset\\n(see Chapter 4\\n):from sklearn.datasets import load_irisfrom sklearn.tree import DecisionTreeClassifieriris = load_iris()X = iris.data[:, 2:] # petal length and widthy = iris.targettree_clf = DecisionTreeClassifier(max_depth=2)tree_clf.fit(X, y)1671Graphviz is an open source graph visualization software package, available at \\nhttp://www.graphviz.org/\\n.You can visualize the trained Decision Tree by first using the \\nexport_graphviz() method to output a graph definition file called \\niris_tree.dot\\n:from sklearn.tree import export_graphvizexport_graphviz(        tree_clf,        out_file=image_path(\"iris_tree.dot\"),        feature_names=iris.feature_names[2:],        class_names=iris.target_names,        rounded=True,        filled=True    )Then you can convert this \\n.dot\\n file to a variety of formats such as PDF or PNG using\\nthe dot command-line tool from the graphviz\\n package.1 This command line converts\\nthe .dot\\n file to a .png\\n image file:$ dot -Tpng iris_tree.dot -o iris_tree.pngYour first decision tree looks like \\nFigure 6-1.Figure 6-1. Iris Decision Tree\\n168 | Chapter 6: Decision Trees\\nMaking PredictionsLet‡s see how the tree represented in \\nFigure 6-1 makes predictions. Suppose you findan iris flower and you want to classify it. You start at the \\nroot node\\n (depth 0, at the\\ntop): this node asks whether the flower‡s petal length is smaller than 2.45 cm. If it is,\\nthen you move down to the root‡s left child node (depth 1, left). In this case, it is a \\nleaf\\nnode\\n (i.e., it does not have any children nodes), so it does not ask any questions: you\\ncan simply look at the predicted class for that node and the Decision Tree predicts\\nthat your flower is an Iris-Setosa (\\nclass=setosa).Now suppose you find another flower, but this time the petal length is greater than\\n2.45 cm. You must move down to the root‡s right child node (depth 1, right), which is\\nnot a leaf node, so it asks another question: is the petal width smaller than 1.75 cm? Ifit is, then your flower is most likely an Iris-Versicolor (depth 2, left). If not, it is likely\\nan Iris-Virginica (depth 2, right). It‡s really that simple.\\nOne of the many qualities of Decision Trees is that they require\\nvery little data preparation. In particular, they don‡t require feature\\nscaling or centering at all.\\nA node‡s \\nsamples attribute counts how many training instances it applies to. For\\nexample, 100 training instances have a petal length greater than 2.45 cm (depth 1,\\nright), among which 54 have a petal width smaller than 1.75 cm (depth 2, left). A\\nnode‡s \\nvalue attribute tells you how many training instances of each class this node\\napplies to: for example, the bottom-right node applies to 0 Iris-Setosa, 1 Iris-\\nVersicolor, and 45 Iris-Virginica. Finally, a node‡s \\ngini attribute measures its \\nimpur…\\nity\\n: a node is ƒpure⁄ (\\ngini=0) if all training instances it applies to belong to the same\\nclass. For example, since the depth-1 left node applies only to Iris-Setosa training\\ninstances, it is pure and its gini score is 0. Equation 6-1\\n shows how the training algo…rithm computes the gini score \\nGi of the ith node. For example, the depth-2 left node\\nhas a gini score equal to 1 – (0/54)2 – (49/54)2 – (5/54)2 Ÿ 0.168. Another impurity\\nmeasure\\n is discussed shortly.\\nEquation 6-1. Gini impurity\\nGi=1”\\n“k=1\\nnpi,k2‹pi,k is the ratio of class \\nk instances among the training instances in the ith node.Making Predictions | 169\\nScikit-Learn uses the \\nCART algorithm, which produces \\nonly binary\\ntrees\\n: nonleaf nodes always have two children (i.e., questions only\\nhave yes/no answers). However, other algorithms such as ID3 can\\nproduce Decision Trees with nodes that have more than two chil…\\ndren.Figure 6-2 shows this Decision Tree‡s decision boundaries. The thick vertical line rep…\\nresents the decision boundary of the root node (depth 0): petal length = 2.45 cm.\\nSince the left area is pure (only Iris-Setosa), it cannot be split any further. However,\\nthe right area is impure, so the depth-1 right node splits it at petal width = 1.75 cm\\n(represented by the dashed line). Since \\nmax_depth was set to 2, the Decision Tree\\nstops right there. However, if you set \\nmax_depth to 3, then the two depth-2 nodeswould each add another decision boundary (represented by the dotted lines).\\nFigure 6-2. Decision Tree decision boundaries\\nModel Interpretation: White Box Versus Black BoxAs you can see Decision Trees are fairly intuitive and their decisions are easy to inter…\\npret. Such models are often called white box models\\n. In contrast, as we will see, Ran…\\ndom Forests or neural networks are generally considered \\nblack box models\\n. Theymake great predictions, and you can easily check the calculations that they performed\\nto make these predictions; nevertheless, it is usually hard to explain in simple terms\\nwhy the predictions were made. For example, if a neural network says that a particu…\\nlar person appears on a picture, it is hard to know what actually contributed to this\\nprediction: did the model recognize that person‡s eyes? Her mouth? Her nose? Her\\nshoes? Or even the couch that she was sitting on? Conversely, Decision Trees provide\\nnice and simple classification rules that can even be applied manually if need be (e.g.,\\nfor flower classification).\\n170 | Chapter 6: Decision Trees\\nEstimating Class ProbabilitiesA Decision Tree can also estimate the probability that an instance belongs to a partic…\\nular class k: first it traverses the tree to find the leaf node for this instance, and then it\\nreturns the ratio of training instances of class \\nk in this node. For example, suppose\\nyou have found a flower whose petals are 5 cm long and 1.5 cm wide. The corre…\\nsponding leaf node is the depth-2 left node, so the Decision Tree should output the\\nfollowing probabilities: 0% for Iris-Setosa (0/54), 90.7% for Iris-Versicolor (49/54),\\nand 9.3% for Iris-Virginica (5/54). And of course if you ask it to predict the class, it\\nshould output Iris-Versicolor (class 1) since it has the highest probability. Let‡s check\\nthis:>>> tree_clf.predict_proba([[5, 1.5]])array([[ 0. ,  0.90740741,  0.09259259]])>>> tree_clf.predict([[5, 1.5]])array([1])Perfect! Notice that the estimated probabilities would be identical anywhere else in\\nthe bottom-right rectangle of \\nFigure 6-2›for example, if the petals were 6 cm long\\nand 1.5 cm wide (even though it seems obvious that it would most likely be an Iris-\\nVirginica in this case).\\nThe CART Training AlgorithmScikit-Learn uses the Classi†cation And Regression Tree\\n (CART) algorithm to train\\nDecision Trees (also called ƒgrowing⁄ trees). The idea is really quite simple: the algo…\\nrithm first splits the training set in two subsets using a single feature \\nk and a thres…hold tk (e.g., ƒpetal length Ž\\n 2.45 cm⁄). How does it choose \\nk and \\ntk? It searches for the\\npair (k, tk) that produces the purest subsets (weighted by their size). The cost function\\nthat the algorithm tries to minimize is given by \\nEquation 6-2\\n.Equation 6-2. CART cost function for \\nclassi†cationJk,tk=mleftmGleft+mrightmGrightwhereGleft/rightmeasurestheimpurityoftheleft/rightsubset,\\nmleft/rightisthenumberofinstancesintheleft/rightsubset.\\nOnce it has successfully split the training set in two, it splits the subsets using the\\nsame logic, then the sub-subsets and so on, recursively. It stops recursing once it rea…\\nches the maximum depth (defined by the \\nmax_depth hyperparameter), or if it cannot\\nfind a split that will reduce impurity. A few other hyperparameters (described in a\\nEstimating Class Probabilities | 171\\n2P is the set of problems that can be solved in polynomial time. NP is the set of problems whose solutions can\\nbe verified in polynomial time. An NP-Hard problem is a problem to which any NP problem can be reduced\\nin polynomial time. An NP-Complete problem is both NP and NP-Hard. A major open mathematical ques…\\ntion is whether or not P = NP. If P ł NP (which seems likely), then no polynomial algorithm will ever be\\nfound for any NP-Complete problem (except perhaps on a quantum computer).\\n3log\\n2 is the binary logarithm. It is equal to \\nlog\\n2(m) = log\\n(m) / log\\n(2).moment) control additional stopping conditions (\\nmin_samples_split, min_samples_leaf, min_weight_fraction_leaf, and max_leaf_nodes).As you can see, the CART algorithm is \\na greedy algorithm\\n: it greed…ily searches for an optimum split at the top level, then repeats the\\nprocess at each level. It does not check whether or not the split will\\nlead to the lowest possible impurity several levels down. A greedy\\nalgorithm often produces a reasonably good solution, but it is notguaranteed to be the optimal solution.\\nUnfortunately, finding the optimal tree is known to be an \\nNP-Complete\\n problem:2 itrequires O(exp(m)) time, making the problem intractable even for fairly small train…\\ning sets. This is why we must settle for a ƒreasonably good⁄ solution.\\nComputational ComplexityMaking predictions requires traversing the Decision Tree from the root to a leaf.\\nDecision Trees are generally approximately balanced, so traversing the Decision Tree\\nrequires going through roughly O(log\\n2(m)) nodes.3 Since each node only requireschecking the value of one feature, the overall prediction complexity is just \\nO(log\\n2(m)),independent of the number of features. So predictions are very fast, even when deal…\\ning with large training sets.However, the training algorithm compares all features (or less if \\nmax_features is set)on all samples at each node. This results in a training complexity of \\nO(n ‰ \\nm log\\n(m)).For small training sets (less than a few thousand instances), Scikit-Learn can speed up\\ntraining by presorting the data (set \\npresort=True), but this slows down training con…siderably for larger training sets.Gini Impurity or Entropy?By default, the Gini impurity measure is used, but you can select the \\nentropy\\n impurity\\nmeasure instead by setting the criterion hyperparameter to \\n\"entropy\". The conceptof entropy originated in thermodynamics as a measure of molecular disorder:\\nentropy approaches zero when molecules are still and well ordered. It later spread to a\\nwide variety of domains, including Shannon‡s \\ninformation theory\\n, where it measures172 | Chapter 6: Decision Trees\\n4A reduction of entropy is often called an \\ninformation gain\\n.5See Sebastian Raschka‡s \\ninteresting analysis for more details\\n.the average information content of a message:\\n4 entropy is zero when all messages are\\nidentical. In Machine Learning, it is frequently used as an impurity measure: a set‡s\\nentropy is zero when it contains instances of only one class. \\nEquation 6-3\\n shows thedefinition of the entropy of the i\\nth node. For example, the depth-2 left node in\\nFigure 6-1 has an entropy equal to \\n”4954log4954”554log554 Ÿ 0.31.Equation 6-3. Entropy\\nHi=”\\n“k=1\\npi,kł0\\nnpi,klogpi,kSo should you use Gini impurity or entropy? The truth is, most of the time it does not\\nmake a big difference: they lead to similar trees. Gini impurity is slightly faster to\\ncompute, so it is a good default. However, when they differ, Gini impurity tends to\\nisolate the most frequent class in its own branch of the tree, while entropy tends to\\nproduce slightly more balanced trees.\\n5Regularization HyperparametersDecision Trees \\nmake very few assumptions about the training data (as opposed to lin…\\near models, which obviously assume that the data is linear, for example). If left\\nunconstrained, the tree structure will adapt itself to the training data, fitting it very\\nclosely, and most likely overfitting it. Such a model is often called a \\nnonparametric\\nmodel\\n, not because it does not have any parameters (it often has a lot) but because the\\nnumber of parameters is not determined prior to training, so the model structure is\\nfree to stick closely to the data. In contrast, \\na parametric model\\n such as a linear modelhas a predetermined number of parameters, so its degree of freedom is limited,\\nreducing the risk of overfitting (but increasing the risk of underfitting).To avoid \\noverfitting the training data, you need to restrict the Decision Tree‡s freedom\\nduring training. As you know by now, this is called regularization. The regularization\\nhyperparameters depend on the algorithm used, but generally you can at least restrict\\nthe maximum depth of the Decision Tree. In Scikit-Learn, this is controlled by the\\nmax_depth hyperparameter (the default value is \\nNone, which means unlimited).Reducing max_depth will regularize the model and thus reduce the risk of overfitting.\\nThe DecisionTreeClassifier class has a few other parameters that similarly restrict\\nthe shape of the Decision Tree: \\nmin_samples_split (the minimum number of sam…\\nRegularization Hyperparameters | 173\\nples a node must have before it can be split), \\nmin_samples_leaf (the minimum num…\\nber of samples a leaf node must have), \\nmin_weight_fraction_leaf (same asmin_samples_leaf but expressed as a fraction of the total number of weighted\\ninstances), max_leaf_nodes (maximum number of leaf nodes), and \\nmax_features(maximum number of features that are evaluated for splitting at each node). Increas…\\ning min_* hyperparameters or reducing \\nmax_* hyperparameters will regularize the\\nmodel.Other algorithms work by first training the Decision Tree without\\nrestrictions, then pruning\\n (deleting) unnecessary nodes. A node\\nwhose children are all leaf nodes is considered unnecessary if the\\npurity improvement it provides is not \\nstatistically \\nsigni†cant. Stan…dard statistical tests, such as the \\nﬁ2 test\\n, are used to estimate the\\nprobability that the improvement is purely the result of chance\\n(which is called the null hypothesis\\n). If this probability, called the \\np-value\\n, is higher than a given threshold (typically 5%, controlled by\\na hyperparameter), then the node is considered unnecessary and its\\nchildren are deleted. The pruning continues until all unnecessary\\nnodes have been pruned.\\nFigure 6-3 shows two Decision Trees trained on the moons dataset (introduced in\\nChapter 5\\n). On the left, the Decision Tree is trained with the default hyperparameters\\n(i.e., no restrictions), and on the right the Decision Tree is trained with \\nmin_samples_leaf=4. It is quite obvious that the model on the left is overfitting, and the\\nmodel on the right will probably generalize better.\\nFigure 6-3. Regularization using min_samples_leaf\\n174 | Chapter 6: Decision Trees\\nRegressionDecision Trees are also capable of performing regression tasks. Let‡s build a regres…\\nsion tree using Scikit-Learn‡s \\nDecisionTreeRegressor class, training it on a noisyquadratic dataset with \\nmax_depth=2:from sklearn.tree import DecisionTreeRegressortree_reg = DecisionTreeRegressor(max_depth=2)tree_reg.fit(X, y)The resulting tree is represented on \\nFigure 6-4.Figure 6-4. A Decision Tree for regression\\nThis tree looks very similar to the classification tree you built earlier. The main differ…\\nence is that instead of predicting a class in each node, it predicts a value. For example,\\nsuppose you want to make a prediction for a new instance with \\nx1 = 0.6. You traverse\\nthe tree starting at the root, and you eventually reach the leaf node that predicts\\nvalue=0.1106. This prediction is simply the average target value of the 110 training\\ninstances associated to this leaf node. This prediction results in a Mean Squared Error\\n(MSE) equal to 0.0151 over these 110 instances.This model‡s predictions are represented on the left of \\nFigure 6-5. If you setmax_depth=3, you get the predictions represented on the right. Notice how the pre…\\ndicted value for each region is always the average target value of the instances in that\\nregion. The algorithm splits each region in a way that makes most training instances\\nas close as possible to that predicted value.\\nRegression | 175\\nFigure 6-5. Predictions of two Decision Tree regression models\\nThe CART algorithm works mostly the same way as earlier, except that instead of try…\\ning to split the training set in a way that minimizes impurity, it now tries to split the\\ntraining set in a way that minimizes the MSE. \\nEquation 6-4\\n shows the cost functionthat the algorithm tries to minimize.\\nEquation 6-4. CART cost function for regression\\nJk,tk=mleftmMSEleft+mrightmMSErightwhereMSEnode=“inodeynode”yi2ynode=1mnode“inodeyiJust like for classification tasks, Decision Trees are prone to overfitting when dealing\\nwith regression tasks. Without any regularization (i.e., using the default hyperpara…\\nmeters), you get the predictions on the left of Figure 6-6. It is obviously overfitting\\nthe training set very badly. Just setting \\nmin_samples_leaf=10 results in a much more\\nreasonable model, represented on the right of \\nFigure 6-6.Figure 6-6. Regularizing a Decision Tree regressor\\n176 | Chapter 6: Decision Trees\\n6It randomly selects the set of features to evaluate at each node.\\nInstabilityHopefully by now you are convinced that Decision Trees have a lot going for them:\\nthey are simple to understand and interpret, easy to use, versatile, and powerful.\\nHowever they do have a few limitations. First, as you may have noticed, Decision\\nTrees love orthogonal decision boundaries (all splits are perpendicular to an axis),\\nwhich makes them sensitive to training set rotation. For example, \\nFigure 6-7 shows asimple linearly separable dataset: on the left, a Decision Tree can split it easily, while\\non the right, after the dataset is rotated by 45‘, the decision boundary looks unneces…\\nsarily convoluted. Although both Decision Trees fit the training set perfectly, it is very\\nlikely that the model on the right will not generalize well. One way to limit this prob…\\nlem is to use PCA (see Chapter 8\\n), which often results in a better orientation of the\\ntraining data.\\nFigure 6-7. Sensitivity to training set rotation\\nMore generally, the main issue with Decision Trees is that they are very sensitive to\\nsmall variations in the training data. For example, if you just remove the widest Iris-\\nVersicolor from the iris training set (the one with petals 4.8 cm long and 1.8 cm wide)\\nand train a new Decision Tree, you may get the model represented in \\nFigure 6-8. Asyou can see, it looks very different from the previous Decision Tree (\\nFigure 6-2).Actually, since the training algorithm used by Scikit-Learn is stochastic\\n6 you may\\nget very different models even on the same training data (unless you set the\\nrandom_state hyperparameter).\\nInstability | 177\\nFigure 6-8. Sensitivity to training set details\\nRandom Forests \\ncan limit this instability by averaging predictions over many trees, as\\nwe will see in the next chapter.\\nExercises1.What is the approximate depth of a Decision Tree trained (without restrictions)\\non a training set with 1 million instances?2.Is a node‡s Gini impurity generally lower or greater than its parent‡s? Is it \\ngener…\\nally\\n lower/greater, or \\nalways\\n lower/greater?\\n3.If a Decision Tree is overfitting the training set, is it a good idea to try decreasing\\nmax_depth?4.If a Decision Tree is underfitting the training set, is it a good idea to try scaling\\nthe input features?\\n5.If it takes one hour to train a Decision Tree on a training set containing 1 million\\ninstances, roughly how much time will it take to train another Decision Tree on a\\ntraining set containing 10 million instances?\\n6.If your training set contains 100,000 instances, will setting \\npresort=True speedup training?7.Train and fine-tune a Decision Tree for the moons dataset.\\na.Generate a moons dataset using \\nmake_moons(n_samples=10000, noise=0.4).b.\\nSplit it into a training set and a test set using \\ntrain_test_split().178 | Chapter 6: Decision Trees\\nc.Use grid search with cross-validation (with the help of the \\nGridSearchCVclass) to find good hyperparameter values for a \\nDecisionTreeClassifier. Hint: try various values for \\nmax_leaf_nodes.d.Train it on the full training set using these hyperparameters, and measure\\nyour model‡s performance on the test set. You should get roughly 85% to 87%\\naccuracy.\\n8.Grow a forest.a.Continuing the previous exercise, generate 1,000 subsets of the training set,\\neach containing 100 instances selected randomly. Hint: you can use Scikit-\\nLearn‡s \\nShuffleSplit class for this.b.\\nTrain one Decision Tree on each subset, using the best hyperparameter values\\nfound above. Evaluate these 1,000 Decision Trees on the test set. Since they\\nwere trained on smaller sets, these Decision Trees will likely perform worse\\nthan the first Decision Tree, achieving only about 80% accuracy.\\nc.Now comes the magic. For each test set instance, generate the predictions of\\nthe 1,000 Decision Trees, and keep only the most frequent prediction (you can\\nuse SciPy‡s \\nmode() function for this). This gives you majority-vote predictions\\nover the test set.d.Evaluate these predictions on the test set: you should obtain a slightly higher\\naccuracy than your first model (about 0.5 to 1.5% higher). Congratulations,\\nyou have trained a Random Forest classifier!\\nSolutions to these exercises are available in \\nAppendix A\\n.Exercises | 179\\nCHAPTER 7Ensemble Learning and Random ForestsSuppose you ask a complex question to thousands of random people, then aggregate\\ntheir answers. In many cases you will find that this aggregated answer is better than\\nan expert‡s answer. This is called the \\nwisdom of the crowd\\n. Similarly, if you aggregate\\nthe predictions of a group of predictors (such as classifiers or regressors), you willoften get better predictions than with the best individual predictor. A group of pre…\\ndictors is called an ensemble\\n; thus, this technique is called \\nEnsemble Learning\\n, and anEnsemble Learning algorithm is called an Ensemble method\\n.For example, you can train a group of Decision Tree classifiers, each on a different\\nrandom subset of the training set. To make predictions, you just obtain the predic…\\ntions of all individual trees, then predict the class that gets the most votes (see the last\\nexercise in Chapter 6\\n). Such an ensemble of Decision Trees is called a \\nRandom Forest\\n, and despite its simplicity, this is one of the most powerful Machine Learning algo…\\nrithms available today.\\nMoreover, as we discussed in \\nChapter 2\\n, you will often use Ensemble methods nearthe end of a project, once you have already built a few good predictors, to combine\\nthem into an even better predictor. In fact, the winning solutions in Machine Learn…\\ning competitions often involve several Ensemble methods (most famously in the \\nNet…\\nflix Prize competition\\n).In this chapter we will discuss the most popular Ensemble methods, including \\nbag…\\nging\\n, boosting\\n, stacking\\n, and a few others. We will also explore Random Forests.\\nVoting Classi•ersSuppose you have trained a few classifiers, each one achieving about 80% accuracy.\\nYou may have a Logistic Regression classifier, an SVM classifier, a Random Forest\\nclassifier, a K-Nearest Neighbors classifier, and perhaps a few more (see \\nFigure 7-1).181Figure 7-1. Training diverse \\nclassi†ersA very simple way to create an even better classifier is to aggregate the predictions of\\neach classifier and predict the class that gets the most votes. This majority-vote classi…\\nfier is called a hard voting\\n classifier (see Figure 7-2).Figure 7-2. Hard voting \\nclassi†er predictions\\nSomewhat surprisingly, this voting classifier often achieves a higher accuracy than the\\nbest classifier in the ensemble. In fact, even if each classifier is a weak learner\\n (mean…ing it does only slightly better than random guessing), the ensemble can still be a\\nstrong learner\\n (achieving high accuracy), provided there are a sufficient number of\\nweak learners and they are sufficiently diverse.\\n182 | Chapter 7: Ensemble Learning and Random Forests\\nHow is this possible? The following analogy can help shed some light on this mystery.\\nSuppose you have a slightly biased coin that has a 51% chance of coming up heads,\\nand 49% chance of coming up tails. If you toss it 1,000 times, you will generally getmore or less 510 heads and 490 tails, and hence a majority of heads. If you do the\\nmath, you will find that the probability of obtaining a majority of heads after 1,000\\ntosses is close to 75%. The more you toss the coin, the higher the probability (e.g.,with 10,000 tosses, the probability climbs over 97%). This is due to the law of large\\nnumbers\\n: as you keep tossing the coin, the ratio of heads gets closer and closer to the\\nprobability of heads (51%). Figure 7-3 shows 10 series of biased coin tosses. You can\\nsee that as the number of tosses increases, the ratio of heads approaches 51%. Eventu…\\nally all 10 series end up so close to 51% that they are consistently above 50%.\\nFigure 7-3. \\n•e law of large numbers\\nSimilarly, suppose you build an ensemble containing 1,000 classifiers that are individ…\\nually correct only 51% of the time (barely better than random guessing). If you pre…dict the majority voted class, you can hope for up to 75% accuracy! However, this is\\nonly true if all classifiers are perfectly independent, making uncorrelated errors,\\nwhich is clearly not the case since they are trained on the same data. They are likely to\\nmake the same types of errors, so there will be many majority votes for the wrong\\nclass, reducing the ensemble‡s accuracy.\\nEnsemble methods work best when the predictors are as independ…ent from one another as possible. One way to get diverse classifiers\\nis to train them using very different algorithms. This increases the\\nchance that they will make very different types of errors, improving\\nthe ensemble‡s accuracy.\\nVoting Classi•ers | 183\\nThe following code creates and trains a voting classifier in Scikit-Learn, composed of\\nthree diverse classifiers (the training set is the moons dataset, \\nintroduced in \\nChap…\\nter 5):from sklearn.ensemble import RandomForestClassifierfrom sklearn.ensemble import VotingClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn.svm import SVClog_clf = LogisticRegression()rnd_clf = RandomForestClassifier()svm_clf = SVC()voting_clf = VotingClassifier(        estimators=[(•lr•, log_clf), (•rf•, rnd_clf), (•svc•, svm_clf)],        voting=•hard•    )voting_clf.fit(X_train, y_train)Let‡s look at each classifier‡s accuracy on the test set:\\n>>> from sklearn.metrics import accuracy_score>>> for clf in (log_clf, rnd_clf, svm_clf, voting_clf):>>>     clf.fit(X_train, y_train)>>>     y_pred = clf.predict(X_test)>>>     print(clf.__class__.__name__, accuracy_score(y_test, y_pred))LogisticRegression 0.864RandomForestClassifier 0.872SVC 0.888VotingClassifier 0.896There you have it! The voting classifier slightly outperforms all the individual classifi…\\ners.If all classifiers are able to estimate class probabilities (i.e., they have a \\npredict_proba() method), then you can tell Scikit-Learn to predict the class with thehighest class probability, averaged over all the individual classifiers. This is called \\nso“voting\\n. It often achieves higher performance than hard voting because it gives more\\nweight to highly confident votes. All you need to do is replace \\nvoting=\"hard\" withvoting=\"soft\" and ensure that all classifiers can estimate class probabilities. This is\\nnot the case of the SVC class by default, so you need to set its \\nprobability hyperpara…\\nmeter to True (this will make the \\nSVC class use cross-validation to estimate class prob…\\nabilities, slowing down training, and it will add a predict_proba() method). If youmodify the preceding code to use soft voting, you will find that the voting classifier\\nachieves over 91% accuracy!184 | Chapter 7: Ensemble Learning and Random Forests\\n1ƒBagging Predictors,⁄ L. Breiman (1996).\\n2In statistics, resampling with replacement is called \\nbootstrapping\\n.3ƒPasting small votes for classification in large databases and on-line,⁄ L. Breiman (1999).\\n4Bias and variance were introduced in \\nChapter 4\\n.Bagging and PastingOne way to get a diverse set of classifiers is to use very different training algorithms,\\nas just discussed. Another approach is to use the same training algorithm for every\\npredictor, but to train them on different random subsets of the training set. When\\nsampling is performed \\nwith\\n replacement, this method is called \\nbagging\\n1 (short forbootstrap aggregating\\n2). When sampling is performed \\nwithout\\n replacement, it is called\\npasting\\n.3In other words, both bagging and pasting allow training instances to be sampled sev…\\neral times across multiple predictors, but only bagging allows training instances to be\\nsampled several times for the same predictor. This sampling and training process is\\nrepresented in \\nFigure 7-4.Figure 7-4. Pasting/bagging training set sampling and training\\nOnce all predictors are trained, the ensemble can make a prediction for a newinstance by simply aggregating the predictions of all predictors. The aggregation\\nfunction is typically the statistical mode\\n (i.e., the most frequent prediction, just like a\\nhard voting classifier) for classification, or the average for regression. Each individual\\npredictor has a higher bias than if it were trained on the original training set, butaggregation reduces both bias and variance.\\n4 Generally, the net result is that the\\nBagging and Pasting | 185\\n5max_samples can alternatively be set to a float between 0.0 and 1.0, in which case the max number of instances\\nto sample is equal to the size of the training set times \\nmax_samples.ensemble has a similar bias but a lower variance than a single predictor trained on theoriginal training set.As you can see in Figure 7-4, predictors can all be trained in parallel, via different\\nCPU cores or even different servers. Similarly, predictions can be made in parallel.\\nThis is one of the reasons why bagging and pasting are such popular methods: they\\nscale very well.\\nBagging and Pasting in Scikit-LearnScikit-Learn offers a simple API for both bagging and pasting with the \\nBaggingClassifier class (or BaggingRegressor for regression). The following code trains anensemble of 500 Decision Tree classifiers,\\n5 each trained on 100 training instances ran…domly sampled from the training set with replacement (this is an example of bagging,\\nbut if you want to use pasting instead, just set \\nbootstrap=False). The n_jobs param…\\neter tells Scikit-Learn the number of CPU cores to use for training and predictions\\n(–1 tells Scikit-Learn to use all available cores):\\nfrom sklearn.ensemble import BaggingClassifierfrom sklearn.tree import DecisionTreeClassifierbag_clf = BaggingClassifier(        DecisionTreeClassifier(), n_estimators=500,        max_samples=100, bootstrap=True, n_jobs=-1    )bag_clf.fit(X_train, y_train)y_pred = bag_clf.predict(X_test)The BaggingClassifier automatically performs soft voting\\ninstead of hard voting if the base classifier can estimate class proba…\\nbilities (i.e., if it has a predict_proba() method), which is the casewith Decision Trees classifiers.\\nFigure 7-5 compares the decision boundary of a single Decision Tree with the deci…\\nsion boundary of a bagging ensemble of 500 trees (from the preceding code), both\\ntrained on the moons dataset. As you can see, the ensemble‡s predictions will likely\\ngeneralize much better than the single Decision Tree‡s predictions: the ensemble has a\\ncomparable bias but a smaller variance (it makes roughly the same number of errors\\non the training set, but the decision boundary is less irregular).\\n186 | Chapter 7: Ensemble Learning and Random Forests\\n6As m grows, this ratio approaches 1 – exp(–1) Ÿ 63.212%.\\nFigure 7-5. A single Decision Tree versus a bagging ensemble of 500 trees\\nBootstrapping introduces a bit more diversity in the subsets that each predictor is\\ntrained on, so bagging ends up with a slightly higher bias than pasting, but this also\\nmeans that predictors end up being less correlated so the ensemble‡s variance is\\nreduced. Overall, bagging often results in better models, which explains why it is gen…\\nerally preferred. However, if you have spare time and CPU power you can use cross-\\nvalidation to evaluate both bagging and pasting and select the one that works best.\\nOut-of-Bag EvaluationWith bagging, some instances may be sampled several times for any given predictor,\\nwhile others may not be sampled at all. By default a \\nBaggingClassifier samples \\nmtraining instances with replacement (\\nbootstrap=True), where m is the size of thetraining set. This means that only about 63% of the training instances are sampled on\\naverage for each predictor.\\n6 The remaining 37% of the training instances that are not\\nsampled are called \\nout-of-bag\\n (oob) instances. Note that they are not the same 37%\\nfor all predictors.Since a predictor never sees the oob instances during training, it can be evaluated on\\nthese instances, without the need for a separate validation set or cross-validation. You\\ncan evaluate the ensemble itself by averaging out the oob evaluations of each predic…\\ntor.\\nIn Scikit-Learn, you can set oob_score=True when creating a \\nBaggingClassifier torequest an automatic oob evaluation after training. The following code demonstrates\\nthis. The resulting evaluation score is available through the \\noob_score_ variable:>>> bag_clf = BaggingClassifier(>>>         DecisionTreeClassifier(), n_estimators=500,>>>         bootstrap=True, n_jobs=-1, oob_score=True)Bagging and Pasting | 187\\n7ƒEnsembles on Random Patches,⁄ G. Louppe and P. Geurts (2012).\\n8ƒThe random subspace method for constructing decision forests,⁄ Tin Kam Ho (1998).\\n>>> bag_clf.fit(X_train, y_train)>>> bag_clf.oob_score_0.93066666666666664According to this oob evaluation, this \\nBaggingClassifier is likely to achieve about93.1% accuracy on the test set. Let‡s verify this:\\n>>> from sklearn.metrics import accuracy_score>>> y_pred = bag_clf.predict(X_test)>>> accuracy_score(y_test, y_pred)0.93600000000000005We get 93.6% accuracy on the test set›close enough!\\nThe oob decision function for each training instance is also available through the\\noob_decision_function_ variable. In this case (since the base estimator has a \\npredict_proba() method) the decision function returns the class probabilities for eachtraining instance. For example, the oob evaluation estimates that the second training\\ninstance has a 60.6% probability of belonging to the positive class (and 39.4% ofbelonging to the positive class):>>> bag_clf.oob_decision_function_array([[ 0.        ,  1.        ],       [ 0.60588235,  0.39411765],       [ 1.        ,  0.        ],       ...       [ 1.        ,  0.        ],       [ 0.        ,  1.        ],       [ 0.48958333,  0.51041667]])Random Patches and Random SubspacesThe BaggingClassifier class supports sampling the features as well. This is con…\\ntrolled by two hyperparameters: \\nmax_features and \\nbootstrap_features. They workthe same way as \\nmax_samples and bootstrap, but for feature sampling instead of\\ninstance sampling. Thus, each predictor will be trained on a random subset of the\\ninput features.\\nThis is particularly useful when you are dealing with high-dimensional inputs (such\\nas images). Sampling both training instances and features is called the \\nRandom\\nPatches\\n method.7 Keeping all training instances (i.e., \\nbootstrap=False and max_samples=1.0) but sampling features (i.e., \\nbootstrap_features=True and/or max_features smaller than 1.0) is called the Random Subspaces\\n method.8188 | Chapter 7: Ensemble Learning and Random Forests\\n9ƒRandom Decision Forests,⁄ T. Ho (1995).\\n10The BaggingClassifier class remains useful if you want a bag of something other than Decision Trees.\\n11There are a few notable exceptions: splitter is absent (forced to \\n\"random\"), presort is absent (forced to\\nFalse), max_samples is absent (forced to \\n1.0), and base_estimator is absent (forced to \\nDecisionTreeClassifier with the provided hyperparameters).\\nSampling features results in even more predictor diversity, trading a bit more bias for\\na lower variance.Random ForestsAs we have discussed, a \\nRandom Forest\\n9 is an ensemble of Decision Trees, generally\\ntrained via the bagging method (or sometimes pasting), typically with max_samplesset to the size of the training set. Instead of building a BaggingClassifier and pass…ing it a DecisionTreeClassifier, you can instead use the RandomForestClassifierclass, which is more convenient and optimized for Decision Trees\\n10 (similarly, there is\\na RandomForestRegressor class for regression tasks). The following code trains aRandom Forest classifier with 500 trees (each limited to maximum 16 nodes), using\\nall available CPU cores:\\nfrom sklearn.ensemble import RandomForestClassifierrnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)rnd_clf.fit(X_train, y_train)y_pred_rf = rnd_clf.predict(X_test)With a few exceptions, a \\nRandomForestClassifier has all the hyperparameters of a\\nDecisionTreeClassifier (to control how trees are grown), plus all the hyperpara…\\nmeters of a BaggingClassifier to control the ensemble itself.\\n11The Random Forest algorithm introduces extra randomness when growing trees;\\ninstead of searching for the very best feature when splitting a node (see \\nChapter 6\\n), itsearches for the best feature among a random subset of features. This results in a\\ngreater tree diversity, which (once again) trades a higher bias for a lower variance,\\ngenerally yielding an overall better model. The following BaggingClassifier isroughly equivalent to the previous \\nRandomForestClassifier:bag_clf = BaggingClassifier(        DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),        n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1    )Random Forests | 189\\n12ƒExtremely randomized trees,⁄ P. Geurts, D. Ernst, L. Wehenkel (2005).\\nExtra-TreesWhen you are growing a tree in a Random Forest, at each node only a random subset\\nof the features is considered for splitting (as discussed earlier). It is possible to make\\ntrees even more random by also using random thresholds for each feature rather than\\nsearching for the best possible thresholds (like regular Decision Trees do).\\nA forest of such extremely random trees is simply called an \\nExtremely Randomized\\nTrees\\n ensemble12 (or Extra-Trees\\n for short). Once again, this trades more bias for alower variance. It also makes Extra-Trees much faster to train than regular Random\\nForests since finding the best possible threshold for each feature at every node is one\\nof the most time-consuming tasks of growing a tree.You can create an Extra-Trees classifier using Scikit-Learn‡s \\nExtraTreesClassifierclass. Its API is identical to the \\nRandomForestClassifier class. Similarly, the \\nExtraTreesRegressor class has the same API as the RandomForestRegressor class.It is hard to tell in advance whether a \\nRandomForestClassifierwill perform better or worse than an ExtraTreesClassifier. Gen…erally, the only way to know is to try both and compare them using\\ncross-validation (and tuning the hyperparameters using grid\\nsearch).Feature ImportanceLastly, if you look at a single Decision Tree, important features are likely to appear\\ncloser to the root of the tree, while unimportant features will often appear closer to\\nthe leaves (or not at all). It is therefore possible to get an estimate of a feature‡s impor…\\ntance by computing the average depth at which it appears across all trees in the forest.\\nScikit-Learn computes this automatically for every feature after training. You can\\naccess the result using the feature_importances_ variable. For example, the follow…\\ning code trains a RandomForestClassifier on the iris dataset (introduced in \\nChap…\\nter 4) and outputs each feature‡s importance. It seems that the most important\\nfeatures are the petal length (44%) and width (42%), while sepal length and width are\\nrather unimportant in comparison (11% and 2%, respectively):\\n>>> from sklearn.datasets import load_iris>>> iris = load_iris()>>> rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)>>> rnd_clf.fit(iris[\"data\"], iris[\"target\"])>>> for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):>>>     print(name, score)sepal length (cm) 0.112492250999190 | Chapter 7: Ensemble Learning and Random Forests\\n13ƒA Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting,⁄ Yoav Freund,\\nRobert E. Schapire (1997).\\nsepal width (cm) 0.0231192882825petal length (cm) 0.441030464364petal width (cm) 0.423357996355Similarly, if you train a Random Forest classifier on the MNIST dataset (introduced\\nin Chapter 3\\n) and plot each pixel‡s importance, you get the image represented in\\nFigure 7-6.Figure 7-6. MNIST pixel importance (according to a Random Forest \\nclassi†er)Random Forests are very handy to get a quick understanding of what features\\nactually matter, in particular if you need to perform feature selection.\\nBoostingBoosting\\n (originally called hypothesis boosting\\n) refers to any Ensemble method that\\ncan combine several weak learners into a strong learner. The general idea of most\\nboosting methods is to train predictors sequentially, each trying to correct its prede…\\ncessor. There are many boosting methods available, but by far the most popular are\\nAdaBoost\\n13 (short for Adaptive Boosting\\n) and Gradient Boosting\\n. Let‡s start with Ada…\\nBoost.\\nBoosting | 191\\n14This is just for illustrative purposes. SVMs are generally not good base predictors for AdaBoost, because they\\nare slow and tend to be unstable with AdaBoost.\\nAdaBoostOne way for a new predictor to correct its predecessor is to pay a bit more attention\\nto the training instances that the predecessor underfitted. This results in new predic…\\ntors focusing more and more on the hard cases. This is the technique used by Ada…\\nBoost.\\nFor example, to build an AdaBoost classifier, a first base classifier (such as a Decision\\nTree) is trained and used to make predictions on the training set. The relative weight\\nof misclassified training instances is then increased. A second classifier is trainedusing the updated weights and again it makes predictions on the training set, weights\\nare updated, and so on (see \\nFigure 7-7).Figure 7-7. AdaBoost sequential training with instance weight updates\\nFigure 7-8 shows the decision boundaries of five consecutive predictors on themoons dataset (in this example, each predictor is a highly regularized SVM classifier\\nwith an RBF kernel14). The first classifier gets many instances wrong, so their weights\\nget boosted. The second classifier therefore does a better job on these instances, andso on. The plot on the right represents the same sequence of predictors except that\\nthe learning rate is halved (i.e., the misclassified instance weights are boosted half as\\nmuch at every iteration). As you can see, this sequential learning technique has some\\nsimilarities with Gradient Descent, except that instead of tweaking a single predictor‡s\\n192 | Chapter 7: Ensemble Learning and Random Forests\\nparameters to minimize a cost function, AdaBoost adds predictors to the ensemble,\\ngradually making it better.\\nFigure 7-8. Decision boundaries of consecutive predictors\\nOnce all predictors are trained, the ensemble makes predictions very much like bag…\\nging or pasting, except that predictors have different weights depending on their\\noverall accuracy on the weighted training set.\\nThere is one important drawback to this sequential learning techni…\\nque: it cannot be parallelized (or only partially), since each predic…tor can only be trained after the previous predictor has beentrained and evaluated. As a result, it does not scale as well as bag…\\nging or pasting.Let‡s take a closer look at the AdaBoost algorithm. Each instance weight \\nw(i) is initially\\nset to 1m. A first predictor is trained and its weighted error rate \\nr1 is computed on the\\ntraining set; see Equation 7-1\\n.Equation 7-1. Weighted error rate of the j\\nth\\n predictor\\nrj=“i=1\\nyjiłyimwi“i=1\\nmwiwhereyjiisthe\\njthpredictor‡spredictionforthe\\nithinstance.Boosting | 193\\n15The original AdaBoost algorithm does not use a learning rate hyperparameter.\\nThe predictor‡s weight \\n‰j is then computed using \\nEquation 7-2\\n, where − is the learn…ing rate hyperparameter (defaults to 1).\\n15 The more accurate the predictor is, the\\nhigher its weight will be. If it is just guessing randomly, then its weight will be close to\\nzero. However, if it is most often wrong (i.e., less accurate than random guessing),\\nthen its weight will be negative.\\nEquation 7-2. Predictor weight\\n‰j=−log1”\\nrjrjNext the instance weights are updated using \\nEquation 7-3\\n: the misclassified instancesare boosted.Equation 7-3. Weight update rule\\nfori=1,2,\\n,mwiwiifyji=yiwiexp‰jifyjiłyiThen all the instance weights are normalized (i.e., divided by \\n“i=1\\nmwi).Finally, a new predictor is trained using the updated weights, and the whole process is\\nrepeated (the new predictor‡s weight is computed, the instance weights are updated,\\nthen another predictor is trained, and so on). The algorithm stops when the desirednumber of predictors is reached, or when a perfect predictor is found.\\nTo make predictions, AdaBoost simply computes the predictions of all the predictors\\nand weighs them using the predictor weights \\n‰j. The predicted class is the one that\\nreceives the majority of weighted votes (see \\nEquation 7-4\\n).Equation 7-4. AdaBoost predictions\\ny=argmax\\nk“j=1\\nyj=kN‰jwhereNisthenumberofpredictors.\\n194 | Chapter 7: Ensemble Learning and Random Forests\\n16For more details, see ƒMulti-Class AdaBoost,⁄ J. Zhu et al. (2006).\\n17First introduced in ƒArcing the Edge,⁄ L. Breiman (1997).\\nScikit-Learn actually uses a multiclass version of AdaBoost called \\nSAMME16 (whichstands for Stagewise Additive Modeling using a Multiclass Exponential loss function\\n).When there are just two classes, SAMME is equivalent to AdaBoost. Moreover, if the\\npredictors can estimate class probabilities (i.e., if they have a \\npredict_proba()method), Scikit-Learn can use a variant of SAMME called \\nSAMME.R (the R standsfor ƒReal⁄), which relies on class probabilities rather than predictions and generally\\nperforms better.\\nThe following code trains an AdaBoost classifier based on 200 \\nDecision Stumps\\n usingScikit-Learn‡s \\nAdaBoostClassifier class (as you might expect, there is also an \\nAdaBoostRegressor class). A Decision Stump is a Decision Tree with \\nmax_depth=1›inother words, a tree composed of a single decision node plus two leaf nodes. This is\\nthe default base estimator for the \\nAdaBoostClassifier class:from sklearn.ensemble import AdaBoostClassifierada_clf = AdaBoostClassifier(        DecisionTreeClassifier(max_depth=1), n_estimators=200,        algorithm=\"SAMME.R\", learning_rate=0.5    )ada_clf.fit(X_train, y_train)If your AdaBoost ensemble is overfitting the training set, you can\\ntry reducing the number of estimators or more strongly regulariz…\\ning the base estimator.\\nGradient BoostingAnother very popular Boosting algorithm is \\nGradient Boosting\\n.17 Just like AdaBoost,\\nGradient Boosting works by sequentially adding predictors to an ensemble, each one\\ncorrecting its predecessor. However, instead of tweaking the instance weights at every\\niteration like AdaBoost does, this method tries to fit the new predictor to the \\nresidual\\nerrors\\n made by the previous predictor.\\nLet‡s go through a simple regression example using Decision Trees as the base predic…\\ntors (of course Gradient Boosting also works great with regression tasks). This is\\ncalled Gradient Tree Boosting\\n, or Gradient Boosted Regression Trees\\n (GBRT\\n). First, let‡s\\nfit a DecisionTreeRegressor to the training set (for example, a noisy quadratic train…\\ning set):Boosting | 195\\nfrom sklearn.tree import DecisionTreeRegressortree_reg1 = DecisionTreeRegressor(max_depth=2)tree_reg1.fit(X, y)Now train a second \\nDecisionTreeRegressor on the residual errors made by the firstpredictor:\\ny2 = y - tree_reg1.predict(X)tree_reg2 = DecisionTreeRegressor(max_depth=2)tree_reg2.fit(X, y2)Then we train a third regressor on the residual errors made by the second predictor:\\ny3 = y2 - tree_reg2.predict(X)tree_reg3 = DecisionTreeRegressor(max_depth=2)tree_reg3.fit(X, y3)Now we have an ensemble containing three trees. It can make predictions on a new\\ninstance simply by adding up the predictions of all the trees:\\ny_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))Figure 7-9 represents the predictions of these three trees in the left column, and the\\nensemble‡s predictions in the right column. In the first row, the ensemble has just one\\ntree, so its predictions are exactly the same as the first tree‡s predictions. In the second\\nrow, a new tree is trained on the residual errors of the first tree. On the right you can\\nsee that the ensemble‡s predictions are equal to the sum of the predictions of the first\\ntwo trees. Similarly, in the third row another tree is trained on the residual errors of\\nthe second tree. You can see that the ensemble‡s predictions gradually get better as\\ntrees are added to the ensemble.A simpler way to train GBRT ensembles is to use Scikit-Learn‡s \\nGradientBoostingRegressor class. Much like the \\nRandomForestRegressor class, it has hyperparameters to\\ncontrol the growth of Decision Trees (e.g., \\nmax_depth, min_samples_leaf, and so on),as well as hyperparameters to control the ensemble training, such as the number of\\ntrees (n_estimators). The following code creates the same ensemble as the previous\\none:from sklearn.ensemble import GradientBoostingRegressorgbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)gbrt.fit(X, y)196 | Chapter 7: Ensemble Learning and Random Forests\\nFigure 7-9. Gradient Boosting\\nThe learning_rate hyperparameter scales the contribution of each tree. If you set it\\nto a low value, such as 0.1, you will need more trees in the ensemble to fit the train…ing set, but the predictions will usually generalize better. This is a regularization tech…\\nnique called shrinkage\\n. Figure 7-10 shows two GBRT ensembles trained with a low\\nlearning rate: the one on the left does not have enough trees to fit the training set,\\nwhile the one on the right has too many trees and overfits the training set.\\nBoosting | 197\\nFigure 7-10. GBRT ensembles with not enough predictors \\n(le“) and too many (right)\\nIn order to find the optimal number of trees, you can use early stopping (see \\nChap…\\nter 4). A simple way to implement this is to use the \\nstaged_predict() method: itreturns an iterator over the predictions made by the ensemble at each stage of train…\\ning (with one tree, two trees, etc.). The following code trains a GBRT ensemble with\\n120 trees, then measures the validation error at each stage of training to find the opti…\\nmal number of trees, and finally trains another GBRT ensemble using the optimal\\nnumber of trees:\\nimport numpy as npfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import mean_squared_errorX_train, X_val, y_train, y_val = train_test_split(X, y)gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)gbrt.fit(X_train, y_train)errors = [mean_squared_error(y_val, y_pred)          for y_pred in gbrt.staged_predict(X_val)]bst_n_estimators = np.argmin(errors)gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators)gbrt_best.fit(X_train, y_train)The validation errors are represented on the left of \\nFigure 7-11, and the best model‡s\\npredictions are represented on the right.\\n198 | Chapter 7: Ensemble Learning and Random Forests\\nFigure 7-11. Tuning the number of trees using early stopping\\nIt is also possible to implement early stopping by actually stopping training early\\n(instead of training a large number of trees first and then looking back to find the\\noptimal number). You can do so by setting \\nwarm_start=True, which makes Scikit-Learn keep existing trees when the fit() method is called, allowing incremental\\ntraining. The following code stops training when the validation error does not\\nimprove for five iterations in a row:\\ngbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)min_val_error = float(\"inf\")error_going_up = 0for n_estimators in range(1, 120):    gbrt.n_estimators = n_estimators    gbrt.fit(X_train, y_train)    y_pred = gbrt.predict(X_val)    val_error = mean_squared_error(y_val, y_pred)    if val_error < min_val_error:        min_val_error = val_error        error_going_up = 0    else:        error_going_up += 1        if error_going_up == 5:            break  # early stoppingThe GradientBoostingRegressor class also supports a subsample hyperparameter,\\nwhich specifies the fraction of training instances to be used for training each tree. For\\nexample, if \\nsubsample=0.25, then each tree is trained on 25% of the training instan…ces, selected randomly. As you can probably guess by now, this trades a higher bias\\nfor a lower variance. It also speeds up training considerably. This technique is called\\nStochastic Gradient Boosting\\n.Boosting | 199\\n18ƒStacked Generalization,⁄ D. Wolpert (1992).\\n19Alternatively, it is possible to use out-of-fold predictions. In some contexts this is called \\nstacking\\n, while using ahold-out set is called blending\\n. However, for many people these terms are synonymous.\\nIt is possible to use Gradient Boosting with other cost functions.\\nThis is controlled by the \\nloss hyperparameter (see Scikit-Learn‡s\\ndocumentation for more details).\\nStackingThe last Ensemble method we will discuss in this chapter is called \\nstacking\\n (short forstacked generalization\\n).18 It is based on a simple idea: instead of using trivial functions\\n(such as hard voting) to aggregate the predictions of all predictors in an ensemble,\\nwhy don‡t we train a model to perform this aggregation? \\nFigure 7-12 shows such anensemble performing a regression task on a new instance. Each of the bottom threepredictors predicts a different value (3.1, 2.7, and 2.9), and then the final predictor \\n(called a blender\\n, or a meta learner\\n) takes these predictions as inputs and makes the\\nfinal prediction (3.0).Figure 7-12. Aggregating predictions using a blending predictor\\nTo train the blender, a common approach is to use a \\nhold-out set.19 Let‡s see how it\\nworks. First, the training set is split in two subsets. The first subset is used to train thepredictors in the first layer (see \\nFigure 7-13).200 | Chapter 7: Ensemble Learning and Random Forests\\nFigure 7-13. Training the \\n†rst layer\\nNext, the first layer predictors are used to make predictions on the second (held-out)\\nset (see Figure 7-14). This ensures that the predictions are ƒclean,⁄ since the predictors\\nnever saw these instances during training. Now for each instance in the hold-out set\\nthere are three predicted values. We can create a new training set using these predic…\\nted values as input features (which makes this new training set three-dimensional),\\nand keeping the target values. The blender is trained on this new training set, so itlearns to predict the target value given the first layer‡s predictions.\\nFigure 7-14. Training the blender\\nStacking | 201\\nIt is actually possible to train several different blenders this way (e.g., one using Lin…\\near Regression, another using Random Forest Regression, and so on): we get a whole\\nlayer of blenders. The trick is to split the training set into three subsets: the first one is\\nused to train the first layer, the second one is used to create the training set used to\\ntrain the second layer (using predictions made by the predictors of the first layer),\\nand the third one is used to create the training set to train the third layer (using pre…\\ndictions made by the predictors of the second layer). Once this is done, we can make\\na prediction for a new instance by going through each layer sequentially, as shown in\\nFigure 7-15.Figure 7-15. Predictions in a multilayer stacking ensemble\\nUnfortunately, Scikit-Learn does not support stacking directly, but it is not too hard\\nto roll out your own implementation (see the following exercises). Alternatively, you\\ncan use an open source implementation such as \\nbrew (available at \\nhttps://github.com/\\nviisar/brew\\n).Exercises1.If you have trained five different models on the exact same training data, and\\nthey all achieve 95% precision, is there any chance that you can combine these\\nmodels to get better results? If so, how? If not, why?\\n2.What is the difference between hard and soft voting classifiers?\\n202 | Chapter 7: Ensemble Learning and Random Forests\\n3.Is it possible to speed up training of a bagging ensemble by distributing it across\\nmultiple servers? What about pasting ensembles, boosting ensembles, random\\nforests, or stacking ensembles?4.What is the benefit of out-of-bag evaluation?\\n5.What makes Extra-Trees more random than regular Random Forests? How can\\nthis extra randomness help? Are Extra-Trees slower or faster than regular Ran…\\ndom Forests?\\n6.If your AdaBoost ensemble underfits the training data, what hyperparameters\\nshould you tweak and how?7.If your Gradient Boosting ensemble overfits the training set, should you increase\\nor decrease the learning rate?\\n8.Load the MNIST data (introduced in \\nChapter 3\\n), and split it into a training set, a\\nvalidation set, and a test set (e.g., use the first 40,000 instances for training, the\\nnext 10,000 for validation, and the last 10,000 for testing). Then train various\\nclassifiers, such as a Random Forest classifier, an Extra-Trees classifier, and an\\nSVM. Next, try to combine them into an ensemble that outperforms them all on\\nthe validation set, using a soft or hard voting classifier. Once you have found one,\\ntry it on the test set. How much better does it perform compared to the individ…\\nual classifiers?9.Run the individual classifiers from the previous exercise to make predictions on\\nthe validation set, and create a new training set with the resulting predictions:\\neach training instance is a vector containing the set of predictions from all your\\nclassifiers for an image, and the target is the image‡s class. Congratulations, you\\nhave just trained a blender, and together with the classifiers they form a stacking\\nensemble! Now let‡s evaluate the ensemble on the test set. For each image in the\\ntest set, make predictions with all your classifiers, then feed the predictions to theblender to get the ensemble‡s predictions. How does it compare to the voting clas…\\nsifier you trained earlier?Solutions to these exercises are available in \\nAppendix A\\n.Exercises | 203\\nCHAPTER 8Dimensionality ReductionMany Machine Learning problems involve thousands or even millions of features for\\neach training instance. Not only does this make training extremely slow, it can also\\nmake it much harder to find a good solution, as we will see. This problem is often\\nreferred to as the curse of dimensionality\\n.Fortunately, in real-world problems, it is often possible to reduce the number of fea…\\ntures considerably, turning an intractable problem into a tractable one. For example,\\nconsider the MNIST images (introduced in \\nChapter 3\\n): the pixels on the image bor…ders are almost always white, so you could completely drop these pixels from the\\ntraining set without losing much information. \\nFigure 7-6 confirms that these pixels\\nare utterly unimportant for the classification task. Moreover, two neighboring pixels\\nare often highly correlated: if you merge them into a single pixel (e.g., by taking the\\nmean of the two pixel intensities), you will not lose much information.\\nReducing dimensionality does lose some information (just like\\ncompressing an image to JPEG can degrade its quality), so even\\nthough it will speed up training, it may also make your system per…\\nform slightly worse. It also makes your pipelines a bit more com…\\nplex and thus harder to maintain. So you should first try to train\\nyour system with the original data before considering using dimen…\\nsionality reduction if training is too slow. In some cases, however,\\nreducing the dimensionality of the training data may filter out\\nsome noise and unnecessary details and thus result in higher per…\\nformance (but in general it won‡t; it will just speed up training).\\nApart from speeding up training, dimensionality reduction is also extremely useful\\nfor data visualization (or \\nDataViz\\n). Reducing the number of dimensions down to two\\n2051Well, four dimensions if you count time, and a few more if you are a string theorist.\\n2Watch a rotating tesseract projected into 3D space at \\nhttp://goo.gl/OM7ktJ\\n. Image by Wikipedia user Nerd…\\nBoy1392 (\\nCreative Commons BY-SA 3.0\\n). Reproduced from https://en.wikipedia.org/wiki/Tesseract\\n.3Fun fact: anyone you know is probably an extremist in at least one dimension (e.g., how much sugar they put\\nin their coffee), if you consider enough dimensions.(or three) makes it possible to plot a high-dimensional training set on a graph and\\noften gain some important insights by visually detecting patterns, such as clusters.\\nIn this chapter we will discuss the curse of dimensionality and get a sense of what\\ngoes on in high-dimensional space. Then, we will present the two main approaches to\\ndimensionality reduction (projection and Manifold Learning), and we will go\\nthrough three of the most popular dimensionality reduction techniques: PCA, Kernel\\nPCA, and LLE.The Curse of DimensionalityWe are so used to living in three dimensions\\n1 that our intuition fails us when we try\\nto imagine a high-dimensional space. Even a basic 4D hypercube is incredibly hard to\\npicture in our mind (see Figure 8-1), let alone a 200-dimensional ellipsoid bent in a\\n1,000-dimensional space.Figure 8-1. Point, segment, square, cube, and tesseract (0D to 4D hypercubes)\\n2It turns out that many things behave very differently in high-dimensional space. For\\nexample, if you pick a random point in a unit square (a 1 ‰ 1 square), it will have only\\nabout a 0.4% chance of being located less than 0.001 from a border (in other words, it\\nis very unlikely that a random point will be ƒextreme⁄ along any dimension). But in a\\n10,000-dimensional unit hypercube (a 1 ‰ 1 ‰ \\n ‰ 1 cube, with ten thousand 1s), thisprobability is greater than 99.999999%. Most points in a high-dimensional hypercube\\nare very close to the border.\\n3206 | Chapter 8: Dimensionality Reduction\\nHere is a more troublesome difference: if you pick two points randomly in a unit\\nsquare, the distance between these two points will be, on average, roughly 0.52. If you\\npick two random points in a unit 3D cube, the average distance will be roughly 0.66.\\nBut what about two points picked randomly in a 1,000,000-dimensional hypercube?\\nWell, the average distance, believe it or not, will be about 408.25 (roughly\\n1,000,000/6\\n)! This is quite counterintuitive: how can two points be so far apart\\nwhen they both lie within the same unit hypercube? This fact implies that high-\\ndimensional datasets are at risk of being very sparse: most training instances are\\nlikely to be far away from each other. Of course, this also means that a new instance\\nwill likely be far away from any training instance, making predictions much less relia…\\nble than in lower dimensions, since they will be based on much larger extrapolations.\\nIn short, the more dimensions the training set has, the greater the risk of overfitting\\nit.In theory, one solution to the curse of dimensionality could be to increase the size of\\nthe training set to reach a sufficient density of training instances. Unfortunately, in\\npractice, the number of training instances required to reach a given density grows\\nexponentially with the number of dimensions. With just 100 features (much less than\\nin the MNIST problem), you would need more training instances than atoms in the\\nobservable universe in order for training instances to be within 0.1 of each other on\\naverage, assuming they were spread out uniformly across all dimensions.\\nMain Approaches for Dimensionality ReductionBefore we dive into specific dimensionality reduction algorithms, let‡s take a look at\\nthe two main approaches to reducing dimensionality: projection and Manifold\\nLearning.ProjectionIn most real-world problems, training instances are not\\n spread out uniformly acrossall dimensions. Many features are almost constant, while others are highly correlated\\n(as discussed earlier for MNIST). As a result, all training instances actually lie within(or close to) a much lower-dimensional \\nsubspace\\n of the high-dimensional space. Thissounds very abstract, so let‡s look at an example. In \\nFigure 8-2 you can see a 3D data…\\nset represented by the circles.\\nMain Approaches for Dimensionality Reduction | 207\\nFigure 8-2. A 3D dataset lying close to a 2D subspace\\nNotice that all training instances lie close to a plane: this is a lower-dimensional (2D)\\nsubspace of the high-dimensional (3D) space. Now if we project every training\\ninstance perpendicularly onto this subspace (as represented by the short lines con…\\nnecting the instances to the plane), we get the new 2D dataset shown in \\nFigure 8-3.Ta-da! We have just reduced the dataset‡s dimensionality from 3D to 2D. Note that\\nthe axes correspond to new features \\nz1 and z2 (the coordinates of the projections on\\nthe plane).Figure 8-3. \\n•e new 2D dataset \\na“er projection\\n208 | Chapter 8: Dimensionality Reduction\\nHowever, projection is not always the best approach to dimensionality reduction. In\\nmany cases the subspace may twist and turn, such as in the famous \\nSwiss roll\\n toy data…\\nset represented in \\nFigure 8-4.Figure 8-4. Swiss roll dataset\\nSimply projecting onto a plane (e.g., by dropping \\nx3) would squash different layers of\\nthe Swiss roll together, as shown on the left of \\nFigure 8-5. However, what you really\\nwant is to unroll the Swiss roll to obtain the 2D dataset on the right of \\nFigure 8-5.Figure 8-5. Squashing by projecting onto a plane \\n(le“) versus unrolling the Swiss roll\\n(right)\\nMain Approaches for Dimensionality Reduction | 209\\nManifold LearningThe Swiss roll is an example of a 2D \\nmanifold\\n. Put simply, a 2D manifold is a 2D\\nshape that can be bent and twisted in a higher-dimensional space. More generally, a\\nd-dimensional manifold is a part of an n-dimensional space (where d < \\nn) that locally\\nresembles a d-dimensional hyperplane. In the case of the Swiss roll, \\nd = 2 and \\nn = 3: it\\nlocally resembles a 2D plane, but it is rolled in the third dimension.Many dimensionality reduction algorithms work by modeling the \\nmanifold\\n on whichthe training instances lie; this is called Manifold Learning\\n. It relies on the \\nmanifold\\nassumption\\n, also called the manifold hypothesis\\n, which holds that most real-world\\nhigh-dimensional datasets lie close to a much lower-dimensional manifold. This\\nassumption is very often empirically observed.\\nOnce again, think about the MNIST dataset: all handwritten digit images have some\\nsimilarities. They are made of connected lines, the borders are white, they are moreor less centered, and so on. If you randomly generated images, only a ridiculously\\ntiny fraction of them would look like handwritten digits. In other words, the degrees\\nof freedom available to you if you try to create a digit image are dramatically lower\\nthan the degrees of freedom you would have if you were allowed to generate any\\nimage you wanted. These constraints tend to squeeze the dataset into a lower-\\ndimensional manifold.The manifold assumption is often accompanied by another implicit assumption: that\\nthe task at hand (e.g., classification or regression) will be simpler if expressed in the\\nlower-dimensional space of the manifold. For example, in the top row of \\nFigure 8-6the Swiss roll is split into two classes: in the 3D space (on the left), the decision\\nboundary would be fairly complex, but in the 2D unrolled manifold space (on the\\nright), the decision boundary is a simple straight line.\\nHowever, this assumption does not always hold. For example, in the bottom row of\\nFigure 8-6, the decision boundary is located at \\nx1 = 5. This decision boundary looks\\nvery simple in the original 3D space (a vertical plane), but it looks more complex in\\nthe unrolled manifold (a collection of four independent line segments).\\nIn short, if you reduce the dimensionality of your training set before training amodel, it will definitely speed up training, but it may not always lead to a better or\\nsimpler solution; it all depends on the dataset.\\nHopefully you now have a good sense of what the curse of dimensionality is and how\\ndimensionality reduction algorithms can fight it, especially when the manifold\\nassumption holds. The rest of this chapter will go through some of the most popular\\nalgorithms.210 | Chapter 8: Dimensionality Reduction\\nFigure 8-6. \\n•e decision boundary may not always be simpler with lower dimensions\\nPCAPrincipal Component Analysis\\n (PCA) is by far the most popular dimensionality reduc…\\ntion algorithm. First it identifies the hyperplane that lies closest to the data, and then\\nit projects the data onto it.\\nPreserving the VarianceBefore you can project the training set onto a lower-dimensional hyperplane, you\\nfirst need to choose the right hyperplane. For example, a simple 2D dataset is repre…\\nsented on the left of \\nFigure 8-7, along with three different axes (i.e., one-dimensional\\nhyperplanes). On the right is the result of the projection of the dataset onto each of\\nthese axes. As you can see, the projection onto the solid line preserves the maximum\\nvariance, while the projection onto the dotted line preserves very little variance, and\\nthe projection onto the dashed line preserves an intermediate amount of variance.\\nPCA | 211\\n4ƒOn Lines and Planes of Closest Fit to Systems of Points in Space,⁄ K. Pearson (1901).\\nFigure 8-7. Selecting the subspace onto which to project\\nIt seems reasonable to select the axis that preserves the maximum amount of var…\\niance, as it will most likely lose less information than the other projections. Another\\nway to justify this choice is that it is the axis that minimizes the mean squared dis…\\ntance between the original dataset and its projection onto that axis. This is the rather\\nsimple idea behind \\nPCA.4Principal ComponentsPCA identifies the axis that accounts for the largest amount of variance in the train…\\ning set. In Figure 8-7, it is the solid line. It also finds a second axis, orthogonal to the\\nfirst one, that accounts for the largest amount of remaining variance. In this 2D\\nexample there is no choice: it is the dotted line. If it were a higher-dimensional data…\\nset, PCA would also find a third axis, orthogonal to both previous axes, and a fourth,a fifth, and so on›as many axes as the number of dimensions in the dataset.\\nThe unit vector that defines the i\\nth axis is called the ith principal component\\n (PC). InFigure 8-7, the 1st PC is c1 and the 2nd PC is c2. In Figure 8-2 the first two PCs arerepresented by the orthogonal arrows in the plane, and the third PC would be\\northogonal to the plane (pointing up or down).\\n212 | Chapter 8: Dimensionality Reduction\\nThe direction of the principal components is not stable: if you per…\\nturb the training set slightly and run PCA again, some of the new\\nPCs may point in the opposite direction of the original PCs. How…\\never, they will generally still lie on the same axes. In some cases, a\\npair of PCs may even rotate or swap, but the plane they define will\\ngenerally remain the same.So how can you find the principal components of a training set? Luckily, there is a\\nstandard matrix factorization technique called \\nSingular Value Decomposition\\n (SVD)that can decompose the training set matrix \\nX into the dot product of three matrices \\nU’ œ ’ VT, where VT contains all the principal components that we are looking for, as\\nshown in Equation 8-1\\n.Equation 8-1. Principal components matrix\\nT=\\n12\\n\\nThe following Python code uses NumPy‡s \\nsvd() function to obtain all the principalcomponents of the training set, then extracts the first two PCs:\\nX_centered = X - X.mean(axis=0)U, s, V = np.linalg.svd(X_centered)c1 = V.T[:, 0]c2 = V.T[:, 1]PCA assumes that the dataset is centered around the origin. As we\\nwill see, Scikit-Learn‡s PCA classes take care of centering the data\\nfor you. However, if you implement PCA yourself (as in the pre…\\nceding example), or if you use other libraries, don‡t forget to center\\nthe data first.\\nProjecting Down to d DimensionsOnce you have identified all the principal components, you can reduce the dimen…\\nsionality of the dataset down to \\nd dimensions by projecting it onto the \\nhyperplane\\ndefined by the first d principal components. Selecting this hyperplane ensures that the\\nprojection will preserve as much variance as possible. For example, in \\nFigure 8-2 the3D dataset is projected down to the 2D plane defined by the first two principal com…\\nponents, preserving a large part of the dataset‡s variance. As a result, the 2D projec…\\ntion looks very much like the original 3D dataset.\\nTo project the training set onto the hyperplane, you can simply compute the dot\\nproduct of the training set matrix \\nX by the matrix \\nWd, defined as the matrix contain…\\nPCA | 213\\ning the first d principal components (i.e., the matrix composed of the first \\nd columnsof VT), as shown in Equation 8-2\\n.Equation 8-2. Projecting the training set down to d dimensions\\nd…proj=’dThe following Python code projects the training set onto the plane defined by the first\\ntwo principal components:\\nW2 = V.T[:, :2]X2D = X_centered.dot(W2)There you have it! You now know how to reduce the dimensionality of any dataset\\ndown to any number of dimensions, while preserving as much variance as possible.\\nUsing Scikit-LearnScikit-Learn‡s \\nPCA class implements PCA using SVD decomposition just like we did\\nbefore. The following code applies PCA to reduce the dimensionality of the dataset\\ndown to two dimensions (note that it automatically takes care of centering the data):\\nfrom sklearn.decomposition import PCApca = PCA(n_components = 2)X2D = pca.fit_transform(X)After fitting the PCA transformer to the dataset, you can access the principal compo…\\nnents using the \\ncomponents_ variable (note that it contains the PCs as horizontal vec…\\ntors, so, for example, the first principal component is equal to \\npca.components_.T[:,0]).Explained Variance RatioAnother very useful piece of information is the \\nexplained variance ratio\\n of each prin…cipal component, available via the \\nexplained_variance_ratio_ variable. It indicates\\nthe proportion of the dataset‡s variance that lies along the axis of each principal com…\\nponent. For example, let‡s look at the explained variance ratios of the first two compo…\\nnents of the 3D dataset represented in \\nFigure 8-2:>>> print(pca.explained_variance_ratio_)array([ 0.84248607,  0.14631839])This tells you that 84.2% of the dataset‡s variance lies along the first axis, and 14.6%\\nlies along the second axis. This leaves less than 1.2% for the third axis, so it is reason…\\nable to assume that it probably carries little information.\\n214 | Chapter 8: Dimensionality Reduction\\nChoosing the Right Number of DimensionsInstead of arbitrarily choosing the number of dimensions to reduce down to, it is\\ngenerally preferable to choose the number of dimensions that add up to a sufficiently\\nlarge portion of the variance (e.g., 95%). Unless, of course, you are reducing dimen…\\nsionality for data visualization›in that case you will generally want to reduce the\\ndimensionality down to 2 or 3.The following code computes PCA without reducing dimensionality, then computes\\nthe minimum number of dimensions required to preserve 95% of the training set‡s\\nvariance:pca = PCA()pca.fit(X)cumsum = np.cumsum(pca.explained_variance_ratio_)d = np.argmax(cumsum >= 0.95) + 1You could then set \\nn_components=d and run PCA again. However, there is a much\\nbetter option: instead of specifying the number of principal components you want to\\npreserve, you can set \\nn_components to be a float between \\n0.0 and 1.0, indicating the\\nratio of variance you wish to preserve:\\npca = PCA(n_components=0.95)X_reduced = pca.fit_transform(X)Yet another option is to plot the explained variance as a function of the number of\\ndimensions (simply plot \\ncumsum; see Figure 8-8). There will usually be an elbow in thecurve, where the explained variance stops growing fast. You can think of this as the\\nintrinsic dimensionality of the dataset. In this case, you can see that reducing the\\ndimensionality down to about 100 dimensions wouldn‡t lose too much explained var…\\niance.Figure 8-8. Explained variance as a function of the number of dimensions\\nPCA | 215\\nPCA for CompressionObviously after dimensionality reduction, the training set takes up much less space.\\nFor example, try applying PCA to the MNIST dataset while preserving 95% of its var…\\niance. You should find that each instance will have just over 150 features, instead of\\nthe original 784 features. So while most of the variance is preserved, the dataset is\\nnow less than 20% of its original size! This is a reasonable compression ratio, and you\\ncan see how this can speed up a classification algorithm (such as an SVM classifier)\\ntremendously.\\nIt is also possible to decompress the reduced dataset back to 784 dimensions by\\napplying the inverse transformation of the PCA projection. Of course this won‡t give\\nyou back the original data, since the projection lost a bit of information (within the\\n5% variance that was dropped), but it will likely be quite close to the original data.\\nThe mean squared distance between the original data and the reconstructed data\\n(compressed and then decompressed) is called the \\nreconstruction error\\n. For example,\\nthe following code compresses the MNIST dataset down to 154 dimensions, then uses\\nthe inverse_transform() method to decompress it back to 784 dimensions.\\nFigure 8-9 shows a few digits from the original training set (on the left), and the cor…responding digits after compression and decompression. You can see that there is a\\nslight image quality loss, but the digits are still mostly intact.\\npca = PCA(n_components = 154)X_mnist_reduced = pca.fit_transform(X_mnist)X_mnist_recovered = pca.inverse_transform(X_mnist_reduced)Figure 8-9. MNIST compression preserving 95% of the variance\\n216 | Chapter 8: Dimensionality Reduction\\n5Scikit-Learn uses the algorithm described in ƒIncremental Learning for Robust Visual Tracking,⁄ D. Ross et al.\\n(2007).The equation of the inverse transformation is shown in \\nEquation 8-3\\n.Equation 8-3. PCA inverse transformation, back to the original number of\\ndimensions\\nrecovered=d…proj’dTIncremental PCAOne problem with the preceding implementation of PCA is that it requires the whole\\ntraining set to fit in memory in order for the SVD algorithm to run. Fortunately,\\nIncremental PCA\\n (IPCA) algorithms have been developed: you can split the training\\nset into mini-batches and feed an IPCA algorithm one mini-batch at a time. This is\\nuseful for large training sets, and also to apply PCA online (i.e., on the fly, as new\\ninstances arrive).The following code splits the MNIST dataset into 100 mini-batches (using NumPy‡s\\narray_split() function) and feeds them to Scikit-Learn‡s \\nIncrementalPCA class5 to reduce the dimensionality of the MNIST dataset down to 154 dimensions (just like\\nbefore). Note that you must call the \\npartial_fit() method with each mini-batch\\nrather than the \\nfit() method with the whole training set:from sklearn.decomposition import IncrementalPCAn_batches = 100inc_pca = IncrementalPCA(n_components=154)for X_batch in np.array_split(X_mnist, n_batches):    inc_pca.partial_fit(X_batch)X_mnist_reduced = inc_pca.transform(X_mnist)Alternatively, you can use NumPy‡s \\nmemmap class, which allows you to manipulate a\\nlarge array stored in a binary file on disk as if it were entirely in memory; the class\\nloads only the data it needs in memory, when it needs it. Since the \\nIncrementalPCAclass uses only a small part of the array at any given time, the memory usage remains\\nunder control. This makes it possible to call the usual \\nfit() method, as you can seein the following code:X_mm = np.memmap(filename, dtype=\"float32\", mode=\"readonly\", shape=(m, n))batch_size = m // n_batchesinc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)inc_pca.fit(X_mm)PCA | 217\\n6ƒKernel Principal Component Analysis,⁄ B. Schšlkopf, A. Smola, K. M™ller (1999).\\nRandomized PCAScikit-Learn offers yet another option to perform PCA, called Randomized PCA\\n. Thisis a stochastic algorithm that quickly finds an approximation of the first \\nd principalcomponents. Its computational complexity is \\nO(m ‰ \\nd2) + O(d3), instead of O(m ‰ \\nn2)+ O(n3), so it is dramatically faster than the previous algorithms when \\nd is much \\nsmaller than n.rnd_pca = PCA(n_components=154, svd_solver=\"randomized\")X_reduced = rnd_pca.fit_transform(X_mnist)Kernel PCAIn Chapter 5\\n we discussed the kernel trick, a mathematical technique that implicitly\\nmaps instances into a very high-dimensional space (called the \\nfeature space\\n), enablingnonlinear classification and regression with Support Vector Machines. Recall that a\\nlinear decision boundary in the high-dimensional feature space corresponds to a\\ncomplex nonlinear decision boundary in the \\noriginal space\\n.It turns out that the same trick can be applied to PCA, making it possible to perform\\ncomplex nonlinear projections for dimensionality reduction. This is called \\nKernel\\nPCA (kPCA).6 It is often good at preserving clusters of instances after projection, or\\nsometimes even unrolling datasets that lie close to a twisted manifold.\\nFor example, the following code uses Scikit-Learn‡s \\nKernelPCA class to perform kPCAwith an RBF kernel (see Chapter 5\\n for more details about the RBF kernel and theother kernels):from sklearn.decomposition import KernelPCArbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04)X_reduced = rbf_pca.fit_transform(X)Figure 8-10 shows the Swiss roll, reduced to two dimensions using a linear kernel(equivalent to simply using the \\nPCA class), an RBF kernel, and a sigmoid kernel(Logistic).218 | Chapter 8: Dimensionality Reduction\\nFigure 8-10. Swiss roll reduced to 2D using kPCA with various kernels\\nSelecting a Kernel and Tuning HyperparametersAs kPCA is an unsupervised learning algorithm, there is no obvious performance\\nmeasure to help you select the best kernel and hyperparameter values. However,\\ndimensionality reduction is often a preparation step for a supervised learning task\\n(e.g., classification), so you can simply use grid search to select the kernel and hyper…\\nparameters that lead to the best performance on that task. For example, the following\\ncode creates a two-step pipeline, first reducing dimensionality to two dimensions\\nusing kPCA, then applying Logistic Regression for classification. Then it uses \\nGridSearchCV to find the best kernel and gamma value for kPCA in order to get the bestclassification accuracy at the end of the pipeline:\\nfrom sklearn.model_selection import GridSearchCVfrom sklearn.linear_model import LogisticRegressionfrom sklearn.pipeline import Pipelineclf = Pipeline([        (\"kpca\", KernelPCA(n_components=2)),        (\"log_reg\", LogisticRegression())    ])param_grid = [{        \"kpca__gamma\": np.linspace(0.03, 0.05, 10),        \"kpca__kernel\": [\"rbf\", \"sigmoid\"]    }]grid_search = GridSearchCV(clf, param_grid, cv=3)grid_search.fit(X, y)The best kernel and hyperparameters are then available through the \\nbest_params_variable:>>> print(grid_search.best_params_){•kpca__gamma•: 0.043333333333333335, •kpca__kernel•: •rbf•}Kernel PCA | 219\\nAnother approach, this time entirely unsupervised, is to select the kernel and hyper…\\nparameters that yield the lowest reconstruction error. However, reconstruction is not\\nas easy as with linear PCA. Here‡s why. \\nFigure 8-11 shows the original Swiss roll 3Ddataset (top left), and the resulting 2D dataset after kPCA is applied using an RBF\\nkernel (top right). Thanks to the kernel trick, this is mathematically equivalent to\\nmapping the training set to an infinite-dimensional feature space (bottom right)\\nusing the feature map\\n ž, then projecting the transformed training set down to 2Dusing linear PCA. Notice that if we could invert the linear PCA step for a given\\ninstance in the reduced space, the reconstructed point would lie in feature space, not\\nin the original space (e.g., like the one represented by an x in the diagram). Since the\\nfeature space is infinite-dimensional, we cannot compute the reconstructed point,\\nand therefore we cannot compute the true reconstruction error. Fortunately, it is pos…\\nsible to find a point in the original space that would map close to the reconstructed\\npoint. This is called the \\nreconstruction pre-image\\n. Once you have this pre-image, you\\ncan measure its squared distance to the original instance. You can then select the ker…\\nnel and hyperparameters that minimize this reconstruction pre-image error.\\nFigure 8-11. Kernel PCA and the reconstruction pre-image error\\n220 | Chapter 8: Dimensionality Reduction\\n7Scikit-Learn uses the algorithm based on K, Jason\\nWeston, and Bernhard Scholkopf, \\nƒLearning to Find Pre-images⁄\\n (Tubingen, Germany: Max Planck Institute\\nfor Biological Cybernetics, 2004).8ƒNonlinear Dimensionality Reduction by Locally Linear Embedding,⁄ S. Roweis, L. Saul (2000).\\nYou may be wondering how to perform this reconstruction. One solution is to train a\\nsupervised regression model, with the projected instances as the training set and the\\noriginal instances as the targets. Scikit-Learn will do this automatically if you \\nsetfit_inverse_transform=True, as shown in the following code:7rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433,                    fit_inverse_transform=True)X_reduced = rbf_pca.fit_transform(X)X_preimage = rbf_pca.inverse_transform(X_reduced)By default, \\nfit_inverse_transform=False and KernelPCA has noinverse_transform() method. This method only gets created\\nwhen you set fit_inverse_transform=True.You can then compute the reconstruction pre-image error:\\n>>> from sklearn.metrics import mean_squared_error>>> mean_squared_error(X, X_preimage)32.786308795766132Now you can use grid search with cross-validation to find the kernel and hyperpara…\\nmeters that minimize this pre-image reconstruction error.\\nLLELocally Linear Embedding\\n (LLE)8 is another very powerful \\nnonlinear dimensionality\\nreduction\\n (NLDR) technique. It is a Manifold Learning technique that does not rely\\non projections like the previous algorithms. In a nutshell, LLE works by first measur…\\ning how each training instance linearly relates to its closest neighbors (c.n.), and then\\nlooking for a low-dimensional representation of the training set where these local\\nrelationships are best preserved (more details shortly). This makes it particularly\\ngood at unrolling twisted manifolds, especially when there is not too much noise.\\nFor example, the following code uses Scikit-Learn‡s \\nLocallyLinearEmbedding class to\\nunroll the Swiss roll. The resulting 2D dataset is shown in \\nFigure 8-12. As you cansee, the Swiss roll is completely unrolled and the distances between instances are\\nlocally well preserved. However, distances are not preserved on a larger scale: the left\\npart of the unrolled Swiss roll is squeezed, while the right part is stretched. Neverthe…\\nless, LLE did a pretty good job at modeling the manifold.\\nLLE | 221\\nfrom sklearn.manifold import LocallyLinearEmbeddinglle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)X_reduced = lle.fit_transform(X)Figure 8-12. Unrolled Swiss roll using LLE\\nHere‡s how LLE works: first, for each training instance \\nx(i), the algorithm identifies its\\nk closest neighbors (in the preceding code k = 10), then tries to reconstruct x(i) as alinear function of these neighbors. More specifically, it finds the weights \\nwi,j such that\\nthe squared distance between x(i) and “j=1\\nmwi,jj is as small as possible, assumingwi,j = 0 if x(j) is not one of the k closest neighbors of x(i). Thus the first step of LLE is\\nthe constrained optimization problem described in \\nEquation 8-4\\n, where W is theweight matrix containing all the weights \\nwi,j. The second constraint simply normalizes\\nthe weights for each training instance \\nx(i).222 | Chapter 8: Dimensionality Reduction\\nEquation 8-4. LLE step 1: linearly modeling local relationships\\n=argmin\\n“i=1\\nm\\ni”“j=1\\nmwi,jj2subjectto\\nwi,j=0if\\njisnotoneofthe\\nkc.n.of\\ni“j=1\\nmwi,j=1for\\ni=1,2,\\n,mAfter this step, the weight matrix \\n (containing the weights \\nwi,j) encodes the locallinear relationships between the training instances. Now the second step is to map the\\ntraining instances into a \\nd-dimensional space (where d < n) while preserving these\\nlocal relationships as much as possible. If \\nz(i) is the image of x(i) in this d-dimensionalspace, then we want the squared distance between \\nz(i) and “j=1\\nmwi,jj to be as smallas possible. This idea leads to the unconstrained optimization problem described in\\nEquation 8-5\\n. It looks very similar to the first step, but instead of keeping the instan…\\nces fixed and finding the optimal weights, we are doing the reverse: keeping the\\nweights fixed and finding the optimal position of the instances‡ images in the low-\\ndimensional space. Note that \\nZ is the matrix containing all \\nz(i).Equation 8-5. LLE step 2: reducing dimensionality while preserving relationships\\n=argmin\\n“i=1\\nm\\ni”“j=1\\nmwi,jj2Scikit-Learn‡s LLE implementation has the following computational complexity:\\nO(m log(m)n log(k)) for finding the k nearest neighbors, O(mnk\\n3) for optimizing theweights, and \\nO(dm\\n2) for constructing the low-dimensional representations. Unfortu…\\nnately, the \\nm2 in the last term makes this algorithm scale poorly to very large datasets.\\nOther Dimensionality Reduction TechniquesThere are many other dimensionality reduction techniques, several of which are\\navailable in Scikit-Learn. Here are some of the most popular:\\n‹Multidimensional Scaling\\n (MDS) reduces dimensionality while trying to preserve\\nthe distances between the instances (see Figure 8-13).Other Dimensionality Reduction Techniques | 223\\n9The geodesic distance between two nodes in a graph is the number of nodes on the shortest path between\\nthese nodes.‹Isomap\\n creates \\na graph by connecting each instance to its nearest neighbors, then\\nreduces dimensionality while trying to preserve the \\ngeodesic distances\\n9 betweenthe instances.‹t-Distributed Stochastic Neighbor Embedding\\n (t-SNE) reduces dimensionalitywhile trying to keep similar instances close and dissimilar instances apart. It is\\nmostly used for visualization, in particular to visualize clusters of instances in\\nhigh-dimensional space (e.g., to visualize the MNIST images in 2D).‹Linear Discriminant Analysis\\n (LDA) is actually a classification algorithm, but dur…\\ning training it learns the most discriminative axes between the classes, and these\\naxes can then be used to define a hyperplane onto which to project the data. The\\nbenefit is that the projection will keep classes as far apart as possible, so LDA is a\\ngood technique to reduce dimensionality before running another classification\\nalgorithm such as an SVM classifier.\\nFigure 8-13. Reducing the Swiss roll to 2D using various techniques\\nExercises1.What are the main motivations for reducing a dataset‡s dimensionality? What are\\nthe main drawbacks?\\n2.What is the curse of dimensionality?\\n3.Once a dataset‡s dimensionality has been reduced, is it possible to reverse the\\noperation? If so, how? If not, why?\\n4.Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?\\n5.Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained\\nvariance ratio to 95%. How many dimensions will the resulting dataset have?\\n224 | Chapter 8: Dimensionality Reduction\\n6.In what cases would you use vanilla PCA, Incremental PCA, Randomized PCA,\\nor Kernel PCA?\\n7.How can you evaluate the performance of a dimensionality reduction algorithm\\non your dataset?\\n8.Does it make any sense to chain two different dimensionality reduction algo…\\nrithms?9.Load the MNIST dataset (introduced in \\nChapter 3\\n) and split it into a training set\\nand a test set (take the first 60,000 instances for training, and the remaining10,000 for testing). Train a Random Forest classifier on the dataset and time how\\nlong it takes, then evaluate the resulting model on the test set. Next, use PCA to\\nreduce the dataset‡s dimensionality, with an explained variance ratio of 95%.\\nTrain a new Random Forest classifier on the reduced dataset and see how long it\\ntakes. Was training much faster? Next evaluate the classifier on the test set: how\\ndoes it compare to the previous classifier?\\n10.Use t-SNE to reduce the MNIST dataset down to two dimensions and plot the\\nresult using Matplotlib. You can use a scatterplot using 10 different colors to rep…\\nresent each image‡s target class. Alternatively, you can write colored digits at the\\nlocation of each instance, or even plot scaled-down versions of the digit images\\nthemselves (if you plot all digits, the visualization will be too cluttered, so you\\nshould either draw a random sample or plot an instance only if no other instance\\nhas already been plotted at a close distance). You should get a nice visualization\\nwith well-separated clusters of digits. Try using other dimensionality reduction\\nalgorithms such as PCA, LLE, or MDS and compare the resulting visualizations.\\nSolutions to these exercises are available in \\nAppendix A\\n.Exercises | 225\\nPART IINeural Networks and Deep LearningCHAPTER 9Up and Running with TensorFlowTensorFlow\\n is a powerful open source software library for numerical computation,\\nparticularly well suited and fine-tuned for large-scale Machine Learning. Its basic\\nprinciple is simple: you first define in Python a graph of computations to perform\\n(for example, the one in \\nFigure 9-1), and then TensorFlow takes that graph and runs\\nit efficiently using optimized C++ code.\\nFigure 9-1. A simple computation graph\\nMost importantly, it is possible to break up the graph into several chunks and run\\nthem in parallel across multiple CPUs or GPUs (as shown in \\nFigure 9-2). TensorFlow\\nalso supports distributed computing, so you can train colossal neural networks on\\nhumongous training sets in a reasonable amount of time by splitting the computa…\\ntions across hundreds of servers (see \\nChapter 12\\n). TensorFlow can train a network\\nwith millions of parameters on a training set composed of billions of instances with\\nmillions of features each. This should come as no surprise, since TensorFlow was\\n2291TensorFlow is not limited to neural networks or even Machine Learning; you could run quantum physics sim…\\nulations if you wanted.\\ndeveloped by the Google Brain team and it powers many of Google‡s large-scale serv…\\nices, such as Google Cloud Speech, Google Photos, and Google Search.Figure 9-2. Parallel computation on multiple CPUs/GPUs/servers\\nWhen TensorFlow was open-sourced in November 2015, there were already many\\npopular open source libraries for Deep Learning (Table 9-1\\n lists a few), and to be fairmost of TensorFlow‡s features already existed in one library or another. Nevertheless,\\nTensorFlow‡s clean design, scalability, flexibility,\\n1 and great documentation (not to\\nmention Google‡s name) quickly boosted it to the top of the list. In short, TensorFlow\\nwas designed to be flexible, scalable, and production-ready, and existing frameworks\\narguably hit only two out of the three of these. Here are some of TensorFlow‡s high…\\nlights:\\n‹It runs not only on Windows, Linux, and macOS, but also on mobile devices,\\nincluding both iOS and Android.230 | Chapter 9: Up and Running with TensorFlow\\n2Not to be confused with the TFLearn library, which is an independent project.\\n‹It provides a very simple Python API called \\nTF.Learn\\n2 (tensorflow.contrib.learn), compatible with Scikit-Learn. As you will see, you can use it to\\ntrain various types of neural networks in just a few lines of code. It was previ…\\nously an independent project called \\nScikit Flow\\n (or sk‡ow).‹It also provides another simple API called \\nTF-slim\\n (tensorflow.contrib.slim)to simplify building, training, and evaluating neural networks.\\n‹Several other high-level APIs have been built independently on top of Tensor…\\nFlow, such as \\nKeras\\n or Pretty Tensor\\n.‹Its main Python API offers much more flexibility (at the cost of higher complex…\\nity) to create all sorts of computations, including any neural network architecture\\nyou can think of.‹It includes highly efficient C++ implementations of many ML operations, partic…\\nularly those needed to build neural networks. There is also a C++ API to defineyour own high-performance operations.\\n‹It provides several advanced optimization nodes to search for the parameters that\\nminimize a cost function. These are very easy to use since TensorFlow automati…\\ncally takes care of computing the gradients of the functions you define. This is \\ncalled automatic \\ndi›erentiating (or autodi›).‹It also comes with a great visualization tool \\ncalled TensorBoard\\n that allows you to\\nbrowse through the computation graph, view learning curves, and more.\\n‹Google also launched a \\ncloud service to run TensorFlow graphs\\n.‹Last but not least, it has a dedicated team of passionate and helpful developers,\\nand a growing community contributing to improving it. It is one of the most\\npopular open source projects on GitHub, and more and more great projects are\\nbeing built on top of it (for examples, check out the resources page on \\nhttps://www.tensor‡ow.org/, or https://github.com/jtoy/awesome-tensor‡ow). To ask\\ntechnical questions, you should use http://stackover‡ow.com/ and tag your ques…tion with \"tensorflow\". You can file bugs and feature requests through GitHub.\\nFor general discussions, join the \\nGoogle group.In this chapter, we will go through the basics of TensorFlow, from installation to cre…\\nating, running, saving, and visualizing simple computational graphs. Mastering these\\nbasics is important before you build your first neural network (which we will do in\\nthe next chapter).\\nUp and Running with TensorFlow | 231\\nTable 9-1. Open source Deep Learning libraries (not an exhaustive list)\\nLibraryAPIPlatformsStarted by\\nYearCa—ePython, C++, Matlab\\nLinux, macOS, Windows\\nY. Jia, UC Berkeley (BVLC)\\n2013Deeplearning4jJava, Scala, Clojure\\nLinux, macOS, Windows, Android\\nA. Gibson, J.Patterson\\n2014H2OPython, R\\nLinux, macOS, Windows\\nH2O.ai2014MXNetPython, C++, others\\nLinux, macOS, Windows, iOS, Android\\nDMLC2015TensorFlowPython, C++\\nLinux, macOS, Windows, iOS, Android\\nGoogle2015TheanoPythonLinux, macOS, iOS\\nUniversity of Montreal\\n2010TorchC++, Lua\\nLinux, macOS, iOS, Android\\nR. Collobert, K. Kavukcuoglu, C.\\nFarabet2002InstallationLet‡s get started! Assuming you installed Jupyter and Scikit-Learn by following the\\ninstallation instructions in \\nChapter 2\\n, you can simply use pip to install TensorFlow. If\\nyou created an isolated environment using virtualenv, you first need to activate it:\\n$ cd $ML_PATH               # Your ML working directory (e.g., $HOME/ml)$ source env/bin/activateNext, install TensorFlow:\\n$ pip3 install --upgrade tensorflowFor GPU support, you need to install \\ntensorflow-gpu instead oftensorflow. See Chapter 12\\n for more details.To test your installation, type the following command. It should output the version of\\nTensorFlow you installed.\\n$ python3 -c •import tensorflow; print(tensorflow.__version__)•1.0.0Creating Your First Graph and Running It in a SessionThe following code creates the graph represented in \\nFigure 9-1:import tensorflow as tfx = tf.Variable(3, name=\"x\")y = tf.Variable(4, name=\"y\")f = x*x*y + y + 2232 | Chapter 9: Up and Running with TensorFlow\\n3In distributed TensorFlow, variable values are stored on the servers instead of the session, as we will see in\\nChapter 12\\n.That‡s all there is to it! The most important thing to understand is that this code does\\nnot actually perform any computation, even though it looks like it does (especially the\\nlast line). It just creates a computation graph. In fact, even the variables are not ini…\\ntialized yet. To evaluate this graph, you need to open a TensorFlow \\nsession\\n and use itto initialize the variables and evaluate \\nf. A TensorFlow session takes care of placing\\nthe operations onto \\ndevices\\n such as CPUs and GPUs and running them, and it holds\\nall the variable values.3 The following code creates a session, initializes the variables,\\nand evaluates, and \\nf then closes the session (which frees up resources):>>> sess = tf.Session()>>> sess.run(x.initializer)>>> sess.run(y.initializer)>>> result = sess.run(f)>>> print(result)42>>> sess.close()Having to repeat \\nsess.run() all \\nthe time is a bit cumbersome, but fortunately there is\\na better way:\\nwith tf.Session() as sess:    x.initializer.run()    y.initializer.run()    result = f.eval()Inside the with block, the session is set as the default session. Calling \\nx.initializer.run() is equivalent to calling \\ntf.get_default_session().run(x.initializer), and similarly f.eval() is equivalent to calling\\ntf.get_default_session().run(f). This makes the code easier to read. Moreover,\\nthe session is automatically closed at the end of the block.\\nInstead of manually running the initializer for every single variable, you can use the\\nglobal_variables_initializer() function. Note that it does not actually perform\\nthe initialization immediately, but rather creates a node in the graph that will initialize\\nall variables when it is run:init = tf.global_variables_initializer()  # prepare an init nodewith tf.Session() as sess:    init.run()  # actually initialize all the variables    result = f.eval()Inside Jupyter or within a Python shell you may prefer to create an \\nInteractiveSession. The only difference from a regular Session is that when an \\nInteractiveSession is created it automatically sets itself as the default session, so you don‡t need a\\nCreating Your First Graph and Running It in a Session | 233\\nwith block (but you do need to close the session manually when you are done with\\nit):>>> sess = tf.InteractiveSession()>>> init.run()>>> result = f.eval()>>> print(result)42>>> sess.close()A TensorFlow program is typically split into two parts: the first part builds a compu…\\ntation graph (this is called the \\nconstruction phase\\n), and the second part runs it (this isthe execution phase\\n). The construction phase typically builds a computation graph\\nrepresenting the ML model and the computations required to train it. The execution\\nphase generally runs a loop that evaluates a training step repeatedly (for example, one\\nstep per mini-batch), gradually improving the model parameters. We will go through\\nan example shortly.\\nManaging GraphsAny node you create is automatically added to the default graph:\\n>>> x1 = tf.Variable(1)>>> x1.graph is tf.get_default_graph()TrueIn most cases this is fine, but sometimes you may want to manage multiple independ…\\nent graphs. You can do this by creating a new \\nGraph and temporarily making it the\\ndefault graph inside a \\nwith block, like so:>>> graph = tf.Graph()>>> with graph.as_default():...     x2 = tf.Variable(2)...>>> x2.graph is graphTrue>>> x2.graph is tf.get_default_graph()FalseIn Jupyter (or in a Python shell), it is common to run the same\\ncommands more than once while you are experimenting. As a\\nresult, you may end up with a default graph containing many\\nduplicate nodes. One solution is to restart the Jupyter kernel (or\\nthe Python shell), but a more convenient solution is to just reset the\\ndefault graph by running \\ntf.reset_default_graph().234 | Chapter 9: Up and Running with TensorFlow\\nLifecycle of a Node ValueWhen you evaluate a node, TensorFlow automatically determines the set of nodes\\nthat it depends on and it evaluates these nodes first. For example, consider the \\nfollow…ing code:w = tf.constant(3)x = w + 2y = x + 5z = x * 3with tf.Session() as sess:    print(y.eval())  # 10    print(z.eval())  # 15First, this code defines a very simple graph. Then it starts a session and runs the\\ngraph to evaluate \\ny: TensorFlow automatically detects that \\ny depends on w, whichdepends on x, so it first evaluates \\nw, then x, then y, and returns the value of y. Finally,\\nthe code runs the graph to evaluate \\nz. Once again, TensorFlow detects that it must\\nfirst evaluate \\nw and x. It is important to note that it will \\nnot\\n reuse the result of theprevious evaluation of \\nw and x. In short, the preceding code evaluates \\nw and x twice.All node values are dropped between graph runs, except variable values, which are\\nmaintained by the session across graph runs (queues and readers also maintain some\\nstate, as we will see in \\nChapter 12\\n). A variable starts its life when its initializer is run,and it ends when the session is closed.If you want to evaluate \\ny and z efficiently, without evaluating \\nw and x twice as in theprevious code, you must ask TensorFlow to evaluate both \\ny and z in just one graph\\nrun, as shown in the following code:with tf.Session() as sess:    y_val, z_val = sess.run([y, z])    print(y_val)  # 10    print(z_val)  # 15In single-process TensorFlow, multiple sessions do not share any\\nstate, even if they reuse the same graph (each session would have its\\nown copy of every variable). In distributed TensorFlow (see \\nChap…\\nter 12), variable state is stored on the servers, not in the sessions, so\\nmultiple sessions can share the same variables.\\nLinear Regression with TensorFlowTensorFlow operations (also called \\nops\\n for short) can take any number of inputs and\\nproduce any number of outputs. For example, the addition and multiplication ops\\neach take two inputs and produce one output. Constants and variables take no input\\nLifecycle of a Node Value | 235\\n4Note that \\nhousing.target is a 1D array, but we need to reshape it to a column vector to compute \\ntheta.Recall that NumPy‡s \\nreshape() function accepts –1 (meaning ƒunspecified⁄) for one of the dimensions: that\\ndimension will be computed based on the array‡s length and the remaining dimensions.\\n(they are called source ops\\n). The inputs and outputs are multidimensional arrays,\\ncalled tensors\\n (hence the name ƒtensor flow⁄). Just like NumPy arrays, tensors have a\\ntype and a shape. In fact, in the Python API tensors are simply represented by NumPy\\nndarrays. They typically contain floats, but you can also use them to carry strings\\n(arbitrary byte arrays).\\nIn the examples so far, the tensors just contained a single scalar value, but you can of\\ncourse perform computations on arrays of any shape. For example, the following code\\nmanipulates 2D arrays to perform Linear Regression on the California housing data…\\nset (introduced in \\nChapter 2\\n). It starts by fetching the dataset; then it adds an extra\\nbias input feature (\\nx0 = 1) to all training instances (it does so using NumPy so it runs\\nimmediately); then it creates two TensorFlow constant nodes, \\nX and y, to hold thisdata and the targets,\\n4 and it uses some of the matrix operations provided by Tensor…\\nFlow to define theta. These matrix functions›\\ntranspose(), matmul(), andmatrix_inverse()›are self-explanatory, but as usual they do not perform any com…\\nputations immediately; instead, they create nodes in the graph that will perform them\\nwhen the graph is run. You may recognize that the definition of \\ntheta corresponds to\\nthe Normal Equation (\\n– = \\nXT ’ \\nX)–1 ’ \\nXT ’ \\ny; see Chapter 4\\n). Finally, the code creates a\\nsession and uses it to evaluate \\ntheta.import numpy as npfrom sklearn.datasets import fetch_california_housinghousing = fetch_california_housing()m, n = housing.data.shapehousing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")XT = tf.transpose(X)theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)with tf.Session() as sess:    theta_value = theta.eval()The main benefit of this code versus computing the Normal Equation directly using\\nNumPy is that TensorFlow will automatically run this on your GPU card if you have\\none (provided you installed TensorFlow with GPU support, of course; see \\nChapter 12\\nfor more details).236 | Chapter 9: Up and Running with TensorFlow\\nImplementing Gradient DescentLet‡s try using Batch Gradient Descent (introduced in \\nChapter 4\\n) instead of the Nor…\\nmal Equation. First we will do this by manually computing the gradients, then we will\\nuse TensorFlow‡s autodiff feature to let TensorFlow compute the gradients automati…\\ncally, and finally we will use a couple of TensorFlow‡s out-of-the-box optimizers.\\nWhen using Gradient Descent, remember that it is important to\\nfirst normalize the input feature vectors, or else training may be\\nmuch slower. You can do this using TensorFlow, NumPy, \\nScikit-Learn‡s \\nStandardScaler, or any other solution you prefer. The fol…\\nlowing code assumes that this normalization has already been\\ndone.Manually Computing the GradientsThe following code should be fairly self-explanatory, except for a few new elements:\\n‹The random_uniform() function creates a node in the graph that will generate a\\ntensor containing random values, given its shape and value range, much like\\nNumPy‡s \\nrand() function.‹The assign() function creates a node that will assign a new value to a variable.\\nIn this case, it implements the Batch Gradient Descent step \\n–(next step) = – –−–MSE(–).‹The main loop executes the training step over and over again (n_epochs times),and every 100 iterations it prints out the current Mean Squared Error (\\nmse). You\\nshould see the MSE go down at every iteration.\\nn_epochs = 1000learning_rate = 0.01X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")y_pred = tf.matmul(X, theta, name=\"predictions\")error = y_pred - ymse = tf.reduce_mean(tf.square(error), name=\"mse\")gradients = 2/m * tf.matmul(tf.transpose(X), error)training_op = tf.assign(theta, theta - learning_rate * gradients)init = tf.global_variables_initializer()with tf.Session() as sess:    sess.run(init)    for epoch in range(n_epochs):Implementing Gradient Descent | 237\\n        if epoch % 100 == 0:            print(\"Epoch\", epoch, \"MSE =\", mse.eval())        sess.run(training_op)    best_theta = theta.eval()Using autodi†The preceding code works fine, but it requires mathematically deriving the gradients\\nfrom the cost function (MSE). In the case of Linear Regression, it is reasonably easy,\\nbut if you had to do this with deep neural networks you would get quite a headache:it would be tedious and error-prone. You could use \\nsymbolic \\ndi›erentiation to auto…\\nmatically find the equations for the partial derivatives for you, but the resulting code\\nwould not necessarily be very efficient.\\nTo understand why, consider the function \\nf(x)= exp(exp(exp(x))). If you know calcu…lus, you can figure out its derivative \\nf(x) = exp(x) ‰ exp(exp(x)) ‰ exp(exp(exp(x))).If you code f(x) and f(x) separately and exactly as they appear, your code will not be\\nas efficient as it could be. A more efficient solution would be to write a function that\\nfirst computes exp(\\nx), then exp(exp(x)), then exp(exp(exp(x))), and returns all three.This gives you f(x) directly (the third term), and if you need the derivative you can\\njust multiply all three terms and you are done. With the na€ve approach you would\\nhave had to call the \\nexp function nine times to compute both \\nf(x) and f(x). With this\\napproach you just need to call it three times.\\nIt gets worse when your function is defined by some arbitrary code. Can you find the\\nequation (or the code) to compute the partial derivatives of the following function?\\nHint: don‡t even try.\\ndef my_func(a, b):    z = 0    for i in range(100):        z = a * np.cos(z + i) + z * np.sin(b - i)    return zFortunately, TensorFlow‡s autodiff feature comes to the rescue: it can automatically\\nand efficiently compute the gradients for you. Simply replace the \\ngradients = ...line in the Gradient Descent code in the previous section with the following line, and\\nthe code will continue to work just fine:\\ngradients = tf.gradients(mse, [theta])[0]The gradients() function takes an op (in this case mse) and a list of variables (in thiscase just theta), and it creates a list of ops (one per variable) to compute the gradi…\\nents of the op with regards to each variable. So the \\ngradients node will compute the\\ngradient vector of the MSE with regards to \\ntheta.238 | Chapter 9: Up and Running with TensorFlow\\nThere are four main approaches to computing gradients automatically. They are sum…\\nmarized in Table 9-2\\n. TensorFlow uses \\nreverse-mode \\nautodi›, which is perfect (effi…cient and accurate) when there are many inputs and few outputs, as is often the case\\nin neural networks. It computes all the partial derivatives of the outputs with regards\\nto all the inputs in just \\nnoutputs + 1 graph traversals.\\nTable 9-2. Main solutions to compute gradients automatically\\nTechniqueNb of graph traversals to\\ncompute all gradients\\nAccuracySupportsarbitrary code\\nCommentNumerical \\ndi—erentiationninputs + 1\\nLowYesTrivial to implement\\nSymbolic \\ndi—erentiationN/AHighNoBuilds a very \\ndi—erent graph\\nForward-mode \\nautodi—ninputsHighYesUses \\ndual numbers\\nReverse-mode \\nautodi—noutputs + 1\\nHighYesImplemented by TensorFlow\\nIf you are interested in how this magic works, check out \\nAppendix D\\n.Using an OptimizerSo TensorFlow computes the gradients for you. But it gets even easier: it also provides\\na number of optimizers out of the box, including a Gradient Descent optimizer. You\\ncan simply replace the preceding \\ngradients = ... and training_op = ... lineswith the following code, and once again everything will just work fine:\\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)training_op = optimizer.minimize(mse)If you want to use a different type of optimizer, you just need to change one line. For\\nexample, you can use a momentum optimizer (which often converges much faster\\nthan Gradient Descent; see \\nChapter 11\\n) by defining the optimizer like this:optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,                                       momentum=0.9)Feeding Data to the Training AlgorithmLet‡s \\ntry to modify the previous code to implement Mini-batch Gradient Descent. For\\nthis, we need a way to replace \\nX and \\ny at every iteration with the next mini-batch. The\\nsimplest way to do this is to use placeholder nodes. These nodes are special because\\nthey don‡t actually perform any computation, they just output the data you tell them\\nto output at runtime. They are typically used to pass the training data to TensorFlow\\nduring training. If you don‡t specify a value at runtime for a placeholder, you get an\\nexception.To create a placeholder node, you must call the \\nplaceholder() function and specify\\nthe output tensor‡s data type. Optionally, you can also specify its shape, if you want to\\nFeeding Data to the Training Algorithm | 239\\nenforce it. If you specify \\nNone for a dimension, it means ƒany size.⁄ For example, the\\nfollowing code creates a placeholder node \\nA, and also a node B = A + 5. When weevaluate \\nB, we pass a feed_dict to the eval() method that specifies the value of \\nA.Note that \\nA must have rank 2 (i.e., it must be two-dimensional) and there must be\\nthree columns (or else an exception is raised), but it can have any number of rows.\\n>>> A = tf.placeholder(tf.float32, shape=(None, 3))>>> B = A + 5>>> with tf.Session() as sess:...     B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]]})...     B_val_2 = B.eval(feed_dict={A: [[4, 5, 6], [7, 8, 9]]})...>>> print(B_val_1)[[ 6.  7.  8.]]>>> print(B_val_2)[[  9.  10.  11.] [ 12.  13.  14.]]You can actually feed the output of \\nany\\n operations, not just place…\\nholders. In this case TensorFlow does not try to evaluate these\\noperations; it uses the values you feed it.\\nTo implement Mini-batch Gradient Descent, we only need to tweak the existing code\\nslightly. First change the definition of \\nX and y in the construction phase to make them placeholder nodes:X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")Then define the batch size and compute the total number of batches:\\nbatch_size = 100n_batches = int(np.ceil(m / batch_size))Finally, in the execution phase, fetch the mini-batches one by one, then provide the\\nvalue of X and y via the feed_dict parameter when evaluating a node that depends\\non either of them.def fetch_batch(epoch, batch_index, batch_size):    [...] # load the data from disk    return X_batch, y_batchwith tf.Session() as sess:    sess.run(init)    for epoch in range(n_epochs):        for batch_index in range(n_batches):            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})240 | Chapter 9: Up and Running with TensorFlow\\n    best_theta = theta.eval()We don‡t need to pass the value of \\nX and y when evaluating \\nthetasince it does not depend on either of them.Saving and Restoring ModelsOnce you have trained your model, you should save its parameters to disk so you can\\ncome back to it whenever you want, use it in another program, compare it to other\\nmodels, and so on. Moreover, you probably want to save checkpoints at regular inter…\\nvals during training so that if your computer crashes during training you can con…\\ntinue from the last checkpoint rather than start over from scratch.\\nTensorFlow makes saving and restoring a model very easy. Just create a \\nSaver node at\\nthe end of the construction phase (after all variable nodes are created); then, in the\\nexecution phase, just call its save() method whenever you want to save the model,\\npassing it the session and path of the checkpoint file:\\n[...]theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")[...]init = tf.global_variables_initializer()saver = tf.train.Saver()with tf.Session() as sess:    sess.run(init)    for epoch in range(n_epochs):        if epoch % 100 == 0:  # checkpoint every 100 epochs            save_path = saver.save(sess, \"/tmp/my_model.ckpt\")        sess.run(training_op)    best_theta = theta.eval()    save_path = saver.save(sess, \"/tmp/my_model_final.ckpt\")Restoring a model is just as easy: you create a \\nSaver at the end of the construction\\nphase just like before, but then at the beginning of the execution phase, instead of ini…\\ntializing the variables using the init node, you call the restore() method of theSaver object:with tf.Session() as sess:    saver.restore(sess, \"/tmp/my_model_final.ckpt\")    [...]Saving and Restoring Models | 241\\nBy default a \\nSaver saves and restores all variables under their own name, but if you\\nneed more control, you can specify which variables to save or restore, and what\\nnames to use. For example, the following \\nSaver will save or restore only the \\nthetavariable under the name weights:saver = tf.train.Saver({\"weights\": theta})Visualizing the Graph and Training Curves UsingTensorBoardSo now we have a computation graph that trains a Linear Regression model using\\nMini-batch Gradient Descent, and we are saving checkpoints at regular intervals.\\nSounds sophisticated, doesn‡t it? However, we are still relying on the \\nprint() func…tion to visualize progress during training. There is a better way: enter TensorBoard. If\\nyou feed it some training stats, it will display nice interactive visualizations of these\\nstats in your web browser (e.g., learning curves). You can also provide it the graph‡s\\ndefinition and it will give you a great interface to browse through it. This is very use…\\nful to identify errors in the graph, to find bottlenecks, and so on.\\nThe first step is to tweak your program a bit so it writes the graph definition and\\nsome training stats›for example, the training error (MSE)›to a log directory that\\nTensorBoard will read from. You need to use a different log directory every time you\\nrun your program, or else TensorBoard will merge stats from different runs, which\\nwill mess up the visualizations. The simplest solution for this is to include a time…\\nstamp in the log directory name. Add the following code at the beginning of the pro…\\ngram:from datetime import datetimenow = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")root_logdir = \"tf_logs\"logdir = \"{}/run-{}/\".format(root_logdir, now)Next, add the following code at the very end of the construction phase:\\nmse_summary = tf.summary.scalar(•MSE•, mse)file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())The first line creates a node in the graph that will evaluate the MSE value and write it\\nto a TensorBoard-compatible binary log string called a \\nsummary\\n. The second line cre…ates a \\nFileWriter that you will use to write summaries to logfiles in the log directory.\\nThe first parameter indicates the path of the log directory (in this case something like\\ntf_logs/run-20160906091959/\\n, relative to the current directory). The second\\n(optional) parameter is the graph you want to visualize. Upon creation, the \\nFileWriter creates the log directory if it does not already exist (and its parent directories\\nif needed), and writes the graph definition in a binary logfile called an \\nevents \\n†le.242 | Chapter 9: Up and Running with TensorFlow\\nNext you need to update the execution phase to evaluate the \\nmse_summary node regu…\\nlarly during training (e.g., every 10 mini-batches). This will output a summary that\\nyou can then write to the events file using the \\nfile_writer. Here is the updated code:\\n    [...]    for batch_index in range(n_batches):        X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)        if batch_index % 10 == 0:            summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})            step = epoch * n_batches + batch_index            file_writer.add_summary(summary_str, step)        sess.run(training_op, feed_dict={X: X_batch, y: y_batch})    [...]Avoid logging training stats at every single training step, as this\\nwould significantly slow down training.\\nFinally, you want to close the \\nFileWriter at the end of the program:\\nfile_writer.close()Now run this program: it will create the log directory and write an events file in this\\ndirectory, containing both the graph definition and the MSE values. Open up a shell\\nand go to your working directory, then type \\nls -l tf_logs/run* to list the contents\\nof the log directory:\\n$ cd $ML_PATH               # Your ML working directory (e.g., $HOME/ml)$ ls -l tf_logs/run*total 40-rw-r--r-- 1 ageron staff 18620 Sep 6 11:10 events.out.tfevents.1472553182.mymacIf you run the program a second time, you should see a second directory in the\\ntf_logs/\\n directory:\\n$ ls -l tf_logs/total 0drwxr-xr-x  3 ageron  staff  102 Sep  6 10:07 run-20160906091959drwxr-xr-x  3 ageron  staff  102 Sep  6 10:22 run-20160906092202Great! Now it‡s time to fire up the TensorBoard server. You need to activate your vir…\\ntualenv environment if you created one, then start the server by running the \\ntensorboard command, pointing it to the root log directory. This starts the TensorBoard\\nweb server, listening on port 6006 (which is ƒgoog⁄ written upside down):\\n$ source env/bin/activate$ tensorboard --logdir tf_logs/Starting TensorBoard  on port 6006(You can navigate to http://0.0.0.0:6006)Visualizing the Graph and Training Curves Using TensorBoard | 243\\nNext open a browser and go to \\nhttp://0.0.0.0:6006/\\n (or http://localhost:6006/\\n). Wel…\\ncome to TensorBoard! In the Events tab you should see MSE on the right. If you click\\non it, you will see a plot of the MSE during training, for both runs (Figure 9-3). You\\ncan check or uncheck the runs you want to see, zoom in or out, hover over the curve\\nto get details, and so on.Figure 9-3. Visualizing training stats using TensorBoard\\nNow click on the Graphs tab. You should see the graph shown in \\nFigure 9-4.To reduce clutter, the nodes that have many \\nedges\\n (i.e., connections to other nodes)are separated out to an auxiliary area on the right (you can move a node back and\\nforth between the main graph and the auxiliary area by right-clicking on it). Some\\nparts of the graph are also collapsed by default. For example, try hovering over the\\ngradients node, then click on the  icon to expand this subgraph. Next, in this sub…\\ngraph, try expanding the \\nmse_grad subgraph.\\n244 | Chapter 9: Up and Running with TensorFlow\\nFigure 9-4. Visualizing the graph using TensorBoard\\nIf you want to take a peek at the graph directly within Jupyter, you\\ncan use the show_graph() function available in the notebook for\\nthis chapter. It was originally written by A. Mordvintsev in his great\\ndeepdream tutorial notebook. Another option is to install E. Jang‡s\\nTensorFlow debugger tool\\n which includes a Jupyter extension \\nforgraph visualization (and more).\\nName ScopesWhen dealing with more complex models such as neural networks, the graph can\\neasily become cluttered with thousands of nodes. To avoid this, you can create \\nname\\nscopes\\n to group related nodes. For example, let‡s modify the previous code to define\\nthe error and mse ops within a name scope called \"loss\":with tf.name_scope(\"loss\") as scope:    error = y_pred - y    mse = tf.reduce_mean(tf.square(error), name=\"mse\")The name of each op defined within the scope is now prefixed with \"loss/\":>>> print(error.op.name)loss/sub>>> print(mse.op.name)loss/mseIn TensorBoard, the \\nmse and error nodes now appear inside the \\nloss namespace,which appears collapsed by default (\\nFigure 9-5).Name Scopes | 245\\nFigure 9-5. A collapsed namescope in TensorBoard\\nModularitySuppose you want to create a graph that adds the output of \\ntwo recti†ed linear units\\n(ReLU). A ReLU computes a linear function of the inputs, and outputs the result if it\\nis positive, and 0 otherwise, as shown in \\nEquation 9-1\\n.Equation 9-1. \\nRecti†ed linear unit\\nh,b=max\\n’+b,0\\nThe following code does the job, but it‡s quite repetitive:\\nn_features = 3X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")w1 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights1\")w2 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights2\")b1 = tf.Variable(0.0, name=\"bias1\")b2 = tf.Variable(0.0, name=\"bias2\")z1 = tf.add(tf.matmul(X, w1), b1, name=\"z1\")z2 = tf.add(tf.matmul(X, w2), b2, name=\"z2\")relu1 = tf.maximum(z1, 0., name=\"relu1\")relu2 = tf.maximum(z1, 0., name=\"relu2\")output = tf.add(relu1, relu2, name=\"output\")Such repetitive code is hard to maintain and error-prone (in fact, this code contains a\\ncut-and-paste error; did you spot it?). It would become even worse if you wanted to\\n246 | Chapter 9: Up and Running with TensorFlow\\nadd a few more ReLUs. Fortunately, TensorFlow lets you stay DRY (Don‡t Repeat\\nYourself): simply create a function to build a ReLU. The following code creates five\\nReLUs and outputs their sum (note that \\nadd_n() creates an operation that will com…\\npute the sum of a list of tensors):def relu(X):    w_shape = (int(X.get_shape()[1]), 1)    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")    b = tf.Variable(0.0, name=\"bias\")    z = tf.add(tf.matmul(X, w), b, name=\"z\")    return tf.maximum(z, 0., name=\"relu\")n_features = 3X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")relus = [relu(X) for i in range(5)]output = tf.add_n(relus, name=\"output\")Note that when you create a node, TensorFlow checks whether its name already\\nexists, and if it does it appends an underscore followed by an index to make the name\\nunique. So the first ReLU contains nodes named \\n\"weights\", \"bias\", \"z\", and \"relu\"(plus many more nodes with their default name, such as \\n\"MatMul\"); the second ReLU\\ncontains nodes named \\n\"weights_1\", \"bias_1\", and so on; the third ReLU contains\\nnodes named \"weights_2\", \"bias_2\", and so on. TensorBoard identifies such series\\nand collapses them together to reduce clutter (as you can see in \\nFigure 9-6).Figure 9-6. Collapsed node series\\nModularity | 247\\nUsing name scopes, you can make the graph much clearer. Simply move all the con…\\ntent of the \\nrelu() function inside a name scope. Figure 9-7 shows the resultinggraph. Notice that TensorFlow also gives the name scopes unique names by append…\\ning _1, _2, and so on.def relu(X):    with tf.name_scope(\"relu\"):        [...]Figure 9-7. A clearer graph using name-scoped units\\nSharing VariablesIf you want to share a variable between various components of your graph, one sim…\\nple option is to create it first, then pass it as a parameter to the functions that need it.\\nFor example, suppose you want to control the ReLU threshold (currently hardcoded\\nto 0) using a shared threshold variable for all ReLUs. You could just create that vari…\\nable first, and then pass it to the relu() function:def relu(X, threshold):    with tf.name_scope(\"relu\"):        [...]        return tf.maximum(z, threshold, name=\"max\")threshold = tf.Variable(0.0, name=\"threshold\")X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")relus = [relu(X, threshold) for i in range(5)]output = tf.add_n(relus, name=\"output\")This works fine: now you can control the threshold for all ReLUs using the \\nthresholdvariable. However, if there are many shared parameters such as this one, it will be\\npainful to have to pass them around as parameters all the time. Many people create a\\nPython dictionary containing all the variables in their model, and pass it around to\\nevery function. Others create a class for each module (e.g., a \\nReLU class using classvariables to handle the shared parameter). Yet another option is to set the shared vari…\\nable as an attribute of the \\nrelu() function upon the first call, like so:248 | Chapter 9: Up and Running with TensorFlow\\n5Creating a \\nReLU class is arguably the cleanest option, but it is rather heavyweight.\\ndef relu(X):    with tf.name_scope(\"relu\"):        if not hasattr(relu, \"threshold\"):            relu.threshold = tf.Variable(0.0, name=\"threshold\")        [...]        return tf.maximum(z, relu.threshold, name=\"max\")TensorFlow \\noffers another option, which may lead to slightly cleaner and more mod…\\nular code than the previous solutions.5 This solution is a bit tricky to understand at\\nfirst, but since it is used a lot in TensorFlow it is worth going into a bit of detail. The\\nidea is to use the get_variable() function to create the shared variable if it does not\\nexist yet, or reuse it if it already exists. The desired behavior (creating or reusing) is\\ncontrolled by an attribute of the \\ncurrent \\nvariable_scope(). For example, the follow…\\ning code will create a variable named \\n\"relu/threshold\" (as a scalar, since \\nshape=(),and using 0.0 as the initial value):with tf.variable_scope(\"relu\"):    threshold = tf.get_variable(\"threshold\", shape=(),                                initializer=tf.constant_initializer(0.0))Note that if the variable has already been created by an earlier call to \\nget_variable(), this code will raise an exception. This behavior prevents reusing variables by\\nmistake. If you want to reuse a variable, you need to explicitly say so by setting the\\nvariable scope‡s \\nreuse attribute to \\nTrue (in which case you don‡t have to specify the\\nshape or the initializer):\\nwith tf.variable_scope(\"relu\", reuse=True):    threshold = tf.get_variable(\"threshold\")This code will fetch the existing \"relu/threshold\" variable, or raise an exception if it\\ndoes not exist or if it was not created using \\nget_variable(). Alternatively, you can\\nset the reuse attribute to \\nTrue inside the block by calling the scope‡s \\nreuse_variables() method:with tf.variable_scope(\"relu\") as scope:    scope.reuse_variables()    threshold = tf.get_variable(\"threshold\")Once reuse is set to True, it cannot be set back to False within the\\nblock. Moreover, if you define other variable scopes inside this one,\\nthey will automatically inherit \\nreuse=True. Lastly, only variables\\ncreated by \\nget_variable() can be reused this way.\\nSharing Variables | 249\\nNow you have all the pieces you need to make the \\nrelu() function access the threshold variable without having to pass it as a parameter:\\ndef relu(X):    with tf.variable_scope(\"relu\", reuse=True):        threshold = tf.get_variable(\"threshold\")  # reuse existing variable        [...]        return tf.maximum(z, threshold, name=\"max\")X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")with tf.variable_scope(\"relu\"):  # create the variable    threshold = tf.get_variable(\"threshold\", shape=(),                                initializer=tf.constant_initializer(0.0))relus = [relu(X) for relu_index in range(5)]output = tf.add_n(relus, name=\"output\")This code first defines the relu() function, then creates the \\nrelu/threshold variable\\n(as a scalar that will later be initialized to \\n0.0) and builds five ReLUs by calling the\\nrelu() function. The relu() function reuses the relu/threshold variable, and cre…ates the other ReLU nodes.\\nVariables created using \\nget_variable() are always named using\\nthe name of their variable_scope as a prefix (e.g., \"relu/threshold\"), but for all other nodes (including variables created with\\ntf.Variable()) the variable scope acts like a new name scope. Inparticular, if a name scope with an identical name was already cre…\\nated, then a suffix is added to make the name unique. For example,\\nall nodes created in the preceding code (except the \\nthreshold vari…\\nable) have a name \\nprefixed with \"relu_1/\" to \\n\"relu_5/\", as shownin Figure 9-8.Figure 9-8. Five ReLUs sharing the threshold variable\\nIt is somewhat unfortunate that the \\nthreshold variable must be defined outside the\\nrelu() function, where all the rest of the ReLU code resides. To fix this, the following\\ncode creates the \\nthreshold variable within the relu() function upon the first call,then reuses it in subsequent calls. Now the \\nrelu() function does not have to worry\\nabout name scopes or variable sharing: it just calls get_variable(), which will create\\n250 | Chapter 9: Up and Running with TensorFlow\\nor reuse the threshold variable (it does not need to know which is the case). The restof the code calls relu() five times, making sure to set reuse=False on the first call,and reuse=True for the other calls.def relu(X):    threshold = tf.get_variable(\"threshold\", shape=(),                                initializer=tf.constant_initializer(0.0))    [...]    return tf.maximum(z, threshold, name=\"max\")X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")relus = []for relu_index in range(5):    with tf.variable_scope(\"relu\", reuse=(relu_index >= 1)) as scope:        relus.append(relu(X))output = tf.add_n(relus, name=\"output\")The resulting graph is slightly different than before, since the shared variable lives\\nwithin the first ReLU (see \\nFigure 9-9).Figure 9-9. Five ReLUs sharing the threshold variable\\nThis concludes this introduction to TensorFlow. We will discuss more advanced top…\\nics as we go through the following chapters, in particular many operations related to\\ndeep neural networks, convolutional neural networks, and recurrent neural networks\\nas well as how to scale up with TensorFlow using multithreading, queues, multiple\\nGPUs, and multiple servers.\\nExercises1.What are the main benefits of creating a computation graph rather than directly\\nexecuting the computations? What are the main drawbacks?\\n2.Is the statement \\na_val = a.eval(session=sess) equivalent to \\na_val =sess.run(a)?3.Is the statement \\na_val, b_val = a.eval(session=sess), b.eval(session=sess) equivalent to \\na_val, b_val = sess.run([a, b])?4.Can you run two graphs in the same session?\\nExercises | 251\\n5.If you create a graph \\ng containing a variable \\nw, then start two threads and open asession in each thread, both using the same graph \\ng, will each session have its\\nown copy of the variable w or will it be shared?6.When is a variable initialized? When is it destroyed?7.What is the difference between a placeholder and a variable?\\n8.What happens when you run the graph to evaluate an operation that depends on\\na placeholder but you don‡t feed its value? What happens if the operation does\\nnot depend on the placeholder?9.When you run a graph, can you feed the output value of any operation, or just\\nthe value of placeholders?10.How can you set a variable to any value you want (during the execution phase)?\\n11.How many times does reverse-mode autodiff need to traverse the graph in order\\nto compute the gradients of the cost function with regards to 10 variables? What\\nabout forward-mode autodiff? And symbolic differentiation?\\n12.Implement Logistic Regression with Mini-batch Gradient Descent using Tensor…\\nFlow. Train it and evaluate it on the moons dataset (introduced in \\nChapter 5\\n). Try\\nadding all the bells and whistles:‹Define the graph within a \\nlogistic_regression() function that can be reused\\neasily.\\n‹Save checkpoints using a \\nSaver at regular intervals during training, and save\\nthe final model at the end of training.\\n‹Restore the last checkpoint upon startup if training was interrupted.\\n‹Define the graph using nice scopes so the graph looks good in TensorBoard.\\n‹Add summaries to visualize the learning curves in TensorBoard.\\n‹Try tweaking some hyperparameters such as the learning rate or the mini-\\nbatch size and look at the shape of the learning curve.\\nSolutions to these exercises are available in \\nAppendix A\\n.252 | Chapter 9: Up and Running with TensorFlow\\n1You can get the best of both worlds by being open to biological inspirations without being afraid to create\\nbiologically unrealistic models, as long as they work well.CHAPTER 10Introduction to Arti•cial Neural NetworksBirds inspired us to fly, burdock plants inspired velcro, and nature has inspired many\\nother inventions. It seems only logical, then, to look at the brain‡s architecture for\\ninspiration on how to build an intelligent machine. This is the key idea that inspired\\narti†cial neural networks\\n (ANNs). However, although planes were inspired by birds,\\nthey don‡t have to flap their wings. Similarly, ANNs have gradually become quite dif…\\nferent from their biological cousins. Some researchers even argue that we should drop\\nthe biological analogy altogether (e.g., by saying ƒunits⁄ rather than ƒneurons⁄), lest\\nwe restrict our creativity to biologically plausible systems.\\n1ANNs are at the very core of Deep Learning. They are versatile, powerful, and scala…\\nble, making them ideal to tackle large and highly complex Machine Learning tasks,\\nsuch as classifying billions of images (e.g., Google Images), powering speech recogni…\\ntion services (e.g., Apple‡s Siri), recommending the best videos to watch to hundreds\\nof millions of users every day (e.g., YouTube), or learning to beat the world champion\\nat the game of \\nGo\\n by examining millions of past games and then playing against \\nitself(DeepMind‡s AlphaGo).\\nIn this chapter, we will introduce artificial neural networks, starting with a quick tour\\nof the very first ANN architectures. Then we will present \\nMulti-Layer Perceptrons\\n(MLPs) \\nand implement one using TensorFlow to tackle the MNIST digit classification\\nproblem (introduced in \\nChapter 3\\n).2532ƒA Logical Calculus of Ideas Immanent in Nervous Activity,⁄ W. McCulloch and W. Pitts (1943).\\nFrom Biological to Arti•cial NeuronsSurprisingly, ANNs have been around for quite a while: they were first introduced\\nback in 1943 by the neurophysiologist Warren McCulloch and the mathematician\\nWalter Pitts. In their \\nlandmark paper\\n,2 ƒA Logical Calculus of Ideas Immanent in\\nNervous Activity,⁄ McCulloch and Pitts presented a simplified computational model\\nof how biological neurons might work together in animal brains to perform complex\\ncomputations using \\npropositional logic\\n. This was the first artificial neural networkarchitecture. Since then many other architectures have been invented, as we will see.\\nThe early successes of ANNs until the 1960s led to the widespread belief that we\\nwould soon be conversing with truly intelligent machines. When it became clear that\\nthis promise would go unfulfilled (at least for quite a while), funding flew elsewhere\\nand ANNs entered a long dark era. In the early 1980s there was a revival of interest in\\nANNs as new network architectures were invented and better training techniques\\nwere developed. But by the 1990s, powerful alternative Machine Learning techniques\\nsuch as Support Vector Machines (see \\nChapter 5\\n) were favored by most researchers,\\nas they seemed to offer better results and stronger theoretical foundations. Finally, we\\nare now witnessing yet another wave of interest in ANNs. Will this wave die out like\\nthe previous ones did? There are a few good reasons to believe that this one is differ…\\nent and will have a much more profound impact on our lives:\\n‹There is now a huge quantity of data available to train neural networks, and\\nANNs frequently outperform other ML techniques on very large and complex\\nproblems.‹The tremendous increase in computing power since the 1990s now makes it pos…\\nsible to train large neural networks in a reasonable amount of time. This is in\\npart due to Moore‡s Law, but also thanks to the gaming industry, which has pro…\\nduced powerful GPU cards by the millions.‹The training algorithms have been improved. To be fair they are only slightly dif…\\nferent from the ones used in the 1990s, but these relatively small tweaks have a\\nhuge positive impact.\\n‹Some theoretical limitations of ANNs have turned out to be benign in practice.\\nFor example, many people thought that ANN training algorithms were doomed\\nbecause they were likely to get stuck in local optima, but it turns out that this is\\nrather rare in practice (or when it is the case, they are usually fairly close to the\\nglobal optimum).\\n‹ANNs seem to have entered a virtuous circle of funding and progress. Amazing\\nproducts based on ANNs regularly make the headline news, which pulls more\\n254 | Chapter 10: Introduction to \\nArti•cial Neural Networks3Image by Bruce Blaus (\\nCreative Commons 3.0\\n). Reproduced from https://en.wikipedia.org/wiki/Neuron\\n.and more attention and funding toward them, resulting in more and more pro…\\ngress, and even more amazing products.Biological NeuronsBefore we discuss artificial neurons, let‡s take a quick look at a biological neuron (rep…\\nresented in \\nFigure 10-1). It is an unusual-looking cell mostly found in animal cerebral\\ncortexes (e.g., your brain), composed of a \\ncell body\\n containing the nucleus and most\\nof the cell‡s complex components, and many branching extensions called \\ndendrites\\n,plus one very long extension called the \\naxon\\n. The axon‡s length may be just a few\\ntimes longer than the cell body, or up to tens of thousands of times longer. Near its\\nextremity the axon splits off into many branches called \\ntelodendria\\n, and at the tip of\\nthese branches are minuscule structures called \\nsynaptic terminals\\n (or simply \\nsynap…\\nses\\n), which are connected to the dendrites (or directly to the cell body) of other neu…rons. Biological neurons receive short electrical impulses called \\nsignals\\n from otherneurons via these synapses. When a neuron receives a sufficient number of signals\\nfrom other neurons within a few milliseconds, it fires its own signals.Figure 10-1. Biological neuron\\n3Thus, individual biological neurons seem to behave in a rather simple way, but they\\nare organized in a vast network of billions of neurons, each neuron typically connec…ted to thousands of other neurons. Highly complex computations can be performed\\nby a vast network of fairly simple neurons, much like a complex anthill can emerge\\nfrom the combined efforts of simple ants. The architecture of biological neural net…\\nFrom Biological to Arti•cial Neurons | 255\\n4In the context of Machine Learning, the phrase ƒneural networks⁄ generally refers to ANNs, not BNNs.\\n5Drawing of a cortical lamination by S. Ramon y Cajal (public domain). Reproduced from \\nhttps://en.wikipe\\ndia.org/wiki/Cerebral_cortex\\n.works (BNN)4 is still the subject of active research, but some parts of the brain have\\nbeen mapped, and it seems that neurons are often organized in consecutive layers, as \\nshown in Figure 10-2.Figure 10-2. Multiple layers in a biological neural network (human cortex)\\n5Logical Computations with NeuronsWarren McCulloch and Walter Pitts proposed a very simple model of the biological\\nneuron, which later became known as an \\narti†cial neuron\\n: it has one or more binary\\n(on/off) inputs and one binary output. The artificial neuron simply activates its out…\\nput when more than a certain number of its inputs are active. McCulloch and Pitts\\nshowed that even with such a simplified model it is possible to build a network of\\nartificial neurons that computes any logical proposition you want. For example, let‡s\\nbuild a few ANNs that perform various logical computations (see \\nFigure 10-3),assuming that a neuron is activated when at least two of its inputs are active.\\nFigure 10-3. ANNs performing simple logical computations\\n256 | Chapter 10: Introduction to \\nArti•cial Neural Networks‹The first network on the left is simply the identity function: if neuron A is activa…\\nted, then neuron C gets activated as well (since it receives two input signals from\\nneuron A), but if neuron A is off, then neuron C is off as well.‹The second network performs a logical AND: neuron C is activated only when\\nboth neurons A and B are activated (a single input signal is not enough to acti…\\nvate neuron C).\\n‹The third network performs a logical OR: neuron C gets activated if either neu…\\nron A or neuron B is activated (or both).\\n‹Finally, if we suppose that an input connection can inhibit the neuron‡s activity\\n(which is the case with biological neurons), then the fourth network computes a\\nslightly more complex logical proposition: neuron C is activated only if neuron A\\nis active and if neuron B is off. If neuron A is active all the time, then you get alogical NOT: neuron C is active when neuron B is off, and vice versa.\\nYou can easily imagine how these networks can be combined to compute complex\\nlogical expressions (see the exercises at the end of the chapter).\\nThe PerceptronThe Perceptron\\n is one of the simplest ANN architectures, invented in 1957 by Frank\\nRosenblatt. It is based on a slightly different artificial neuron (see \\nFigure 10-4) called a linear threshold unit\\n (LTU): the inputs and output are now numbers (instead of\\nbinary on/off values) and each input connection is associated with a weight. The LTU\\ncomputes a weighted sum of its inputs (\\nz = w1 x1 + w2 x2 +  + wn xn = wT ’ x), thenapplies a \\nstep function\\n to that sum and outputs the result: \\nhw(x) = step (z) = step (wT ’\\nx).Figure 10-4. Linear threshold unit\\nThe most common step function used in Perceptrons is the \\nHeaviside step function\\n(see Equation 10-1\\n). Sometimes the sign function is used instead.From Biological to Arti•cial Neurons | 257\\n6The name Perceptron\\n is sometimes used to mean a tiny network with a single LTU.\\nEquation 10-1. Common step functions used in Perceptrons\\nheavisidez=0if\\nz<0\\n1if\\nzŠ0\\nsgnz=”1if\\nz<0\\n0if\\nz=0\\n+1if\\nz>0\\nA single LTU can be used for simple linear binary classification. It computes a linear\\ncombination of the inputs and if the result exceeds a threshold, it outputs the positive\\nclass or else outputs the negative class (just like a Logistic Regression classifier or a\\nlinear SVM). For example, you could use a single LTU to classify iris flowers based on\\nthe petal length and width (also adding an extra bias feature \\nx0 = 1, just like we did inprevious chapters). Training an LTU means finding the right values for \\nw0, w1, and w2(the training algorithm is discussed shortly).A Perceptron is simply composed of a single layer of LTUs,\\n6 with each neuron con…nected to all the inputs. These connections are often represented using special pass…\\nthrough neurons called input neurons\\n: they just output whatever input they are fed.\\nMoreover, an extra bias feature is generally added (\\nx0 = 1). This bias feature is typi…\\ncally represented using a special type of neuron called a \\nbias neuron\\n, which just out…puts 1 all the time.A Perceptron with two inputs and three outputs is represented in \\nFigure 10-5. ThisPerceptron can classify instances simultaneously into three different binary classes,\\nwhich makes it a multioutput classifier.\\nFigure 10-5. Perceptron diagram\\nSo how is a Perceptron trained? The Perceptron training algorithm proposed by\\nFrank Rosenblatt was largely inspired by \\nHebb‹s rule\\n. In his book •e Organization of\\nBehavior\\n, published in 1949, Donald Hebb suggested that when a biological neuron\\n258 | Chapter 10: Introduction to \\nArti•cial Neural Networks7Note that this solution is generally not unique: in general when the data are linearly separable, there is an\\ninfinity of hyperplanes that can separate them.\\noften triggers another neuron, the connection between these two neurons growsstronger. This idea was later summarized by Siegrid Lšwel in this catchy phrase:\\nƒCells that fire together, wire together.⁄ This rule later became known as Hebb‡s rule \\n(or Hebbian learning\\n); that is, the connection weight between two neurons is\\nincreased whenever they have the same output. Perceptrons are trained using a var…\\niant of this rule that takes into account the error made by the network; it does not\\nreinforce connections that lead to the wrong output. More specifically, the Perceptron\\nis fed one training instance at a time, and for each instance it makes its predictions.\\nFor every output neuron that produced a wrong prediction, it reinforces the connec…\\ntion weights from the inputs that would have contributed to the correct prediction.\\nThe rule is shown in Equation 10-2\\n.Equation 10-2. Perceptron learning rule (weight update)\\nwi,jnextstep\\n=wi,j+−yj”yjxi‹wi, j is the connection weight between the i\\nth input neuron and the j\\nth output neu…ron.‹xi is the ith input value of the current training instance.\\n‹yj is the output of the jth output neuron for the current training instance.\\n‹yj is the target output of the jth output neuron for the current training instance.\\n‹− is the learning rate.\\nThe decision boundary of each output neuron is linear, so Perceptrons are incapable\\nof learning complex patterns (just like Logistic Regression classifiers). However, if the\\ntraining instances are linearly separable, Rosenblatt demonstrated that this algorithm\\nwould converge to a solution.\\n7 This is called the Perceptron convergence theorem\\n.Scikit-Learn provides a Perceptron class that implements a single LTU network. It\\ncan be used pretty much as you would expect›for example, on the iris dataset (intro…\\nduced in Chapter 4\\n):import numpy as npfrom sklearn.datasets import load_irisfrom sklearn.linear_model import Perceptroniris = load_iris()X = iris.data[:, (2, 3)]  # petal length, petal widthy = (iris.target == 0).astype(np.int)  # Iris Setosa?From Biological to Arti•cial Neurons | 259\\nper_clf = Perceptron(random_state=42)per_clf.fit(X, y)y_pred = per_clf.predict([[2, 0.5]])You may have recognized that the Perceptron learning algorithm strongly resembles \\nStochastic Gradient Descent. In fact, Scikit-Learn‡s \\nPerceptron class is equivalent to\\nusing an SGDClassifier with the following hyperparameters: \\nloss=\"perceptron\",learning_rate=\"constant\", eta0=1 (the learning rate), and \\npenalty=None (no regu…\\nlarization).\\nNote that contrary to Logistic Regression classifiers, Perceptrons do not output a class\\nprobability; rather, they just make predictions based on a hard threshold. This is one\\nof the good reasons to prefer Logistic Regression over Perceptrons.\\nIn their 1969 monograph titled \\nPerceptrons\\n, Marvin Minsky and Seymour Papert\\nhighlighted a number of serious weaknesses of Perceptrons, in particular the fact that\\nthey are incapable of solving some trivial problems (e.g., the \\nExclusive OR\\n (XOR)\\nclassification problem; see the left side of \\nFigure 10-6). Of course this is true of any\\nother linear classification model as well (such as Logistic Regression classifiers), but\\nresearchers had expected much more from Perceptrons, and their disappointment\\nwas great: as a result, many researchers dropped \\nconnectionism\\n altogether (i.e., thestudy of neural networks) in favor of higher-level problems such as logic, problem\\nsolving, and search.However, it turns out that some of the limitations of Perceptrons can be eliminated by\\nstacking multiple Perceptrons. The resulting ANN is called a \\nMulti-Layer Perceptron\\n(MLP). In particular, an MLP can solve the XOR problem, as you can verify by com…\\nputing the output of the MLP represented on the right of \\nFigure 10-6, for each com…bination of inputs: with inputs (0, 0) or (1, 1) the network outputs 0, and with \\ninputs\\n(0, 1) or (1, 0) it outputs 1.Figure 10-6. XOR \\nclassi†cation problem and an MLP that solves it\\n260 | Chapter 10: Introduction to \\nArti•cial Neural Networks8ƒLearning Internal Representations by Error Propagation,⁄ D. Rumelhart, G. Hinton, R. Williams (1986).\\n9This algorithm was actually invented several times by various researchers in different fields, starting with\\nP. Werbos in 1974.\\nMulti-Layer Perceptron and BackpropagationAn MLP is composed of one (passthrough) input layer, one or more layers of LTUs,\\ncalled hidden layers\\n, and one final layer of LTUs called the \\noutput layer\\n (seeFigure 10-7). Every layer except the output layer includes a bias neuron and is fully\\nconnected to the next layer. When an ANN has two or more hidden layers, it is called \\na deep neural network\\n (DNN).Figure 10-7. Multi-Layer Perceptron\\nFor many years researchers struggled to find a way to train MLPs, without success.\\nBut in 1986, D. E. Rumelhart et al. published a \\ngroundbreaking article8 introducing\\nthe backpropagation\\n training algorithm.9 Today we would describe it as Gradient\\nDescent using reverse-mode autodiff (Gradient Descent was introduced in \\nChapter 4\\n,and autodiff was discussed in \\nChapter 9\\n).For each training instance, the algorithm feeds it to the network and computes the\\noutput of every neuron in each consecutive layer (this is the forward pass, just like\\nwhen making predictions). Then it measures the network‡s output error (i.e., the dif…\\nference between the desired output and the actual output of the network), and itcomputes how much each neuron in the last hidden layer contributed to each output\\nneuron‡s error. It then proceeds to measure how much of these error contributions\\ncame from each neuron in the previous hidden layer›and so on until the algorithm\\nreaches the input layer. This reverse pass efficiently measures the error gradient\\nacross all the connection weights in the network by propagating the error gradient\\nbackward in the network (hence the name of the algorithm). If you check out theFrom Biological to Arti•cial Neurons | 261\\nreverse-mode autodiff algorithm in \\nAppendix D\\n, you will find that the forward and\\nreverse passes of backpropagation simply perform reverse-mode autodiff. The last\\nstep of the backpropagation algorithm is a Gradient Descent step on all the connec…\\ntion weights in the network, using the error gradients measured earlier.\\nLet‡s make this even shorter: for each training instance the backpropagation algo…\\nrithm first makes a prediction (forward pass), measures the error, then goes through\\neach layer in reverse to measure the error contribution from each connection (reverse\\npass), and finally slightly tweaks the connection weights to reduce the \\nerror (Gradient\\nDescent step).\\nIn order for this algorithm to work properly, the authors made a key change to the\\nMLP‡s architecture: they replaced the step function with the logistic function, \\n„(z) =1 / (1 + exp(–z)). This was essential because the step function contains only flat seg…\\nments, so there is no gradient to work with (Gradient Descent cannot move on a flat\\nsurface), while the logistic function has a well-defined nonzero derivative every…\\nwhere, allowing Gradient Descent to make some progress at every step. The backpro…\\npagation algorithm may be used with other \\nactivation functions\\n, instead of the logisticfunction. Two other popular activation functions are:\\n•e hyperbolic tangent function tanh (z) = 2„(2z) ‘ 1\\nJust like the logistic function it is S-shaped, continuous, and differentiable, but its\\noutput value ranges from –1 to 1 (instead of 0 to 1 in the case of the logistic func…tion), which tends to make each layer‡s output more or less normalized (i.e., cen…\\ntered around 0) at the beginning of training. This often helps speed up\\nconvergence.\\n•e ReLU function (introduced in \\nChapter 9\\n)ReLU (\\nz) = max (0, z). It is continuous but unfortunately not differentiable at \\nz =\\n0 (the slope changes abruptly, which can make Gradient Descent bounce\\naround). However, in practice it works very well and has the advantage of being\\nfast to compute. Most importantly, the fact that it does not have a maximum out…\\nput value also helps reduce some issues during Gradient Descent (we will come\\nback to this in Chapter 11\\n).These popular activation functions and their derivatives are represented in\\nFigure 10-8.262 | Chapter 10: Introduction to \\nArti•cial Neural NetworksFigure 10-8. Activation functions and their derivatives\\nAn MLP is often used for classification, with each output corresponding to a different\\nbinary class (e.g., spam/ham, urgent/not-urgent, and so on). When the classes are\\nexclusive (e.g., classes 0 through 9 for digit image classification), the output layer is\\ntypically modified by replacing the individual activation functions by a shared \\nso“…max function (see Figure 10-9). The softmax function was introduced in \\nChapter 3\\n.The output of each neuron corresponds to the estimated probability of the corre…\\nsponding class. Note that the signal flows only in one direction (from the inputs to\\nthe outputs), so this architecture is an example of a \\nfeedforward neural network\\n(FNN).Figure 10-9. A modern MLP (including ReLU and \\nso“max) for \\nclassi†cationFrom Biological to Arti•cial Neurons | 263\\nBiological neurons seem to implement a roughly sigmoid (S-\\nshaped) activation function, so researchers stuck to sigmoid func…\\ntions for a very long time. But it turns out that the ReLU activation\\nfunction generally works better in ANNs. This is one of the cases\\nwhere the biological analogy was misleading.\\nTraining an MLP with TensorFlow‡s High-Level APIThe simplest way to train an MLP with TensorFlow is to use the high-level API\\nTF.Learn, which is quite similar to Scikit-Learn‡s API. The \\nDNNClassifier classmakes it trivial to train a deep neural network with any number of hidden layers, and\\na softmax output layer to output estimated class probabilities. For example, the fol…\\nlowing code trains a DNN for classification with two hidden layers (one with 300\\nneurons, and the other with 100 neurons) and a softmax output layer with 10\\nneurons:import tensorflow as tffeature_columns = tf.contrib.learn.infer_real_valued_columns_from_input(X_train)dnn_clf = tf.contrib.learn.DNNClassifier(hidden_units=[300, 100], n_classes=10,                                         feature_columns=feature_columns)dnn_clf.fit(x=X_train, y=y_train, batch_size=50, steps=40000)If you run this code on the MNIST dataset (after scaling it, e.g., by using Scikit-\\nLearn‡s \\nStandardScaler), you may actually get a model that achieves over 98.1%\\naccuracy on the test set! That‡s better than the best model we trained in \\nChapter 3\\n:>>> from sklearn.metrics import accuracy_score>>> y_pred = list(dnn_clf.predict(X_test))>>> accuracy_score(y_test, y_pred)0.98180000000000001The TF.Learn library also provides some convenience functions to evaluate models:\\n>>> dnn_clf.evaluate(X_test, y_test){•accuracy•: 0.98180002, •global_step•: 40000, •loss•: 0.073678359}Under the hood, the \\nDNNClassifier class creates all the neuron layers, based on the\\nReLU activation function (we can change this by setting the \\nactivation_fn hyper…\\nparameter). The output layer relies on the softmax function, and the cost function is\\ncross entropy (introduced in \\nChapter 4\\n).The TF.Learn API is still quite new, so some of the names and func…\\ntions used in these examples may evolve a bit by the time you read\\nthis book. However, the general ideas should not change.\\n264 | Chapter 10: Introduction to \\nArti•cial Neural NetworksTraining a DNN Using Plain TensorFlowIf you want more control over the architecture of the network, you may prefer to use\\nTensorFlow‡s lower-level Python API (introduced in \\nChapter 9\\n). In this section wewill build the same model as before using this API, and we will implement Mini-\\nbatch Gradient Descent to train it on the MNIST dataset. The first step is the con…\\nstruction phase, building the TensorFlow graph. The second step is the execution\\nphase, where you actually run the graph to train the model.\\nConstruction PhaseLet‡s start. First we need to import the \\ntensorflow library. Then we must specify the\\nnumber of inputs and outputs, and set the number of hidden neurons in each layer:\\nimport tensorflow as tfn_inputs = 28*28  # MNISTn_hidden1 = 300n_hidden2 = 100n_outputs = 10Next, just like you did in \\nChapter 9\\n, you can use placeholder nodes to represent the\\ntraining data and targets. The shape of \\nX is only partially defined. We know that it will\\nbe a 2D tensor (i.e., a matrix), with instances along the first dimension and features\\nalong the second dimension, and we know that the number of features is going to be\\n28 x 28 (one feature per pixel), but we don‡t know yet how many instances each train…\\ning batch will contain. So the shape of \\nX is (None, n_inputs). Similarly, we know\\nthat \\ny will be a 1D tensor with one entry per instance, but again we don‡t know the\\nsize of the training batch at this point, so the shape is \\n(None).X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")y = tf.placeholder(tf.int64, shape=(None), name=\"y\")Now let‡s create the actual neural network. The placeholder \\nX will act as the input\\nlayer; during the execution phase, it will be replaced with one training batch at a time\\n(note that all the instances in a training batch will be processed simultaneously by the\\nneural network). Now you need to create the two hidden layers and the output layer.\\nThe two hidden layers are almost identical: they differ only by the inputs they are\\nconnected to and by the number of neurons they contain. The output layer is also\\nvery similar, but it uses a softmax activation function instead of a ReLU activation\\nfunction. So let‡s create a \\nneuron_layer() function that we will use to create one layer\\nat a time. It will need parameters to specify the inputs, the number of neurons, the\\nactivation function, and the name of the layer:\\ndef neuron_layer(X, n_neurons, name, activation=None):    with tf.name_scope(name):        n_inputs = int(X.get_shape()[1])Training a DNN Using Plain TensorFlow | 265\\n10Using a truncated normal distribution rather than a regular normal distribution ensures that there won‡t be\\nany large weights, which could slow down training.\\n11For example, if you set all the weights to 0, then all neurons will output 0, and the error gradient will be the\\nsame for all neurons in a given hidden layer. The Gradient Descent step will then update all the weights in\\nexactly the same way in each layer, so they will all remain equal. In other words, despite having hundreds of\\nneurons per layer, your model will act as if there were only one neuron per layer. It is not going to fly.\\n        stddev = 2 / np.sqrt(n_inputs)        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)        W = tf.Variable(init, name=\"weights\")        b = tf.Variable(tf.zeros([n_neurons]), name=\"biases\")        z = tf.matmul(X, W) + b        if activation==\"relu\":            return tf.nn.relu(z)        else:            return zLet‡s go through this code line by line:\\n1.First we create a name scope using the name of the layer: it will contain all the\\ncomputation nodes for this neuron layer. This is optional, but the graph will look\\nmuch nicer in TensorBoard if its nodes are well organized.\\n2.Next, we get the number of inputs by looking up the input matrix‡s shape and\\ngetting the size of the second dimension (the first dimension is for instances).3.The next three lines create a \\nW variable that will hold the weights matrix. It will be\\na 2D tensor containing all the connection weights between each input and each\\nneuron; hence, its shape will be \\n(n_inputs, n_neurons). It will be initialized\\nrandomly, using a truncated\\n10 normal (Gaussian) distribution with a standard\\ndeviation of \\n2/\\nninputs. Using this specific standard deviation helps the algorithm\\nconverge much faster (we will discuss this further in \\nChapter 11\\n; it is one of thosesmall tweaks to neural networks that have had a tremendous impact on their effi…\\nciency). It is important to initialize connection weights randomly for all hidden\\nlayers to avoid any symmetries that the Gradient Descent algorithm would be\\nunable to break.114.The next line creates a \\nb variable for biases, initialized to 0 (no symmetry issue in\\nthis case), with one bias parameter per neuron.5.Then we create a subgraph to compute \\nz = \\nX ’ \\nW + \\nb. This vectorized implemen…\\ntation will efficiently compute the weighted sums of the inputs plus the bias term\\nfor each and every neuron in the layer, for all the instances in the batch in just\\none shot.6.Finally, if the \\nactivation parameter is set to \"relu\", the code returns relu(z) (i.e., max (0, z)), or else it just returns z.266 | Chapter 10: Introduction to \\nArti•cial Neural NetworksOkay, so now you have a nice function to create a neuron layer. Let‡s use it to create\\nthe deep neural network! The first hidden layer takes \\nX as its input. The second takes\\nthe output of the first hidden layer as its input. And finally, the output layer takes the\\noutput of the second hidden layer as its input.\\nwith tf.name_scope(\"dnn\"):    hidden1 = neuron_layer(X, n_hidden1, \"hidden1\", activation=\"relu\")    hidden2 = neuron_layer(hidden1, n_hidden2, \"hidden2\", activation=\"relu\")    logits = neuron_layer(hidden2, n_outputs, \"outputs\")Notice that once again we used a name scope for clarity. Also note that \\nlogits is theoutput of the neural network before\\n going through the softmax activation function:\\nfor optimization reasons, we will handle the softmax computation later.\\nAs you might expect, TensorFlow comes with many handy functions to create\\nstandard neural network layers, so there‡s often no need to define your own\\nneuron_layer() function like we just did. For example, TensorFlow‡s \\nfully_connected() function creates a fully connected layer, where all the inputs are connected to\\nall the neurons in the layer. It takes care of creating the \\nweights and \\nbiases variables,\\nwith the proper initialization strategy, and it uses the ReLU activation function by\\ndefault (we can change this using the \\nactivation_fn argument). As we will see in\\nChapter 11\\n, it also supports regularization and normalization parameters. Let‡s tweak\\nthe preceding code to use the fully_connected() function instead of our neuron_layer() function. Simply import the function and replace the \\ndnn constructionsection with the following code:from tensorflow.contrib.layers import fully_connectedwith tf.name_scope(\"dnn\"):    hidden1 = fully_connected(X, n_hidden1, scope=\"hidden1\")    hidden2 = fully_connected(hidden1, n_hidden2, scope=\"hidden2\")    logits = fully_connected(hidden2, n_outputs, scope=\"outputs\",                             activation_fn=None)The tensorflow.contrib package contains many useful functions,\\nbut it is a place for experimental code that has not yet graduated to\\nbe part of the main TensorFlow API. So the \\nfully_connected()function (and any other \\ncontrib code) may change or move in the\\nfuture.Now that we have the neural network model ready to go, we need to define the \\ncostfunction that we will use to train it. Just as we did for Softmax Regression in \\nChap…\\nter 4, we will use cross entropy. As we discussed earlier, cross entropy will penalize\\nmodels that estimate a low probability for the target class. TensorFlow provides\\nseveral functions to compute cross entropy. We will use \\nsparse_softmax_cross_entropy_with_logits(): it computes the cross entropy based on the\\nTraining a DNN Using Plain TensorFlow | 267\\nƒlogits⁄ (i.e., the output of the network \\nbefore\\n going through the softmax activation\\nfunction), and it expects labels in the form of integers ranging from 0 to the number\\nof classes minus 1 (in our case, from 0 to 9). This will give us a 1D tensor containing\\nthe cross entropy for each instance. We can then use TensorFlow‡s \\nreduce_mean()function to compute the mean cross entropy over all instances.\\nwith tf.name_scope(\"loss\"):    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(                   labels=y, logits=logits)    loss = tf.reduce_mean(xentropy, name=\"loss\")The sparse_softmax_cross_entropy_with_logits() function isequivalent to applying the softmax activation function and then\\ncomputing the cross entropy, but it is more efficient, and it prop…\\nerly takes care of corner cases like logits equal to 0. This is why we\\ndid not apply the softmax activation function earlier. There is also\\nanother function called softmax_cross_entropy_with_logits(),which takes labels in the form of one-hot vectors (instead of ints\\nfrom 0 to the number of classes minus 1).\\nWe have the neural network model, we have the cost function, and now we need to\\ndefine a GradientDescentOptimizer that will tweak the model parameters to mini…\\nmize the cost function. Nothing new; it‡s just like we did in \\nChapter 9\\n:learning_rate = 0.01with tf.name_scope(\"train\"):    optimizer = tf.train.GradientDescentOptimizer(learning_rate)    training_op = optimizer.minimize(loss)The last important step in the construction phase is to specify how to evaluate the\\nmodel. We will simply use accuracy as our performance measure. First, for each\\ninstance, determine if the neural network‡s prediction is correct by checking whether\\nor not the highest logit corresponds to the target class. For this you can use the\\nin_top_k() function. This returns a 1D tensor full of boolean values, so we need tocast these booleans to floats and then compute the average. This will give us the net…\\nwork‡s overall accuracy.\\nwith tf.name_scope(\"eval\"):    correct = tf.nn.in_top_k(logits, y, 1)    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))And, as usual, we need to create a node to initialize all variables, and we will also cre…\\nate a \\nSaver to save our trained model parameters to disk:\\ninit = tf.global_variables_initializer()saver = tf.train.Saver()268 | Chapter 10: Introduction to \\nArti•cial Neural NetworksPhew! This concludes the construction phase. This was fewer than 40 lines of code,but it was pretty intense: we created placeholders for the inputs and the targets, we\\ncreated a function to build a neuron layer, we used it to create the DNN, we defined\\nthe cost function, we created an optimizer, and finally we defined the performance\\nmeasure. Now on to the execution phase.\\nExecution PhaseThis part is much shorter and simpler. First, let‡s load MNIST. We could use Scikit-\\nLearn for that as we did in previous chapters, but TensorFlow offers its own helper\\nthat fetches the data, scales it (between 0 and 1), shuffles it, and provides a simple\\nfunction to load one mini-batches a time. So let‡s use it instead:\\nfrom tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets(\"/tmp/data/\")Now we define the number of epochs that we want to run, as well as the size of the\\nmini-batches:\\nn_epochs = 400batch_size = 50And now we can train the model:with tf.Session() as sess:    init.run()    for epoch in range(n_epochs):        for iteration in range(mnist.train.num_examples // batch_size):            X_batch, y_batch = mnist.train.next_batch(batch_size)            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})        acc_test = accuracy.eval(feed_dict={X: mnist.test.images,                                            y: mnist.test.labels})        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)    save_path = saver.save(sess, \"./my_model_final.ckpt\")This code opens a TensorFlow session, and it runs the \\ninit node that initializes all\\nthe variables. Then it runs the main training loop: at each epoch, the code iterates\\nthrough a number of mini-batches that corresponds to the training set size. Each\\nmini-batch is fetched via the \\nnext_batch() method, and then the code simply runs\\nthe training operation, feeding it the current mini-batch input data and targets. Next,\\nat the end of each epoch, the code evaluates the model on the last mini-batch and on\\nthe full training set, and it prints out the result. Finally, the model parameters are\\nsaved to disk.\\nTraining a DNN Using Plain TensorFlow | 269\\nUsing the Neural NetworkNow that the neural network is trained, you can use it to make predictions. To do\\nthat, you can reuse the same construction phase, but change the execution phase like\\nthis:with tf.Session() as sess:    saver.restore(sess, \"./my_model_final.ckpt\")    X_new_scaled = [...]  # some new images (scaled from 0 to 1)    Z = logits.eval(feed_dict={X: X_new_scaled})    y_pred = np.argmax(Z, axis=1)First the code loads the model parameters from disk. Then it loads some new imagesthat you want to classify. Remember to apply the same feature scaling as for the train…\\ning data (in this case, scale it from 0 to 1). Then the code evaluates the \\nlogits node.If you wanted to know all the estimated class probabilities, you would need to apply\\nthe softmax() function to the logits, but if you just want to predict a class, you can\\nsimply pick the class that has the highest logit value (using the \\nargmax() functiondoes the trick).Fine-Tuning Neural Network HyperparametersThe flexibility of neural networks is also one of their main drawbacks: there are many\\nhyperparameters to tweak. Not only can you use any \\nimaginable network topology\\n(how neurons are interconnected), but even in a simple MLP you can change the\\nnumber of layers, the number of neurons per layer, the type of activation function to\\nuse in each layer, the weight initialization logic, and much more. How do you know\\nwhat combination of hyperparameters is the best for your task?\\nOf course, you can use grid search with cross-validation to find the right hyperpara…\\nmeters, like you did in previous chapters, but since there are many hyperparameters\\nto tune, and since training a neural network on a large dataset takes a lot of time, you\\nwill only be able to explore a tiny part of the hyperparameter space in a reasonable\\namount of time. It is much better to use \\nrandomized search, as we discussed in Chap…\\nter 2. Another option is to use a tool such as Oscar, which implements more complex\\nalgorithms to help you find a good set of hyperparameters quickly.\\nIt helps to have an idea of what values are reasonable for each hyperparameter, so you\\ncan restrict the search space. Let‡s start with the number of hidden layers.\\nNumber of Hidden LayersFor many problems, you can just begin with a single hidden layer and you will get\\nreasonable results. It has actually been shown that an MLP with just one hidden layer\\ncan model even the most complex functions provided it has enough neurons. For a\\nlong time, these facts convinced researchers that there was no need to investigate any\\n270 | Chapter 10: Introduction to \\nArti•cial Neural Networksdeeper neural networks. But they overlooked the fact that deep networks have a much\\nhigher parameter \\ne⁄ciency than shallow ones: they can model complex functions\\nusing exponentially fewer neurons than shallow nets, making them much faster to\\ntrain.To understand why, suppose you are asked to draw a forest using some drawing soft…\\nware, but you are forbidden to use copy/paste. You would have to draw each tree\\nindividually, branch per branch, leaf per leaf. If you could instead draw one leaf,\\ncopy/paste it to draw a branch, then copy/paste that branch to create a tree, and\\nfinally copy/paste this tree to make a forest, you would be finished in no time. Real-world data is often structured in such a hierarchical way and DNNs automatically\\ntake advantage of this fact: lower hidden layers model low-level structures (e.g., line\\nsegments of various shapes and orientations), intermediate hidden layers combine\\nthese low-level structures to model intermediate-level structures (e.g., squares, cir…\\ncles), and the highest hidden layers and the output layer combine these intermediate\\nstructures to model high-level structures (e.g., faces).Not only does this hierarchical architecture help DNNs converge faster to a good sol…\\nution, it also improves their ability to generalize to new datasets. For example, if you\\nhave already trained a model to recognize faces in pictures, and you now want to\\ntrain a new neural network to recognize hairstyles, then you can kickstart training byreusing the lower layers of the first network. Instead of randomly initializing the\\nweights and biases of the first few layers of the new neural network, you can initialize\\nthem to the value of the weights and biases of the lower layers of the first network.\\nThis way the network will not have to learn from scratch all the low-level structures\\nthat occur in most pictures; it will only have to learn the higher-level structures (e.g.,\\nhairstyles).In summary, for many problems you can start with just one or two hidden layers and\\nit will work just fine (e.g., you can easily reach above 97% accuracy on the MNISTdataset using just one hidden layer with a few hundred neurons, and above 98% accu…\\nracy using two hidden layers with the same total amount of neurons, in roughly the\\nsame amount of training time). For more complex problems, you can gradually ramp\\nup the number of hidden layers, until you start overfitting the training set. Very com…\\nplex tasks, such as large image classification or speech recognition, typically require\\nnetworks with dozens of layers (or even hundreds, but not fully connected ones, as\\nwe will see in Chapter 13\\n), and they need a huge amount of training data. However,\\nyou will rarely have to train such networks from scratch: it is much more common to\\nreuse parts of a pretrained state-of-the-art network that performs a similar task.\\nTraining will be a lot faster and require much less data (we will discuss this in \\nChap…\\nter 11).Fine-Tuning Neural Network Hyperparameters | 271\\n12By Vincent Vanhoucke in his \\nDeep Learning class on Udacity.com.\\n13A few extra ANN architectures are presented in \\nAppendix E\\n.Number of Neurons per Hidden LayerObviously the number of neurons in the input and output layers is determined by the\\ntype of input and output your task requires. For example, the MNIST task requires 28\\nx 28 = 784 input neurons and 10 output neurons. As for the hidden layers, a common\\npractice is to size them to form a funnel, with fewer and fewer neurons at each layer›\\nthe rationale being that many low-level features can coalesce into far fewer high-level\\nfeatures. For example, a typical neural network for MNIST may have two hidden lay…\\ners, the first with 300 neurons and the second with 100. However, this practice is not\\nas common now, and you may simply use the same size for all hidden layers›for\\nexample, all hidden layers with 150 neurons: that‡s just one hyperparameter to tune\\ninstead of one per layer. Just like for the number of layers, you can try increasing the\\nnumber of neurons gradually until the network starts overfitting. In general you will\\nget more bang for the buck by increasing the number of layers than the number of\\nneurons per layer. Unfortunately, as you can see, finding the perfect amount of neu…\\nrons is still somewhat of a black art.\\nA simpler approach is to pick a model with more layers and neurons than you\\nactually need, then use early stopping to prevent it from overfitting (and other regu…\\nlarization techniques, especially \\ndropout\\n, as we will see in Chapter 11\\n). This has beendubbed the ƒstretch pants⁄ approach:\\n12 instead of wasting time looking for pants that\\nperfectly match your size, just use large stretch pants that will shrink down to the\\nright size.\\nActivation FunctionsIn most cases you can use the ReLU activation function in the hidden layers (or one\\nof its variants, as we will see in \\nChapter 11\\n). It is a bit faster to compute than other\\nactivation functions, and Gradient Descent does not get stuck as much on plateaus,\\nthanks to the fact that it does not saturate for large input values (as opposed to the\\nlogistic function or the hyperbolic tangent function, which saturate at 1).\\nFor the output layer, the softmax activation function is generally a good choice for\\nclassification tasks (when the classes are mutually exclusive). For regression tasks,\\nyou can simply use no activation function at all.\\nThis concludes this introduction to artificial neural networks. In the following chap…\\nters, we will discuss techniques to train very deep nets, and distribute training across\\nmultiple servers and GPUs. Then we will explore a few other popular neural network\\narchitectures: convolutional neural networks, recurrent neural networks, and \\nautoen…\\ncoders.13272 | Chapter 10: Introduction to \\nArti•cial Neural NetworksExercises1.Draw an ANN using the original artificial neurons (like the ones in \\nFigure 10-3)that computes \\nA  B (where \\n represents the XOR operation). Hint: \\nA  B = (\\nA ° B)  (° A  B).2.Why is it generally preferable to use a Logistic Regression classifier rather than a\\nclassical Perceptron (i.e., a single layer of linear threshold units trained using the\\nPerceptron training algorithm)? How can you tweak a Perceptron to make it\\nequivalent to a Logistic Regression classifier?\\n3.Why was the logistic activation function a key ingredient in training the first\\nMLPs?\\n4.Name three popular activation functions. Can you draw them?\\n5.Suppose you have an MLP composed of one input layer with 10 passthrough\\nneurons, followed by one hidden layer with 50 artificial neurons, and finally one\\noutput layer with 3 artificial neurons. All artificial neurons use the ReLU activa…\\ntion function.‹What is the shape of the input matrix \\nX?‹What about the shape of the hidden layer‡s weight vector \\nWh, and the shape of\\nits bias vector bh?‹What is the shape of the output layer‡s weight vector \\nWo, and its bias vector bo?‹What is the shape of the network‡s output matrix \\nY?‹Write the equation that computes the network‡s output matrix \\nY as a functionof X, Wh, bh, Wo and bo.6.How many neurons do you need in the output layer if you want to classify email\\ninto spam or ham? What activation function should you use in the output layer?\\nIf instead you want to tackle MNIST, how many neurons do you need in the out…\\nput layer, using what activation function? Answer the same questions for getting\\nyour network to predict housing prices as in Chapter 2\\n.7.What is backpropagation and how does it work? What is the difference between\\nbackpropagation and reverse-mode autodiff?\\n8.Can you list all the hyperparameters you can tweak in an MLP? If the MLP over…\\nfits the training data, how could you tweak these hyperparameters to try to solve\\nthe problem?9.Train a deep MLP on the MNIST dataset and see if you can get over 98% preci…\\nsion. Just like in the last exercise of \\nChapter 9\\n, try adding all the bells and whistles\\nExercises | 273\\n(i.e., save checkpoints, restore the last checkpoint in case of an interruption, add\\nsummaries, plot learning curves using TensorBoard, and so on).\\nSolutions to these exercises are available in \\nAppendix A\\n.274 | Chapter 10: Introduction to \\nArti•cial Neural NetworksCHAPTER 11Training Deep Neural NetsIn Chapter 10\\n we introduced artificial neural networks and trained our first deep\\nneural network. But it was a very shallow DNN, with only two hidden layers. What if\\nyou need to tackle a very complex problem, such as detecting hundreds of types of\\nobjects in high-resolution images? You may need to train a much deeper DNN, per…\\nhaps with (say) 10 layers, each containing hundreds of neurons, connected by hun…\\ndreds of thousands of connections. This would not be a walk in the park:‹First, you would be faced with the tricky vanishing gradients\\n problem (or therelated \\nexploding gradients\\n problem) that affects deep neural networks and makes\\nlower layers very hard to train.\\n‹Second, with such a large network, training would be extremely slow.\\n‹Third, a model with millions of parameters would severely risk overfitting thetraining set.In this chapter, we will go through each of these problems in turn and present techni…\\nques to solve them. We will start by explaining the vanishing gradients problem and\\nexploring some of the most popular solutions to this problem. Next we will look at\\nvarious optimizers that can speed up training large models tremendously compared\\nto plain Gradient Descent. Finally, we will go through a few popular regularization\\ntechniques for large neural networks.With these tools, you will be able to train very deep nets: welcome to Deep Learning!\\nVanishing/Exploding Gradients ProblemsAs we discussed in Chapter 10\\n, the backpropagation algorithm works by going from\\nthe output layer to the input layer, propagating the error gradient on the way. Once\\nthe algorithm has computed the gradient of the cost function with regards to each\\n2751ƒUnderstanding the Difficulty of Training Deep Feedforward Neural Networks,⁄ X. Glorot, Y Bengio (2010).\\nparameter in the network, it uses these gradients to update each parameter with a\\nGradient Descent step.\\nUnfortunately, gradients often get smaller and smaller as the algorithm progresses\\ndown to the lower layers. As a result, the Gradient Descent update leaves the lower\\nlayer connection weights virtually unchanged, and training never converges to a good\\nsolution. This is called the vanishing gradients\\n problem. In some cases, the oppositecan happen: the gradients can grow bigger and bigger, so many layers get insanely\\nlarge weight updates and the algorithm diverges. This is the \\nexploding gradients\\n prob…\\nlem, which is mostly encountered in recurrent neural networks (see \\nChapter 14\\n).More generally, deep neural networks suffer from unstable gradients; different layers\\nmay learn at widely different speeds.\\nAlthough this unfortunate behavior has been empirically observed for quite a while\\n(it was one of the reasons why deep neural networks were mostly abandoned for a\\nlong time), it is only around 2010 that significant progress was made in understand…\\ning it. A paper titled \\nƒUnderstanding the Difficulty of Training Deep Feedforward\\nNeural Networks⁄\\n by Xavier Glorot and Yoshua Bengio\\n1 found a few suspects, includ…ing the combination of the popular logistic sigmoid activation function and the\\nweight initialization technique that was most popular at the time, namely random ini…\\ntialization using a normal distribution with a mean of 0 and a standard deviation of 1.\\nIn short, they showed that with this activation function and this initialization scheme,\\nthe variance of the outputs of each layer is much greater than the variance of its\\ninputs. Going forward in the network, the variance keeps increasing after each layer\\nuntil the activation function saturates at the top layers. This is actually made worse by\\nthe fact that the logistic function has a mean of 0.5, not 0 (the hyperbolic tangent\\nfunction has a mean of 0 and behaves slightly better than the logistic function in deep\\nnetworks).Looking at the logistic activation function (see \\nFigure 11-1), you can see that when\\ninputs become large (negative or positive), the function saturates at 0 or 1, with a\\nderivative extremely close to 0. Thus when backpropagation kicks in, it has virtually\\nno gradient to propagate back through the network, and what little gradient exists\\nkeeps getting diluted as backpropagation progresses down through the top layers, so\\nthere is really nothing left for the lower layers.\\n276 | Chapter 11: Training Deep Neural Nets\\n2Here‡s an analogy: if you set a microphone amplifier‡s knob too close to zero, people won‡t hear your voice, but\\nif you set it too close to the max, your voice will be saturated and people won‡t understand what you are say…\\ning. Now imagine a chain of such amplifiers: they all need to be set properly in order for your voice to come\\nout loud and clear at the end of the chain. Your voice has to come out of each amplifier at the same amplitude\\nas it came in.Figure 11-1. Logistic activation function saturation\\nXavier and He InitializationIn their paper, Glorot and Bengio propose a way to significantly alleviate this prob…\\nlem. We need the signal to flow properly in both directions: in the forward direction\\nwhen making predictions, and in the reverse direction when backpropagating gradi…\\nents. We don‡t want the signal to die out, nor do we want it to explode and saturate.\\nFor the signal to flow properly, the authors argue that we need the variance of the\\noutputs of each layer to be equal to the variance of its inputs,\\n2 and we also need thegradients to have equal variance before and after flowing through a layer in the\\nreverse direction (please check out the paper if you are interested in the mathematical\\ndetails). It is actually not possible to guarantee both unless the layer has an equal\\nnumber of input and output connections, but they proposed a good compromise that\\nhas proven to work very well in practice: the connection weights must be initialized\\nrandomly as described in Equation 11-1\\n, where ninputs\\n and noutputs are the number of\\ninput and output connections for the layer whose weights are being initialized \\n(alsocalled fan-in\\n and fan-out\\n). This initialization strategy is often called \\nXavier initializa…\\ntion\\n (after the author‡s first name), or sometimes \\nGlorot initialization\\n.Vanishing/Exploding Gradients Problems | 277\\n3This simplified strategy was actually already proposed much earlier›for example, in the 1998 book \\nNeural\\nNetworks: Tricks of the Trade\\n by Genevieve Orr and Klaus-Robert M™ller (Springer).\\n4Such as ƒDelving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,⁄ K.\\nHe et al. (2015).\\nEquation 11-1. Xavier initialization (when using the logistic activation function)\\nNormaldistributionwithmean0andstandarddeviation\\n„=2ninputs+noutputsOrauniformdistributionbetween…rand+r,with\\nr=6ninputs+noutputsWhen the number of input connections is roughly equal to the number of output\\nconnections, you get simpler equations (e.g., \\n„=1/\\nninputs or r=3/\\nninputs). We\\nused this simplified strategy in \\nChapter 10\\n.3Using the Xavier initialization strategy can speed up training considerably, and it is\\none of the tricks that led to the current success of Deep Learning. Some \\nrecent papers\\n4have provided similar strategies for different activation functions, as shown in\\nTable 11-1\\n. The initialization strategy for the ReLU activation function (and its var…\\niants, including the ELU activation described shortly) is sometimes \\ncalled He initiali…\\nzation\\n (after the last name of its author).\\nTable 11-1. Initialization parameters for each type of activation function\\nActivation function\\nUniform distribution [—r, r]\\nNormal distribution\\nLogisticr=6ninputs+noutputs•=2ninputs+noutputsHyperbolic tangent\\nr=4\\n6ninputs+noutputs•=4\\n2ninputs+noutputsReLU (and its variants)\\nr=26ninputs+noutputs•=22ninputs+noutputsBy default, the \\nfully_connected() function (introduced in \\nChapter 10\\n) uses Xavier\\ninitialization (with a uniform distribution). You can change this to He initialization\\nby using the variance_scaling_initializer() function like this:he_init = tf.contrib.layers.variance_scaling_initializer()hidden1 = fully_connected(X, n_hidden1, weights_initializer=he_init, scope=\"h1\")278 | Chapter 11: Training Deep Neural Nets\\n5ƒEmpirical Evaluation of Rectified Activations in Convolution Network,⁄ B. Xu et al. (2015).\\nHe initialization considers only the fan-in, not the average between\\nfan-in and fan-out like in Xavier initialization. This is also the\\ndefault for the \\nvariance_scaling_initializer() function, butyou can change this by setting the argument \\nmode=\"FAN_AVG\".Nonsaturating Activation FunctionsOne of the insights in the 2010 paper by Glorot and Bengio was that the vanishing/\\nexploding gradients problems were in part due to a poor choice of activation func…\\ntion. Until then most people had assumed that if Mother Nature had chosen to use\\nroughly sigmoid activation functions in biological neurons, they must be an excellent\\nchoice. But it turns out that other activation functions behave much better in deep\\nneural networks, in particular the ReLU activation function, mostly because it does\\nnot saturate for positive values (and also because it is quite fast to compute).\\nUnfortunately, the ReLU activation function is not perfect. It suffers from a problem\\nknown as the dying ReLUs\\n: during training, some neurons effectively die, meaningthey stop outputting anything other than 0. In some cases, you may find that half of\\nyour network‡s neurons are dead, especially if you used a large learning rate. During\\ntraining, if a neuron‡s weights get updated such that the weighted sum of the neuron‡s\\ninputs is negative, it will start outputting 0. When this happen, the neuron is unlikely\\nto come back to life since the gradient of the ReLU function is 0 when its input is\\nnegative.\\nTo solve this problem, you may want to use a variant of the ReLU function, such as\\nthe leaky ReLU\\n. This function is defined as LeakyReLU\\n‰(z) = max(‰z, z) (seeFigure 11-2). The hyperparameter \\n‰ defines how much the function ƒleaks⁄: it is the\\nslope of the function for z < 0, and is typically set to 0.01. This small slope ensuresthat leaky ReLUs never die; they can go into a long coma, but they have a chance to\\neventually wake up. A \\nrecent paper\\n5 compared several variants of the ReLU activation\\nfunction and one of its conclusions was that the leaky variants always outperformed\\nthe strict ReLU activation function. In fact, setting \\n‰ = 0.2 (huge leak) seemed to\\nresult in better performance than ‰ = 0.01 (small leak). They also evaluated the\\nrandomized leaky ReLU\\n (RReLU), where \\n‰ is picked randomly in a given range during\\ntraining, and it is fixed to an average value during testing. It also performed fairly well\\nand seemed to act as a regularizer (reducing the risk of overfitting the training set).Finally, they also evaluated the \\nparametric leaky ReLU\\n (PReLU), where \\n‰ is authorized\\nto be learned during training (instead of being a hyperparameter, it becomes a\\nparameter that can be modified by backpropagation like any other parameter). This\\nVanishing/Exploding Gradients Problems | 279\\n6ƒFast and Accurate Deep Network Learning by Exponential Linear Units (ELUs),⁄ D. Clevert, T. Unterthiner,\\nS. Hochreiter (2015).\\nwas reported to strongly outperform ReLU on large image datasets, but on smaller\\ndatasets it runs the risk of overfitting the training set.\\nFigure 11-2. Leaky ReLU\\nLast but not least, a 2015 paper\\n by Djork-Arn• Clevert et al.6 proposed a new activa…tion function called the exponential linear unit\\n (ELU) \\nthat outperformed all the ReLU\\nvariants in their experiments: training time was reduced and the neural network per…\\nformed better on the test set. It is represented in \\nFigure 11-3, and Equation 11-2\\nshows its definition.Equation 11-2. ELU activation function\\nELU‰z=‰expz”1\\nifz<0\\nzifz\\nŠ0\\nFigure 11-3. ELU activation function\\n280 | Chapter 11: Training Deep Neural Nets\\nIt looks a lot like the ReLU function, with a few major differences:\\n‹First it takes on negative values when \\nz < 0, which allows the unit to have an\\naverage output closer to 0. This helps alleviate the vanishing gradients problem,\\nas discussed earlier. The hyperparameter \\n‰ defines the value that the ELU func…\\ntion approaches when \\nz is a large negative number. It is usually set to 1, but you\\ncan tweak it like any other hyperparameter if you want.\\n‹Second, it has a nonzero gradient for \\nz < 0, which avoids the dying units issue.\\n‹Third, the function is smooth everywhere, including around \\nz = 0, which helpsspeed up Gradient Descent, since it does not bounce as much left and right of \\nz =\\n0.The main drawback of the ELU activation function is that it is slower to compute\\nthan the ReLU and its variants (due to the use of the exponential function), but dur…\\ning training this is compensated by the faster convergence rate. However, at test time\\nan ELU network will be slower than a ReLU network.\\nSo which activation function should you use for the hidden layers\\nof your deep neural networks? Although your mileage will vary, in\\ngeneral ELU > leaky ReLU (and its variants) > ReLU > tanh > logis…\\ntic. If you care a lot about runtime performance, then you may pre…\\nfer leaky ReLUs over ELUs. If you don‡t want to tweak yet another\\nhyperparameter, you may just use the default \\n‰ values suggestedearlier (0.01 for the leaky ReLU, and 1 for ELU). If you have spare\\ntime and computing power, you can use cross-validation to evalu…\\nate other activation functions, in particular RReLU if your network\\nis overfitting, or PReLU if you have a huge training set.\\nTensorFlow offers an \\nelu() function that you can use to build your neural network.\\nSimply set the \\nactivation_fn argument when calling the \\nfully_connected() func…tion, like this:hidden1 = fully_connected(X, n_hidden1, activation_fn=tf.nn.elu)TensorFlow does not have a predefined function for leaky ReLUs, but it is easy\\nenough to define:def leaky_relu(z, name=None):    return tf.maximum(0.01 * z, z, name=name)hidden1 = fully_connected(X, n_hidden1, activation_fn=leaky_relu)Vanishing/Exploding Gradients Problems | 281\\n7ƒBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,⁄ S. Ioffe\\nand C. Szegedy (2015).Batch NormalizationAlthough using He initialization along with ELU (or any variant of ReLU) can signifi…\\ncantly reduce the vanishing/exploding gradients problems at the beginning of train…\\ning, it doesn‡t guarantee that they won‡t come back during training.\\nIn a 2015 paper\\n,7 Sergey Ioffe and Christian Szegedy proposed a technique called\\nBatch Normalization\\n (BN) to address the vanishing/exploding gradients problems,\\nand more generally the problem that the distribution of each layer‡s inputs changes\\nduring training, as the parameters of the previous layers change (which they call the\\nInternal Covariate \\nShi“ problem).The technique consists of adding an operation in the model just before the activation\\nfunction of each layer, simply zero-centering and normalizing the inputs, then scaling\\nand shifting the result using two new parameters per layer (one for scaling, the other\\nfor shifting). In other words, this operation lets the model learn the optimal scale and\\nmean of the inputs for each layer.\\nIn order to zero-center and normalize the inputs, the algorithm needs to estimate the\\ninputs‡ mean and standard deviation. It does so by evaluating the mean and standard\\ndeviation of the inputs over the current mini-batch (hence the name ƒBatch Normal…\\nization⁄). The whole operation is summarized in \\nEquation 11-3\\n.Equation 11-3. Batch Normalization algorithm\\n1.\\nﬂB=1mB“i=1\\nmBi2.\\n„B2=1mB“i=1\\nmBi”ﬂB23.\\ni=i”ﬂB„B2+4.\\ni=’i+Ł‹ﬂB is the empirical mean, evaluated over the whole mini-batch \\nB.‹„B is the empirical standard deviation, also evaluated over the whole mini-batch.\\n‹mB is the number of instances in the mini-batch.\\n282 | Chapter 11: Training Deep Neural Nets\\n‹(i) is the zero-centered and normalized input.\\n‹’ is the scaling parameter for the layer.\\n‹Ł is the shifting parameter (offset) for the layer.\\n‹ is a tiny number to avoid division by zero (typically 10\\n–3). This is called asmoothing term\\n.‹z(i) is the output of the BN operation: it is a scaled and shifted version of the\\ninputs.\\nAt test time, there is no mini-batch to compute the empirical mean and standard\\ndeviation, so instead you simply use the whole training set‡s mean and standard devi…\\nation. These are typically efficiently computed during training using a moving aver…\\nage. So, in total, four parameters are learned for each batch-normalized layer: \\n’(scale), Ł (offset), ﬂ (mean), and „ (standard deviation).\\nThe authors demonstrated that this technique considerably improved all the deep\\nneural networks they experimented with. The vanishing gradients problem was\\nstrongly reduced, to the point that they could use saturating activation functions such\\nas the tanh and even the logistic activation function. The networks were also much\\nless sensitive to the weight initialization. They were able to use much larger learning\\nrates, significantly speeding up the learning process. Specifically, they note that\\nƒApplied to a state-of-the-art image classification model, Batch Normalization ach…\\nieves the same accuracy with 14 times fewer training steps, and beats the original\\nmodel by a significant margin. [µ] Using an ensemble of batch-normalized net…\\nworks, we improve upon the best published result on ImageNet classification: reach…\\ning 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of\\nhuman raters.⁄ Finally, like a gift that keeps on giving, Batch Normalization also acts\\nlike a regularizer, reducing the need for other regularization techniques (such as\\ndropout, described later in the chapter).\\nBatch Normalization does, however, add some complexity to the model (although it\\nremoves the need for normalizing the input data since the first hidden layer will take\\ncare of that, provided it is batch-normalized). Moreover, there is a runtime penalty:\\nthe neural network makes slower predictions due to the extra computations required\\nat each layer. So if you need predictions to be lightning-fast, you may want to check\\nhow well plain ELU + He initialization perform before playing with Batch Normaliza…\\ntion.You may find that training is rather slow at first while Gradient\\nDescent is searching for the optimal scales and offsets for each\\nlayer, but it accelerates once it has found reasonably good values.\\nVanishing/Exploding Gradients Problems | 283\\nImplementing Batch Normalization with TensorFlowTensorFlow provides a \\nbatch_normalization() function that simply centers and\\nnormalizes the inputs, but you must compute the mean and standard deviation your…\\nself (based on the mini-batch data during training or on the full dataset during test…\\ning, as just discussed) and pass them as parameters to this function, and you must\\nalso handle the creation of the scaling and offset parameters (and pass them to this\\nfunction). It is doable, but not the most convenient approach. Instead, you should use\\nthe batch_norm() function, which handles all this for you. You can either call it\\ndirectly or tell the fully_connected() function to use it, such as in the followingcode:import tensorflow as tffrom tensorflow.contrib.layers import batch_normn_inputs = 28 * 28n_hidden1 = 300n_hidden2 = 100n_outputs = 10X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")is_training = tf.placeholder(tf.bool, shape=(), name=•is_training•)bn_params = {    •is_training•: is_training,    •decay•: 0.99,    •updates_collections•: None}hidden1 = fully_connected(X, n_hidden1, scope=\"hidden1\",                          normalizer_fn=batch_norm, normalizer_params=bn_params)hidden2 = fully_connected(hidden1, n_hidden2, scope=\"hidden2\",                          normalizer_fn=batch_norm, normalizer_params=bn_params)logits = fully_connected(hidden2, n_outputs, activation_fn=None,scope=\"outputs\",                         normalizer_fn=batch_norm, normalizer_params=bn_params)Let‡s walk through this code. The first lines are fairly self-explanatory, until we define\\nthe is_training placeholder, which will either be \\nTrue or \\nFalse. This will be used totell the batch_norm() function whether it should use the current mini-batch‡s mean\\nand standard deviation (during training) or the running averages that it keeps track\\nof (during testing).Next we define \\nbn_params, which is a dictionary that defines the parameters that will\\nbe passed to the batch_norm() function, including is_training of course. The algo…rithm uses exponential decay\\n to compute the running averages, which is why it\\nrequires the decay parameters. Given a new value v, the running average \\nv is updated\\nthrough the equation \\nvv‰decay+\\nv‰1”decay\\n. A good decay value is typically\\nclose to 1›for example, 0.9, 0.99, or 0.999 (you want more 9s for larger datasets and\\n284 | Chapter 11: Training Deep Neural Nets\\nsmaller mini-batches). Finally, \\nupdates_collections should be set to None if youwant the \\nbatch_norm() function to update the running averages right before it per…\\nforms batch normalization during training (i.e., when \\nis_training=True). If youdon‡t set this parameter, by default TensorFlow will just add the operations that\\nupdate the running averages to a collection of operations that you must run yourself.\\nLastly, we create the layers by calling the \\nfully_connected() function, just like wedid in Chapter 10\\n, but this time we tell it to use the batch_norm() function (with theparameters nb_params) to normalize the inputs right before calling the activation\\nfunction.Note that by default \\nbatch_norm() only centers, normalizes, and shifts the inputs; it\\ndoes not scale them (i.e., ’ is fixed to 1). This makes sense for layers with no activa…\\ntion function or with the ReLU activation function, since the next layer‡s weights can\\ntake care of scaling, but for any other activation function, you should add \\n\"scale\":True to bn_params.You may have noticed that defining the preceding three layers was fairly repetitive\\nsince several parameters were identical. To avoid repeating the same parameters over\\nand over again, you can create an \\nargument scope\\n using the arg_scope() function:the first parameter is a list of functions, and the other parameters will be passed tothese functions automatically. The last three lines of the preceding code can be \\nmodi…fied like so:[...]with tf.contrib.framework.arg_scope(        [fully_connected],        normalizer_fn=batch_norm,        normalizer_params=bn_params):    hidden1 = fully_connected(X, n_hidden1, scope=\"hidden1\")    hidden2 = fully_connected(hidden1, n_hidden2, scope=\"hidden2\")    logits = fully_connected(hidden2, n_outputs, scope=\"outputs\",                             activation_fn=None)It may not look much better than before in this small example, but if you have 10 lay…\\ners and want to set the activation function, the initializers, the normalizers, the regu…\\nlarizers, and so on, it will make your code much more readable.\\nThe rest of the construction phase is the same as in Chapter 10\\n: define the cost func…tion, create an optimizer, tell it to minimize the cost function, define the evaluation\\noperations, create a \\nSaver, and so on.The execution phase is also pretty much the same, with one exception. Whenever you\\nrun an operation that depends on the \\nbatch_norm layer, you need to set the \\nis_training placeholder to True or False:Vanishing/Exploding Gradients Problems | 285\\n8ƒOn the difficulty of training recurrent neural networks,⁄ R. Pascanu et al. (2013).\\nwith tf.Session() as sess:    sess.run(init)    for epoch in range(n_epochs):        [...]        for X_batch, y_batch in zip(X_batches, y_batches):            sess.run(training_op,                     feed_dict={is_training: True, X: X_batch, y: y_batch})        accuracy_score = accuracy.eval(            feed_dict={is_training: False, X: X_test_scaled, y: y_test}))        print(accuracy_score)That‡s all! In this tiny example with just two layers, it‡s unlikely that Batch Normaliza…\\ntion will have a very positive impact, but for deeper networks it can make a tremen…\\ndous difference.Gradient ClippingA popular technique to lessen the exploding gradients problem is to simply clip the\\ngradients during backpropagation so that they never exceed some threshold (this is\\nmostly useful for recurrent neural networks; see \\nChapter 14\\n). This is called Gradient\\nClipping\\n.8 In general people now prefer Batch Normalization, but it‡s still useful to\\nknow about Gradient Clipping and how to implement it.\\nIn TensorFlow, the optimizer‡s \\nminimize() function takes care of both computing the\\ngradients and applying them, so you must instead call the optimizer‡s \\ncompute_gradients() method first, then create an operation to clip the gradients using the\\nclip_by_value() function, and finally create an operation to apply the clipped gradi…\\nents using the optimizer‡s \\napply_gradients() method:threshold = 1.0optimizer = tf.train.GradientDescentOptimizer(learning_rate)grads_and_vars = optimizer.compute_gradients(loss)capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)              for grad, var in grads_and_vars]training_op = optimizer.apply_gradients(capped_gvs)You would then run this \\ntraining_op at every training step, as usual. It will compute\\nthe gradients, clip them between –1.0 and 1.0, and apply them. The threshold is a\\nhyperparameter you can tune.\\nReusing Pretrained LayersIt is generally not a good idea to train a very large DNN from scratch: instead, you\\nshould always try to find an existing neural network that accomplishes a similar task\\n286 | Chapter 11: Training Deep Neural Nets\\nto the one you are trying to tackle, then just reuse the lower layers of this network:\\nthis is called transfer learning\\n. It will not only speed up training considerably, but will\\nalso require much less training data.\\nFor example, suppose that you have access to a DNN that was trained to classify pic…\\ntures into 100 different categories, including animals, plants, vehicles, and everyday\\nobjects. You now want to train a DNN to classify specific types of vehicles. These\\ntasks are very similar, so you should try to reuse parts of the first network (see\\nFigure 11-4).Figure 11-4. Reusing pretrained layers\\nIf the input pictures of your new task don‡t have the same size as\\nthe ones used in the original task, you will have to add a prepro…\\ncessing step to resize them to the size expected by the originalmodel. More generally, transfer learning will work only well if the\\ninputs have similar low-level features.\\nReusing a TensorFlow ModelIf the original model was trained using TensorFlow, you can simply restore it and\\ntrain it on the new task:[...] # construct the original modelwith tf.Session() as sess:    saver.restore(sess, \"./my_original_model.ckpt\")    [...] # Train it on your new taskReusing Pretrained Layers | 287\\nHowever, in general you will want to reuse only part of the original model (as we will\\ndiscuss in a moment). A simple solution is to configure the \\nSaver to restore only asubset of the variables from the original model. For example, the following code\\nrestores only hidden layers 1, 2, and 3:\\n[...] # build new model with the same definition as before for hidden layers 1-3init = tf.global_variables_initializer()reuse_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,                               scope=\"hidden[123]\")reuse_vars_dict = dict([(var.name, var.name) for var in reuse_vars])original_saver = tf.Saver(reuse_vars_dict) # saver to restore the original modelnew_saver = tf.Saver() # saver to save the new modelwith tf.Session() as sess:    sess.run(init)    original_saver.restore(\"./my_original_model.ckpt\") # restore layers 1 to 3    [...] # train the new model    new_saver.save(\"./my_new_model.ckpt\") # save the whole modelFirst we build the new model, making sure to copy the original model‡s hidden layers\\n1 to 3. We also create a node to initialize all variables. Then we get the list of all vari…\\nables that were just created with \\n\"trainable=True\" (which is the default), and we\\nkeep only the ones whose scope matches the regular expression \\n\"hidden[123]\" (i.e.,we get all trainable variables in hidden layers 1 to 3). Next we create a dictionary\\nmapping the name of each variable in the original model to its name in the new\\nmodel (generally you want to keep the exact same names). Then we create a \\nSaverthat will restore only these variables, and we create another \\nSaver to save the entire\\nnew model, not just layers 1 to 3. We then start a session and initialize all variables in\\nthe model, then restore the variable values from the original model‡s layers 1 to 3.\\nFinally, we train the model on the new task and save it.\\nThe more similar the tasks are, the more layers you want to reuse\\n(starting with the lower layers). For very similar tasks, you can try\\nkeeping all the hidden layers and just replace the output layer.\\nReusing Models from Other FrameworksIf the model was trained using another framework, you will need to load the weights\\nmanually (e.g., using Theano code if it was trained with Theano), then assign them to\\nthe appropriate variables. This can be quite tedious. For example, the following code\\nshows how you would copy the weight and biases from the first hidden layer of a \\nmodel trained using another framework:288 | Chapter 11: Training Deep Neural Nets\\noriginal_w = [...] # Load the weights from the other frameworkoriginal_b = [...] # Load the biases from the other frameworkX = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")hidden1 = fully_connected(X, n_hidden1, scope=\"hidden1\")[...] # # Build the rest of the model# Get a handle on the variables created by fully_connected()with tf.variable_scope(\"\", default_name=\"\", reuse=True):  # root scope    hidden1_weights = tf.get_variable(\"hidden1/weights\")    hidden1_biases = tf.get_variable(\"hidden1/biases\")# Create nodes to assign arbitrary values to the weights and biasesoriginal_weights = tf.placeholder(tf.float32, shape=(n_inputs, n_hidden1))original_biases = tf.placeholder(tf.float32, shape=(n_hidden1))assign_hidden1_weights = tf.assign(hidden1_weights, original_weights)assign_hidden1_biases = tf.assign(hidden1_biases, original_biases)init = tf.global_variables_initializer()with tf.Session() as sess:    sess.run(init)    sess.run(assign_hidden1_weights, feed_dict={original_weights: original_w})    sess.run(assign_hidden1_biases, feed_dict={original_biases: original_b})    [...] # Train the model on your new taskFreezing the Lower LayersIt is likely that the lower layers of the first DNN have learned to detect low-level fea…\\ntures in pictures that will be useful across both image classification tasks, so you can\\njust reuse these layers as they are. It is generally a good idea to ƒfreeze⁄ their \\nweights\\nwhen training the new DNN: if the lower-layer weights are fixed, then the higher-\\nlayer weights will be easier to train (because they won‡t have to learn a moving target).\\nTo freeze the lower layers during training, the simplest solution is to give the opti…\\nmizer the list of variables to train, excluding the variables from the lower layers:\\ntrain_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,                               scope=\"hidden[34]|outputs\")training_op = optimizer.minimize(loss, var_list=train_vars)The first line gets the list of all trainable variables in hidden layers 3 and 4 and in the\\noutput layer. This leaves out the variables in the hidden layers 1 and 2. Next we pro…\\nvide this restricted list of trainable variables to the optimizer‡s \\nminimize() function.Ta-da! Layers 1 and 2 are now frozen: they will not budge during training (these are\\noften called frozen layers\\n).Reusing Pretrained Layers | 289\\nCaching the Frozen LayersSince the frozen layers won‡t change, it is possible to cache the output of the topmost\\nfrozen layer for each training instance. Since training goes through the whole dataset\\nmany times, this will give you a huge speed boost as you will only need to go through\\nthe frozen layers once per training instance (instead of once per epoch). For example,\\nyou could first run the whole training set through the lower layers (assuming you\\nhave enough RAM):\\nhidden2_outputs = sess.run(hidden2, feed_dict={X: X_train})Then during training, instead of building batches of training instances, you would\\nbuild batches of outputs from hidden layer 2 and feed them to the training operation:\\nimport numpy as npn_epochs = 100n_batches = 500for epoch in range(n_epochs):    shuffled_idx = rnd.permutation(len(hidden2_outputs))    hidden2_batches = np.array_split(hidden2_outputs[shuffled_idx], n_batches)    y_batches = np.array_split(y_train[shuffled_idx], n_batches)    for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):        sess.run(training_op, feed_dict={hidden2: hidden2_batch, y: y_batch})The last line runs the training operation defined earlier (which freezes layers 1 and 2),\\nand feeds it a batch of outputs from the second hidden layer (as well as the targets for\\nthat batch). Since we give TensorFlow the output of hidden layer 2, it does not try to\\nevaluate it (or any node it depends on).\\nTweaking, Dropping, or Replacing the Upper LayersThe output layer of the original model should usually be replaced since it is most\\nlikely not useful at all for the new task, and it may not even have the right number of\\noutputs for the new task.Similarly, the upper hidden layers of the original model are less likely to be as useful\\nas the lower layers, since the high-level features that are most useful for the new task\\nmay differ significantly from the ones that were most useful for the original task. You\\nwant to find the right number of layers to reuse.\\nTry freezing all the copied layers first, then train your model and see how it performs.\\nThen try unfreezing one or two of the top hidden layers to let backpropagation tweak\\nthem and see if performance improves. The more training data you have, the more\\nlayers you can unfreeze.\\nIf you still cannot get good performance, and you have little training data, try drop…\\nping the top hidden layer(s) and freeze all remaining hidden layers again. You can\\n290 | Chapter 11: Training Deep Neural Nets\\niterate until you find the right number of layers to reuse. If you have plenty of train…\\ning data, you may try replacing the top hidden layers instead of dropping them, and\\neven add more hidden layers.\\nModel ZoosWhere can you find a neural network trained for a task similar to the one you want to\\ntackle? The first place to look is obviously in your own catalog of models. This is one\\ngood reason to save all your models and organize them so you can retrieve them later\\neasily. Another option is to search in a \\nmodel zoo\\n. Many people train Machine Learn…\\ning models for various tasks and kindly release their pretrained models to the public.TensorFlow has its own model zoo available at \\nhttps://github.com/tensor‡ow/models.In particular, it contains most of the state-of-the-art image classification nets such as\\nVGG, Inception, and ResNet (see \\nChapter 13\\n, and check out the models/slim\\n direc…tory), including the code, the pretrained models, and tools to download popular\\nimage datasets.\\nAnother popular model zoo is Caffe‡s \\nModel Zoo\\n. It also contains many computer\\nvision models (e.g., LeNet, AlexNet, ZFNet, GoogLeNet, VGGNet, inception) trained\\non various datasets (e.g., ImageNet, Places Database, CIFAR10, etc.). Saumitro Das…\\ngupta wrote a converter, which is available at \\nhttps://github.com/ethereon/ca›e-tensor‡ow.Unsupervised PretrainingSuppose you want to tackle a complex task for which you don‡t have much labeled\\ntraining data, but unfortunately you cannot find a model trained on a similar task.\\nDon‡t lose all hope! First, you should of course try to gather more labeled training\\ndata, but if this is too hard or too expensive, you may still be able to perform \\nunsuper…\\nvised pretraining\\n (see Figure 11-5). That is, if you have plenty of unlabeled training\\ndata, you can try to train the layers one by one, starting with the lowest layer and then\\ngoing up, using an unsupervised feature detector algorithm such as \\nRestricted Boltz…\\nmann Machines\\n (RBMs; see Appendix E\\n) or autoencoders (see \\nChapter 15\\n). Eachlayer is trained on the output of the previously trained layers (all layers except the one\\nbeing trained are frozen). Once all layers have been trained this way, you can fine-\\ntune the network using supervised learning (i.e., with backpropagation).\\nThis is a rather long and tedious process, but it often works well; in fact, it is this\\ntechnique that Geoffrey Hinton and his team used in 2006 and which led to the\\nrevival of neural networks and the success of Deep Learning. Until 2010, unsuper…\\nvised pretraining (typically using RBMs) was the norm for deep nets, and it was onlyafter the vanishing gradients problem was alleviated that it became much more com…\\nmon to train DNNs purely using backpropagation. However, unsupervised pretrain…\\ning (today typically using autoencoders rather than RBMs) is still a good option when\\nReusing Pretrained Layers | 291\\n9Another option is to come up with a supervised task for which you can easily gather a lot of labeled training\\ndata, then use transfer learning, as explained earlier. For example, if you want to train a model to identify your\\nfriends in pictures, you could download millions of faces on the internet and train a classifier to detect\\nwhether two faces are identical or not, then use this classifier to compare a new picture with each picture of\\nyour friends.you have a complex task to solve, no similar model you can reuse, and little labeled\\ntraining data but plenty of unlabeled training data.\\n9Figure 11-5. Unsupervised pretraining\\nPretraining on an Auxiliary TaskOne last option is to train a first neural network on an auxiliary task for which you\\ncan easily obtain or generate labeled training data, then reuse the lower layers of that\\nnetwork for your actual task. The first neural network‡s lower layers will learn feature\\ndetectors that will likely be reusable by the second neural network.\\nFor example, if you want to build a system to recognize faces, you may only have a\\nfew pictures of each individual›clearly not enough to train a good classifier. Gather…\\ning hundreds of pictures of each person would not be practical. However, you could\\ngather a lot of pictures of random people on the internet and train a first neural net…\\nwork to detect whether or not two different pictures feature the same person. Such a\\n292 | Chapter 11: Training Deep Neural Nets\\n10At least for now: \\nresearch is moving fast, especially in the field of optimization\\n. Be sure to take a look at the\\nlatest and greatest optimizers every time a new version of TensorFlow is released.\\nnetwork would learn good feature detectors for faces, so reusing its lower layers\\nwould allow you to train a good face classifier using little training data.\\nIt is often rather cheap to gather unlabeled training examples, but quite expensive to\\nlabel them. In this situation, a common technique is to label all your training exam…\\nples as ƒgood,⁄ then generate many new training instances by corrupting the good\\nones, and label these corrupted instances as ƒbad.⁄ Then you can train a first neural\\nnetwork to classify instances as good or bad. For example, you could download mil…\\nlions of sentences, label them as ƒgood,⁄ then randomly change a word in each sen…\\ntence and label the resulting sentences as ƒbad.⁄ If a neural network can tell that ƒThe\\ndog sleeps⁄ is a good sentence but ƒThe dog they⁄ is bad, it probably knows quite a lot\\nabout language. Reusing its lower layers will likely help in many language processing\\ntasks.Another approach is to train a first network to output a score for each training\\ninstance, and use a cost function that ensures that a good instance‡s score is greater\\nthan a bad instance‡s score by at least some margin. This is called \\nmax margin learn…\\ning\\n.Faster OptimizersTraining a very large deep neural network can be painfully slow. So far we have seen\\nfour ways to speed up training (and reach a better solution): applying a good initiali…\\nzation strategy for the connection weights, using a good activation function, using\\nBatch Normalization, and reusing parts of a pretrained network. Another huge speed\\nboost comes from using a faster optimizer than the regular Gradient Descent opti…\\nmizer. In this section we will present the most popular ones: Momentum optimiza…\\ntion, Nesterov Accelerated Gradient, AdaGrad, RMSProp, and finally Adam\\noptimization.\\nSpoiler alert: the conclusion of this section is that you should almost always use \\nAdam optimization,\\n10 so if you don‡t care about how it works, simply replace your\\nGradientDescentOptimizer with an AdamOptimizer and skip to the next section!With just this small change, training will typically be several times faster. However,\\nAdam optimization does have three hyperparameters that you can tune (plus the\\nlearning rate); the default values usually work fine, but if you ever need to tweak them\\nit may be helpful to know what they do. Adam optimization combines several ideas\\nfrom other optimization algorithms, so it is useful to look at these algorithms first.\\nFaster Optimizers | 293\\n11ƒSome methods of speeding up the convergence of iteration methods,⁄ B. Polyak (1964).\\nMomentum optimizationImagine a bowling ball rolling down a gentle slope on a smooth surface: it will start\\nout slowly, but it will quickly pick up momentum until it eventually reaches terminal\\nvelocity (if there is some friction or air resistance). This is the very simple idea behind\\nMomentum optimization\\n, proposed by Boris Polyak in 1964\\n.11 In contrast, regular\\nGradient Descent will simply take small regular steps down the slope, so it will take\\nmuch more time to reach the bottom.\\nRecall that Gradient Descent simply updates the weights \\n– by directly subtracting thegradient of the cost function \\nJ(–) with regards to the weights (\\n–J(–)) multiplied by\\nthe learning rate \\n−. The equation is: \\n– ¾ – – −–J(–). It does not care about what the\\nearlier gradients were. If the local gradient is tiny, it goes very slowly.\\nMomentum optimization cares a great deal about what previous gradients were: at\\neach iteration, it adds the local gradient to the \\nmomentum vector\\n m (multiplied by the\\nlearning rate \\n−), and it updates the weights by simply subtracting this momentum\\nvector (see Equation 11-4\\n). In other words, the gradient is used as an acceleration, not\\nas a speed. To simulate some sort of friction mechanism and prevent the momentum\\nfrom growing too large, the algorithm introduces a new hyperparameter \\nŁ, simply\\ncalled the momentum\\n, which must be set between 0 (high friction) and 1 (no friction).\\nA typical momentum value is 0.9.\\nEquation 11-4. Momentum algorithm\\n1.\\nŁ+−–J–2.\\n––”You can easily verify that if the gradient remains constant, the terminal velocity (i.e.,\\nthe maximum size of the weight updates) is equal to that gradient multiplied by the\\nlearning rate \\n− multiplied by \\n11”\\nŁ. For example, if \\nŁ = 0.9, then the terminal velocityis equal to 10 times the gradient times the learning rate, so Momentum optimization\\nends up going 10 times faster than Gradient Descent! This allows Momentum opti…\\nmization to escape from plateaus much faster than Gradient Descent. In particular,\\nwe saw in \\nChapter 4\\n that when the inputs have very different scales the cost function\\nwill look like an elongated bowl (see \\nFigure 4-7). Gradient Descent goes down the\\nsteep slope quite fast, but then it takes a very long time to go down the valley. In con…\\ntrast, Momentum optimization will roll down the bottom of the valley faster and\\nfaster until it reaches the bottom (the optimum). In deep neural networks that don‡t\\nuse Batch Normalization, the upper layers will often end up having inputs with very\\n294 | Chapter 11: Training Deep Neural Nets\\n12ƒA Method for Unconstrained Convex Minimization Problem with the Rate of Convergence O(1/k\\n2),⁄ Yurii\\nNesterov (1983).\\ndifferent scales, so using Momentum optimization helps a lot. It can also help roll\\npast local optima.Due to the momentum, the optimizer may overshoot a bit, then\\ncome back, overshoot again, and oscillate like this many times\\nbefore stabilizing at the minimum. This is one of the reasons why it\\nis good to have a bit of friction in the system: it gets rid of these\\noscillations and thus speeds up convergence.\\nImplementing Momentum optimization in TensorFlow is a no-brainer: just replace\\nthe GradientDescentOptimizer with the MomentumOptimizer, then lie back andprofit!optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,                                       momentum=0.9)The one drawback of Momentum optimization is that it adds yet another hyperpara…\\nmeter to tune. However, the momentum value of 0.9 usually works well in practice\\nand almost always goes faster than Gradient Descent.\\nNesterov Accelerated GradientOne small variant to Momentum optimization, proposed by \\nYurii Nesterov in 1983\\n,12is almost always faster than vanilla Momentum optimization. The idea of \\nNesterov\\nMomentum optimization\\n, or Nesterov Accelerated Gradient\\n (NAG), is to measure the\\ngradient of the cost function not at the local position but slightly ahead in the direc…\\ntion of the momentum (see \\nEquation 11-5\\n). The only difference from vanillaMomentum optimization is that the gradient is measured at \\n– + Łm rather than at \\n–.Equation 11-5. Nesterov Accelerated Gradient algorithm\\n1.\\nŁ+−–J–+Ł2.\\n––”This small tweak works because in general the momentum vector will be pointing in\\nthe right direction (i.e., toward the optimum), so it will be slightly more accurate to\\nuse the gradient measured a bit farther in that direction rather than using the gradi…\\nent at the original position, as you can see in \\nFigure 11-6 (where 1 represents the\\ngradient of the cost function measured at the starting point \\n–, and 2 represents the\\ngradient at the point located at \\n– + \\nŁm). As you can see, the Nesterov update ends up\\nFaster Optimizers | 295\\nslightly closer to the optimum. After a while, these small improvements add up and\\nNAG ends up being significantly faster than regular Momentum optimization. More…\\nover, note that when the momentum pushes the weights across a valley, \\n1 continues\\nto push further across the valley, while \\n2 pushes back toward the bottom of the val…ley. This helps reduce oscillations and thus converges faster.\\nFigure 11-6. Regular versus Nesterov Momentum optimization\\nNAG will almost always speed up training compared to regular Momentum optimi…\\nzation. To use it, simply set \\nuse_nesterov=True when creating the \\nMomentumOptimizer:optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,                                       momentum=0.9, use_nesterov=True)AdaGradConsider the elongated bowl problem again: Gradient Descent starts by quickly going\\ndown the steepest slope, then slowly goes down the bottom of the valley. It would be\\nnice if the algorithm could detect this early on and correct its direction to point a bit\\nmore toward the global optimum.\\n296 | Chapter 11: Training Deep Neural Nets\\n13ƒAdaptive Subgradient Methods for Online Learning and Stochastic Optimization,⁄ J. Duchi et al. (2011).\\nThe AdaGrad\\n algorithm13 achieves this by scaling down the gradient vector along the\\nsteepest dimensions (see Equation 11-6\\n):Equation 11-6. AdaGrad algorithm\\n1.\\n+–J–\\n–J–2.\\n––”−–J–+The first step accumulates the square of the gradients into the vector \\ns (the \\n symbolrepresents the element-wise multiplication). This vectorized form is equivalent to\\ncomputing \\nsi ¾ si + (ﬂ / ﬂ –i J(–))2 for each element \\nsi of the vector s; in other words,each si accumulates the squares of the partial derivative of the cost function with\\nregards to parameter –i. If the cost function is steep along the ith dimension, then siwill get larger and larger at each iteration.\\nThe second step is almost identical to Gradient Descent, but with one big difference:\\nthe gradient vector is scaled down by a factor of \\n+ (the  symbol represents the\\nelement-wise division, and \\n is a smoothing term to avoid division by zero, typically\\nset to 10–10). This vectorized form is equivalent to computing\\n–i–i”−ﬂ/ﬂ\\n–iJ–/si+ for all parameters –i (simultaneously).\\nIn short, this algorithm decays the learning rate, but it does so faster for steep dimen…\\nsions than for dimensions with gentler slopes. This is called an \\nadaptive learning rate\\n. It helps point the resulting updates more directly toward the global optimum (see\\nFigure 11-7). One additional benefit is that it requires much less tuning of the learn…\\ning rate hyperparameter \\n−.Figure 11-7. AdaGrad versus Gradient Descent\\nFaster Optimizers | 297\\n14This algorithm was created by Tijmen Tieleman and Geoffrey Hinton in 2012, and presented by Geoffrey\\nHinton in his Coursera class on neural networks (slides: \\nhttp://goo.gl/RsQeis\\n; video: https://goo.gl/XUbIyJ\\n).Amusingly, since the authors have not written a paper to describe it, researchers often cite ƒslide 29 in lecture\\n6⁄ in their papers.\\n15ƒAdam: A Method for Stochastic Optimization,⁄ D. Kingma, J. Ba (2015).\\nAdaGrad often performs well for simple quadratic problems, but unfortunately it\\noften stops too early when training neural networks. The learning rate gets scaled\\ndown so much that the algorithm ends up stopping entirely before reaching the\\nglobal optimum. So even though TensorFlow has an \\nAdagradOptimizer, you shouldnot use it to train deep neural networks (it may be efficient for simpler tasks such as\\nLinear Regression, though).RMSPropAlthough AdaGrad slows down a bit too fast and ends up never converging to the\\nglobal optimum, the \\nRMSProp\\n algorithm14 fixes this by accumulating only the gradi…\\nents from the most recent iterations (as opposed to all the gradients since the begin…\\nning of training). It does so by using exponential decay in the first step (see \\nEquation\\n11-7).Equation 11-7. RMSProp algorithm\\n1.\\nŁ+1”\\nŁ–J–\\n–J–2.\\n––”−–J–+The decay rate \\nŁ is typically set to 0.9. Yes, it is once again a new hyperparameter, but\\nthis default value often works well, so you may not need to tune it at all.\\nAs you might expect, TensorFlow has an \\nRMSPropOptimizer class:optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate,                                      momentum=0.9, decay=0.9, epsilon=1e-10)Except on very simple problems, this optimizer almost always performs much better\\nthan AdaGrad. It also generally performs better than Momentum optimization and\\nNesterov Accelerated Gradients. In fact, it was the preferred optimization algorithm\\nof many researchers until Adam optimization came around.\\nAdam OptimizationAdam\\n,15 which stands for \\nadaptive moment estimation\\n, combines the ideas of Momen…\\ntum optimization and RMSProp: just like Momentum optimization it keeps track of\\nan exponentially decaying average of past gradients, and just like RMSProp it keeps\\n298 | Chapter 11: Training Deep Neural Nets\\n16These are estimations of the mean and (uncentered) variance of the gradients. The mean is often called the\\n†rst moment\\n, while the variance is often called the second moment\\n, hence the name of the algorithm.track of an exponentially decaying average of past squared gradients (see \\nEquation\\n11-8).16Equation 11-8. Adam algorithm\\n1.\\nŁ1+1”\\nŁ1–J–2.\\nŁ2+1”\\nŁ2–J–\\n–J–3.\\n1”\\nŁ1T4.\\n1”\\nŁ2T5.\\n––”−\\n+‹T represents the iteration number (starting at 1).\\nIf you just look at steps 1, 2, and 5, you will notice Adam‡s close similarity to both\\nMomentum optimization and RMSProp. The only difference is that step 1 computes\\nan exponentially decaying average rather than an exponentially decaying sum, but\\nthese are actually equivalent except for a constant factor (the decaying average is just\\n1 – Ł1 times the decaying sum). Steps 3 and 4 are somewhat of a technical detail: since\\nm and s are initialized at 0, they will be biased toward 0 at the beginning of training,\\nso these two steps will help boost m and s at the beginning of training.\\nThe momentum decay hyperparameter \\nŁ1 is typically initialized to 0.9, while the scal…ing decay hyperparameter \\nŁ2 is often initialized to 0.999. As earlier, the \\nsmoothingterm  is usually initialized to a tiny number such as 10\\n–8. These are the default values\\nfor TensorFlow‡s \\nAdamOptimizer class, so you can simply use:\\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)In fact, since Adam is an adaptive learning rate algorithm (like AdaGrad and\\nRMSProp), it requires less tuning of the learning rate hyperparameter \\n−. You can\\noften use the default value \\n− = 0.001, making Adam even easier to use than Gradient\\nDescent.\\nFaster Optimizers | 299\\n17ƒPrimal-Dual Subgradient Methods for Convex Problems,⁄ Yurii Nesterov (2005).\\n18ƒAd Click Prediction: a View from the Trenches,⁄ H. McMahan et al. (2013).\\nAll the optimization techniques discussed so far only rely on the\\n†rst-order partial derivatives\\n (Jacobians\\n). The optimization litera…\\nture contains amazing algorithms based on the \\nsecond-order partial\\nderivatives\\n (the Hessians\\n). Unfortunately, these algorithms are very\\nhard to apply to deep neural networks because there are \\nn2 Hessi…\\nans per output (where n is the number of parameters), as opposed\\nto just n Jacobians per output. Since DNNs typically have tens of\\nthousands of parameters, the second-order optimization algo…\\nrithms often don‡t even fit in memory, and even when they do,\\ncomputing the Hessians is just too slow.\\nTraining Sparse ModelsAll the optimization algorithms just presented produce dense models, meaning that\\nmost parameters will be nonzero. If you need a blazingly fast model at runtime, or if\\nyou need it to take up less memory, you may prefer to end up with a sparse model\\ninstead.One trivial way to achieve this is to train the model as usual, then get rid of the tiny\\nweights (set them to 0).\\nAnother option is to apply strong —\\n1 regularization during training, as it pushes the\\noptimizer to zero out as many weights as it can (as discussed in \\nChapter 4\\n about Lasso\\nRegression).However, in some cases these techniques may remain insufficient. One last option is\\nto apply \\nDual Averaging\\n, often called Follow \\n•e Regularized Leader\\n (FTRL), a techni…que proposed by Yurii Nesterov\\n.17 When used with —1 regularization, this technique\\noften leads to very sparse models. TensorFlow implements a variant of FTRL called\\nFTRL-Proximal\\n18 in the FTRLOptimizer class.Learning Rate SchedulingFinding a good learning rate can be tricky. If you set it way too high, training may\\nactually diverge (as we discussed in Chapter 4\\n). If you set it too low, training will\\neventually converge to the optimum, but it will take a very long time. If you set it\\nslightly too high, it will make progress very quickly at first, but it will end up dancing\\naround the optimum, never settling down (unless you use an adaptive learning rate\\noptimization algorithm such as AdaGrad, RMSProp, or Adam, but even then it may\\ntake time to settle). If you have a limited computing budget, you may have to inter…\\n300 | Chapter 11: Training Deep Neural Nets\\nrupt training before it has converged properly, yielding a suboptimal solution (see\\nFigure 11-8).Figure 11-8. Learning curves for various learning rates −\\nYou may be able to find a fairly good learning rate by training your network several\\ntimes during just a few epochs using various learning rates and comparing the learn…\\ning curves. The ideal learning rate will learn quickly and converge to good solution.\\nHowever, you can do better than a constant learning rate: if you start with a high\\nlearning rate and then reduce it once it stops making fast progress, you can reach a\\ngood solution faster than with the optimal constant learning rate. There are many dif…\\nferent strategies to reduce the learning rate during training. These strategies are called\\nlearning schedules\\n (we briefly introduced this concept in \\nChapter 4\\n), the most com…mon of which are:Predetermined piecewise constant learning rate\\nFor example, set the learning rate to \\n−0 = 0.1 at first, then to \\n−1 = 0.001 after 50epochs. Although this solution can work very well, it often requires fiddling\\naround to figure out the right learning rates and when to use them.\\nPerformance scheduling\\nMeasure the validation error every \\nN steps (just like for early stopping) andreduce the learning rate by a factor of \\nŒ when the error stops dropping.Exponential scheduling\\nSet the learning rate to a function of the iteration number \\nt: −(t) = −0 10‘t/r. Thisworks great, but it requires tuning \\n−0 and r. The learning rate will drop by a fac…\\ntor of 10 every \\nr steps.Power scheduling\\nSet the learning rate to \\n−(t) = −0 (1 + t/r)‘c. The hyperparameter \\nc is typically setto 1. This is similar to exponential scheduling, but the learning rate drops much\\nmore slowly.\\nFaster Optimizers | 301\\n19ƒAn Empirical Study of Learning Rates in Deep Neural Networks for Speech Recognition,⁄ A. Senior et al.\\n(2013).A 2013 paper\\n19 by Andrew Senior et al. compared the performance of some of the\\nmost popular learning schedules when training deep neural networks for speech rec…ognition using Momentum optimization. The authors concluded that, in this setting,\\nboth performance scheduling and exponential scheduling performed well, but they\\nfavored exponential scheduling because it is simpler to implement, is easy to tune,\\nand converged slightly faster to the optimal solution.\\nImplementing a learning schedule with TensorFlow is fairly straightforward:\\ninitial_learning_rate = 0.1decay_steps = 10000decay_rate = 1/10global_step = tf.Variable(0, trainable=False)learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,                                           decay_steps, decay_rate)optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)training_op = optimizer.minimize(loss, global_step=global_step)After setting the hyperparameter values, we create a nontrainable variable\\nglobal_step (initialized to 0) to keep track of the current training iteration number.\\nThen we define an exponentially decaying learning rate (with \\n−0 = 0.1 and r = 10,000)\\nusing TensorFlow‡s \\nexponential_decay() function. Next, we create an optimizer (in\\nthis example, a \\nMomentumOptimizer) using this decaying learning rate. Finally, we cre…\\nate the training operation by calling the optimizer‡s \\nminimize() method; since wepass it the global_step variable, it will kindly take care of incrementing it. That‡s it!\\nSince AdaGrad, RMSProp, and Adam optimization automatically reduce the learning\\nrate during training, it is not necessary to add an extra learning schedule. For other\\noptimization algorithms, using exponential decay or performance scheduling can\\nconsiderably speed up convergence.\\nAvoiding Over•tting Through RegularizationWith four parameters I can fit an elephant and with five I can make him wiggle his\\ntrunk.›John von Neumann, \\ncited by Enrico Fermi in Nature 427\\nDeep neural networks typically have tens of thousands of parameters, sometimes\\neven millions. With so many parameters, the network has an incredible amount of\\nfreedom and can fit a huge variety of complex datasets. But this great flexibility also\\nmeans that it is prone to overfitting the training set.\\n302 | Chapter 11: Training Deep Neural Nets\\nWith millions of parameters you can fit the whole zoo. In this section we will present\\nsome of the most popular regularization techniques for neural networks, and how to\\nimplement them with TensorFlow: early stopping, —\\n1 and —2 regularization, dropout,\\nmax-norm regularization, and data augmentation.\\nEarly StoppingTo avoid overfitting the training set, a great solution is early stopping (introduced in\\nChapter 4\\n): just interrupt training when its performance on the validation set starts\\ndropping.One way to implement this with TensorFlow is to evaluate the model on a validation\\nset at regular intervals (e.g., every 50 steps), and save a ƒwinner⁄ snapshot if it outper…\\nforms previous ƒwinner⁄ snapshots. Count the number of steps since the last ƒwin…\\nner⁄ snapshot was saved, and interrupt training when this number reaches some limit\\n(e.g., 2,000 steps). Then restore the last ƒwinner⁄ snapshot.\\nAlthough early stopping works very well in practice, you can usually get much higher\\nperformance out of your network by combining it with other regularization techni…\\nques.–1 and –2 RegularizationJust \\nlike you did in Chapter 4\\n for simple linear models, you can use —\\n1 and —\\n2 regulari…\\nzation to constrain a neural network‡s connection weights (but typically not its bia…\\nses).One way to do this using TensorFlow is to simply add the appropriate regularization\\nterms to your cost function. For example, assuming you have just one hidden layer\\nwith weights \\nweights1 and one output layer with weights \\nweights2, then you canapply —\\n1 regularization like this:\\n[...] # construct the neural networkbase_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")reg_losses = tf.reduce_sum(tf.abs(weights1)) + tf.reduce_sum(tf.abs(weights2))loss = tf.add(base_loss, scale * reg_losses, name=\"loss\")However, if there are many layers, this approach is not very convenient. Fortunately,\\nTensorFlow provides a better option. Many functions that create variables (such as\\nget_variable() or fully_connected()) accept a *_regularizer argument for each\\ncreated variable (e.g., \\nweights_regularizer). You can pass any function that takes\\nweights as an argument and returns the corresponding regularization loss. The\\nl1_regularizer(), l2_regularizer(), and l1_l2_regularizer() functions return such functions. The following code puts all this together:\\nwith arg_scope(        [fully_connected],Avoiding Over•tting Through Regularization | 303\\n20ƒImproving neural networks by preventing co-adaptation of feature detectors,⁄ G. Hinton et al. (2012).\\n21ƒDropout: A Simple Way to Prevent Neural Networks from Overfitting,⁄ N. Srivastava et al. (2014).\\n        weights_regularizer=tf.contrib.layers.l1_regularizer(scale=0.01)):    hidden1 = fully_connected(X, n_hidden1, scope=\"hidden1\")    hidden2 = fully_connected(hidden1, n_hidden2, scope=\"hidden2\")    logits = fully_connected(hidden2, n_outputs, activation_fn=None,scope=\"out\")This code creates a neural network with two hidden layers and one output layer, and\\nit also creates nodes in the graph to compute the —\\n1 regularization loss corresponding\\nto each layer‡s weights. TensorFlow automatically adds these nodes to a special collec…\\ntion containing all the regularization losses. You just need to add these regularization\\nlosses to your overall loss, like this:reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")Don‡t forget to add the regularization losses to your overall loss, or\\nelse they will simply be ignored.\\nDropoutThe most popular regularization technique for deep neural networks is arguably\\ndropout\\n. It was \\nproposed20 by G. E. Hinton in 2012 and further detailed in a \\npaper\\n21 by\\nNitish Srivastava et al., and it has proven to be highly successful: even the state-of-\\nthe-art neural networks got a 1–2% accuracy boost simply by adding dropout. This\\nmay not sound like a lot, but when a model already has 95% accuracy, getting a 2%\\naccuracy boost means dropping the error rate by almost 40% (going from 5% error to\\nroughly 3%).It is a fairly simple algorithm: at every training step, every neuron (including the\\ninput neurons but excluding the output neurons) has a probability \\np of being tem…porarily ƒdropped out,⁄ meaning it will be entirely ignored during this training step,\\nbut it may be active during the next step (see \\nFigure 11-9). The hyperparameter \\np iscalled the dropout rate\\n, and it is typically set to 50%. After training, neurons don‡t get\\ndropped anymore. And that‡s all (except for a technical detail we will discuss momen…\\ntarily).304 | Chapter 11: Training Deep Neural Nets\\nFigure 11-9. Dropout regularization\\nIt is quite surprising at first that this rather brutal technique works at all. Would a\\ncompany perform better if its employees were told to toss a coin every morning to\\ndecide whether or not to go to work? Well, who knows; perhaps it would! The com…\\npany would obviously be forced to adapt its organization; it could not rely on any sin…\\ngle person to fill in the coffee machine or perform any other critical tasks, so this\\nexpertise would have to be spread across several people. Employees would have to\\nlearn to cooperate with many of their coworkers, not just a handful of them. The\\ncompany would become much more resilient. If one person quit, it wouldn‡t make\\nmuch of a difference. It‡s unclear whether this idea would actually work for compa…\\nnies, but it certainly does for neural networks. Neurons trained with dropout cannot\\nco-adapt with their neighboring neurons; they have to be as useful as possible on\\ntheir own. They also cannot rely excessively on just a few input neurons; they must\\npay attention to each of their input neurons. They end up being less sensitive to slight\\nchanges in the inputs. In the end you get a more robust network that generalizes bet…\\nter.\\nAnother way to understand the power of dropout is to realize that a unique neural\\nnetwork is generated at each training step. Since each neuron can be either present or\\nabsent, there is a total of 2\\nN possible networks (where N is the total number of drop…\\npable neurons). This is such a huge number that it is virtually impossible for the same\\nneural network to be sampled twice. Once you have run a 10,000 training steps, you\\nhave essentially trained 10,000 different neural networks (each with just one training\\ninstance). These neural networks are obviously not independent since they share\\nmany of their weights, but they are nevertheless all different. The resulting neural\\nnetwork can be seen as an averaging ensemble of all these smaller neural networks.\\nThere is one small but important technical detail. Suppose \\np = 50, in which case dur…ing testing a neuron will be connected to twice as many input neurons as it was (on\\naverage) during training. To compensate for this fact, we need to multiply each neu…\\nAvoiding Over•tting Through Regularization | 305\\nron‡s input connection weights by 0.5 after training. If we don‡t, each neuron will get a\\ntotal input signal roughly twice as large as what the network was trained on, and it is\\nunlikely to perform well. More generally, we need to multiply each input connection\\nweight by the \\nkeep probability\\n (1 – p) after training. Alternatively, we can divide each\\nneuron‡s output by the keep probability during training (these alternatives are not\\nperfectly equivalent, but they work equally well).\\nTo implement dropout using TensorFlow, you can simply apply the \\ndropout() func…tion to the input layer and to the output of every hidden layer. During training, this\\nfunction randomly drops some items (setting them to 0) and divides the remainingitems by the keep probability. After training, this function does nothing at all. The\\nfollowing code applies dropout regularization to our three-layer neural network:\\nfrom tensorflow.contrib.layers import dropout[...]is_training = tf.placeholder(tf.bool, shape=(), name=•is_training•)keep_prob = 0.5X_drop = dropout(X, keep_prob, is_training=is_training)hidden1 = fully_connected(X_drop, n_hidden1, scope=\"hidden1\")hidden1_drop = dropout(hidden1, keep_prob, is_training=is_training)hidden2 = fully_connected(hidden1_drop, n_hidden2, scope=\"hidden2\")hidden2_drop = dropout(hidden2, keep_prob, is_training=is_training)logits = fully_connected(hidden2_drop, n_outputs, activation_fn=None,                         scope=\"outputs\")You want to use the \\ndropout() function in tensorflow.contrib.layers, not the one in tensorflow.nn. The first one turns off(no-op) when not training, which is what you want, while the sec…\\nond one does not.Of course, just like you did earlier for Batch Normalization, you need to set \\nis_training to True when training, and to False when testing.If you observe that the model is overfitting, you can increase the dropout rate (i.e.,\\nreduce the keep_prob hyperparameter). Conversely, you should try decreasing the\\ndropout rate (i.e., increasing \\nkeep_prob) if the model underfits the training set. It can\\nalso help to increase the dropout rate for large layers, and reduce it for small ones.\\nDropout does tend to significantly slow down convergence, but it usually results in a\\nmuch better model when tuned properly. So, it is generally well worth the extra time\\nand effort.306 | Chapter 11: Training Deep Neural Nets\\nDropconnect\\n is a variant of dropout where individual connections\\nare dropped randomly rather than whole neurons. In general drop…\\nout performs better.\\nMax-Norm RegularizationAnother regularization technique that is quite popular for neural networks is called\\nmax-norm regularization\\n: for each neuron, it constrains the weights \\nw of the incom…ing connections such that \\n w 2 Ž r, where r is the max-norm hyperparameter and\\n ’ 2 is the —2 norm.We typically implement this constraint by computing \\nw2 after each training stepand clipping w if needed (r\\n2).Reducing r increases the amount of regularization and helps reduce overfitting. Max-\\nnorm regularization can also help alleviate the vanishing/exploding gradients prob…\\nlems (if you are not using Batch Normalization).\\nTensorFlow does not provide an off-the-shelf max-norm regularizer, but it is not too\\nhard to implement. The following code creates a node \\nclip_weights that will clip the\\nweights variable \\nalong the second axis so that each row vector has a maximum norm\\nof 1.0:threshold = 1.0clipped_weights = tf.clip_by_norm(weights, clip_norm=threshold, axes=1)clip_weights = tf.assign(weights, clipped_weights)You would then apply this operation after each training step, like so:\\nwith tf.Session() as sess:    [...]    for epoch in range(n_epochs):        [...]        for X_batch, y_batch in zip(X_batches, y_batches):            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})            clip_weights.eval()You may wonder how to get access to the \\nweights variable of each layer. For this you\\ncan simply use a variable scope like this:\\nhidden1 = fully_connected(X, n_hidden1, scope=\"hidden1\")with tf.variable_scope(\"hidden1\", reuse=True):    weights1 = tf.get_variable(\"weights\")Alternatively, you can use the root variable scope:\\nhidden1 = fully_connected(X, n_hidden1, scope=\"hidden1\")hidden2 = fully_connected(hidden1, n_hidden2, scope=\"hidden2\")Avoiding Over•tting Through Regularization | 307\\n[...]with tf.variable_scope(\"\", default_name=\"\", reuse=True):  # root scope    weights1 = tf.get_variable(\"hidden1/weights\")    weights2 = tf.get_variable(\"hidden2/weights\")If you don‡t know what the name of a variable is, you can either use TensorBoard to\\nfind out or simply use the \\nglobal_variables() function and print out all the variable\\nnames:for variable in tf.global_variables():    print(variable.name)Although the preceding solution should work fine, it is a bit messy. A cleaner solution\\nis to create a \\nmax_norm_regularizer() function \\nand use it just like the earlier l1_regularizer() function:def max_norm_regularizer(threshold, axes=1, name=\"max_norm\",                         collection=\"max_norm\"):    def max_norm(weights):        clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)        clip_weights = tf.assign(weights, clipped, name=name)        tf.add_to_collection(collection, clip_weights)        return None  # there is no regularization loss term    return max_normThis function returns a parametrized max_norm() function that you can use like any\\nother regularizer:\\nmax_norm_reg = max_norm_regularizer(threshold=1.0)hidden1 = fully_connected(X, n_hidden1, scope=\"hidden1\",                          weights_regularizer=max_norm_reg)Note that max-norm regularization does not require adding a regularization loss term\\nto your overall loss function, so the max_norm() function returns None. But you stillneed to be able to run the clip_weights operation after each training step, so you\\nneed to be able to get a handle on it. This is why the \\nmax_norm() function adds theclip_weights node to a collection of max-norm clipping operations. You need to\\nfetch these clipping operations and run them after each training step:\\nclip_all_weights = tf.get_collection(\"max_norm\")with tf.Session() as sess:    [...]    for epoch in range(n_epochs):        [...]        for X_batch, y_batch in zip(X_batches, y_batches):            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})            sess.run(clip_all_weights)Much\\n cleaner code, isn‡t it?\\n308 | Chapter 11: Training Deep Neural Nets\\nData AugmentationOne last regularization technique, data augmentation, consists of generating new\\ntraining instances from existing ones, artificially boosting the size of the training set.This will reduce overfitting, making this a regularization technique. The trick is to\\ngenerate realistic training instances; ideally, a human should not be able to tell which\\ninstances were generated and which ones were not. Moreover, simply adding white\\nnoise will not help; the modifications you apply should be learnable (white noise is\\nnot).For example, if your model is meant to classify pictures of mushrooms, you can\\nslightly shift, rotate, and resize every picture in the training set by various amounts\\nand add the resulting pictures to the training set (see Figure 11-10). This forces themodel to be more tolerant to the position, orientation, and size of the mushrooms in\\nthe picture. If you want the model to be more tolerant to lighting conditions, you can\\nsimilarly generate many images with various contrasts. Assuming the mushrooms are\\nsymmetrical, you can also flip the pictures horizontally. By combining these transfor…\\nmations you can greatly increase the size of your training set.\\nFigure 11-10. Generating new training instances from existing ones\\nIt is often preferable to generate training instances on the fly during training rather\\nthan wasting storage space and network bandwidth. TensorFlow offers several image\\nmanipulation operations such as transposing (shifting), rotating, resizing, flipping,\\nand cropping, as well as adjusting the brightness, contrast, saturation, and hue (see\\nAvoiding Over•tting Through Regularization | 309\\nthe API documentation for more details). This makes it easy to implement data aug…\\nmentation for image datasets.\\nAnother powerful technique to train very deep neural networks is\\nto add skip connections\\n (a skip connection is when you add theinput of a layer to the output of a higher layer). We will explore this\\nidea in Chapter 13\\n when we talk about deep residual networks.Practical GuidelinesIn this chapter, we have covered a wide range of techniques and you may be wonder…\\ning which ones you should use. The configuration in \\nTable 11-2\\n will work fine inmost cases.Table 11-2. Default DNN \\ncon†gurationInitializationHe initialization\\nActivation function\\nELUNormalizationBatch Normalization\\nRegularizationDropoutOptimizerAdamLearning rate schedule\\nNoneOf course, you should try to reuse parts of a pretrained neural network if you can\\nfind one that solves a similar problem.\\nThis default configuration may need to be tweaked:\\n‹If you can‡t find a good learning rate (convergence was too slow, so you increased\\nthe training rate, and now convergence is fast but the network‡s accuracy is sub…\\noptimal), then you can try adding a learning schedule such as exponential decay.\\n‹If your training set is a bit too small, you can implement data augmentation.\\n‹If you need a sparse model, you can add some —1 regularization to the mix (and\\noptionally zero out the tiny weights after training). If you need an even sparser\\nmodel, you can try using FTRL instead of Adam optimization, along with —\\n1 reg…ularization.\\n‹If you need a lightning-fast model at runtime, you may want to drop Batch Nor…\\nmalization, and possibly replace the ELU activation function with the leaky\\nReLU. Having a sparse model will also help.\\nWith these guidelines, you are now ready to train very deep nets›well, if you are\\nvery patient, that is! If you use a single machine, you may have to wait for days or\\n310 | Chapter 11: Training Deep Neural Nets\\neven months for training to complete. In the next chapter we will discuss how to use\\ndistributed TensorFlow to train and run models across many servers and GPUs.\\nExercises1.Is it okay to initialize all the weights to the same value as long as that value is\\nselected randomly using He initialization?\\n2.Is it okay to initialize the bias terms to 0?\\n3.Name three advantages of the ELU activation function over ReLU.\\n4.In which cases would you want to use each of the following activation functions:\\nELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\\n5.What may happen if you set the \\nmomentum hyperparameter too close to 1 (e.g.,\\n0.99999) when using a MomentumOptimizer?6.Name three ways you can produce a sparse model.\\n7.Does dropout slow down training? Does it slow down inference (i.e., makingpredictions on new instances)?8.Deep Learning.a.Build a DNN with five hidden layers of 100 neurons each, He initialization,\\nand the ELU activation function.\\nb.\\nUsing Adam optimization and early stopping, try training it on MNIST but\\nonly on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in thenext exercise. You will need a softmax output layer with five neurons, and as\\nalways make sure to save checkpoints at regular intervals and save the final\\nmodel so you can reuse it later.\\nc.Tune the hyperparameters using cross-validation and see what precision you\\ncan achieve.d.Now try adding Batch Normalization and compare the learning curves: is it\\nconverging faster than before? Does it produce a better model?\\ne.Is the model overfitting the training set? Try adding dropout to every layer\\nand try again. Does it help?\\n9.Transfer learning.\\na.Create a new DNN that reuses all the pretrained hidden layers of the previous\\nmodel, freezes them, and replaces the softmax output layer with a fresh new\\none.b.\\nTrain this new DNN on digits 5 to 9, using only 100 images per digit, and time\\nhow long it takes. Despite this small number of examples, can you achieve\\nhigh precision?Exercises | 311\\nc.Try caching the frozen layers, and train the model again: how much faster is it\\nnow?d.Try again reusing just four hidden layers instead of five. Can you achieve a\\nhigher precision?e.Now unfreeze the top two hidden layers and continue training: can you get\\nthe model to perform even better?10.Pretraining on an auxiliary task.\\na.In this exercise you will build a DNN that compares two MNIST digit images\\nand predicts whether they represent the same digit or not. Then you will reuse\\nthe lower layers of this network to train an MNIST classifier using very little\\ntraining data. Start by building two DNNs (let‡s call them DNN A and B), both\\nsimilar to the one you built earlier but without the output layer: each DNN\\nshould have five hidden layers of 100 neurons each, He initialization, and ELU\\nactivation. Next, add a single output layer on top of both DNNs. You should\\nuse TensorFlow‡s \\nconcat() function with axis=1 to concatenate the outputs\\nof both DNNs along the horizontal axis, then feed the result to the output\\nlayer. This output layer should contain a single neuron using the logistic acti…\\nvation function.\\nb.\\nSplit the MNIST training set in two sets: split #1 should containing 55,000\\nimages, and split #2 should contain contain 5,000 images. Create a function\\nthat generates a training batch where each instance is a pair of MNIST images\\npicked from split #1. Half of the training instances should be pairs of images\\nthat belong to the same class, while the other half should be images from dif…\\nferent classes. For each pair, the training label should be 0 if the images are\\nfrom the same class, or 1 if they are from different classes.\\nc.Train the DNN on this training set. For each image pair, you can simultane…\\nously feed the first image to DNN A and the second image to DNN B. The\\nwhole network will gradually learn to tell whether two images belong to thesame class or not.d.Now create a new DNN by reusing and freezing the hidden layers of DNN A\\nand adding a softmax output layer on with 10 neurons. Train this network on\\nsplit #2 and see if you can achieve high performance despite having only 500\\nimages per class.Solutions to these exercises are available in \\nAppendix A\\n.312 | Chapter 11: Training Deep Neural Nets\\nCHAPTER 12Distributing TensorFlow AcrossDevices and ServersIn Chapter 11\\n we discussed several techniques that can considerably speed up train…\\ning: better weight initialization, Batch Normalization, sophisticated optimizers, and\\nso on. However, even with all of these techniques, training a large neural network on\\na single machine with a single CPU can take days or even weeks.\\nIn this chapter we will see how to use TensorFlow to distribute computations across\\nmultiple devices (CPUs and GPUs) and run them in parallel (see \\nFigure 12-1). Firstwe will distribute computations across multiple devices on just one machine, then on\\nmultiple devices across multiple machines.\\nFigure 12-1. Executing a TensorFlow graph across multiple devices in parallel\\n313TensorFlow‡s support of distributed computing is one of its main highlights com…\\npared to other neural network frameworks. It gives you full control over how to split\\n(or replicate) your computation graph across devices and servers, and it lets you par…\\nallelize and synchronize operations in flexible ways so you can choose between all\\nsorts of parallelization approaches.\\nWe will look at some of the most popular approaches to parallelizing the execution\\nand training of a neural network. Instead of waiting for weeks for a training algo…rithm to complete, you may end up waiting for just a few hours. Not only does this\\nsave an enormous amount of time, it also means that you can experiment with vari…\\nous models much more easily, and frequently retrain your models on fresh data.\\nOther great use cases of parallelization include exploring a much larger hyperparame…\\nter space when fine-tuning your model, and running large ensembles of neural net…works efficiently.\\nBut we must learn to walk before we can run. Let‡s start by parallelizing simple graphs\\nacross several GPUs on a single machine.\\nMultiple Devices on a Single MachineYou \\ncan often get a major performance boost simply by adding GPU cards to a single\\nmachine. In fact, in many cases this will suffice; you won‡t need to use multiple\\nmachines at all. For example, you can typically train a neural network just as fast\\nusing 8 GPUs on a single machine rather than 16 GPUs across multiple machines\\n(due to the extra delay imposed by network communications in a multimachine\\nsetup).In this section we will look at how to set up your environment so that TensorFlow can\\nuse multiple GPU cards on one machine. Then we will look at how you can distribute\\noperations across available devices and execute them in parallel.\\nInstallationIn order to run TensorFlow on multiple GPU cards, you first need to make sure your\\nGPU cards have NVidia Compute Capability (greater or equal to 3.0). This includes\\nNvidia‡s Titan, Titan X, K20, and K40 cards (if you own another card, you can check\\nits compatibility at \\nhttps://developer.nvidia.com/cuda-gpus\\n).314 | Chapter 12: Distributing TensorFlow Across Devices and Servers\\nIf you don‡t own any GPU cards, you can use a hosting service with\\nGPU capability such as Amazon AWS. Detailed instructions to set\\nup TensorFlow 0.9 with Python 3.5 on an Amazon AWS GPU\\ninstance are available in Àiga Avsec‡s \\nhelpful blog post. It should\\nnot be too hard to update it to the latest version of TensorFlow.\\nGoogle also released a cloud service called \\nCloud Machine Learning\\nto run TensorFlow graphs. In May 2016, they announced that their\\nplatform now includes servers equipped with \\ntensor processing units\\n(TPUs), \\nprocessors specialized for Machine Learning that are much\\nfaster than GPUs for many ML tasks. Of course, another option is\\nsimply to buy your own GPU card. Tim Dettmers wrote a \\ngreat\\nblog post to help you choose, and he updates it fairly regularly.\\nYou must then download and install the appropriate version of the CUDA and\\ncuDNN libraries (CUDA 8.0 and cuDNN 5.1 if you are using the binary installation\\nof TensorFlow 1.0.0), and set a few environment variables so TensorFlow knows\\nwhere to find CUDA and cuDNN. The detailed installation instructions are likely to\\nchange fairly quickly, so it is best that you follow the instructions on TensorFlow‡s\\nwebsite.Nvidia‡s \\nCompute \\nUni†ed Device Architecture\\n library (CUDA) allows developers to\\nuse CUDA-enabled GPUs for all sorts of computations (not just graphics accelera…\\ntion). Nvidia‡s \\nCUDA Deep Neural Network\\n library (cuDNN) is a GPU-accelerated\\nlibrary of primitives for DNNs. It provides optimized implementations of common\\nDNN computations such as activation layers, normalization, forward and backward\\nconvolutions, and pooling (see \\nChapter 13\\n). It is part of Nvidia‡s Deep Learning SDK\\n(note that it requires creating an Nvidia developer account in order to download it).\\nTensorFlow uses CUDA and cuDNN to control the GPU cards and accelerate com…\\nputations (see \\nFigure 12-2).Figure 12-2. TensorFlow uses CUDA and cuDNN to control GPUs and boost DNNs\\nYou can use the \\nnvidia-smi command to check that CUDA is properly installed. It\\nlists the available GPU cards, as well as processes running on each card:\\nMultiple Devices on a Single Machine | 315\\n$ nvidia-smiWed Sep 16 09:50:03 2016+------------------------------------------------------+| NVIDIA-SMI 352.63     Driver Version: 352.63         ||-------------------------------+----------------------+----------------------+| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC || Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. ||===============================+======================+======================||   0  GRID K520           Off  | 0000:00:03.0     Off |                  N/A || N/A   27C    P8    17W / 125W |     11MiB /  4095MiB |      0%      Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes:                                                       GPU Memory ||  GPU       PID  Type  Process name                               Usage      ||=============================================================================||  No running processes found                                                 |+-----------------------------------------------------------------------------+Finally, you must install TensorFlow with GPU support. If you created an isolated\\nenvironment using virtualenv, you first need to activate it:\\n$ cd $ML_PATH               # Your ML working directory (e.g., $HOME/ml)$ source env/bin/activateThen install the appropriate GPU-enabled version of TensorFlow:\\n$ pip3 install --upgrade tensorflow-gpuNow you can open up a Python shell and check that TensorFlow detects and uses\\nCUDA and cuDNN properly by importing TensorFlow and creating a session:\\n>>> import tensorflow as tfI [...]/dso_loader.cc:108] successfully opened CUDA library libcublas.so locallyI [...]/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locallyI [...]/dso_loader.cc:108] successfully opened CUDA library libcufft.so locallyI [...]/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locallyI [...]/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally>>> sess = tf.Session()[...]I [...]/gpu_init.cc:102] Found device 0 with properties:name: GRID K520major: 3 minor: 0 memoryClockRate (GHz) 0.797pciBusID 0000:00:03.0Total memory: 4.00GiBFree memory: 3.95GiBI [...]/gpu_init.cc:126] DMA: 0I [...]/gpu_init.cc:136] 0:   YI [...]/gpu_device.cc:839] Creating TensorFlow device(/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)Looks good! TensorFlow detected the CUDA and cuDNN libraries, and it used the\\nCUDA library to detect the GPU card (in this case an Nvidia Grid K520 card).\\n316 | Chapter 12: Distributing TensorFlow Across Devices and Servers\\nManaging the GPU RAMBy default TensorFlow automatically grabs all the RAM in all available GPUs the first\\ntime you run a graph, so you will not be able to start a second TensorFlow program\\nwhile the first one is still running. If you try, you will get the following error:\\nE [...]/cuda_driver.cc:965] failed to allocate 3.66G (3928915968 bytes) fromdevice: CUDA_ERROR_OUT_OF_MEMORYOne solution is to run each process on different GPU cards. To do this, the simplest\\noption is to set the CUDA_VISIBLE_DEVICES environment variable so that each process\\nonly sees the appropriate GPU cards. For example, you could start two programs like\\nthis:$ CUDA_VISIBLE_DEVICES=0,1 python3 program_1.py# and in another terminal:$ CUDA_VISIBLE_DEVICES=3,2 python3 program_2.pyProgram #1 will only see GPU cards 0 and 1 (numbered 0 and 1, respectively), and\\nprogram #2 will only see GPU cards 2 and 3 (numbered 1 and 0, respectively). Every…\\nthing will work fine (see Figure 12-3).Figure 12-3. Each program gets two GPUs for itself\\nAnother option is to tell TensorFlow to grab only a fraction of the memory. For\\nexample, to make TensorFlow grab only 40% of each GPU‡s memory, you must create\\na ConfigProto object, set its gpu_options.per_process_gpu_memory_fraction option to 0.4, and create the session using this configuration:\\nconfig = tf.ConfigProto()config.gpu_options.per_process_gpu_memory_fraction = 0.4session = tf.Session(config=config)Now two programs like this one can run in parallel using the same GPU cards (but\\nnot three, since 3 ‰ 0.4 > 1). See Figure 12-4.Multiple Devices on a Single Machine | 317\\n1ƒTensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems,⁄ Google Research\\n(2015).Figure 12-4. Each program gets all four GPUs, but with only 40% of the RAM each\\nIf you run the nvidia-smi command while both programs are running, you shouldsee that each process holds roughly 40% of the total RAM of each card:\\n$ nvidia-smi[...]+-----------------------------------------------------------------------------+| Processes:                                                       GPU Memory ||  GPU       PID  Type  Process name                               Usage      ||=============================================================================||    0      5231    C   python                                        1677MiB ||    0      5262    C   python                                        1677MiB ||    1      5231    C   python                                        1677MiB ||    1      5262    C   python                                        1677MiB |[...]Yet another option is to tell TensorFlow to grab memory only when it needs it. To do\\nthis you must \\nset config.gpu_options.allow_growth to \\nTrue. However, TensorFlow\\nnever releases memory once it has grabbed it (to avoid memory fragmentation) so\\nyou may still run out of memory after a while. It may be harder to guarantee a deter…\\nministic behavior using this option, so in general you probably want to stick with one\\nof the previous options.Okay, now you have a working GPU-enabled TensorFlow installation. Let‡s see how\\nto use it!Placing Operations on DevicesThe TensorFlow \\nwhitepaper\\n1 presents a friendly \\ndynamic placer\\n algorithm that auto…\\nmagically distributes operations across all available devices, taking into account\\nthings like the measured computation time in previous runs of the graph, estimations\\nof the size of the input and output tensors to each operation, the amount of RAM\\navailable in each device, communication delay when transferring data in and out of\\n318 | Chapter 12: Distributing TensorFlow Across Devices and Servers\\ndevices, hints and constraints from the user, and more. Unfortunately, this sophistica…\\nted algorithm is internal to Google; it was not released in the open source version of\\nTensorFlow. The reason it was left out seems to be that in practice a small set of place…\\nment rules specified by the user actually results in more efficient placement than what\\nthe dynamic placer is capable of. However, the TensorFlow team is working on\\nimproving the dynamic placer, and perhaps it will eventually be good enough to be\\nreleased.Until then TensorFlow relies on the \\nsimple placer\\n, which (as its name suggests) is very\\nbasic.Simple placementWhenever you run a graph, if TensorFlow needs to evaluate a node that is not placed\\non a device yet, it uses the simple placer to place it, along with all other nodes that are\\nnot placed yet. The simple placer respects the following rules:\\n‹If a node was already placed on a device in a previous run of the graph, it is left\\non that device.\\n‹Else, if the user pinned\\n a node to a device (described next), the placer places it onthat device.\\n‹Else, it defaults to GPU #0, or the CPU if there is no GPU.\\nAs you can see, placing operations on the appropriate device is mostly up to you. If\\nyou don‡t do anything, the whole graph will be placed on the default device. To pin\\nnodes onto a device, you must create a device block using the \\ndevice() function. For\\nexample, the following code pins the variable \\na and the constant \\nb on the CPU, but\\nthe multiplication node \\nc is not pinned on any device, so it will be placed on the \\ndefault device:\\nwith tf.device(\"/cpu:0\"):    a = tf.Variable(3.0)    b = tf.constant(4.0)c = a * bThe \"/cpu:0\" device aggregates all CPUs on a multi-CPU system.\\nThere is currently no way to pin nodes on specific CPUs or to use\\njust a subset of all CPUs.\\nMultiple Devices on a Single Machine | 319\\nLogging placementsLet‡s check that the simple placer respects the placement constraints we have just\\ndefined. For this you can set the \\nlog_device_placement option to True; this tells theplacer to log a message whenever it places a node. For example:\\n>>> config = tf.ConfigProto()>>> config.log_device_placement = True>>> sess = tf.Session(config=config)I [...] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520,pci bus id: 0000:00:03.0)[...]>>> x.initializer.run(session=sess)I [...] a: /job:localhost/replica:0/task:0/cpu:0I [...] a/read: /job:localhost/replica:0/task:0/cpu:0I [...] mul: /job:localhost/replica:0/task:0/gpu:0I [...] a/Assign: /job:localhost/replica:0/task:0/cpu:0I [...] b: /job:localhost/replica:0/task:0/cpu:0I [...] a/initial_value: /job:localhost/replica:0/task:0/cpu:0>>> sess.run(c)12The lines starting with \"I\" for Info are the log messages. When we create a session,\\nTensorFlow logs a message to tell us that it has found a GPU card (in this case the\\nGrid K520 card). Then the first time we run the graph (in this case when initializing\\nthe variable a), the simple placer is run and places each node on the device it was\\nassigned to. As expected, the log messages show that all nodes are placed on \\n\"/cpu:0\"except the multiplication node, which ends up on the default device \\n\"/gpu:0\" (youcan safely ignore the prefix /job:localhost/replica:0/task:0 for now; we will talk\\nabout it in a moment). Notice that the second time we run the graph (to compute \\nc),the placer is not used since all the nodes TensorFlow needs to compute \\nc are alreadyplaced.Dynamic placement functionWhen you create a device block, you can specify a function instead of a device name.\\nTensorFlow will call this function for each operation it needs to place in the device\\nblock, and the function must return the name of the device to pin the operation on.\\nFor example, the following code pins all the variable nodes to \\n\"/cpu:0\" (in this casejust the variable a) and all other nodes to \"/gpu:0\":def variables_on_cpu(op):    if op.type == \"Variable\":        return \"/cpu:0\"    else:        return \"/gpu:0\"with tf.device(variables_on_cpu):    a = tf.Variable(3.0)320 | Chapter 12: Distributing TensorFlow Across Devices and Servers\\n    b = tf.constant(4.0)    c = a * bYou can easily implement more complex algorithms, such as pinning variables across\\nGPUs in a round-robin fashion.\\nOperations and kernelsFor a TensorFlow operation to run on a device, it needs to have an implementation\\nfor that device; this is called a \\nkernel\\n. Many operations have kernels for both CPUs\\nand GPUs, but not all of them. For example, TensorFlow does not have a GPU kernel\\nfor integer variables, so the following code will fail when TensorFlow tries to place the\\nvariable i on GPU #0:>>> with tf.device(\"/gpu:0\"):...     i = tf.Variable(3)[...]>>> sess.run(i.initializer)Traceback (most recent call last):[...]tensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a deviceto node •Variable•: Could not satisfy explicit device specificationNote that TensorFlow infers that the variable must be of type \\nint32 since the initiali…zation value is an integer. If you change the initialization value to \\n3.0 instead of 3, orif you explicitly set dtype=tf.float32 when creating the variable, everything will\\nwork fine.Soft placementBy default, if you try to pin an operation on a device for which the operation has no\\nkernel, you get the exception shown earlier when TensorFlow tries to place the opera…\\ntion on the device. If you prefer TensorFlow to fall back to the CPU instead, you can\\nset the allow_soft_placement configuration option to \\nTrue:with tf.device(\"/gpu:0\"):    i = tf.Variable(3)config = tf.ConfigProto()config.allow_soft_placement = Truesess = tf.Session(config=config)sess.run(i.initializer)  # the placer runs and falls back to /cpu:0So far we have discussed how to place nodes on different devices. Now let‡s see how\\nTensorFlow will run these nodes in parallel.\\nParallel ExecutionWhen TensorFlow runs a graph, it starts by finding out the list of nodes that need to\\nbe evaluated, and it counts how many dependencies each of them has. TensorFlow\\nMultiple Devices on a Single Machine | 321\\nthen starts evaluating the nodes with zero dependencies (i.e., source nodes). If these\\nnodes are placed on separate devices, they obviously get evaluated in parallel. If they\\nare placed on the same device, they get evaluated in different threads, so they may run\\nin parallel too (in separate GPU threads or CPU cores).\\nTensorFlow manages a thread pool on each device to parallelize operations (see\\nFigure 12-5). These are called the inter-op thread pools\\n. Some operations have multi…\\nthreaded kernels: they can use other thread pools (one per device) called the intra-op\\nthread pools\\n.Figure 12-5. Parallelized execution of a TensorFlow graph\\nFor example, in \\nFigure 12-5, operations A, B, and C are source ops, so they can\\nimmediately be evaluated. Operations A and B are placed on GPU #0, so they are sent\\nto this device‡s inter-op thread pool, and immediately evaluated in parallel. Operation\\nA happens to have a multithreaded kernel; its computations are split in three parts,\\nwhich are executed in parallel by the intra-op thread pool. Operation C goes to GPU\\n#1‡s inter-op thread pool.\\nAs soon as operation C finishes, the dependency counters of operations D and E will\\nbe decremented and will both reach 0, so both operations will be sent to the inter-op\\nthread pool to be executed.You can control the number of threads per inter-op pool by setting\\nthe inter_op_parallelism_threads option. Note that the first ses…\\nsion you start creates the inter-op thread pools. All other sessions\\nwill just reuse them unless you set the use_per_session_threadsoption to True. You can control the number of threads per intra-op\\npool by setting the intra_op_parallelism_threads option.322 | Chapter 12: Distributing TensorFlow Across Devices and Servers\\nControl DependenciesIn some cases, it may be wise to postpone the evaluation of an operation even though\\nall the operations it depends on have been executed. For example, if it uses a lot of\\nmemory but its value is needed only much further in the graph, it would be best to\\nevaluate it at the last moment to avoid needlessly occupying RAM that other opera…\\ntions may need. Another example is a set of operations that depend on data located\\noutside of the device. If they all run at the same time, they may saturate the device‡s\\ncommunication bandwidth, and they will end up all waiting on I/O. Other operations\\nthat need to communicate data will also be blocked. It would be preferable to execute\\nthese communication-heavy operations sequentially, allowing the device to perform\\nother operations in parallel.\\nTo postpone evaluation of some nodes, a simple solution is to add \\ncontrol dependen…\\ncies\\n. For example, the following code tells TensorFlow to evaluate \\nx and \\ny only after aand b have been evaluated:\\na = tf.constant(1.0)b = a + 2.0with tf.control_dependencies([a, b]):    x = tf.constant(3.0)    y = tf.constant(4.0)z = x + yObviously, since \\nz depends on \\nx and \\ny, evaluating \\nz also implies waiting for \\na and \\nb to\\nbe evaluated, even though it is not explicitly in the \\ncontrol_dependencies() block.Also, since \\nb depends on a, we could simplify the preceding code by just creating a\\ncontrol dependency on \\n[b] instead of [a, b], but in some cases ƒexplicit is better\\nthan implicit.⁄\\nGreat! Now you know:\\n‹How to place operations on multiple devices in any way you please\\n‹How these operations get executed in parallel\\n‹How to create control dependencies to optimize parallel execution\\nIt‡s time to distribute computations across multiple servers!\\nMultiple Devices Across Multiple ServersTo run a graph across multiple servers, you first need to define a \\ncluster\\n. A cluster iscomposed of one or more TensorFlow servers, called \\ntasks\\n, typically spread acrossseveral machines (see Figure 12-6). Each task belongs to a job\\n. A job is just a namedgroup of tasks that typically have a common role, such as keeping track of the model\\nMultiple Devices Across Multiple Servers | 323\\nparameters (such a job is usually named \"ps\" for parameter server\\n), or performingcomputations (such a job is usually named \\n\"worker\").Figure 12-6. TensorFlow cluster\\nThe following cluster \\nspeci†cation defines two jobs, \"ps\" and \"worker\", containing\\none task and two tasks, respectively. In this example, machine A hosts two Tensor…\\nFlow servers (i.e., tasks), listening on different ports: one is part of the \\n\"ps\" job, and\\nthe other is part of the \"worker\" job. Machine B just hosts one TensorFlow server,\\npart of the \"worker\" job.\\ncluster_spec = tf.train.ClusterSpec({    \"ps\": [        \"machine-a.example.com:2221\",  # /job:ps/task:0    ],    \"worker\": [        \"machine-a.example.com:2222\",  # /job:worker/task:0        \"machine-b.example.com:2222\",  # /job:worker/task:1    ]})To start a TensorFlow server, you must create a \\nServer object, passing it the clusterspecification (so it can communicate with other servers) and its own job name and\\ntask number. For example, to start the first worker task, you would run the following\\ncode on machine A:server = tf.train.Server(cluster_spec, job_name=\"worker\", task_index=0)It is usually simpler to just run one task per machine, but the previous example dem…\\nonstrates that TensorFlow allows you to run multiple tasks on the same machine if\\n324 | Chapter 12: Distributing TensorFlow Across Devices and Servers\\n2You can even start multiple tasks in the same process. It may be useful for tests, but it is not recommended in\\nproduction.3It is the next version of Google‡s internal \\nStubby\\n service, which Google has used successfully for over a decade.\\nSee http://grpc.io/\\n for more details.you want.\\n2 If you have several servers on one machine, you will need to ensure that\\nthey don‡t all try to grab all the RAM of every GPU, as explained earlier. For example,\\nin Figure 12-6 the \"ps\" task does not see the GPU devices, since presumably its pro…cess was launched with \\nCUDA_VISIBLE_DEVICES=\"\". Note that the CPU is shared by\\nall tasks located on the same machine.\\nIf you want the process to do nothing other than run the TensorFlow server, you can\\nblock the main thread by telling it to wait for the server to finish using the \\njoin() method (otherwise the server will be killed as soon as your main thread exits). Since\\nthere is currently no way to stop the server, this will actually block forever:\\nserver.join()  # blocks until the server stops (i.e., never)Opening a SessionOnce all the tasks are up and running (doing nothing yet), you can open a session onany of the servers, from a client located in any process on any machine (even from a\\nprocess running one of the tasks), and use that session like a regular local session. \\nFor\\nexample:\\na = tf.constant(1.0)b = a + 2c = a * 3with tf.Session(\"grpc://machine-b.example.com:2222\") as sess:    print(c.eval())  # 9.0This client code first creates a simple graph, then opens a session on the TensorFlow\\nserver located on machine B (which we will call the \\nmaster\\n), and instructs it to evalu…ate \\nc. The master starts by placing the operations on the appropriate devices. In this\\nexample, since we did not pin any operation on any device, the master simply places\\nthem all on its own default device›in this case, machine B‡s GPU device. Then it just\\nevaluates \\nc as instructed by the client, and it returns the result.\\nThe Master and Worker ServicesThe client uses the \\ngRPC protocol (Google Remote Procedure Call\\n) to communicate\\nwith the server. This is an efficient open source framework to call remote functions\\nand get their outputs across a variety of platforms and languages.\\n3 It is based on\\nHTTP2, which opens a connection and leaves it open during the whole session,\\nallowing efficient bidirectional communication once the connection is established.\\nMultiple Devices Across Multiple Servers | 325\\nData is transmitted in the form of \\nprotocol \\nbu›ers, another open source Google tech…nology. This is a lightweight binary data interchange format.\\nAll servers in a TensorFlow cluster may communicate with any\\nother server in the cluster, so make sure to open the appropriate\\nports on your firewall.Every TensorFlow server provides two services: the \\nmaster service\\n and the worker ser…\\nvice\\n. The master service allows clients to open sessions and use them to run graphs. It\\ncoordinates the computations across tasks, relying on the worker service to actually\\nexecute computations on other tasks and get their results.\\nThis architecture gives you a lot of flexibility. One client can connect to multiple\\nservers by opening multiple sessions in different threads. One server can handle mul…\\ntiple sessions simultaneously from one or more clients. You can run one client per\\ntask (typically within the same process), or just one client to control all tasks. All\\noptions are open.Pinning Operations Across TasksYou can use device blocks to pin operations on any device managed by any task, by\\nspecifying the job name, task index, device type, and device index. For example, the\\nfollowing code pins a to the CPU of the first task in the \"ps\" job (that‡s the CPU on\\nmachine A), and it pins b to the second GPU managed by the first task of the\"worker\" job (that‡s GPU #1 on machine A). Finally, \\nc is not pinned to any device, so\\nthe master places it on its own default device (machine B‡s GPU #0 device).\\nwith tf.device(\"/job:ps/task:0/cpu:0\")    a = tf.constant(1.0)with tf.device(\"/job:worker/task:0/gpu:1\")    b = a + 2c = a + bAs earlier, if you omit the device type and index, TensorFlow will default to the task‡s\\ndefault device; for example, pinning an operation to \\n\"/job:ps/task:0\" will place iton the default device of the first task of the \\n\"ps\" job (machine A‡s CPU). If you also\\nomit the task index (e.g., \"/job:ps\"), TensorFlow defaults to \\n\"/task:0\". If you omitthe job name and the task index, TensorFlow defaults to the session‡s master task.\\n326 | Chapter 12: Distributing TensorFlow Across Devices and Servers\\nSharding Variables Across Multiple Parameter ServersAs we will see shortly, a common pattern when training a neural network on a dis…\\ntributed setup is to store the model parameters on a set of parameter servers (i.e., the\\ntasks in the \"ps\" job) while other tasks focus on computations (i.e., the tasks in the\\n\"worker\" job). For large models with millions of parameters, it is useful to shard\\nthese parameters across multiple parameter servers, to reduce the risk of saturating a\\nsingle parameter server‡s network card. If you were to manually pin every variable to\\na different parameter server, it would be quite tedious. Fortunately, TensorFlow pro…\\nvides the replica_device_setter() function, which distributes variables across allthe \"ps\" tasks in a round-robin fashion. For example, the following code pins five\\nvariables to two parameter servers:\\nwith tf.device(tf.train.replica_device_setter(ps_tasks=2):    v1 = tf.Variable(1.0)  # pinned to /job:ps/task:0    v2 = tf.Variable(2.0)  # pinned to /job:ps/task:1    v3 = tf.Variable(3.0)  # pinned to /job:ps/task:0    v4 = tf.Variable(4.0)  # pinned to /job:ps/task:1    v5 = tf.Variable(5.0)  # pinned to /job:ps/task:0Instead of passing the number of \\nps_tasks, you can pass the cluster spec cluster=cluster_spec and TensorFlow will simply count the number of tasks in the \\n\"ps\"job.\\nIf you create other operations in the block, beyond just variables, TensorFlow auto…\\nmatically pins them to \\n\"/job:worker\", which will default to the first device managed\\nby the first task in the \"worker\" job. You can pin them to another device by setting\\nthe worker_device parameter, but a better approach is to use embedded device\\nblocks. An inner device block can override the job, task, or device defined in an outer\\nblock. For example:\\nwith tf.device(tf.train.replica_device_setter(ps_tasks=2)):    v1 = tf.Variable(1.0)  # pinned to /job:ps/task:0 (+ defaults to /cpu:0)    v2 = tf.Variable(2.0)  # pinned to /job:ps/task:1 (+ defaults to /cpu:0)    v3 = tf.Variable(3.0)  # pinned to /job:ps/task:0 (+ defaults to /cpu:0)    [...]    s = v1 + v2            # pinned to /job:worker (+ defaults to task:0/gpu:0)    with tf.device(\"/gpu:1\"):        p1 = 2 * s         # pinned to /job:worker/gpu:1 (+ defaults to /task:0)        with tf.device(\"/task:1\"):            p2 = 3 * s     # pinned to /job:worker/task:1/gpu:1This example assumes that the parameter servers are CPU-only,\\nwhich is typically the case since they only need to store and com…municate parameters, not perform intensive computations.\\nMultiple Devices Across Multiple Servers | 327\\nSharing State Across Sessions Using Resource ContainersWhen you are using a plain local session\\n (not the distributed kind), each variable‡s\\nstate is managed by the session itself; as soon as it ends, all variable values are lost.\\nMoreover, multiple local sessions cannot share any state, even if they both run the\\nsame graph; each session has its own copy of every variable (as we discussed in \\nChap…\\nter 9). In contrast, when you are using \\ndistributed sessions\\n, variable state is managed\\nby resource containers\\n located on the cluster itself, not by the sessions. So if you create\\na variable named x using one client session, it will automatically be available to any\\nother session on the same cluster (even if both sessions are connected to a different\\nserver). For example, consider the following client code:\\n# simple_client.pyimport tensorflow as tfimport sysx = tf.Variable(0.0, name=\"x\")increment_x = tf.assign(x, x + 1)with tf.Session(sys.argv[1]) as sess:    if sys.argv[2:]==[\"init\"]:        sess.run(x.initializer)    sess.run(increment_x)    print(x.eval())Let‡s suppose you have a TensorFlow cluster up and running on machines A and B,\\nport 2222. You could launch the client, have it open a session with the server on\\nmachine A, and tell it to initialize the variable, increment it, and print its value by\\nlaunching the following command:\\n$ python3 simple_client.py grpc://machine-a.example.com:2222 init1.0Now if you launch the client with the following command, it will connect to the\\nserver on machine B and magically reuse the same variable \\nx (this time we don‡t ask\\nthe server to initialize the variable):\\n$ python3 simple_client.py grpc://machine-b.example.com:22222.0This feature cuts both ways: it‡s great if you want to share variables across multiple\\nsessions, but if you want to run completely independent computations on the same\\ncluster you will have to be careful not to use the same variable names by accident.\\nOne way to ensure that you won‡t have name clashes is to wrap all of your construc…\\ntion phase inside a variable scope with a unique name for each computation, for\\nexample:\\nwith tf.variable_scope(\"my_problem_1\"):    [...] # Construction phase of problem 1328 | Chapter 12: Distributing TensorFlow Across Devices and Servers\\nA better option is to use a container block:\\nwith tf.container(\"my_problem_1\"):    [...] # Construction phase of problem 1This will use a container dedicated to problem #1, instead of the default one (whose\\nname is an empty string \\n\"\"). One advantage is that variable names remain nice and\\nshort. Another advantage is that you can easily reset a named container. For example,\\nthe following command will connect to the server on machine A and ask it to reset\\nthe container named \\n\"my_problem_1\", which will free all the resources this container\\nused (and also close all sessions open on the server). Any variable managed by this\\ncontainer must be initialized before you can use it again:\\ntf.Session.reset(\"grpc://machine-a.example.com:2222\", [\"my_problem_1\"])Resource containers make it easy to share variables across sessions in flexible ways.\\nFor example, \\nFigure 12-7 shows four clients running different graphs on the same\\ncluster, but sharing some variables. Clients A and B share the same variable \\nx man…aged by the default container, while clients C and D share another variable named \\nxmanaged by the container named \\n\"my_problem_1\". Note that client C even uses vari…\\nables from both containers.\\nFigure 12-7. Resource containers\\nResource containers also take care of preserving the state of other stateful operations,\\nnamely queues and readers. Let‡s take a look at queues first.\\nAsynchronous Communication Using TensorFlow QueuesQueues are another great way to exchange data between multiple sessions; for exam…\\nple, one common use case is to have a client create a graph that loads the training data\\nand pushes it into a queue, while another client creates a graph that pulls the data\\nfrom the queue and trains a model (see Figure 12-8). This can speed up training con…Multiple Devices Across Multiple Servers | 329\\nsiderably because the training operations don‡t have to wait for the next mini-batch at\\nevery step.\\nFigure 12-8. Using queues to load the training data asynchronously\\nTensorFlow provides various kinds of queues. The simplest kind is the \\n†rst-in †rst-out (FIFO) queue. For example, the following code creates a FIFO queue that can\\nstore up to 10 tensors containing two float values each:\\nq = tf.FIFOQueue(capacity=10, dtypes=[tf.float32], shapes=[[2]],                 name=\"q\", shared_name=\"shared_q\")To share variables across sessions, all you had to do was to specify\\nthe same name and container on both ends. With queues Tensor…\\nFlow does not use the name attribute but instead uses \\nshared_name,so it is important to specify it (even if it is the same as the \\nname).And, of course, use the same container.\\nEnqueuing dataTo \\npush data to a queue, you must create an \\nenqueue operation. For example, the fol…\\nlowing code pushes three training instances to the queue:# training_data_loader.pyimport tensorflow as tfq = [...]training_instance = tf.placeholder(tf.float32, shape=(2))enqueue = q.enqueue([training_instance])with tf.Session(\"grpc://machine-a.example.com:2222\") as sess:   sess.run(enqueue, feed_dict={training_instance: [1., 2.]})   sess.run(enqueue, feed_dict={training_instance: [3., 4.]})   sess.run(enqueue, feed_dict={training_instance: [5., 6.]})330 | Chapter 12: Distributing TensorFlow Across Devices and Servers\\nInstead of enqueuing instances one by one, you can enqueue several at a time using\\nan enqueue_many operation:\\n[...]training_instances = tf.placeholder(tf.float32, shape=(None, 2))enqueue_many = q.enqueue([training_instances])with tf.Session(\"grpc://machine-a.example.com:2222\") as sess:   sess.run(enqueue_many,            feed_dict={training_instances: [[1., 2.], [3., 4.], [5., 6.]]})Both examples enqueue the same three tensors to the queue.\\nDequeuing dataTo pull the instances out of the queue, on the other end, you need to use a \\ndequeueoperation:\\n# trainer.pyimport tensorflow as tfq = [...]dequeue = q.dequeue()with tf.Session(\"grpc://machine-a.example.com:2222\") as sess:   print(sess.run(dequeue))  # [1., 2.]   print(sess.run(dequeue))  # [3., 4.]   print(sess.run(dequeue))  # [5., 6.]In general you will want to pull a whole mini-batch at once, instead of pulling just\\none instance at a time. To do so, you must use a \\ndequeue_many operation, specifying\\nthe mini-batch size:\\n[...]batch_size = 2dequeue_mini_batch= q.dequeue_many(batch_size)with tf.Session(\"grpc://machine-a.example.com:2222\") as sess:   print(sess.run(dequeue_mini_batch))  # [[1., 2.], [4., 5.]]   print(sess.run(dequeue_mini_batch))  # blocked waiting for another instanceWhen a queue is full, the enqueue operation will block until items are pulled out by a\\ndequeue operation. Similarly, when a queue is empty (or you are using\\ndequeue_many() and there are fewer items than the mini-batch size), the dequeue\\noperation will block until enough items are pushed into the queue using an enqueue\\noperation.\\nMultiple Devices Across Multiple Servers | 331\\nQueues of tuplesEach item in a queue can be a tuple of tensors (of various types and shapes) instead of\\njust a single tensor. For example, the following queue stores pairs of tensors, one of\\ntype int32 and shape \\n(), and the other of type float32 and shape \\n[3,2]:q = tf.FIFOQueue(capacity=10, dtypes=[tf.int32, tf.float32], shapes=[[],[3,2]],                 name=\"q\", shared_name=\"shared_q\")The enqueue operation must be given pairs of tensors (note that each pair represents\\nonly one item in the queue):a = tf.placeholder(tf.int32, shape=())b = tf.placeholder(tf.float32, shape=(3, 2))enqueue = q.enqueue((a, b))with tf.Session([...]) as sess:    sess.run(enqueue, feed_dict={a: 10, b:[[1., 2.], [3., 4.], [5., 6.]]})    sess.run(enqueue, feed_dict={a: 11, b:[[2., 4.], [6., 8.], [0., 2.]]})    sess.run(enqueue, feed_dict={a: 12, b:[[3., 6.], [9., 2.], [5., 8.]]})On the other end, the dequeue() function now creates a pair of dequeue operations:\\ndequeue_a, dequeue_b = q.dequeue()In general, you should run these operations together:\\nwith tf.Session([...]) as sess:    a_val, b_val = sess.run([dequeue_a, dequeue_b])    print(a_val) # 10    print(b_val) # [[1., 2.], [3., 4.], [5., 6.]]If you run dequeue_a on its own, it will dequeue a pair and returnonly the first element; the second element will be lost (and simi…\\nlarly, if you run \\ndequeue_b on its own, the first element will be\\nlost).The dequeue_many() function also returns a pair of operations:\\nbatch_size = 2dequeue_as, dequeue_bs = q.dequeue_many(batch_size)You can use it as you would expect:\\nwith tf.Session([...]) as sess:    a, b = sess.run([dequeue_a, dequeue_b])    print(a) # [10, 11]    print(b) # [[[1., 2.], [3., 4.], [5., 6.]], [[2., 4.], [6., 8.], [0., 2.]]]    a, b = sess.run([dequeue_a, dequeue_b])  # blocked waiting for another pair332 | Chapter 12: Distributing TensorFlow Across Devices and Servers\\nClosing a queueIt is possible to close a queue to signal to the other sessions that no more data will be\\nenqueued:close_q = q.close()with tf.Session([...]) as sess:    [...]    sess.run(close_q)Subsequent executions of \\nenqueue or enqueue_many operations will raise an excep…\\ntion. By default, any pending enqueue request will be honored, unless you call\\nq.close(cancel_pending_enqueues=True).Subsequent executions of \\ndequeue or dequeue_many operations will continue to suc…\\nceed as long as there are items in the queue, but they will fail when there are notenough items left in the queue. If you are using a dequeue_many operation and there\\nare a few instances left in the queue, but fewer than the mini-batch size, they will be\\nlost. You may prefer to use a \\ndequeue_up_to operation instead; it behaves exactly like\\ndequeue_many except when a queue is closed and there are fewer than batch_sizeinstances left in the queue, in which case it just returns them.RandomShuƒeQueueTensorFlow also supports a couple more types of queues, including \\nRandomShuffleQueue, which can be used just like a FIFOQueue except that items are dequeued in a\\nrandom order. This can be useful to shuffle training instances at each epoch during\\ntraining. First, let‡s create the queue:\\nq = tf.RandomShuffleQueue(capacity=50, min_after_dequeue=10,                          dtypes=[tf.float32], shapes=[()],                          name=\"q\", shared_name=\"shared_q\")The min_after_dequeue specifies the minimum number of items that must remain in\\nthe queue after a dequeue operation. This ensures that there will be enough instances\\nin the queue to have enough randomness (once the queue is closed, the\\nmin_after_dequeue limit is ignored). Now suppose that you enqueued 22 items in\\nthis queue (floats \\n1. to 22.). Here is how you could dequeue them:\\ndequeue = q.dequeue_many(5)with tf.Session([...]) as sess:   print(sess.run(dequeue)) # [ 20.  15.  11.  12.   4.]   (17 items left)   print(sess.run(dequeue)) # [  5.  13.   6.   0.  17.]   (12 items left)   print(sess.run(dequeue)) # 12 - 5 < 10: blocked waiting for 3 more instancesMultiple Devices Across Multiple Servers | 333\\nPaddingFifoQueueA PaddingFIFOQueue can also be used just like a FIFOQueue except that it accepts ten…\\nsors of variable sizes along any dimension (but with a fixed rank). When you are\\ndequeuing them with a dequeue_many or dequeue_up_to operation, each tensor is\\npadded with zeros along every variable dimension to make it the same size as the\\nlargest tensor in the mini-batch. For example, you could enqueue 2D tensors (matri…\\nces) of arbitrary sizes:\\nq = tf.PaddingFIFOQueue(capacity=50, dtypes=[tf.float32], shapes=[(None, None)]                        name=\"q\", shared_name=\"shared_q\")v = tf.placeholder(tf.float32, shape=(None, None))enqueue = q.enqueue([v])with tf.Session([...]) as sess:   sess.run(enqueue, feed_dict={v: [[1., 2.], [3., 4.], [5., 6.]]})       # 3x2   sess.run(enqueue, feed_dict={v: [[1.]]})                               # 1x1   sess.run(enqueue, feed_dict={v: [[7., 8., 9., 5.], [6., 7., 8., 9.]]}) # 2x4If we just dequeue one item at a time, we get the exact same tensors that were\\nenqueued. But if we dequeue several items at a time (using \\ndequeue_many() ordequeue_up_to()), the queue automatically pads the tensors appropriately. For exam…\\nple, if we dequeue all three items at once, all tensors will be padded with zeros to\\nbecome 3 ‰ 4 tensors, since the maximum size for the first dimension is 3 (first item)\\nand the maximum size for the second dimension is 4 (third item):\\n>>> q = [...]>>> dequeue = q.dequeue_many(3)>>> with tf.Session([...]) as sess:...     print(sess.run(dequeue))[[[ 1.  2.  0.  0.]  [ 3.  4.  0.  0.]  [ 5.  6.  0.  0.]] [[ 1.  0.  0.  0.]  [ 0.  0.  0.  0.]  [ 0.  0.  0.  0.]] [[ 7.  8.  9.  5.]  [ 6.  7.  8.  9.]  [ 0.  0.  0.  0.]]]This type of queue can be useful when you are dealing with variable length inputs,\\nsuch as sequences of words (see Chapter 14\\n).Okay, now let‡s pause for a second: so far you have learned to distribute computations\\nacross multiple devices and servers, share variables across sessions, and communicate\\nasynchronously using queues. Before you start training neural networks, though,\\nthere‡s one last topic we need to discuss: how to efficiently load training data.\\n334 | Chapter 12: Distributing TensorFlow Across Devices and Servers\\nLoading Data Directly from the GraphSo far we have assumed that the clients would load the training data and feed it to the\\ncluster using placeholders. This is simple and works quite well for simple setups, but\\nit is rather inefficient since it transfers the training data several times:\\n1.From the filesystem to the client\\n2.From the client to the master task\\n3.Possibly from the master task to other tasks where the data is needed\\nIt gets worse if you have several clients training various neural networks using the\\nsame training data (for example, for hyperparameter tuning): if every client loads the\\ndata simultaneously, you may end up even saturating your file server or the network‡s\\nbandwidth.Preload the data into a variableFor datasets that can fit in memory, a better option is to load the training data once\\nand assign it to a variable, then just use that variable in your graph. This is \\ncalledpreloading\\n the training set. This way the data will be transferred only once from the\\nclient to the cluster (but it may still need to be moved around from task to task\\ndepending on which operations need it). The following code shows how to load the\\nfull training set into a variable:\\ntraining_set_init = tf.placeholder(tf.float32, shape=(None, n_features))training_set = tf.Variable(training_set_init, trainable=False, collections=[],                           name=\"training_set\")with tf.Session([...]) as sess:    data = [...]  # load the training data from the datastore    sess.run(training_set.initializer, feed_dict={training_set_init: data})You must set \\ntrainable=False so the optimizers don‡t try to tweak this variable. You\\nshould also set collections=[] to ensure that this variable won‡t get added to the\\nGraphKeys.GLOBAL_VARIABLES collection, which is used for saving and restoring\\ncheckpoints.\\nThis example assumes that all of your training set (including the\\nlabels) consists only of float32 values. If that‡s not the case, you\\nwill need one variable per type.Reading the training data directly from the graphIf the training set does not fit in memory, a good solution is to use \\nreader operations\\n: these are operations capable of reading data directly from the filesystem. This way the\\nMultiple Devices Across Multiple Servers | 335\\ntraining data never needs to flow through the clients at all. TensorFlow provides read…\\ners for various file formats:\\n‹CSV‹Fixed-length binary records\\n‹TensorFlow‡s own \\nTFRecords format, based on protocol buffers\\nLet‡s look at a simple example reading from a CSV file (for other formats, please\\ncheck out the API documentation). Suppose you have file named \\nmy_test.csv\\n that\\ncontains training instances, and you want to create operations to read it. Suppose it\\nhas the following content, with two float features \\nx1 and x2 and one integer \\ntargetrepresenting a binary class:\\nx1,  x2,  target1. , 2. , 04. , 5  , 17. ,    , 0First, let‡s create a \\nTextLineReader to read this file. A TextLineReader opens a file(once we tell it which one to open) and reads lines one by one. It is a stateful opera…\\ntion, like variables and queues: it preserves its state across multiple runs of the graph,\\nkeeping track of which file it is currently reading and what its current position is in\\nthis file.reader = tf.TextLineReader(skip_header_lines=1)Next, we create a queue that the reader will pull from to know which file to read next.\\nWe also create an enqueue operation and a placeholder to push any filename we want\\nto the queue, and we create an operation to close the queue once we have no more \\nfiles to read:filename_queue = tf.FIFOQueue(capacity=10, dtypes=[tf.string], shapes=[()])filename = tf.placeholder(tf.string)enqueue_filename = filename_queue.enqueue([filename])close_filename_queue = filename_queue.close()Now we are ready to create a \\nread operation that will read one record (i.e., a line) at a\\ntime and return a key/value pair. The key is the record‡s unique identifier›a string\\ncomposed of the filename, a colon (\\n:), and the line number›and the value is simply\\na string containing the content of the line:\\nkey, value = reader.read(filename_queue)We have all we need to read the file line by line! But we are not quite done yet›we\\nneed to parse this string to get the features and target:\\nx1, x2, target = tf.decode_csv(value, record_defaults=[[-1.], [-1.], [-1]])features = tf.stack([x1, x2])336 | Chapter 12: Distributing TensorFlow Across Devices and Servers\\nThe first line uses TensorFlow‡s CSV parser to extract the values from the current\\nline. The default values are used when a field is missing (in this example the third\\ntraining instance‡s \\nx2 feature), and they are also used to determine the type of each\\nfield (in this case two floats and one integer).\\nFinally, we can push this training instance and its target to a \\nRandomShuffleQueuethat we will share with the training graph (so it can pull mini-batches from it), and we\\ncreate an operation to close that queue when we are done pushing instances to it:\\ninstance_queue = tf.RandomShuffleQueue(    capacity=10, min_after_dequeue=2,    dtypes=[tf.float32, tf.int32], shapes=[[2],[]],    name=\"instance_q\", shared_name=\"shared_instance_q\")enqueue_instance = instance_queue.enqueue([features, target])close_instance_queue = instance_queue.close()Wow! That was a lot of work just to read a file. Plus we only created the graph, so now\\nwe need to run it:with tf.Session([...]) as sess:    sess.run(enqueue_filename, feed_dict={filename: \"my_test.csv\"})    sess.run(close_filename_queue)    try:        while True:            sess.run(enqueue_instance)    except tf.errors.OutOfRangeError as ex:        pass # no more records in the current file and no more files to read    sess.run(close_instance_queue)First we open the session, and then we enqueue the filename \"my_test.csv\" andimmediately close that queue since we will not enqueue any more filenames. Then we\\nrun an infinite loop to enqueue instances one by one. The enqueue_instancedepends on the reader reading the next line, so at every iteration a new record is read\\nuntil it reaches the end of the file. At that point it tries to read the filename queue to\\nknow which file to read next, and since the queue is closed it throws an OutOfRangeError exception (if we did not close the queue, it would just remain blocked until\\nwe pushed another filename or closed the queue). Lastly, we close the instance queue\\nso that the training operations pulling from it won‡t get blocked forever. \\nFigure 12-9summarizes what we have learned; it represents a typical graph for reading training\\ninstances from a set of CSV files.Multiple Devices Across Multiple Servers | 337\\nFigure 12-9. A graph dedicated to reading training instances from CSV \\n†lesIn the training graph, you need to create the shared instance queue and simply\\ndequeue mini-batches from it:\\ninstance_queue = tf.RandomShuffleQueue([...], shared_name=\"shared_instance_q\")mini_batch_instances, mini_batch_targets = instance_queue.dequeue_up_to(2)[...] # use the mini_batch instances and targets to build the training graphtraining_op = [...]with tf.Session([...]) as sess:    try:        for step in range(max_steps):            sess.run(training_op)    except tf.errors.OutOfRangeError as ex:        pass # no more training instancesIn this example, the first mini-batch will contain the first two instances of the CSV\\nfile, and the second mini-batch will contain the last instance.\\nTensorFlow queues don‡t handle sparse tensors well, so if your\\ntraining instances are sparse you should parse the records after theinstance queue.This architecture will only use one thread to read records and push them to theinstance queue. You can get a much higher throughput by having multiple threads\\nread simultaneously from multiple files using multiple readers. Let‡s see how.\\nMultithreaded readers using a Coordinator and a QueueRunnerTo have multiple threads read instances simultaneously, you could create \\nPythonthreads (using the threading module) and manage them yourself. However, Tensor…\\nFlow provides some tools to make this simpler: the \\nCoordinator class and the \\nQueueRunner class.338 | Chapter 12: Distributing TensorFlow Across Devices and Servers\\nA Coordinator is a very simple object whose sole purpose is to coordinate stopping\\nmultiple threads. First you create a \\nCoordinator:coord = tf.train.Coordinator()Then you give it to all threads that need to stop jointly, and their main loop looks like\\nthis:while not coord.should_stop():    [...] # do somethingAny thread can request that every thread stop by calling the \\nCoordinator‡s\\nrequest_stop() method:coord.request_stop()Every thread will stop as soon as it finishes its current iteration. You can wait for all of\\nthe threads to finish by calling the Coordinator‡s \\njoin() method, \\npassing it the list ofthreads:coord.join(list_of_threads)A QueueRunner runs multiple threads that each run an enqueue operation repeatedly,\\nfilling up a queue as fast as possible. As soon as the queue is closed, the next threadthat tries to push an item to the queue will get an \\nOutOfRangeError; this threadcatches the error and immediately tells other threads to stop using a \\nCoordinator.The following code shows how you can use a QueueRunner to have five threads read…\\ning instances simultaneously and pushing them to an instance queue:\\n[...] # same construction phase as earlierqueue_runner = tf.train.QueueRunner(instance_queue, [enqueue_instance] * 5)with tf.Session() as sess:    sess.run(enqueue_filename, feed_dict={filename: \"my_test.csv\"})    sess.run(close_filename_queue)    coord = tf.train.Coordinator()    enqueue_threads = queue_runner.create_threads(sess, coord=coord, start=True)The first line creates the \\nQueueRunner and tells it to run five threads, all running thesame enqueue_instance operation repeatedly. Then we start a session and we\\nenqueue the name of the files to read (in this case just \"my_test.csv\"). Next we cre…\\nate a \\nCoordinator that the \\nQueueRunner will use to stop gracefully, as just explained.\\nFinally, we tell the \\nQueueRunner to create the threads and start them. The threads will\\nread all training instances and push them to the instance queue, and then they will allstop gracefully.\\nThis will be a bit more efficient than earlier, but we can do better. Currently all\\nthreads are reading from the same file. We can make them read simultaneously from\\nseparate files instead (assuming the training data is sharded across multiple CSV files)\\nby creating multiple readers (see \\nFigure 12-10).Multiple Devices Across Multiple Servers | 339\\nFigure 12-10. Reading simultaneously from multiple \\n†lesFor this we need to write a small function to create a reader and the nodes that will\\nread and push one instance to the instance queue:def read_and_push_instance(filename_queue, instance_queue):    reader = tf.TextLineReader(skip_header_lines=1)    key, value = reader.read(filename_queue)    x1, x2, target = tf.decode_csv(value, record_defaults=[[-1.], [-1.], [-1]])    features = tf.stack([x1, x2])    enqueue_instance = instance_queue.enqueue([features, target])    return enqueue_instanceNext we define the queues:\\nfilename_queue = tf.FIFOQueue(capacity=10, dtypes=[tf.string], shapes=[()])filename = tf.placeholder(tf.string)enqueue_filename = filename_queue.enqueue([filename])close_filename_queue = filename_queue.close()instance_queue = tf.RandomShuffleQueue([...])And finally we create the \\nQueueRunner, but this time we give it a list of different\\nenqueue operations. Each operation will use a different reader, so the threads will\\nsimultaneously read from different files:\\nread_and_enqueue_ops = [    read_and_push_instance(filename_queue, instance_queue)    for i in range(5)]queue_runner = tf.train.QueueRunner(instance_queue, read_and_enqueue_ops)The execution phase is then the same as before: first push the names of the files toread, then create a \\nCoordinator and create and start the \\nQueueRunner threads. Thistime all threads will read from different files simultaneously until all files are read\\nentirely, and then the \\nQueueRunner will close the instance queue so that other ops\\npulling from it don‡t get blocked.\\n340 | Chapter 12: Distributing TensorFlow Across Devices and Servers\\nOther convenience functionsTensorFlow also offers a few convenience functions to simplify some common tasks\\nwhen reading training instances. We will go over just a few (see the API documenta…\\ntion for the full list).The string_input_producer() takes a 1D tensor containing a list of filenames, cre…\\nates a thread that pushes one filename at a time to the filename queue, and then\\ncloses the queue. If you specify a number of epochs, it will cycle through the file…\\nnames once per epoch before closing the queue. By default, it shuffles the filenames at\\neach epoch. It creates a \\nQueueRunner to manage its thread, and adds it to the GraphKeys.QUEUE_RUNNERS collection. To start every \\nQueueRunner in that collection, you\\ncan call the tf.train.start_queue_runners() function. Note that if you forget to\\nstart the QueueRunner, the filename queue will be open and empty, and your readers\\nwill be blocked forever.\\nThere are a few other producer\\n functions that similarly create a queue and a corre…\\nsponding QueueRunner for running an enqueue operation (e.g., \\ninput_producer(),range_input_producer(), and slice_input_producer()).The shuffle_batch() function \\ntakes a list of tensors (e.g., [features, target]) andcreates:\\n‹A RandomShuffleQueue‹A QueueRunner to enqueue the tensors to the queue (added to the GraphKeys.QUEUE_RUNNERS collection)‹A dequeue_many operation to extract a mini-batch from the queue\\nThis makes it easy to manage in a single process a multithreaded input pipeline feed…\\ning a queue and a training pipeline reading mini-batches from that queue. Also check\\nout the batch(), batch_join(), and shuffle_batch_join() functions that \\nprovidesimilar functionality.\\nOkay! You now have all the tools you need to start training and running neural net…\\nworks efficiently across multiple devices and servers on a TensorFlow cluster. Let‡s\\nreview what you have learned:\\n‹Using multiple GPU devices\\n‹Setting up and starting a TensorFlow cluster\\n‹Distributing computations across multiple devices and servers\\n‹Sharing variables (and other stateful ops such as queues and readers) across ses…\\nsions using containers\\n‹Coordinating multiple graphs working asynchronously using queues\\nMultiple Devices Across Multiple Servers | 341\\n4Not 100% linear if you wait for all devices to finish, since the total time will be the time taken by the slowest\\ndevice.‹Reading inputs efficiently using readers, queue runners, and coordinators\\nNow let‡s use all of this to parallelize neural networks!\\nParallelizing Neural Networks on a TensorFlow ClusterIn this section, first we will look at how to parallelize several neural networks by sim…\\nply placing each one on a different device. Then we will look at the much trickier\\nproblem of training a single neural network across multiple devices and servers.\\nOne Neural Network per DeviceThe most trivial way to train and run neural networks on a TensorFlow cluster is to\\ntake the exact same code you would use for a single device on a single machine, andspecify the master server‡s address when creating the session. That‡s it›you‡re done!\\nYour code will be running on the server‡s default device. You can change the device\\nthat will run your graph simply by putting your code‡s construction phase within a\\ndevice block.By running several client sessions in parallel (in different threads or different pro…\\ncesses), connecting them to different servers, and configuring them to use different\\ndevices, you can quite easily train or run many neural networks in parallel, across all\\ndevices and all machines in your cluster (see Figure 12-11). The speedup is almostlinear.\\n4 Training 100 neural networks across 50 servers with 2 GPUs each will not take\\nmuch longer than training just 1 neural network on 1 GPU.\\nFigure 12-11. Training one neural network per device\\n342 | Chapter 12: Distributing TensorFlow Across Devices and Servers\\nThis solution is perfect for hyperparameter tuning: each device in the cluster will\\ntrain a different model with its own set of hyperparameters. The more computing\\npower you have, the larger the hyperparameter space you can explore.\\nIt also works perfectly if you host a web service that receives a large number \\nof queries\\nper second\\n (QPS) and you need your neural network to make a prediction for eachquery. Simply replicate the neural network across all devices on the cluster and dis…\\npatch queries across all devices. By adding more servers you can handle an unlimited\\nnumber of QPS (however, this will not reduce the time it takes to process a single\\nrequest since it will still have to wait for a neural network to make a prediction).\\nAnother option is to serve your neural networks using \\nTensorFlow\\nServing\\n. It is an open source system, released by Google in Febru…\\nary 2016, designed to serve a high volume of queries to Machine\\nLearning models (typically built with TensorFlow). It handles\\nmodel versioning, so you can easily deploy a new version of yournetwork to production, or experiment with various algorithms\\nwithout interrupting your service, and it can sustain a heavy load\\nby adding more servers. For more details, check out \\nhttps://tensor\\n‡ow.github.io/serving/.In-Graph Versus Between-Graph ReplicationYou \\ncan also parallelize the training of a large ensemble of neural networks by simply\\nplacing every neural network on a different device (ensembles were introduced in\\nChapter 7\\n). However, once you want to \\nrun\\n the ensemble, you will need to aggregate\\nthe individual predictions made by each neural network to produce the ensemble‡s\\nprediction, and this requires a bit of coordination.\\nThere are two major approaches to handling a neural network ensemble (or any other\\ngraph that contains large chunks of independent computations):\\n‹You can create one big graph, containing every neural network, each pinned to a\\ndifferent device, plus the computations needed to aggregate the individual pre…\\ndictions from all the neural networks (see Figure 12-12). Then you just create\\none session to any server in the cluster and let it take care of everything (includ…\\ning waiting for all individual predictions to be available before aggregating them).\\nThis approach is called \\nin-graph replication\\n.Parallelizing Neural Networks on a TensorFlow Cluster | 343\\nFigure 12-12. In-graph replication\\n‹Alternatively, you can create one separate graph for each neural network and\\nhandle synchronization between these graphs yourself. This approach is \\ncalledbetween-graph replication\\n. One typical implementation is to coordinate the exe…\\ncution of these graphs using queues (see \\nFigure 12-13). A set of clients handles\\none neural network each, reading from its dedicated input queue, and writing to\\nits dedicated prediction queue. Another client is in charge of reading the inputs\\nand pushing them to all the input queues (copying all inputs to every queue).\\nFinally, one last client is in charge of reading one prediction from each prediction\\nqueue and aggregating them to produce the ensemble‡s prediction.\\nFigure 12-13. Between-graph replication\\n344 | Chapter 12: Distributing TensorFlow Across Devices and Servers\\nThese solutions have their pros and cons. In-graph replication is somewhat simpler to\\nimplement since you don‡t have to manage multiple clients and multiple queues.\\nHowever, between-graph replication is a bit easier to organize into well-bounded and\\neasy-to-test modules. Moreover, it gives you more flexibility. For example, you could\\nadd a dequeue timeout in the aggregator client so that the ensemble would not fail\\neven if one of the neural network clients crashes or if one neural network takes too\\nlong to produce its prediction. TensorFlow lets you specify a timeout when calling \\ntherun() function by passing a RunOptions with timeout_in_ms:with tf.Session([...]) as sess:    [...]    run_options = tf.RunOptions()    run_options.timeout_in_ms = 1000  # 1s timeout    try:        pred = sess.run(dequeue_prediction, options=run_options)    except tf.errors.DeadlineExceededError as ex:        [...] # the dequeue operation timed out after 1sAnother way you can specify a timeout is to set the session‡s \\noperation_timeout_in_ms configuration option, but in this case the \\nrun() function times out if any\\noperation takes longer than the timeout delay:\\nconfig = tf.ConfigProto()config.operation_timeout_in_ms = 1000  # 1s timeout for every operationwith tf.Session([...], config=config) as sess:    [...]    try:        pred = sess.run(dequeue_prediction)    except tf.errors.DeadlineExceededError as ex:        [...]  # the dequeue operation timed out after 1sModel ParallelismSo far we have run each neural network on a single device. What if we want to run a\\nsingle neural network across multiple devices? This requires chopping your model\\ninto separate chunks and running each chunk on a different device. This is called\\nmodel parallelism\\n. Unfortunately, model parallelism turns out to be pretty tricky, and\\nit really depends on the architecture of your neural network. For fully connected net…\\nworks, there is generally not much to be gained from this approach (see\\nFigure 12-14). Intuitively, it may seem that an easy way to split the model is to place\\neach layer on a different device, but this does not work since each layer needs to wait\\nfor the output of the previous layer before it can do anything. So perhaps you can\\nslice it vertically›for example, with the left half of each layer on one device, and the\\nright part on another device? This is slightly better, since both halves of each layer can\\nindeed work in parallel, but the problem is that each half of the next layer requires the\\noutput of both halves, so there will be a lot of cross-device communication (repre…\\nParallelizing Neural Networks on a TensorFlow Cluster | 345\\nsented by the dashed arrows). This is likely to completely cancel out the benefit of the\\nparallel computation, since cross-device communication is slow (especially if it is\\nacross separate machines).\\nFigure 12-14. Splitting a fully connected neural network\\nHowever, as we will see in \\nChapter 13\\n, some neural network architectures, such asconvolutional neural networks, contain layers that are only partially connected to the\\nlower layers, so it is much easier to distribute chunks across devices in an efficient\\nway.\\nFigure 12-15. Splitting a partially connected neural network\\nMoreover, as we will see in \\nChapter 14\\n, some deep recurrent neural networks are\\ncomposed of several layers of \\nmemory cells\\n (see the left side of Figure 12-16). A cell‡s\\noutput at time \\nt is fed back to its input at time \\nt + 1 (as you can see more clearly onthe right side of \\nFigure 12-16). If you split such a network horizontally, placing each\\nlayer on a different device, then at the first step only one device will be active, at the\\nsecond step two will be active, and by the time the signal propagates to the output\\nlayer all devices will be active simultaneously. There is still a lot of cross-device com…\\nmunication going on, but since each cell may be fairly complex, the benefit of run…\\nning multiple cells in parallel often outweighs the communication penalty.\\n346 | Chapter 12: Distributing TensorFlow Across Devices and Servers\\nFigure 12-16. Splitting a deep recurrent neural network\\nIn short, model parallelism can speed up running or training some types of neuralnetworks, but not all, and it requires special care and tuning, such as making surethat devices that need to communicate the most run on the same machine.\\nData ParallelismAnother way to parallelize the training of a neural network is to replicate it on each\\ndevice, run a training step simultaneously on all replicas using a different mini-batch\\nfor each, and then aggregate the gradients to update the model parameters. This is\\ncalled data parallelism\\n (see Figure 12-17).Figure 12-17. Data parallelism\\nThere are two variants of this approach: \\nsynchronous updates\\n and asynchronous\\nupdates\\n.Parallelizing Neural Networks on a TensorFlow Cluster | 347\\n5This name is slightly confusing since it sounds like some replicas are special, doing nothing. In reality, all rep…\\nlicas are equivalent: they all work hard to be among the fastest at each training step, and the losers vary at\\nevery step (unless some devices are really slower than others).\\nSynchronous updatesWith \\nsynchronous updates\\n, the aggregator waits for all gradients to be available before\\ncomputing the average and applying the result (i.e., using the aggregated gradients to\\nupdate the model parameters). Once a replica has finished computing its gradients, it\\nmust wait for the parameters to be updated before it can proceed to the next mini-\\nbatch. The downside is that some devices may be slower than others, so all other\\ndevices will have to wait for them at every step. Moreover, the parameters will be\\ncopied to every device almost at the same time (immediately after the gradients are\\napplied), which may saturate the parameter servers‡ bandwidth.\\nTo reduce the waiting time at each step, you could ignore the gradi…\\nents from the slowest few replicas (typically ~10%). For example,\\nyou could run 20 replicas, but only aggregate the gradients from\\nthe fastest 18 replicas at each step, and just ignore the gradients\\nfrom the last 2. As soon as the parameters are updated, the first 18\\nreplicas can start working again immediately, without having to\\nwait for the 2 slowest replicas. This setup is generally described ashaving 18 replicas plus 2 \\nspare replicas\\n.5Asynchronous updatesWith asynchronous updates, whenever a replica has finished computing the gradi…\\nents, it immediately uses them to update the model parameters. There is no aggrega…\\ntion (remove the ƒmean⁄ step in \\nFigure 12-17), and no synchronization. Replicas just\\nwork independently of the other replicas. Since there is no waiting for the other repli…\\ncas, this approach runs more training steps per minute. Moreover, although the\\nparameters still need to be copied to every device at every step, this happens at differ…\\nent times for each replica so the risk of bandwidth saturation is reduced.\\nData parallelism with asynchronous updates is an attractive choice, because of its\\nsimplicity, the absence of synchronization delay, and a better use of the bandwidth.\\nHowever, although it works reasonably well in practice, it is almost surprising that it\\nworks at all! Indeed, by the time a replica has finished computing the gradients based\\non some parameter values, these parameters will have been updated several times by\\nother replicas (on average \\nN – 1 times if there are N replicas) and there is no guaran…tee that the computed gradients will still be pointing in the right direction (see\\nFigure 12-18). When gradients are severely out-of-date, they are called \\nstale gradients\\n: they can slow down convergence, introducing noise and wobble effects (the learning\\n348 | Chapter 12: Distributing TensorFlow Across Devices and Servers\\ncurve may contain temporary oscillations), or they can even make the training algo…\\nrithm diverge.Figure 12-18. Stale gradients when using asynchronous updates\\nThere are a few ways to reduce the effect of stale gradients:\\n‹Reduce the learning rate.\\n‹Drop stale gradients or scale them down.\\n‹Adjust the mini-batch size.\\n‹Start the first few epochs using just one replica (this is called the warmup phase\\n).Stale gradients tend to be more damaging at the beginning of training, when gra…\\ndients are typically large and the parameters have not settled into a valley of the \\ncost function yet, so different replicas may push the parameters in quite different\\ndirections.A paper published by the Google Brain team in April 2016\\n benchmarked variousapproaches and found that data parallelism with synchronous updates using a few\\nspare replicas was the most efficient, not only converging faster but also producing a\\nbetter model. However, this is still an active area of research, so you should not rule\\nout asynchronous updates quite yet.\\nBandwidth saturationWhether you use synchronous or asynchronous updates, data parallelism still\\nrequires communicating the model parameters from the parameter servers to every\\nreplica at the beginning of every training step, and the gradients in the other direction\\nat the end of each training step. Unfortunately, this means that there always comes a\\npoint where adding an extra GPU will not improve performance at all because the\\nParallelizing Neural Networks on a TensorFlow Cluster | 349\\ntime spent moving the data in and out of GPU RAM (and possibly across the net…\\nwork) will outweigh the speedup obtained by splitting the computation load. At that\\npoint, adding more GPUs will just increase saturation and slow down training.\\nFor some models, typically relatively small and trained on a very\\nlarge training set, you are often better off training the model on asingle machine with a single GPU.\\nSaturation is more severe for large dense models, since they have a lot of parameters\\nand gradients to transfer. It is less severe for small models (but the parallelization gain\\nis small) and also for large sparse models since the gradients are typically mostly\\nzeros, so they can be communicated efficiently. Jeff Dean, initiator and lead of the\\nGoogle Brain project, reported typical speedups of 25–40x when distributing compu…\\ntations across 50 GPUs for dense models, and 300x speedup for sparser models\\ntrained across 500 GPUs. As you can see, sparse models really do scale better. Here\\nare a few concrete examples:\\n‹Neural Machine Translation: 6x speedup on 8 GPUs\\n‹Inception/ImageNet: 32x speedup on 50 GPUs\\n‹RankBrain: 300x speedup on 500 GPUs\\nThese numbers represent the state of the art in Q1 2016. Beyond a few dozen GPUs\\nfor a dense model or few hundred GPUs for a sparse model, saturation kicks in and\\nperformance degrades. There is plenty of research going on to solve this problem\\n(exploring peer-to-peer architectures rather than centralized parameter servers, using\\nlossy model compression, optimizing when and what the replicas need to communi…\\ncate, and so on), so there will likely be a lot of progress in parallelizing neural net…\\nworks in the next few years.In the meantime, here are a few simple steps you can take to reduce the saturation\\nproblem:‹Group your GPUs on a few servers rather than scattering them across many\\nservers. This will avoid unnecessary network hops.\\n‹Shard the parameters across multiple parameter servers (as discussed earlier).\\n‹Drop the model parameters‡ float precision from 32 bits (\\ntf.float32) to 16 bits(tf.bfloat16). This will cut in half the amount of data to transfer, without much\\nimpact on the convergence rate or the model‡s performance.\\n350 | Chapter 12: Distributing TensorFlow Across Devices and Servers\\nAlthough 16-bit precision is the minimum for training neural net…\\nwork, you can actually drop down to 8-bit precision after trainingto reduce the size of the model and speed up computations. This is\\ncalled quantizing\\n the neural network. It is particularly useful for\\ndeploying and running pretrained models on mobile phones. SeePete Warden‡s \\ngreat post\\n on the subject.TensorFlow implementationTo implement data parallelism using TensorFlow, you first need to choose whether\\nyou want in-graph replication or between-graph replication, and whether you want\\nsynchronous updates or asynchronous updates. Let‡s look at how you would imple…\\nment each combination (see the exercises and the Jupyter notebooks for complete\\ncode examples).\\nWith in-graph replication + synchronous updates, you build one big graph contain…\\ning all the model replicas (placed on different devices), and a few nodes to aggregate\\nall their gradients and feed them to an optimizer. Your code opens a session to the\\ncluster and simply runs the training operation repeatedly.\\nWith in-graph replication + asynchronous updates, you also create one big graph, but\\nwith one optimizer per replica, and you run one thread per replica, repeatedly run…\\nning the replica‡s optimizer.\\nWith between-graph replication + asynchronous updates, you run multiple inde…\\npendent clients (typically in separate processes), each training the model replica as if\\nit were alone in the world, but the parameters are actually shared with other replicas(using a resource container).\\nWith between-graph replication + synchronous updates, once again you run multiple\\nclients, each training a model replica based on shared parameters, but this time you\\nwrap the optimizer (e.g., a \\nMomentumOptimizer) within a SyncReplicasOptimizer.Each replica uses this optimizer as it would use any other optimizer, but under the\\nhood this optimizer sends the gradients to a set of queues (one per variable), which is\\nread by one of the replica‡s \\nSyncReplicasOptimizer, called the chief\\n. The chief aggre…gates the gradients and applies them, then writes a token to a \\ntoken queue\\n for eachreplica, signaling it that it can go ahead and compute the next gradients. This\\napproach supports having \\nspare replicas\\n.If you go through the exercises, you will implement each of these four solutions. You\\nwill easily be able to apply what you have learned to train large deep neural networks\\nacross dozens of servers and GPUs! In the following chapters we will go through a\\nfew more important neural network architectures before we tackle \\nReinforcement\\nLearning.Parallelizing Neural Networks on a TensorFlow Cluster | 351\\nExercises1.If you get a CUDA_ERROR_OUT_OF_MEMORY when starting your TensorFlow pro…\\ngram, what is probably going on? What can you do about it?\\n2.What is the difference between pinning an operation on a device and placing an\\noperation on a device?\\n3.If you are running on a GPU-enabled TensorFlow installation, and you just use\\nthe default placement, will all operations be placed on the first GPU?\\n4.If you pin a variable to \"/gpu:0\", can it be used by operations placed on \\n/gpu:1?Or by operations placed on \\n\"/cpu:0\"? Or by operations pinned to devices loca…\\nted on other servers?\\n5.Can two operations placed on the same device run in parallel?\\n6.What is a control dependency and when would you want to use one?\\n7.Suppose you train a DNN for days on a TensorFlow cluster, and immediately\\nafter your training program ends you realize that you forgot to save the model\\nusing a Saver. Is your trained model lost?\\n8.Train several DNNs in parallel on a TensorFlow cluster, using different hyper…\\nparameter values. This could be DNNs for MNIST classification or any other task\\nyou are interested in. The simplest option is to write a single client program that\\ntrains only one DNN, then run this program in multiple processes in parallel,\\nwith different hyperparameter values for each client. The program should have\\ncommand-line options to control what server and device the DNN should be\\nplaced on, and what resource container and hyperparameter values to use (make\\nsure to use a different resource container for each DNN). Use a validation set or\\ncross-validation to select the top three models.\\n9.Create an ensemble using the top three models from the previous exercise.\\nDefine it in a single graph, ensuring that each DNN runs on a different device.\\nEvaluate it on the validation set: does the ensemble perform better than the indi…\\nvidual DNNs?\\n10.Train a DNN using between-graph replication and data parallelism with asyn…\\nchronous updates, timing how long it takes to reach a satisfying performance.\\nNext, try again using synchronous updates. Do synchronous updates produce a\\nbetter model? Is training faster? Split the DNN vertically and place each vertical\\nslice on a different device, and train the model again. Is training any faster? Is \\ntheperformance any different?\\nSolutions to these exercises are available in \\nAppendix A\\n.352 | Chapter 12: Distributing TensorFlow Across Devices and Servers\\nCHAPTER 13Convolutional Neural NetworksAlthough IBM‡s Deep Blue supercomputer beat the chess world champion Garry Kas…\\nparov back in 1996, until quite recently computers were unable to reliably perform\\nseemingly trivial tasks such as detecting a puppy in a picture or recognizing spokenwords. Why are these tasks so effortless to us humans? The answer lies in the fact that\\nperception largely takes place outside the realm of our consciousness, within special…ized visual, auditory, and other sensory modules in our brains. By the time sensory\\ninformation reaches our consciousness, it is already adorned with high-level features;\\nfor example, when you look at a picture of a cute puppy, you cannot choose \\nnot\\n to see\\nthe puppy, or \\nnot\\n to notice its cuteness. Nor can you explain \\nhow\\n you recognize a cute\\npuppy; it‡s just obvious to you. Thus, we cannot trust our subjective experience: per…\\nception is not trivial at all, and to understand it we must look at how the sensory\\nmodules work.Convolutional neural networks (CNNs) emerged from the study of the brain‡s visual\\ncortex, and they have been used in image recognition since the 1980s. In the last few\\nyears, thanks to the increase in computational power, the amount of available training\\ndata, and the tricks presented in \\nChapter 11\\n for training deep nets, CNNs have man…\\naged to achieve superhuman performance on some complex visual tasks. They power\\nimage search services, self-driving cars, automatic video classification systems, and\\nmore. Moreover, CNNs are not restricted to visual perception: they are also successful\\nat other tasks, such as \\nvoice recognition\\n or \\nnatural language processing\\n (NLP); however,\\nwe will focus on visual applications for now.\\nIn this chapter we will present where CNNs came from, what their building blocks\\nlook like, and how to implement them using TensorFlow. Then we will present some\\nof the best CNN architectures.3531ƒSingle Unit Activity in Striate Cortex of Unrestrained Cats,⁄ D. Hubel and T. Wiesel (1958).\\n2ƒReceptive Fields of Single Neurones in the Cat‡s Striate Cortex,⁄ D. Hubel and T. Wiesel (1959).\\n3ƒReceptive Fields and Functional Architecture of Monkey Striate Cortex,⁄ D. Hubel and T. Wiesel (1968).\\n4ƒNeocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected\\nby Shift in Position,⁄ K. Fukushima (1980).\\n5ƒGradient-Based Learning Applied to Document Recognition,⁄ Y. LeCun et al. (1998).\\nThe Architecture of the Visual CortexDavid H. Hubel and Torsten Wiesel performed a series of experiments on cats in\\n19581 and 19592 (and a few years later on monkeys\\n3), giving crucial insights on the\\nstructure of the visual cortex (the authors received the Nobel Prize in Physiology or\\nMedicine in 1981 for their work). In particular, they showed that many neurons in\\nthe visual cortex have a small \\nlocal receptive \\n†eld, meaning they react only to visualstimuli located in a limited region of the visual field (see \\nFigure 13-1, in which thelocal receptive fields of five neurons are represented by dashed circles). The receptive\\nfields of different neurons may overlap, and together they tile the whole visual field.\\nMoreover, the authors showed that some neurons react only to images of horizontal\\nlines, while others react only to lines with different orientations (two neurons may\\nhave the same receptive field but react to different line orientations). They also\\nnoticed that some neurons have larger receptive fields, and they react to more com…\\nplex patterns that are combinations of the lower-level patterns. These observations\\nled to the idea that the higher-level neurons are based on the outputs of neighboring\\nlower-level neurons (in \\nFigure 13-1, notice that each neuron is connected only to a\\nfew neurons from the previous layer). This powerful architecture is able to detect all\\nsorts of complex patterns in any area of the visual field.\\nFigure 13-1. Local receptive \\n†elds in the visual cortex\\nThese studies of the visual cortex inspired the neocognitron, introduced in 1980\\n,4which gradually evolved into what we now call \\nconvolutional neural networks\\n. Animportant milestone was a \\n1998 paper\\n5 by Yann LeCun, L•on Bottou, Yoshua Bengio,\\n354 | Chapter 13: Convolutional Neural Networks\\n6A convolution is a mathematical operation that slides one function over another and measures the integral of\\ntheir pointwise multiplication. It has deep connections with the Fourier transform and the Laplace transform,\\nand is heavily used in signal processing. Convolutional layers actually use cross-correlations, which are very\\nsimilar to convolutions (see \\nhttp://goo.gl/HAfxXd\\n for more details).and Patrick Haffner, which introduced the famous \\nLeNet-5\\n architecture, widely usedto recognize handwritten check numbers. This architecture has some building blocks\\nthat you already know, such as fully connected layers and sigmoid activation func…\\ntions, but it also introduces two new building blocks: \\nconvolutional layers\\n and \\npooling\\nlayers\\n. Let‡s look at them now.\\nWhy not simply use a regular deep neural network with fully con…\\nnected layers for image recognition tasks? Unfortunately, although\\nthis works fine for small images (e.g., MNIST), it breaks down forlarger images because of the huge number of parameters it\\nrequires. For example, a 100 ‰ 100 image has 10,000 pixels, and if\\nthe first layer has just 1,000 neurons (which already severely\\nrestricts the amount of information transmitted to the next layer),\\nthis means a total of 10 million connections. And that‡s just the first\\nlayer. CNNs solve this problem using partially connected layers.\\nConvolutional LayerThe most important building block of a CNN is the \\nconvolutional layer\\n:6 neurons inthe first convolutional layer are not connected to every single pixel in the input image\\n(like they were in previous chapters), but only to pixels in their receptive fields (see\\nFigure 13-2). In turn, each neuron in the second convolutional layer is connected\\nonly to neurons located within a small rectangle in the first layer. This architecture\\nallows the network to concentrate on low-level features in the first hidden layer, then\\nassemble them into higher-level features in the next hidden layer, and so on. This\\nhierarchical structure is common in real-world images, which is one of the reasonswhy CNNs work so well for image recognition.\\nConvolutional Layer | 355\\nFigure 13-2. CNN layers with rectangular local receptive \\n†eldsUntil now, all multilayer neural networks we looked at had layers\\ncomposed of a long line of neurons, and we had to flatten input\\nimages to 1D before feeding them to the neural network. Now each\\nlayer is represented in 2D, which makes it easier to match neurons\\nwith their corresponding inputs.\\nA neuron located in row \\ni, column j of a given layer is connected to the outputs of the\\nneurons in the previous layer located in rows \\ni to i + fh – 1, columns j to j + fw – 1,where fh and fw are the height and width of the receptive field (see \\nFigure 13-3). Inorder for a layer to have the same height and width as the previous layer, it is com…\\nmon to add zeros around the inputs, as shown in the diagram. This is \\ncalled zero pad…\\nding\\n.Figure 13-3. Connections between layers and zero padding\\n356 | Chapter 13: Convolutional Neural Networks\\nIt is also possible to connect a large input layer to a much smaller layer by spacing out\\nthe receptive fields, as shown in Figure 13-4. The distance between two consecutivereceptive fields is called the stride\\n. In the diagram, a 5 ‰ 7 input layer (plus zero pad…\\nding) is connected to a 3 ‰ 4 layer, using 3 ‰ 3 receptive fields and a stride of 2 (in this\\nexample the stride is the same in both directions, but it does not have to be so). A\\nneuron located in row \\ni, column j in the upper layer is connected to the outputs of the\\nneurons in the previous layer located in rows \\ni ‰ sh to i ‰ sh + fh – 1, columns j ‰ sw +fw – 1, where sh and sw are the vertical and horizontal strides.\\nFigure 13-4. Reducing dimensionality using a stride\\nFiltersA neuron‡s weights can be represented as a small image the size of the receptive field.\\nFor example, \\nFigure 13-5 shows two possible sets of weights, called \\n†lters (or \\nconvolu…\\ntion kernels\\n). The first one is represented as a black square with a vertical white line in\\nthe middle (it is a 7 ‰ 7 matrix full of 0s except for the central column, which is full of\\n1s); neurons using these weights will ignore everything in their receptive field except\\nfor the central vertical line (since all inputs will get multiplied by 0, except for the\\nones located in the central vertical line). The second filter is a black square with a\\nhorizontal white line in the middle. Once again, neurons using these weights will\\nignore everything in their receptive field except for the central horizontal line.\\nNow if all neurons in a layer use the same vertical line filter (and the same bias term),\\nand you feed the network the input image shown in \\nFigure 13-5 (bottom image), thelayer will output the top-left image. Notice that the vertical white lines get enhanced\\nwhile the rest gets blurred. Similarly, the upper-right image is what you get if all neu…\\nrons use the horizontal line filter; notice that the horizontal white lines get enhanced\\nwhile the rest is blurred out. Thus, a layer full of neurons using the same filter gives \\nConvolutional Layer | 357\\nyou a feature map\\n, which highlights the areas in an image that are most similar to the\\nfilter. During training, a CNN finds the most useful filters for its task, and it learns to\\ncombine them into more complex patterns (e.g., a cross is an area in an image where\\nboth the vertical filter and the horizontal filter are active).\\nFigure 13-5. Applying two \\ndi›erent †lters to get two feature maps\\nStacking Multiple Feature MapsUp to now, for simplicity, we have represented each convolutional layer as a thin 2D\\nlayer, but in reality it is composed of several feature maps of equal sizes, so it is more\\naccurately represented in 3D (see \\nFigure 13-6). Within one feature map, all neurons\\nshare the same parameters (weights and bias term), but different feature maps may\\nhave different parameters. A neuron‡s receptive field is the same as described earlier,\\nbut it extends across all the previous layers‡ feature maps. In short, a convolutional\\nlayer simultaneously applies multiple filters to its inputs, making it capable of detect…\\ning multiple features anywhere in its inputs.\\nThe fact that all neurons in a feature map share the same parame…\\nters dramatically reduces the number of parameters in the model,\\nbut most importantly it means that once the CNN has learned to\\nrecognize a pattern in one location, it can recognize it in any other\\nlocation. In contrast, once a regular DNN has learned to recognize\\na pattern in one location, it can recognize it only in that particular\\nlocation.\\n358 | Chapter 13: Convolutional Neural Networks\\nMoreover, input images are also composed of multiple sublayers: one per \\ncolor chan…\\nnel\\n. There are typically three: red, green, and blue (RGB). Grayscale images have just\\none channel, but some images may have much more›for example, satellite images\\nthat capture extra light frequencies (such as infrared).\\nFigure 13-6. Convolution layers with multiple feature maps, and images with three\\nchannels\\nSpecifically, a neuron located in row \\ni, column j of the feature map \\nk in a given convo…\\nlutional layer \\nl is connected to the outputs of the neurons in the previous layer \\nl – 1,located in rows \\ni ‰ sw to i ‰ sw + fw – 1 and columns j ‰ sh to j ‰ sh + fh – 1, across allfeature maps (in layer \\nl – 1). Note that all neurons located in the same row \\ni and col…umn j but in different feature maps are connected to the outputs of the exact same\\nneurons in the previous layer.\\nEquation 13-1\\n summarizes the preceding explanations in one big mathematical equa…\\ntion: it shows how to compute the output of a given neuron in a convolutional layer.\\nConvolutional Layer | 359\\nIt is a bit ugly due to all the different indices, but all it does is calculate the weighted\\nsum of all the inputs, plus the bias term.\\nEquation 13-1. Computing the output of a neuron in a convolutional layer\\nzi,j,k=bk+“u=1\\nfh“v=1\\nfw“k=1\\nfnxi,j,k.wu,v,k,kwithi=u.sh+fh”1\\nj=v.sw+fw”1\\n‹zi, j, k\\n is the output of the neuron located in row \\ni, column j in feature map \\nk of the\\nconvolutional layer (layer \\nl).‹As explained earlier, \\nsh and sw are the vertical and horizontal strides, \\nfh and fw arethe height and width of the receptive field, and \\nfn is the number of feature maps\\nin the previous layer (layer \\nl – 1).‹xi, j, k is the output of the neuron located in layer \\nl – 1, row i, column j, feature\\nmap \\nk(or channel kif the previous layer is the input layer).\\n‹bk is the bias term for feature map \\nk (in layer \\nl). You can think of it as a knob that\\ntweaks the overall brightness of the feature map \\nk.‹wu, v, k,k is the connection weight between any neuron in feature map \\nk of the layer\\nl and its input located at row \\nu, column v (relative to the neuron‡s receptive field),\\nand feature map \\nk.TensorFlow ImplementationIn TensorFlow, each input image is typically represented as a 3D tensor of \\nshape[height, width, channels]. A mini-batch is represented as a 4D tensor of \\nshape[mini-batch size, height, width, channels]. The weights of a convolutional\\nlayer are represented as a 4D tensor of shape [\\nfh, fw, fn, fn]. The bias terms of a convo…\\nlutional layer are simply represented as a 1D tensor of \\nshape [fn].Let‡s look at a simple example. The following code loads two sample images, using\\nScikit-Learn‡s \\nload_sample_images() (which loads two color images, one of a Chi…nese temple, and the other of a flower). Then it creates two 7 ‰ 7 filters (one with a\\nvertical white line in the middle, and the other with a horizontal white line), and\\napplies them to both images using a convolutional layer built using TensorFlow‡s\\nconv2d() function (with zero padding and a stride of 2). Finally, it plots one of the\\nresulting feature maps (similar to the top-right image in \\nFigure 13-5).360 | Chapter 13: Convolutional Neural Networks\\nimport numpy as npfrom sklearn.datasets import load_sample_images# Load sample imagesdataset = np.array(load_sample_images().images, dtype=np.float32)batch_size, height, width, channels = dataset.shape# Create 2 filtersfilters_test = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)filters_test[:, 3, :, 0] = 1  # vertical linefilters_test[3, :, :, 1] = 1  # horizontal line# Create a graph with input X plus a convolutional layer applying the 2 filtersX = tf.placeholder(tf.float32, shape=(None, height, width, channels))convolution = tf.nn.conv2d(X, filters, strides=[1,2,2,1], padding=\"SAME\")with tf.Session() as sess:    output = sess.run(convolution, feed_dict={X: dataset})plt.imshow(output[0, :, :, 1])  # plot 1st image•s 2nd feature mapplt.show()Most of this code is self-explanatory, but the \\nconv2d() line deserves a bit of explana…\\ntion:‹X is the input mini-batch (a 4D tensor, as explained earlier).\\n‹filters is the set of filters to apply (also a 4D tensor, as explained earlier).\\n‹strides is a four-element 1D array, where the two central elements are the verti…\\ncal and horizontal strides (\\nsh and sw). The first and last elements must currently\\nbe equal to 1. They may one day be used to specify a batch stride (to skip some\\ninstances) and a channel stride (to skip some of the previous layer‡s feature maps\\nor channels).‹padding must be either \\n\"VALID\" or \"SAME\":›If set to \"VALID\", the convolutional layer does \\nnot\\n use zero padding, and may\\nignore some rows and columns at the bottom and right of the input image,\\ndepending on the stride, as shown in Figure 13-7 (for simplicity, only the hor…\\nizontal dimension is shown here, but of course the same logic applies to the\\nvertical dimension).›If set to \"SAME\", the convolutional layer uses zero padding if necessary. In this\\ncase, the number of output neurons is equal to the number of input neurons\\ndivided by the stride, rounded up (in this example, ceil (13 / 5) = 3). Then\\nzeros are added as evenly as possible around the inputs.\\nConvolutional Layer | 361\\n7A fully connected layer with 150 ‰ 100 neurons, each connected to all 150 ‰ 100 ‰ 3 inputs, would have 150\\n2‰ 1002 ‰ 3 = 675 million parameters!Figure 13-7. Padding optionsŠinput width: 13, \\n†lter width: 6, stride: 5\\nUnfortunately, convolutional layers have quite a few hyperparameters: you must\\nchoose the number of filters, their height and width, the strides, and the padding\\ntype. As always, you can use cross-validation to find the right hyperparameter values,\\nbut this is very time-consuming. We will discuss common CNN architectures later, to\\ngive you some idea of what hyperparameter values work best in practice.\\nMemory RequirementsAnother problem with CNNs is that the convolutional layers require a huge amount\\nof RAM, especially during training, because the reverse pass of backpropagation\\nrequires all the intermediate values computed during the forward pass.\\nFor example, consider a convolutional layer with 5 ‰ 5 filters, outputting 200 feature\\nmaps of size 150 ‰ 100, with stride 1 and SAME padding. If the input is a 150 ‰ 100\\nRGB image (three channels), then the number of parameters is (5 ‰ 5 ‰ 3 + 1) ‰ 200\\n= 15,200 (the +1 corresponds to the bias terms), which is fairly small compared to a\\nfully connected layer.\\n7 However, each of the 200 feature maps contains 150 ‰ 100 neu…\\nrons, and each of these neurons needs to compute a weighted sum of its 5 ‰ 5 ‰ 3 =\\n75 inputs: that‡s a total of 225 million float multiplications. Not as bad as a fully con…\\n362 | Chapter 13: Convolutional Neural Networks\\n81 MB = 1,024 kB = 1,024 ‰ 1,024 bytes = 1,024 ‰ 1,024 ‰ 8 bits.nected layer, but still quite computationally intensive. Moreover, if the feature maps\\nare represented using 32-bit floats, then the convolutional layer‡s output will occupy\\n200 ‰ 150 ‰ 100 ‰ 32 = 96 million bits (about 11.4 MB) of RAM.8 And that‡s just for\\none instance! If a training batch contains 100 instances, then this layer will use up\\nover 1 GB of RAM!During inference (i.e., when making a prediction for a new instance) the RAM occu…pied by one layer can be released as soon as the next layer has been computed, so you\\nonly need as much RAM as required by two consecutive layers. But during training\\neverything computed during the forward pass needs to be preserved for the reverse\\npass, so the amount of RAM needed is (at least) the total amount of RAM required by\\nall layers.\\nIf training crashes because of an out-of-memory error, you can try\\nreducing the mini-batch size. Alternatively, you can try reducing\\ndimensionality using a stride, or removing a few layers. Or you can\\ntry using 16-bit floats instead of 32-bit floats. Or you could distrib…\\nute the CNN across multiple devices.\\nNow let‡s look at the second common building block of CNNs: the \\npooling layer\\n.Pooling LayerOnce you understand how convolutional layers work, the pooling layers are quite\\neasy to grasp. Their goal is to \\nsubsample\\n (i.e., shrink) the input image in order to\\nreduce the computational load, the memory usage, and the number of parameters\\n(thereby limiting the risk of overfitting). Reducing the input image size also makes\\nthe neural network tolerate a little bit of image shift (\\nlocation invariance\\n).Just like in convolutional layers, each neuron in a pooling layer is connected to the\\noutputs of a limited number of neurons in the previous layer, located within a small\\nrectangular receptive field. You must define its size, the stride, and the padding type,\\njust like before. However, a pooling neuron has no weights; all it does is aggregate the\\ninputs using an aggregation function such as the max or mean. \\nFigure 13-8 shows amax pooling layer\\n, which is the most common type of pooling layer. In this example,\\nwe use a 2 ‰ 2 pooling kernel\\n, a stride of 2, and no padding. Note that only the max\\ninput value in each kernel makes it to the next layer. The other inputs are dropped.\\nPooling Layer | 363\\nFigure 13-8. Max pooling layer (2 Ÿ 2 pooling kernel, stride 2, no padding)\\nThis is obviously a very destructive kind of layer: even with a tiny 2 ‰ 2 kernel and a\\nstride of 2, the output will be two times smaller in both directions (so its area will befour times smaller), simply dropping 75% of the input values.\\nA pooling layer typically works on every input channel independently, so the output\\ndepth is the same as the input depth. You may alternatively pool over the depth\\ndimension, as we will see next, in which case the image‡s spatial dimensions (height\\nand width) remain unchanged, but the number of channels is reduced.\\nImplementing a max pooling layer in TensorFlow is quite easy. The following code\\ncreates a max pooling layer using a 2 ‰ 2 kernel, stride 2, and no padding, then\\napplies it to all the images in the dataset:\\n[...] # load the image dataset, just like above# Create a graph with input X plus a max pooling layerX = tf.placeholder(tf.float32, shape=(None, height, width, channels))max_pool = tf.nn.max_pool(X, ksize=[1,2,2,1], strides=[1,2,2,1],padding=\"VALID\")with tf.Session() as sess:    output = sess.run(max_pool, feed_dict={X: dataset})plt.imshow(output[0].astype(np.uint8))  # plot the output for the 1st imageplt.show()The ksize argument contains the kernel shape along all four dimensions of the input\\ntensor: \\n[batch size, height, width, channels]. TensorFlow currently does not\\nsupport pooling over multiple instances, so the first element of \\nksize must be equal\\nto 1. Moreover, it does not support pooling over both the spatial dimensions (height\\nand width) and the depth dimension, so either ksize[1] and ksize[2] must both be\\nequal to 1, or ksize[3] must be equal to 1.\\nTo create an \\naverage pooling layer\\n, just use the avg_pool() function instead ofmax_pool().364 | Chapter 13: Convolutional Neural Networks\\nNow you know all the building blocks to create a convolutional neural network. Let‡s\\nsee how to assemble them.CNN ArchitecturesTypical CNN architectures stack a few convolutional layers (each one generally fol…\\nlowed by a ReLU layer), then a pooling layer, then another few convolutional layers\\n(+ReLU), then another pooling layer, and so on. The image gets smaller and smaller\\nas it progresses through the network, but it also typically gets deeper and deeper (i.e.,with more feature maps) thanks to the convolutional layers (see \\nFigure 13-9). At the\\ntop of the stack, a regular feedforward neural network is added, composed of a few\\nfully connected layers (+ReLUs), and the final layer outputs the prediction (e.g., a\\nsoftmax layer that outputs estimated class probabilities).\\nFigure 13-9. Typical CNN architecture\\nA common mistake is to use convolution kernels that are too large.\\nYou can often get the same effect as a 9 ‰ 9 kernel by stacking two 3\\n‰ 3 kernels on top of each other, for a lot less compute.\\nOver the years, variants of this fundamental architecture have been developed, lead…\\ning to amazing advances in the field. A good measure of this progress is the error rate\\nin competitions such as the ILSVRC \\nImageNet challenge\\n. In this competition the\\ntop-5 error rate for image classification fell from over 26% to barely over 3% in just\\nfive years. The top-five error rate is the number of test images for which the system‡s\\ntop 5 predictions did not include the correct answer. The images are large (256 pixels\\nhigh) and there are 1,000 classes, some of which are really subtle (try distinguishing\\n120 dog breeds). Looking at the evolution of the winning entries is a good way to\\nunderstand how CNNs work.\\nWe will first look at the classical LeNet-5 architecture (1998), then three of the win…\\nners of the ILSVRC challenge: AlexNet (2012), GoogLeNet (2014), and ResNet\\n(2015).CNN Architectures | 365\\nOther Visual TasksThere was stunning progress as well in other visual tasks such as object detection andlocalization, and image segmentation. In object detection and localization, the neural\\nnetwork typically outputs a sequence of bounding boxes around various objects inthe image. For example, see Maxine Oquab et al.‡s 2015 \\npaper\\n that outputs a heat map\\nfor each object class, or Russell Stewart et al.‡s 2015 \\npaper\\n that uses a combination of a\\nCNN to detect faces and a recurrent neural network to output a sequence of bound…\\ning boxes around them. In image segmentation, the net outputs an image (usually of\\nthe same size as the input) where each pixel indicates the class of the object to which\\nthe corresponding input pixel belongs. For example, check out Evan Shelhamer et al.‡s\\n2016 paper\\n.LeNet-5The LeNet-5 architecture is perhaps the most widely known CNN architecture. As\\nmentioned earlier, it was created by Yann LeCun in 1998 and widely used for hand…\\nwritten digit recognition (MNIST). It is composed of the layers shown in \\nTable 13-1\\n.Table 13-1. LeNet-5 architecture\\nLayerTypeMapsSizeKernel size\\nStrideActivationOutFully Connected\\n–10––RBFF6Fully Connected\\n–84––tanhC5Convolution1201 † 1\\n5 † 5\\n1tanhS4Avg Pooling\\n165 † 5\\n2 † 2\\n2tanhC3Convolution1610 † 10\\n5 † 5\\n1tanhS2Avg Pooling\\n614 † 14\\n2 † 2\\n2tanhC1Convolution628 † 28\\n5 † 5\\n1tanhInInput132 † 32\\n–––There are a few extra details to be noted:‹MNIST images are 28 ‰ 28 pixels, but they are zero-padded to 32 ‰ 32 pixels andnormalized before being fed to the network. The rest of the network does not useany padding, which is why the size keeps shrinking as the image progresses\\nthrough the network.‹The average pooling layers are slightly more complex than usual: each neuron\\ncomputes the mean of its inputs, then multiplies the result by a learnable coeffi…\\ncient (one per map) and adds a learnable bias term (again, one per map), then\\nfinally applies the activation function.\\n366 | Chapter 13: Convolutional Neural Networks\\n9ƒImageNet Classification with Deep Convolutional Neural Networks,⁄ A. Krizhevsky et al. (2012).\\n‹Most neurons in C3 maps are connected to neurons in only three or four S2\\nmaps (instead of all six S2 maps). See table 1 in the original paper for details.\\n‹The output layer is a bit special: instead of computing the dot product of the\\ninputs and the weight vector, each neuron outputs the square of the Euclidian\\ndistance between its input vector and its weight vector. Each output measures\\nhow much the image belongs to a particular digit class. The cross entropy \\ncostfunction is now preferred, as it penalizes bad predictions much more, producing\\nlarger gradients and thus converging faster.\\nYann LeCun‡s \\nwebsite (ƒLENET⁄ section) features great demos of LeNet-5 classifying \\ndigits.AlexNetThe AlexNet\\n CNN architecture\\n9 won the 2012 ImageNet ILSVRC challenge by a large\\nmargin: it achieved 17% top-5 error rate while the second best achieved only 26%! It\\nwas developed by Alex Krizhevsky (hence the name), Ilya Sutskever, and Geoffrey\\nHinton. It is quite similar to LeNet-5, only much larger and deeper, and it was the\\nfirst to stack convolutional layers directly on top of each other, instead of stacking a\\npooling layer on top of each convolutional layer. \\nTable 13-2\\n presents this architecture.\\nTable 13-2. AlexNet architecture\\nLayerTypeMapsSizeKernel size\\nStridePaddingActivationOutFully Connected\\n–1,000–––SoftmaxF9Fully Connected\\n–4,096–––ReLUF8Fully Connected\\n–4,096–––ReLUC7Convolution25613 † 13\\n3 † 3\\n1SAMEReLUC6Convolution38413 † 13\\n3 † 3\\n1SAMEReLUC5Convolution38413 † 13\\n3 † 3\\n1SAMEReLUS4Max Pooling\\n25613 † 13\\n3 † 3\\n2VALID–C3Convolution25627 † 27\\n5 † 5\\n1SAMEReLUS2Max Pooling\\n9627 † 27\\n3 † 3\\n2VALID–C1Convolution9655 † 55\\n11 † 11\\n4SAMEReLUInInput3 (RGB)\\n224 † 224\\n––––To reduce overfitting, the authors used two regularization techniques we discussed in\\nprevious chapters: first they applied dropout (with a 50% dropout rate) during train…\\ning to the outputs of layers F8 and F9. Second, they performed data augmentation by\\nCNN Architectures | 367\\n10ƒGoing Deeper with Convolutions,⁄ C. Szegedy et al. (2015).\\nrandomly shifting the training images by various offsets, flipping them horizontally,\\nand changing the lighting conditions.\\nAlexNet also uses a competitive normalization step immediately after the ReLU step\\nof layers C1 and C3, called \\nlocal response normalization\\n. This form of normalization\\nmakes the neurons that most strongly activate inhibit neurons at the same location\\nbut in neighboring feature maps (such competitive activation has been observed in\\nbiological neurons). This encourages different feature maps to specialize, pushing\\nthem apart and forcing them to explore a wider range of features, ultimately improv…\\ning generalization. \\nEquation 13-2\\n shows how to apply LRN.\\nEquation 13-2. Local response normalization\\nbi=aik+‰“j=jlowjhighaj2”Łwithjhigh=min\\ni+r2,fn”1\\njlow=max\\n0,i”r2‹bi is the normalized output of the neuron located in feature map \\ni, at some row \\nuand column v (note that in this equation we consider only neurons located at this\\nrow and column, so u and v are not shown).‹ai is the activation of that neuron after the ReLU step, but before normalization.\\n‹k, ‰, Ł, and r are hyperparameters. \\nk is called the bias\\n, and r is called the depth\\nradius\\n.‹fn is the number of feature maps.\\nFor example, if \\nr = 2 and a neuron has a strong activation, it will inhibit the activation\\nof the neurons located in the feature maps immediately above and below its own.\\nIn AlexNet, the hyperparameters are set as follows: \\nr = 2, ‰ = 0.00002, Ł = 0.75, and k= 1. This step can be implemented using TensorFlow‡s \\nlocal_response_normalization() operation.\\nA variant of AlexNet called \\nZF Net\\n was developed by Matthew Zeiler and Rob Fergus\\nand won the 2013 ILSVRC challenge. It is essentially AlexNet with a few tweaked \\nhyperparameters (number of feature maps, kernel size, stride, etc.).\\nGoogLeNetThe GoogLeNet architecture\\n was developed by Christian Szegedy et al. from GoogleResearch,10 and it won the ILSVRC 2014 challenge by pushing the top-5 error rate\\n368 | Chapter 13: Convolutional Neural Networks\\n11In the 2010 movie Inception\\n, the characters keep going deeper and deeper into multiple layers of dreams,\\nhence the name of these modules.below 7%. This great performance came in large part from the fact that the network\\nwas much deeper than previous CNNs (see \\nFigure 13-11). This was made possible bysub-networks called inception modules\\n,11 which allow GoogLeNet to use parameters\\nmuch more efficiently than previous architectures: GoogLeNet actually has 10 times\\nfewer parameters than AlexNet (roughly 6 million instead of 60 million).\\nFigure 13-10 shows the architecture of an inception module. The notation ƒ3 ‰ 3 +\\n2(S)⁄ means that the layer uses a 3 ‰ 3 kernel, stride 2, and SAME padding. The input\\nsignal is first copied and fed to four different layers. All convolutional layers use the\\nReLU activation function. Note that the second set of convolutional layers uses differ…\\nent kernel sizes (1 ‰ 1, 3 ‰ 3, and 5 ‰ 5), allowing them to capture patterns at different\\nscales. Also note that every single layer uses a stride of 1 and SAME padding (even\\nthe max pooling layer), so their outputs all have the same height and width as their\\ninputs. This makes it possible to concatenate all the outputs along the depth dimen…\\nsion in the final depth concat layer\\n (i.e., stack the feature maps from all four top con…\\nvolutional layers). This concatenation layer can be implemented in TensorFlow using\\nthe concat() operation, with \\naxis=3 (axis 3 is the depth).Figure 13-10. Inception module\\nYou may wonder why inception modules have convolutional layers with 1 ‰ 1 ker…\\nnels. Surely these layers cannot capture any features since they look at only one pixel\\nat a time? In fact, these layers serve two purposes:\\n‹First, they are configured to output many fewer feature maps than their inputs, so\\nthey serve as \\nbottleneck layers\\n, meaning they reduce dimensionality. This is par…\\nCNN Architectures | 369\\nticularly useful before the 3 ‰ 3 and 5 ‰ 5 convolutions, since these are very com…\\nputationally expensive layers.\\n‹Second, each pair of convolutional layers ([1 ‰ 1, 3 ‰ 3] and [1 ‰ 1, 5 ‰ 5]) acts\\nlike a single, powerful convolutional layer, capable of capturing more complex\\npatterns. Indeed, instead of sweeping a simple linear classifier across the image\\n(as a single convolutional layer does), this pair of convolutional layers sweeps a\\ntwo-layer neural network across the image.\\nIn short, you can think of the whole inception module as a convolutional layer on\\nsteroids, able to output feature maps that capture complex patterns at various scales.\\nThe number of convolutional kernels for each convolutional layer\\nis a hyperparameter. Unfortunately, this means that you have six\\nmore hyperparameters to tweak for every inception layer you add.\\nNow let‡s look at the architecture of the GoogLeNet CNN (see \\nFigure 13-11). It is so\\ndeep that we had to represent it in three columns, but GoogLeNet is actually one tall\\nstack, including nine inception modules (the boxes with the spinning tops) that\\nactually contain three layers each. The number of feature maps output by each convo…\\nlutional layer and each pooling layer is shown before the kernel size. The six numbers\\nin the inception modules represent the number of feature maps output by each con…\\nvolutional layer in the module (in the same order as in \\nFigure 13-10). Note that all the\\nconvolutional layers use the ReLU activation function.\\n370 | Chapter 13: Convolutional Neural Networks\\nFigure 13-11. GoogLeNet architecture\\nLet‡s go through this network:\\n‹The first two layers divide the image‡s height and width by 4 (so its area is divided\\nby 16), to reduce the computational load.\\n‹Then the local response normalization layer ensures that the previous layers learn\\na wide variety of features (as discussed earlier).\\n‹Two convolutional layers follow, where the first acts like a \\nbottleneck layer\\n. Asexplained earlier, you can think of this pair as a single smarter convolutional\\nlayer.\\n‹Again, a local response normalization layer ensures that the previous layers cap…\\nture a wide variety of patterns.\\n‹Next a max pooling layer reduces the image height and width by 2, again to speed\\nup computations.\\nCNN Architectures | 371\\n12ƒDeep Residual Learning for Image Recognition,⁄ K. He (2015).\\n‹Then comes the tall stack of nine inception modules, interleaved with a couple\\nmax pooling layers to reduce dimensionality and speed up the net.\\n‹Next, the average pooling layer uses a kernel the size of the feature maps with\\nVALID padding, outputting 1 ‰ 1 feature maps: this surprising strategy is \\ncalledglobal average pooling\\n. It effectively forces the previous layers to produce feature\\nmaps that are actually confidence maps for each target class (since other kinds of\\nfeatures would be destroyed by the averaging step). This makes it unnecessary to\\nhave several fully connected layers at the top of the CNN (like in AlexNet), con…\\nsiderably reducing the number of parameters in the network and limiting the risk\\nof overfitting.‹The last layers are self-explanatory: dropout for regularization, then a fully con…\\nnected layer with a softmax activation function to output estimated class proba…\\nbilities.This diagram is slightly simplified: the original GoogLeNet architecture also included\\ntwo auxiliary classifiers plugged on top of the third and sixth inception modules.\\nThey were both composed of one average pooling layer, one convolutional layer, two\\nfully connected layers, and a softmax activation layer. During training, their loss\\n(scaled down by 70%) was added to the overall loss. The goal was to fight the vanish…\\ning gradients problem and regularize the network. However, it was shown that their\\neffect was relatively minor.\\nResNetLast but not least, the winner of the ILSVRC 2015 challenge was the Residual Network\\n(or ResNet\\n), developed by Kaiming He et al.,\\n12 which delivered an astounding top-5error rate under 3.6%, using an extremely deep CNN composed of 152 layers. The\\nkey to being able to train such a deep network is to use skip connections\\n (also calledshortcut connections\\n): the signal feeding into a layer is also added to the output of a\\nlayer located a bit higher up the stack. Let‡s see why this is useful.\\nWhen training a neural network, the goal is to make it model a target function h(x).If you add the input \\nx to the output of the network (i.e., you add a skip connection),then the network will be forced to model f(x) = h(x) – x rather than \\nh(x). This iscalled residual learning\\n (see Figure 13-12).372 | Chapter 13: Convolutional Neural Networks\\nFigure 13-12. Residual learning\\nWhen you initialize a regular neural network, its weights are close to zero, so the net…\\nwork just outputs values close to zero. If you add a skip connection, the resulting net…\\nwork just outputs a copy of its inputs; in other words, it initially models the identity\\nfunction. If the target function is fairly close to the identity function (which is often\\nthe case), this will speed up training considerably.\\nMoreover, if you add many skip connections, the network can start making progress\\neven if several layers have not started learning yet (see \\nFigure 13-13). Thanks to skipconnections, the signal can easily make its way across the whole network. The deep\\nresidual network can be seen as a stack of residual units\\n, where each residual unit is asmall neural network with a skip connection.Figure 13-13. Regular deep neural network \\n(le“) and deep residual network (right)\\nCNN Architectures | 373\\nNow let‡s look at ResNet‡s architecture (see \\nFigure 13-14). It is actually surprisingly\\nsimple. It starts and ends exactly like GoogLeNet (except without a dropout layer),\\nand in between is just a very deep stack of simple residual units. Each residual unit is\\ncomposed of two convolutional layers, with Batch Normalization (BN) and ReLU\\nactivation, using 3 ‰ 3 kernels and preserving spatial dimensions (stride 1, SAME\\npadding).Figure 13-14. ResNet architecture\\nNote that the number of feature maps is doubled every few residual units, at the same\\ntime as their height and width are halved (using a convolutional layer with stride 2).\\nWhen this happens the inputs cannot be added directly to the outputs of the residual\\nunit since they don‡t have the same shape (for example, this problem affects the skip\\nconnection represented by the dashed arrow in \\nFigure 13-14). To solve this problem,\\nthe inputs are passed through a 1 ‰ 1 convolutional layer with stride 2 and the right\\nnumber of output feature maps (see \\nFigure 13-15).Figure 13-15. Skip connection when changing feature map size and depth\\n374 | Chapter 13: Convolutional Neural Networks\\n13ƒVery Deep Convolutional Networks for Large-Scale Image Recognition,⁄ K. Simonyan and A. Zisserman\\n(2015).14ƒInception-v4, Inception-ResNet and the Impact of Residual Connections on Learning,⁄ C. Szegedy et al.\\n(2016).ResNet-34 is the ResNet with 34 layers (only counting the convolutional layers and\\nthe fully connected layer) containing three residual units that output 64 feature maps,\\n4 RUs with 128 maps, 6 RUs with 256 maps, and 3 RUs with 512 maps.\\nResNets deeper than that, such as ResNet-152, use slightly different residual units.\\nInstead of two 3 ‰ 3 convolutional layers with (say) 256 feature maps, they use three\\nconvolutional layers: first a 1 ‰ 1 convolutional layer with just 64 feature maps (4\\ntimes less), which acts a a bottleneck layer (as discussed already), then a 3 ‰ 3 layer\\nwith 64 feature maps, and finally another 1 ‰ 1 convolutional layer with 256 feature\\nmaps (4 times 64) that restores the original depth. ResNet-152 contains three such\\nRUs that output 256 maps, then 8 RUs with 512 maps, a whopping 36 RUs with 1,024\\nmaps, and finally 3 RUs with 2,048 maps.\\nAs you can see, the field is moving rapidly, with all sorts of architectures popping out\\nevery year. One clear trend is that CNNs keep getting deeper and deeper. They are\\nalso getting lighter, requiring fewer and fewer parameters. At present, the ResNet\\narchitecture is both the most powerful and arguably the simplest, so it is really the\\none you should probably use for now, but keep looking at the ILSVRC challenge\\nevery year. The 2016 winners were the Trimps-Soushen team from China with an\\nastounding 2.99% error rate. To achieve this they trained combinations of the previ…\\nous models and joined them into an ensemble. Depending on the task, the reduced\\nerror rate may or may not be worth the extra complexity.\\nThere are a few other architectures that you may want to look at, in particular\\nVGGNet\\n13 (runner-up of the ILSVRC 2014 challenge) and \\nInception-v414 (whichmerges the ideas of GoogLeNet and ResNet and achieves close to 3% top-5 error rate\\non ImageNet classification).\\nThere is really nothing special about implementing the various\\nCNN architectures we just discussed. We saw earlier how to build\\nall the individual building blocks, so now all you need is to assem…ble them to create the desired architecture. We will build ResNet-34\\nin the upcoming exercises and you will find full working code inthe Jupyter notebooks.\\nCNN Architectures | 375\\n15This name is quite misleading since this layer does \\nnot\\n perform a deconvolution, which is a well-defined\\nmathematical operation (the inverse of a convolution).\\nTensorFlow Convolution OperationsTensorFlow also offers a few other kinds of convolutional layers:\\n‹conv1d() creates a convolutional layer for 1D inputs. This is useful, for example,\\nin natural language processing, where a sentence may be represented as a 1D\\narray of words, and the receptive field covers a few neighboring words.\\n‹conv3d() creates a convolutional layer for 3D inputs, such as 3D PET scan.\\n‹atrous_conv2d() creates an \\natrous convolutional layer\\n (ƒÉ trous⁄ is French for\\nƒwith holes⁄). This is equivalent to using a regular convolutional layer with a fil…\\nter dilated by inserting rows and columns of zeros (i.e., holes). For example, a 1 ‰\\n3 filter equal to [[1,2,3]] may be dilated with a \\ndilation rate\\n of 4, resulting in adilated \\n†lter [[1, 0, 0, 0, 2, 0, 0, 0, 3]]. This allows the convolutional\\nlayer to have a larger receptive field at no computational price and using no extra\\nparameters.‹conv2d_transpose() creates a \\ntranspose convolutional layer\\n, sometimes called adeconvolutional layer\\n,15 which upsamples\\n an image. It does so by inserting zeros\\nbetween the inputs, so you can think of this as a regular convolutional layer using\\na fractional stride. Upsampling is useful, for example, in image segmentation: in a\\ntypical CNN, feature maps get smaller and smaller as you progress through the\\nnetwork, so if you want to output an image of the same size as the input, you\\nneed an upsampling layer.\\n‹depthwise_conv2d() creates a \\ndepthwise convolutional layer\\n that applies every fil…\\nter to every individual input channel independently. Thus, if there are \\nfn filtersand fn input channels, then this will output \\nfn ‰ fn feature maps.\\n‹separable_conv2d() creates a \\nseparable convolutional layer\\n that first acts like a\\ndepthwise convolutional layer, then applies a 1 ‰ 1 convolutional layer to the\\nresulting feature maps. This makes it possible to apply filters to arbitrary sets of\\ninputs channels.\\nExercises1.What are the advantages of a CNN over a fully connected DNN for image classi…\\nfication?\\n2.Consider a CNN composed of three convolutional layers, each with 3 ‰ 3 kernels,\\na stride of 2, and SAME padding. The lowest layer outputs 100 feature maps, the\\n376 | Chapter 13: Convolutional Neural Networks\\nmiddle one outputs 200, and the top one outputs 400. The input images are RGB\\nimages of 200 ‰ 300 pixels. What is the total number of parameters in the CNN?\\nIf we are using 32-bit floats, at least how much RAM will this network require\\nwhen making a prediction for a single instance? What about when training on a\\nmini-batch of 50 images?\\n3.If your GPU runs out of memory while training a CNN, what are five things you\\ncould try to solve the problem?\\n4.Why would you want to add a max pooling layer rather than a convolutional\\nlayer with the same stride?\\n5.When would you want to add a \\nlocal response normalization\\n layer?\\n6.Can you name the main innovations in AlexNet, compared to LeNet-5? What\\nabout the main innovations in GoogLeNet and ResNet?\\n7.Build your own CNN and try to achieve the highest possible accuracy on MNIST.\\n8.Classifying large images using Inception v3.\\na.Download some images of various animals. Load them in Python, for example\\nusing the matplotlib.image.mpimg.imread() function. Resize and/or cropthem to 299 ‰ 299 pixels, and ensure that they have just three channels (RGB),\\nwith no transparency channel.b.\\nDownload the latest pretrained Inception v3 model: the checkpoint is avail…\\nable at \\nhttps://goo.gl/nxSQvl\\n.c.Create the Inception v3 model by calling the \\ninception_v3() function, asshown below. This must be done within an argument scope created by the\\ninception_v3_arg_scope() function. Also, you must set \\nis_training=Falseand num_classes=1001 like so:from tensorflow.contrib.slim.nets import inceptionimport tensorflow.contrib.slim as slimX = tf.placeholder(tf.float32, shape=[None, 299, 299, 3])with slim.arg_scope(inception.inception_v3_arg_scope()):    logits, end_points = inception.inception_v3(                             X, num_classes=1001, is_training=False)predictions = end_points[\"Predictions\"]saver = tf.train.Saver()d.Open a session and use the Saver to restore the pretrained model checkpoint\\nyou downloaded earlier.\\ne.Run the model to classify the images you prepared. Display the top five pre…\\ndictions for each image, along with the estimated probability (the list of class\\nnames is available at \\nhttps://goo.gl/brXRtZ\\n). How accurate is the model?\\n9.Transfer learning for large image classification.\\nExercises | 377\\na.Create a training set containing at least 100 images per class. For example, you\\ncould classify your own pictures based on the location (beach, mountain, city,\\netc.), or alternatively you can just use an existing dataset, such as the \\nflowersdataset\\n or MIT‡s \\nplaces dataset\\n (requires registration, and it is huge).\\nb.\\nWrite a preprocessing step that will resize and crop the image to 299 ‰ 299,\\nwith some randomness for data augmentation.\\nc.Using the pretrained Inception v3 model from the previous exercise, freeze all\\nlayers up to the bottleneck layer (i.e., the last layer before the output layer),\\nand replace the output layer with the appropriate number of outputs for your\\nnew classification task (e.g., the flowers dataset has five mutually exclusive\\nclasses so the output layer must have five neurons and use the softmax activa…\\ntion function).d.Split your dataset into a training set and a test set. Train the model on the\\ntraining set and evaluate it on the test set.\\n10.Go through TensorFlow‡s \\nDeepDream tutorial. It is a fun way to familiarize your…\\nself with various ways of visualizing the patterns learned by a CNN, and to gener…\\nate art using Deep Learning.\\nSolutions to these exercises are available in \\nAppendix A\\n.378 | Chapter 13: Convolutional Neural Networks\\nCHAPTER 14Recurrent Neural NetworksThe batter hits the ball. You immediately start running, anticipating the ball‡s trajec…\\ntory. You track it and adapt your movements, and finally catch it (under a thunder of\\napplause). Predicting the future is what you do all the time, whether you are finishing\\na friend‡s sentence or anticipating the smell of coffee at breakfast. In this chapter, we\\nare going to discuss recurrent neural networks\\n (RNN), a class of nets that can predict\\nthe future (well, up to a point, of course). They can analyze \\ntime series\\n data such as\\nstock prices, and tell you when to buy or sell. In autonomous driving systems, they\\ncan anticipate car trajectories and help avoid accidents. More generally, they can work\\non sequences\\n of arbitrary lengths, rather than on fixed-sized inputs like all the nets we\\nhave discussed so far. For example, they can take sentences, documents, or audio\\nsamples as input, making them extremely useful for natural language processing\\n(NLP) systems such as automatic translation, speech-to-text, or \\nsentiment analysis\\n (e.g., reading movie reviews and extracting the rater‡s feeling about the movie).\\nMoreover, RNNs‡ ability to anticipate also makes them capable of surprising creativ…\\nity. You can ask them to predict which are the most likely next notes in a melody, then\\nrandomly pick one of these notes and play it. Then ask the net for the next most likely\\nnotes, play it, and repeat the process again and again. Before you know it, your net\\nwill compose a melody such as \\nthe one produced by Google‡s \\nMagenta project\\n. Simi…larly, RNNs can \\ngenerate sentences\\n, image captions\\n, and much more. The result is not\\nexactly Shakespeare or Mozart yet, but who knows what they will produce a few years\\nfrom now?In this chapter, we will look at the fundamental concepts underlying RNNs, the main\\nproblem they face (namely, vanishing/exploding gradients, discussed in \\nChapter 11\\n),and the solutions widely used to fight it: LSTM and GRU cells. Along the way, as\\nalways, we will show how to implement RNNs using TensorFlow. Finally, we will take\\na look at the architecture of a machine translation system.\\n379Recurrent NeuronsUp to now we have mostly looked at feedforward neural networks, where the activa…\\ntions flow only in one direction, from the input layer to the output layer (except for a\\nfew networks in Appendix E\\n). A recurrent neural network looks very much like a\\nfeedforward neural network, except it also has connections pointing backward. Let‡s\\nlook at the simplest possible RNN, composed of just one neuron receiving inputs,\\nproducing an output, and sending that output back to itself, as shown in \\nFigure 14-1(left). At each \\ntime step\\n t (also called a \\nframe\\n), this recurrent neuron\\n receives the inputs\\nx(t) as well as its own output from the previous time step, \\ny(t–1). We can represent this\\ntiny network against the time axis, as shown in \\nFigure 14-1 (right). This is called\\nunrolling the network through time\\n.Figure 14-1. A recurrent neuron \\n(le“), unrolled through time (right)\\nYou can easily create a layer of recurrent neurons. At each time step \\nt, every neuron\\nreceives both the input vector \\nx(t) and the output vector from the previous time stepy(t–1), as shown in Figure 14-2. Note that both the inputs and outputs are vectors now\\n(when there was just a single neuron, the output was a scalar).Figure 14-2. A layer of recurrent neurons \\n(le“), unrolled through time (right)\\nEach recurrent neuron has two sets of weights: one for the inputs \\nx(t) and the other forthe outputs of the previous time step, \\ny(t–1). Let‡s call these weight vectors \\nwx and wy.380 | Chapter 14: Recurrent Neural Networks\\n1Note that many researchers prefer to use the hyperbolic tangent (tanh) activation function in RNNs rather\\nthan the ReLU activation function. For example, take a look at by Vu Pham et al.‡s paper \\nƒDropout Improves\\nRecurrent Neural Networks for Handwriting Recognition⁄\\n. However, ReLU-based RNNs are also possible, as\\nshown in Quoc V. Le et al.‡s paper \\nƒA Simple Way to Initialize Recurrent Networks of Rectified Linear Units⁄\\n.The output of a single recurrent neuron can be computed pretty much as you might\\nexpect, as shown in Equation 14-1\\n (b is the bias term and Ì(’) is the activation func…\\ntion, e.g., ReLU\\n1).Equation 14-1. Output of a single recurrent neuron for a single instance\\nt=‚tT’x+t”1\\nT’y+bJust like for feedforward neural networks, we can compute a whole layer‡s output in\\none shot for a whole mini-batch using a vectorized form of the previous equation (see\\nEquation 14-2\\n).Equation 14-2. Outputs of a layer of recurrent neurons for all instances in a mini-\\nbatch\\nt=‚t’x+t”1\\n’y+=‚tt”1\\n’+with=xy‹Y(t) is an m ‰ nneurons matrix containing the layer‡s outputs at time step \\nt for eachinstance in the mini-batch (\\nm is the number of instances in the mini-batch and\\nnneurons is the number of neurons).\\n‹X(t) is an m ‰ ninputs\\n matrix containing the inputs for all instances (\\nninputs\\n is thenumber of input features).\\n‹Wx is an ninputs\\n ‰ nneurons matrix containing the connection weights for the inputs\\nof the current time step.\\n‹Wy is an nneurons ‰ nneurons matrix containing the connection weights for the out…\\nputs of the previous time step.\\n‹The weight matrices \\nWx and Wy are often concatenated into a single weight\\nmatrix \\nW of shape (\\nninputs\\n + nneurons) ‰ nneurons (see the second line of Equation\\n14-2).‹b is a vector of size nneurons containing each neuron‡s bias term.\\nRecurrent Neurons | 381\\nNotice that \\nY(t) is a function of X(t) and Y(t–1), which is a function of X(t–1) and Y(t–2),which is a function of X(t–2) and Y(t–3), and so on. This makes Y(t) a function of all theinputs since time \\nt = 0 (that is, \\nX(0), X(1), µ, X(t)). At the first time step, \\nt = 0, there areno previous outputs, so they are typically assumed to be all zeros.Memory CellsSince the output of a recurrent neuron at time step \\nt is a function of all the inputs\\nfrom previous time steps, you could say it has a form of \\nmemory\\n. A part of a neuralnetwork that preserves some state across time steps is called a \\nmemory cell\\n (or simply\\na cell\\n). A single recurrent neuron, or a layer of recurrent neurons, is a very \\nbasic cell\\n,but later in this chapter we will look at some more complex and powerful types of\\ncells.In general a cell‡s state at time step \\nt, denoted h(t) (the ƒh⁄ stands for ƒhidden⁄), is a\\nfunction of some inputs at that time step and its state at the previous time step: \\nh(t) =f(h(t–1), x(t)). Its output at time step \\nt, denoted y(t), is also a function of the previousstate and the current inputs. In the case of the basic cells we have discussed so far, the\\noutput is simply equal to the state, but in more complex cells this is not always the\\ncase, as shown in Figure 14-3.Figure 14-3. A cell‹s hidden state and its output may be \\ndi›erentInput and Output SequencesAn RNN can simultaneously take a sequence of inputs and produce a sequence of\\noutputs (see Figure 14-4, top-left network). For example, this type of network is use…\\nful for predicting time series such as stock prices: you feed it the prices over the last Ndays, and it must output the prices shifted by one day into the future (i.e., from \\nN – 1\\ndays ago to tomorrow).\\nAlternatively, you could feed the network a sequence of inputs, and ignore all outputs\\nexcept for the last one (see the top-right network). In other words, this is a sequence-\\nto-vector network. For example, you could feed the network a sequence of words cor…\\n382 | Chapter 14: Recurrent Neural Networks\\nresponding to a movie review, and the network would output a sentiment score (e.g.,\\nfrom –1 [hate] to +1 [love]).\\nConversely, you could feed the network a single input at the first time step (and zeros\\nfor all other time steps), and let it output a sequence (see the bottom-left network).This is a vector-to-sequence network. For example, the input could be an image, and\\nthe output could be a caption for that image.\\nLastly, you could have a sequence-to-vector network, called \\nan encoder\\n, followed by avector-to-sequence network, called a \\ndecoder\\n (see the bottom-right network). For\\nexample, this can be used for translating a sentence from one language to another.\\nYou would feed the network a sentence in one language, the encoder would convert\\nthis sentence into a single vector representation, and then the decoder would decode\\nthis vector into a sentence in another language. This two-step model, called an\\nEncoder–Decoder, works much better than trying to translate on the fly with a single\\nsequence-to-sequence RNN (like the one represented on the top left), since the last\\nwords of a sentence can affect the first words of the translation, so you need to wait\\nuntil you have heard the whole sentence before translating it.\\nFigure 14-4. Seq to seq (top \\nle“), seq to vector (top right), vector to seq (bottom \\nle“),delayed seq to seq (bottom right)\\nSounds promising, so let‡s start coding!\\nRecurrent Neurons | 383\\nBasic RNNs in TensorFlowFirst, let‡s implement a very simple RNN model, without using any of TensorFlow‡s\\nRNN operations, to better understand what goes on under the hood. We will create\\nan RNN composed of a layer of five recurrent neurons (like the RNN represented in\\nFigure 14-2), using the tanh activation function. We will assume that the RNN runs\\nover only two time steps, taking input vectors of size 3 at each time step. The follow…\\ning code builds this RNN, unrolled through two time steps:n_inputs = 3n_neurons = 5X0 = tf.placeholder(tf.float32, [None, n_inputs])X1 = tf.placeholder(tf.float32, [None, n_inputs])Wx = tf.Variable(tf.random_normal(shape=[n_inputs, n_neurons],dtype=tf.float32))Wy = tf.Variable(tf.random_normal(shape=[n_neurons,n_neurons],dtype=tf.float32))b = tf.Variable(tf.zeros([1, n_neurons], dtype=tf.float32))Y0 = tf.tanh(tf.matmul(X0, Wx) + b)Y1 = tf.tanh(tf.matmul(Y0, Wy) + tf.matmul(X1, Wx) + b)init = tf.global_variables_initializer()This network looks much like a two-layer feedforward neural network, with a few\\ntwists: first, the same weights and bias terms are shared by both layers, and second,\\nwe feed inputs at each layer, and we get outputs from each layer. To run the model, we\\nneed to feed it the inputs at both time steps, like so:\\nimport numpy as np# Mini-batch:        instance 0,instance 1,instance 2,instance 3X0_batch = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 0, 1]]) # t = 0X1_batch = np.array([[9, 8, 7], [0, 0, 0], [6, 5, 4], [3, 2, 1]]) # t = 1with tf.Session() as sess:    init.run()    Y0_val, Y1_val = sess.run([Y0, Y1], feed_dict={X0: X0_batch, X1: X1_batch})This mini-batch contains four instances, each with an input sequence composed of\\nexactly two inputs. At the end, \\nY0_val and \\nY1_val contain the outputs of the network\\nat both time steps for all neurons and all instances in the mini-batch:\\n>>> print(Y0_val)  # output at t = 0[[-0.2964572   0.82874775 -0.34216955 -0.75720584  0.19011548]  # instance 0 [-0.12842922  0.99981797  0.84704727 -0.99570125  0.38665548]  # instance 1 [ 0.04731077  0.99999976  0.99330056 -0.999933    0.55339795]  # instance 2 [ 0.70323634  0.99309105  0.99909431 -0.85363263  0.7472108 ]] # instance 3>>> print(Y1_val)  # output at t = 1[[ 0.51955646  1.          0.99999022 -0.99984968 -0.24616946]  # instance 0 [-0.70553327 -0.11918639  0.48885304  0.08917919 -0.26579669]  # instance 1384 | Chapter 14: Recurrent Neural Networks\\n [-0.32477224  0.99996376  0.99933046 -0.99711186  0.10981458]  # instance 2 [-0.43738723  0.91517633  0.97817528 -0.91763324  0.11047263]] # instance 3That wasn‡t too hard, but of course if you want to be able to run an RNN over 100\\ntime steps, the graph is going to be pretty big. Now let‡s look at how to create the\\nsame model using TensorFlow‡s RNN operations.\\nStatic Unrolling Through Time\\nThe static_rnn() function creates an unrolled RNN network by chaining cells. The\\nfollowing code creates the exact same model as the previous one:\\nX0 = tf.placeholder(tf.float32, [None, n_inputs])X1 = tf.placeholder(tf.float32, [None, n_inputs])basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)output_seqs, states = tf.contrib.rnn.static_rnn(                          basic_cell, [X0, X1], dtype=tf.float32)Y0, Y1 = output_seqsFirst we create the input placeholders, as before. Then we create a \\nBasicRNNCell,which you can think of as a factory that creates copies of the cell to build the unrolled\\nRNN (one for each time step). Then we call static_rnn(), giving it the cell factory\\nand the input tensors, and telling it the data type of the inputs (this is used to create\\nthe initial state matrix, which by default is full of zeros). The \\nstatic_rnn() functioncalls the cell factory‡s \\n__call__() function once per input, creating two copies of the\\ncell (each containing a layer of five recurrent neurons), with shared weights and bias\\nterms, and it chains them just like we did earlier. The \\nstatic_rnn() function returns\\ntwo objects. The first is a Python list containing the output tensors for each time step.\\nThe second is a tensor containing the final states of the network. When you are using\\nbasic cells, the final state is simply equal to the last output.\\nIf there were 50 time steps, it would not be very convenient to have to define 50 input\\nplaceholders and 50 output tensors. Moreover, at execution time you would have to\\nfeed each of the 50 placeholders and manipulate the 50 outputs. Let‡s simplify this.\\nThe following code builds the same RNN again, but this time it takes a single input\\nplaceholder of shape \\n[None, n_steps, n_inputs] where the first dimension is themini-batch size. Then it extracts the list of input sequences for each time step. \\nX_seqsis a Python list of n_steps tensors of shape \\n[None, n_inputs], where once again thefirst dimension is the mini-batch size. To do this, we first swap the first two dimen…\\nsions using the transpose() function, so that the time steps are now the first dimen…\\nsion. Then we extract a Python list of tensors along the first dimension (i.e., onetensor per time step) using the unstack() function. The next two lines are the sameas before. Finally, we merge all the output tensors into a single tensor using the\\nstack() function, and we swap the first two dimensions to get a final \\noutputs tensor\\nBasic RNNs in TensorFlow | 385\\nof shape \\n[None, n_steps, n_neurons] (again the first dimension is the mini-batch\\nsize).X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])X_seqs = tf.unstack(tf.transpose(X, perm=[1, 0, 2]))basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)output_seqs, states = tf.contrib.rnn.static_rnn(                          basic_cell, X_seqs, dtype=tf.float32)outputs = tf.transpose(tf.stack(output_seqs), perm=[1, 0, 2])Now we can run the network by feeding it a single tensor that contains all the mini-\\nbatch sequences:\\nX_batch = np.array([         # t = 0     t = 1        [[0, 1, 2], [9, 8, 7]], # instance 0        [[3, 4, 5], [0, 0, 0]], # instance 1        [[6, 7, 8], [6, 5, 4]], # instance 2        [[9, 0, 1], [3, 2, 1]], # instance 3    ])with tf.Session() as sess:    init.run()    outputs_val = outputs.eval(feed_dict={X: X_batch})And we get a single outputs_val tensor for all instances, all time steps, and all neu…rons:>>> print(outputs_val)[[[-0.2964572   0.82874775 -0.34216955 -0.75720584  0.19011548]  [ 0.51955646  1.          0.99999022 -0.99984968 -0.24616946]] [[-0.12842922  0.99981797  0.84704727 -0.99570125  0.38665548]  [-0.70553327 -0.11918639  0.48885304  0.08917919 -0.26579669]] [[ 0.04731077  0.99999976  0.99330056 -0.999933    0.55339795]  [-0.32477224  0.99996376  0.99933046 -0.99711186  0.10981458]] [[ 0.70323634  0.99309105  0.99909431 -0.85363263  0.7472108 ]  [-0.43738723  0.91517633  0.97817528 -0.91763324  0.11047263]]]However, this approach still builds a graph containing one cell per time step. If there\\nwere 50 time steps, the graph would look pretty ugly. It is a bit like writing a program\\nwithout ever using loops (e.g., Y0=f(0, X0); Y1=f(Y0, X1); Y2=f(Y1, X2); ...;Y50=f(Y49, X50)). With such as large graph, you may even get out-of-memory\\n(OOM) errors during backpropagation (especially with the limited memory of GPU\\ncards), since it must store all tensor values during the forward pass so it can use them\\nto compute gradients during the reverse pass.\\nFortunately, there is a better solution: the \\ndynamic_rnn() function.386 | Chapter 14: Recurrent Neural Networks\\nDynamic Unrolling Through Time\\nThe dynamic_rnn() function uses a while_loop() operation to run over the cell the\\nappropriate number of times, and you can set \\nswap_memory=True if you want it to\\nswap the GPU‡s memory to the CPU‡s memory during backpropagation to avoid\\nOOM errors. Conveniently, it also accepts a single tensor for all inputs at every time\\nstep (shape \\n[None, n_steps, n_inputs]) and it outputs a single tensor for all out…puts at every time step (shape \\n[None, n_steps, n_neurons]); there is no need tostack, unstack, or transpose. The following code creates the same RNN as earlier\\nusing the dynamic_rnn() function. It‡s so much nicer!\\nX = tf.placeholder(tf.float32, [None, n_steps, n_inputs])basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)During backpropagation, the \\nwhile_loop() operation does the\\nappropriate magic: it stores the tensor values for each iteration dur…\\ning the forward pass so it can use them to compute gradients dur…\\ning the reverse pass.Handling Variable Length Input SequencesSo far we have used only fixed-size input sequences (all exactly two steps long). What\\nif the input sequences have variable lengths (e.g., like sentences)? In this case you\\nshould set the sequence_length parameter when calling the dynamic_rnn() (orstatic_rnn()) function; it must be a 1D tensor indicating the length of the input\\nsequence for each instance. For example:\\nseq_length = tf.placeholder(tf.int32, [None])[...]outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32,                                    sequence_length=seq_length)For example, suppose the second input sequence contains only one input instead of\\ntwo. It must be padded with a zero vector in order to fit in the input tensor \\nX (because\\nthe input tensor‡s second dimension is the size of the longest sequence›i.e., 2).\\nX_batch = np.array([        # step 0     step 1        [[0, 1, 2], [9, 8, 7]], # instance 0        [[3, 4, 5], [0, 0, 0]], # instance 1 (padded with a zero vector)        [[6, 7, 8], [6, 5, 4]], # instance 2        [[9, 0, 1], [3, 2, 1]], # instance 3    ])seq_length_batch = np.array([2, 1, 2, 2])Basic RNNs in TensorFlow | 387\\nOf course, you now need to feed values for both placeholders X and seq_length:with tf.Session() as sess:    init.run()    outputs_val, states_val = sess.run(        [outputs, states], feed_dict={X: X_batch, seq_length: seq_length_batch})Now the RNN outputs zero vectors for every time step past the input sequence length\\n(look at the second instance‡s output for the second time step):\\n>>> print(outputs_val)[[[-0.2964572   0.82874775 -0.34216955 -0.75720584  0.19011548]  [ 0.51955646  1.          0.99999022 -0.99984968 -0.24616946]]  # final state [[-0.12842922  0.99981797  0.84704727 -0.99570125  0.38665548]   # final state  [ 0.          0.          0.          0.          0.        ]]  # zero vector [[ 0.04731077  0.99999976  0.99330056 -0.999933    0.55339795]  [-0.32477224  0.99996376  0.99933046 -0.99711186  0.10981458]]  # final state [[ 0.70323634  0.99309105  0.99909431 -0.85363263  0.7472108 ]  [-0.43738723  0.91517633  0.97817528 -0.91763324  0.11047263]]] # final stateMoreover, the \\nstates tensor contains the final state of each cell (excluding the zero\\nvectors):>>> print(states_val)[[ 0.51955646  1.          0.99999022 -0.99984968 -0.24616946]    # t = 1 [-0.12842922  0.99981797  0.84704727 -0.99570125  0.38665548]    # t = 0 !!! [-0.32477224  0.99996376  0.99933046 -0.99711186  0.10981458]    # t = 1 [-0.43738723  0.91517633  0.97817528 -0.91763324  0.11047263]]   # t = 1Handling Variable-Length Output SequencesWhat if the output sequences have variable lengths as well? If you know in advance\\nwhat length each sequence will have (for example if you know that it will be the same\\nlength as the input sequence), then you can set the \\nsequence_length parameter asdescribed above. Unfortunately, in general this will not be possible: for example, the\\nlength of a translated sentence is generally different from the length of the input sen…\\ntence. In this case, the most common solution is to define a special output called anend-of-sequence token\\n (EOS token). Any output past the EOS should be ignored (we\\nwill discuss this later in this chapter).\\nOkay, now you know how to build an RNN network (or more precisely an RNN net…\\nwork unrolled through time). But how do you train it?388 | Chapter 14: Recurrent Neural Networks\\nTraining RNNsTo train an RNN, the trick is to unroll it through time (like we just did) and then\\nsimply use regular backpropagation (see \\nFigure 14-5). This strategy is called \\nbackpro…\\npagation through time\\n (BPTT).\\nFigure 14-5. Backpropagation through time\\nJust like in regular backpropagation, there is a first forward pass through the unrolled\\nnetwork (represented by the dashed arrows); then the output sequence is evaluated\\nusing a cost function \\nCtmin,tmin+1\\n,,tmax (where \\ntmin and \\ntmax are the first\\nand last output time steps, not counting the ignored outputs), and the gradients of\\nthat cost function are propagated backward through the unrolled network (repre…\\nsented by the solid arrows); and finally the model parameters are updated using the\\ngradients computed during BPTT. Note that the gradients flow backward through all\\nthe outputs used by the cost function, not just through the final output (for example,\\nin Figure 14-5 the cost function is computed using the last three outputs of the net…\\nwork, Y(2), Y(3), and Y(4), so gradients flow through these three outputs, but not\\nthrough Y(0) and \\nY(1)). Moreover, since the same parameters \\nW and \\nb are used at each\\ntime step, backpropagation will do the right thing and sum over all time steps.\\nTraining a Sequence Classi•erLet‡s \\ntrain an RNN to classify MNIST images. A convolutional neural network would\\nbe better suited for image classification (see \\nChapter 13\\n), but this makes for a simple\\nexample that you are already familiar with. We will treat each image as a sequence of\\n28 rows of 28 pixels each (since each MNIST image is 28 ‰ 28 pixels). We will use\\ncells of 150 recurrent neurons, plus a fully connected layer containing 10 neurons\\nTraining RNNs | 389\\n(one per class) connected to the output of the last time step, followed by a softmax\\nlayer (see \\nFigure 14-6).Figure 14-6. Sequence \\nclassi†erThe construction phase is quite straightforward; it‡s pretty much the same as the\\nMNIST classifier we built in Chapter 10\\n except that an unrolled RNN replaces the\\nhidden layers. Note that the fully connected layer is connected to the \\nstates tensor,\\nwhich contains only the final state of the RNN (i.e., the 28\\nth output). Also note that \\nyis a placeholder for the target classes.from tensorflow.contrib.layers import fully_connectedn_steps = 28n_inputs = 28n_neurons = 150n_outputs = 10learning_rate = 0.001X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])y = tf.placeholder(tf.int32, [None])basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)logits = fully_connected(states, n_outputs, activation_fn=None)xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(               labels=y, logits=logits)loss = tf.reduce_mean(xentropy)optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)training_op = optimizer.minimize(loss)correct = tf.nn.in_top_k(logits, y, 1)accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))390 | Chapter 14: Recurrent Neural Networks\\ninit = tf.global_variables_initializer()Now let‡s load the MNIST data and reshape the test data to \\n[batch_size, n_steps,n_inputs] as is expected by the network. We will take care of reshaping the training\\ndata in a moment.\\nfrom tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets(\"/tmp/data/\")X_test = mnist.test.images.reshape((-1, n_steps, n_inputs))y_test = mnist.test.labelsNow we are ready to train the RNN. The execution phase is exactly the same as for\\nthe MNIST classifier in Chapter 10\\n, except that we reshape each training batch before\\nfeeding it to the network.n_epochs = 100batch_size = 150with tf.Session() as sess:    init.run()    for epoch in range(n_epochs):        for iteration in range(mnist.train.num_examples // batch_size):            X_batch, y_batch = mnist.train.next_batch(batch_size)            X_batch = X_batch.reshape((-1, n_steps, n_inputs))            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)The output should look like this:0 Train accuracy: 0.713333 Test accuracy: 0.72991 Train accuracy: 0.766667 Test accuracy: 0.7977...98 Train accuracy: 0.986667 Test accuracy: 0.977799 Train accuracy: 0.986667 Test accuracy: 0.9809We get over 98% accuracy›not bad! Plus you would certainly get a better result by\\ntuning the hyperparameters, initializing the RNN weights using He initialization,\\ntraining longer, or adding a bit of regularization (e.g., dropout).\\nYou can specify an initializer for the RNN by wrapping its \\nconstruction code in a variable scope (e.g., usevariable_scope(\"rnn\", initializer=variance_scaling_initializer()) to use He initialization).\\nTraining RNNs | 391\\nTraining to Predict Time Series\\nNow let‡s take a look at how to handle time series, such as stock prices, air tempera…\\nture, brain wave patterns, and so on. In this section we will train an RNN to predict\\nthe next value in a generated time series. Each training instance is a randomly\\nselected sequence of 20 consecutive values from the time series, and the targetsequence is the same as the input sequence, except it is shifted by one time step into\\nthe future (see Figure 14-7).Figure 14-7. Time series \\n(le“), and a training instance from that series (right)\\nFirst, let‡s create the RNN. It will contain 100 recurrent neurons and we will unroll it\\nover 20 time steps since each training instance will be 20 inputs long. Each input will\\ncontain only one feature (the value at that time). The targets are also sequences of 20\\ninputs, each containing a single value. The code is almost the same as earlier:\\nn_steps = 20n_inputs = 1n_neurons = 100n_outputs = 1X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu)outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)In general you would have more than just one input feature. For\\nexample, if you were trying to predict stock prices, you would\\nlikely have many other input features at each time step, such as pri…\\nces of competing stocks, ratings from analysts, or any other feature\\nthat might help the system make its predictions.\\nAt each time step we now have an output vector of size 100. But what we actually\\nwant is a single output value at each time step. The simplest solution is to wrap the\\ncell in an OutputProjectionWrapper. A cell wrapper acts like a normal cell, proxying\\n392 | Chapter 14: Recurrent Neural Networks\\nevery method call to an underlying cell, but it also adds some functionality. The \\nOutputProjectionWrapper adds a fully connected layer of linear neurons (i.e., without\\nany activation function) on top of each output (but it does not affect the cell state).\\nAll these fully connected layers share the same (trainable) weights and bias terms.\\nThe resulting RNN is represented in \\nFigure 14-8.Figure 14-8. RNN cells using output projections\\nWrapping a cell is quite easy. Let‡s tweak the preceding code by wrapping the\\nBasicRNNCell into an \\nOutputProjectionWrapper:cell = tf.contrib.rnn.OutputProjectionWrapper(    tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu),    output_size=n_outputs)So far, so good. Now we need to define the cost function. We will use the Mean\\nSquared Error (MSE), as we did in previous regression tasks. Next we will create an\\nAdam optimizer, the training op, and the variable initialization op, as usual:\\nlearning_rate = 0.001loss = tf.reduce_mean(tf.square(outputs - y))optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)training_op = optimizer.minimize(loss)init = tf.global_variables_initializer()Now on to the execution phase:\\nn_iterations = 10000batch_size = 50with tf.Session() as sess:Training RNNs | 393\\n    init.run()    for iteration in range(n_iterations):        X_batch, y_batch = [...]  # fetch the next training batch        sess.run(training_op, feed_dict={X: X_batch, y: y_batch})        if iteration % 100 == 0:            mse = loss.eval(feed_dict={X: X_batch, y: y_batch})            print(iteration, \"\\\\tMSE:\", mse)The program‡s output should look like this:\\n0    MSE: 379.586100  MSE: 14.58426200  MSE: 7.14066300  MSE: 3.98528400  MSE: 2.00254[...]Once the model is trained, you can make predictions:X_new = [...]  # New sequencesy_pred = sess.run(outputs, feed_dict={X: X_new})Figure 14-9 shows the predicted sequence for the instance we looked at earlier (in\\nFigure 14-7), after just 1,000 training iterations.\\nFigure 14-9. Time series prediction\\nAlthough using an OutputProjectionWrapper is the simplest solution to reduce the\\ndimensionality of the RNN‡s output sequences down to just one value per time step\\n(per instance), it is not the most efficient. There is a trickier but more efficient solu…\\ntion: you can reshape the RNN outputs from \\n[batch_size, n_steps, n_neurons]to [batch_size * n_steps, n_neurons], then apply a single fully connected layer\\nwith the appropriate output size (in our case just 1), which will result in an output\\ntensor of shape \\n[batch_size * n_steps, n_outputs], and then reshape this tensor\\n394 | Chapter 14: Recurrent Neural Networks\\nto [batch_size, n_steps, n_outputs]. These operations are represented in\\nFigure 14-10.Figure 14-10. Stack all the outputs, apply the projection, then unstack the result\\nTo implement this solution, we first revert to a basic cell, without the \\nOutputProjectionWrapper:cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu)rnn_outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)Then we stack all the outputs using the reshape() operation, apply the fully connec…\\nted linear layer (without using any activation function; this is just a projection), and\\nfinally unstack all the outputs, again using reshape():stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons])stacked_outputs = fully_connected(stacked_rnn_outputs, n_outputs,                                  activation_fn=None)outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])Training RNNs | 395\\nThe rest of the code is the same as earlier. This can provide a significant speed boost\\nsince there is just one fully connected layer instead of one per time step.\\nCreative RNNNow that we have a model that can predict the future, we can use it to generate some\\ncreative sequences, as explained at the beginning of the chapter. All we need is to pro…\\nvide it a seed sequence containing \\nn_steps values (e.g., full of zeros), use the model to\\npredict the next value, append this predicted value to the sequence, feed the last\\nn_steps values to the model to predict the next value, and so on. This process gener…ates a new sequence that has some resemblance to the original time series (see\\nFigure 14-11).sequence = [0.] * n_stepsfor iteration in range(300):    X_batch = np.array(sequence[-n_steps:]).reshape(1, n_steps, 1)    y_pred = sess.run(outputs, feed_dict={X: X_batch})    sequence.append(y_pred[0, -1, 0])Figure 14-11. Creative sequences, seeded with zeros \\n(le“) or with an instance (right)\\nNow you can try to feed all your John Lennon albums to an RNN and see if it can\\ngenerate the next ƒImagine.⁄ However, you will probably need a much more powerful\\nRNN, with more neurons, and also much deeper. Let‡s look at deep RNNs now.\\nDeep RNNsIt is quite common to stack multiple layers of cells, as shown in \\nFigure 14-12. Thisgives you a deep RNN\\n.396 | Chapter 14: Recurrent Neural Networks\\nFigure 14-12. Deep RNN \\n(le“), unrolled through time (right)\\nTo implement a deep RNN in TensorFlow, you can create several cells and stack them\\ninto a \\nMultiRNNCell. In the following code we stack three identical cells (but you\\ncould very well use various kinds of cells with a different number of neurons):\\nn_neurons = 100n_layers = 3basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)multi_layer_cell = tf.contrib.rnn.MultiRNNCell([basic_cell] * n_layers)outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)That‡s all there is to it! The \\nstates variable is a tuple containing one tensor per layer,\\neach representing the final state of that layer‡s cell (with shape \\n[batch_size, n_neurons]). If you set state_is_tuple=False when creating the \\nMultiRNNCell, thenstates becomes a single tensor containing the states from every layer, concatenated\\nalong the column axis (i.e., its shape is \\n[batch_size, n_layers * n_neurons]).Note that before TensorFlow 0.11.0, this behavior was the default.\\nDistributing a Deep RNN Across Multiple GPUsChapter 12\\n pointed out that we can efficiently distribute deep RNNs across multiple\\nGPUs by pinning each layer to a different GPU (see \\nFigure 12-16). However, if you\\ntry to create each cell in a different \\ndevice() block, it will not work:with tf.device(\"/gpu:0\"):  # BAD! This is ignored.    layer1 = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)with tf.device(\"/gpu:1\"):  # BAD! Ignored again.    layer2 = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)This fails because a \\nBasicRNNCell is \\na cell factory, not a cell \\nper se\\n (as mentioned ear…\\nlier); no cells get created when you create the factory, and thus no variables do either.\\nDeep RNNs | 397\\n2This uses the decorator\\n design pattern.\\nThe device block is simply ignored. The cells actually get created later. When you call\\ndynamic_rnn(), it calls the \\nMultiRNNCell, which calls each individual BasicRNNCell, which create the actual cells (including their variables). Unfortunately, none of these\\nclasses provide any way to control the devices on which the variables get created. If\\nyou try to put the \\ndynamic_rnn() call within a device block, the whole RNN gets pin…ned to a single device. So are you stuck? Fortunately not! The trick is to create your\\nown cell wrapper:\\nimport tensorflow as tfclass DeviceCellWrapper(tf.contrib.rnn.RNNCell):  def __init__(self, device, cell):    self._cell = cell    self._device = device  @property  def state_size(self):    return self._cell.state_size  @property  def output_size(self):    return self._cell.output_size  def __call__(self, inputs, state, scope=None):    with tf.device(self._device):        return self._cell(inputs, state, scope)This wrapper simply proxies every method call to another cell, except it wraps the\\n__call__() function within a device block.2 Now you can distribute each layer on a\\ndifferent GPU:\\ndevices = [\"/gpu:0\", \"/gpu:1\", \"/gpu:2\"]cells = [DeviceCellWrapper(dev,tf.contrib.rnn.BasicRNNCell(num_units=n_neurons))         for dev in devices]multi_layer_cell = tf.contrib.rnn.MultiRNNCell(cells)outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)Do not set state_is_tuple=False, or the MultiRNNCell will con…catenate all the cell states into a single tensor, on a single GPU.\\n398 | Chapter 14: Recurrent Neural Networks\\nApplying DropoutIf you build a very deep RNN, it may end up overfitting the training set. To prevent\\nthat, a common technique is to apply dropout (introduced in \\nChapter 11\\n). You can\\nsimply add a dropout layer before or after the RNN as usual, but if you also want to\\napply dropout between the RNN layers, you need to use a \\nDropoutWrapper. The fol…lowing code applies dropout to the inputs of each layer in the RNN, dropping each\\ninput with a 50% probability:\\nkeep_prob = 0.5cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)cell_drop = tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=keep_prob)multi_layer_cell = tf.contrib.rnn.MultiRNNCell([cell_drop] * n_layers)rnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)Note that it is also possible to apply dropout to the outputs by setting \\noutput_keep_prob.The main problem with this code is that it will apply dropout not only during train…\\ning but also during testing, which is not what you want (recall that dropout should be\\napplied only during training). Unfortunately, the \\nDropoutWrapper does not supportan is_training placeholder (yet?), so you must either write your own dropout wrap…\\nper class, or have two different graphs: one for training, and the other for testing. The\\nsecond option looks like this:import sysis_training = (sys.argv[-1] == \"train\")X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)if is_training:    cell = tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=keep_prob)multi_layer_cell = tf.contrib.rnn.MultiRNNCell([cell] * n_layers)rnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)[...] # build the rest of the graphinit = tf.global_variables_initializer()saver = tf.train.Saver()with tf.Session() as sess:    if is_training:        init.run()        for iteration in range(n_iterations):            [...] # train the model        save_path = saver.save(sess, \"/tmp/my_model.ckpt\")    else:        saver.restore(sess, \"/tmp/my_model.ckpt\")        [...] # use the modelDeep RNNs | 399\\nWith that you should be able to train all sorts of RNNs! Unfortunately, if you want to\\ntrain an RNN on long sequences, things will get a bit harder. Let‡s see why and what\\nyou can do about it.The Di…culty of Training over Many Time Steps\\nTo train an RNN on long sequences, you will need to run it over many time steps,\\nmaking the unrolled RNN a very deep network. Just like any deep neural network it\\nmay suffer from the vanishing/exploding gradients problem (discussed in \\nChap…\\nter 11) and take forever to train. Many of the tricks we discussed to alleviate this\\nproblem can be used for deep unrolled RNNs as well: good parameter initialization,\\nnonsaturating activation functions (e.g., ReLU), Batch Normalization, Gradient Clip…\\nping, and faster optimizers. However, if the RNN needs to handle even moderately\\nlong sequences (e.g., 100 inputs), then training will still be very slow.\\nThe simplest and most common solution to this problem is to unroll the RNN only\\nover a limited number of time steps during training. This is called \\ntruncated backpro…\\npagation through time\\n. In TensorFlow you can implement it simply by truncating the\\ninput sequences. For example, in the time series prediction problem, you would sim…\\nply reduce n_steps during training. The problem, of course, is that the model will\\nnot be able to learn long-term patterns. One workaround could be to make sure that\\nthese shortened sequences contain both old and recent data, so that the model can\\nlearn to use both (e.g., the sequence could contain monthly data for the last five\\nmonths, then weekly data for the last five weeks, then daily data over the last five\\ndays). But this workaround has its limits: what if fine-grained data from last year is\\nactually useful? What if there was a brief but significant event that absolutely must be\\ntaken into account, even years later (e.g., the result of an election)?\\nBesides the long training time, a second problem faced by long-running RNNs is the\\nfact that the memory of the first inputs gradually fades away. Indeed, due to the trans…\\nformations that the data goes through when traversing an RNN, some information is\\nlost after each time step. After a while, the RNN‡s state contains virtually no trace of\\nthe first inputs. This can be a showstopper. For example, say you want to perform\\nsentiment analysis on a long review that starts with the four words ƒI loved this\\nmovie,⁄ but the rest of the review lists the many things that could have made the\\nmovie even better. If the RNN gradually forgets the first four words, it will completely\\nmisinterpret the review. To solve this problem, various types of cells with long-term\\nmemory have been introduced. They have proved so successful that the basic cells are\\nnot much used anymore. Let‡s first look at the most popular of these long memory\\ncells: the LSTM cell.400 | Chapter 14: Recurrent Neural Networks\\n3ƒLong Short-Term Memory,⁄ S. Hochreiter and J. Schmidhuber (1997).\\n4ƒLong Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling,⁄ H.\\nSak et al. (2014).5ƒRecurrent Neural Network Regularization,⁄ W. Zaremba et al. (2015).\\nLSTM CellThe Long Short-Term Memory\\n (LSTM) cell was \\nproposed in 19973 by Sepp Hochreiter\\nand J™rgen Schmidhuber, and it was gradually improved over the years by several\\nresearchers, such as Alex Graves, \\nHaÑim Sak\\n,4 Wojciech Zaremba\\n,5 and many more. If\\nyou consider the LSTM cell as a black box, it can be used very much like a basic cell,\\nexcept it will perform much better; training will converge faster and it will detect\\nlong-term dependencies in the data. In TensorFlow, you can simply use a \\nBasicLSTMCell instead of a BasicRNNCell:lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=n_neurons)LSTM cells manage two state vectors, and for performance reasons they are kept\\nseparate\\n by default. You can change this default behavior by setting\\nstate_is_tuple=False when creating the \\nBasicLSTMCell.So how does an LSTM cell work? The architecture of a basic LSTM cell is shown inFigure 14-13.Figure 14-13. LSTM cell\\nLSTM Cell | 401\\nIf you don‡t look at what‡s inside the box, the LSTM cell looks exactly like a regular\\ncell, except that its state is split in two vectors: \\nh(t) and c(t) (ƒc⁄ stands for ƒcell⁄). You\\ncan think of h(t) as the short-term state and \\nc(t) as the long-term state.\\nNow let‡s open the box! The key idea is that the network can learn what to store in the\\nlong-term state, what to throw away, and what to read from it. As the long-term state\\nc(t–1) traverses the network from left to right, you can see that it first goes through \\naforget gate\\n, dropping some memories, and then it adds some new memories via theaddition operation (which adds the memories that were selected by an \\ninput gate\\n).The result c(t) is sent straight out, without any further transformation. So, at each time\\nstep, some memories are dropped and some memories are added. Moreover, after the\\naddition operation, the long-term state is copied and passed through the tanh func…\\ntion, and then the result is filtered by the output gate\\n. This produces the short-termstate \\nh(t) (which is equal to the cell‡s output for this time step \\ny(t)). Now let‡s look at\\nwhere new memories come from and how the gates work.\\nFirst, the current input vector \\nx(t) and the previous short-term state \\nh(t–1) are fed tofour different fully connected layers. They all serve a different purpose:\\n‹The main layer is the one that outputs \\ng(t). It has the usual role of analyzing the\\ncurrent inputs \\nx(t) and the previous (short-term) state \\nh(t–1). In a basic cell, there isnothing else than this layer, and its output goes straight out to \\ny(t) and \\nh(t). In con…trast, in an LSTM cell this layer‡s output does not go straight out, but instead it is\\npartially stored in the long-term state.\\n‹The three other layers are \\ngate controllers\\n. Since they use the logistic activation\\nfunction, their outputs range from 0 to 1. As you can see, their outputs are fed toelement-wise multiplication operations, so if they output 0s, they close the gate,\\nand if they output 1s, they open it. Specifically:\\n›The forget gate\\n (controlled by \\nf(t)) controls which parts of the long-term state\\nshould be erased.›The input gate\\n (controlled by \\ni(t)) controls which parts of \\ng(t) should be addedto the long-term state (this is why we said it was only ƒpartially stored⁄).\\n›Finally, the \\noutput gate\\n (controlled by \\no(t)) controls which parts of the long-\\nterm state should be read and output at this time step (both to \\nh(t)) and y(t).In short, an LSTM cell can learn to recognize an important input (that‡s the role of the\\ninput gate), store it in the long-term state, learn to preserve it for as long as it is\\nneeded (that‡s the role of the forget gate), and learn to extract it whenever it is needed.\\nThis explains why they have been amazingly successful at capturing long-term pat…\\nterns in time series, long texts, audio recordings, and more.\\n402 | Chapter 14: Recurrent Neural Networks\\n6ƒRecurrent Nets that Time and Count,⁄ F. Gers and J. Schmidhuber (2000).\\nEquation 14-3\\n summarizes how to compute the cell‡s long-term state, its short-term\\nstate, and its output at each time step for a single instance (the equations for a whole\\nmini-batch are very similar).\\nEquation 14-3. LSTM computations\\nt=„xiT’t+hiT’t”1\\n+it=„xf\\nT’t+hf\\nT’t”1\\n+ft=„xoT’t+hoT’t”1\\n+ot=tanh\\nxg\\nT’t+hg\\nT’t”1\\n+gt=t\\nt”1\\n+t\\ntt=t=ttanht‹Wxi, Wxf, Wxo\\n, Wxg are the weight matrices of each of the four layers for their con…\\nnection to the input vector \\nx(t).‹Whi\\n, Whf\\n, Who\\n, and Whg\\n are the weight matrices of each of the four layers for their\\nconnection to the previous short-term state \\nh(t–1).‹bi, bf, bo, and bg are the bias terms for each of the four layers. Note that Tensor…\\nFlow initializes bf to a vector full of 1s instead of 0s. This prevents forgetting\\neverything at the beginning of training.\\nPeephole ConnectionsIn a basic LSTM cell, the gate controllers can look only at the input \\nx(t) and the previ…ous short-term state \\nh(t–1). It may be a good idea to give them a bit more context by\\nletting them peek at the long-term state as well. This idea was \\nproposed by Felix Gers\\nand J™rgen Schmidhuber in 2000\\n.6 They proposed an LSTM variant with extra con…\\nnections called peephole connections\\n: the previous long-term state \\nc(t–1) is added as aninput to the controllers of the forget gate and the input gate, and the current long-\\nterm state \\nc(t) is added as input to the controller of the output gate.\\nTo implement peephole connections in TensorFlow, you must use the \\nLSTMCellinstead of the BasicLSTMCell and set use_peepholes=True:lstm_cell = tf.contrib.rnn.LSTMCell(num_units=n_neurons, use_peepholes=True)LSTM Cell | 403\\n7ƒLearning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation,⁄ K. Cho\\net al. (2014).8A 2015 paper by Klaus Greff et al., \\nƒLSTM: A Search Space Odyssey,⁄\\n seems to show that all LSTM variants\\nperform roughly the same.There are many other variants of the LSTM cell. One particularly popular variant is\\nthe GRU cell, which we will look at now.\\nGRU CellThe Gated Recurrent Unit\\n (GRU) \\ncell (see Figure 14-14) was proposed by Kyunghyun\\nCho et al. in a 2014 paper\\n7 that also introduced the Encoder–Decoder network we\\nmentioned earlier.\\nFigure 14-14. GRU cell\\nThe GRU cell is a simplified version of the LSTM cell, and it seems to perform just as\\nwell8 (which explains its growing popularity). The main simplifications are:\\n‹Both state vectors are merged into a single vector \\nh(t).‹A single gate controller controls both the forget gate and the input gate. If the\\ngate controller outputs a 1, the input gate is open and the forget gate is closed. If\\n404 | Chapter 14: Recurrent Neural Networks\\nit outputs a 0, the opposite happens. In other words, whenever a memory must\\nbe stored, the location where it will be stored is erased first. This is actually a fre…\\nquent variant to the LSTM cell in and of itself.\\n‹There is no output gate; the full state vector is output at every time step. How…\\never, there is a new gate controller that controls which part of the previous state\\nwill be shown to the main layer.\\nEquation 14-4\\n summarizes how to compute the cell‡s state at each time step for a sin…\\ngle instance.Equation 14-4. GRU computations\\nt=„xzT’t+hzT’t”1\\nt=„xrT’t+hrT’t”1\\nt=tanh\\nxg\\nT’t+hg\\nT’t\\nt”1\\nt=1”\\nttanhxg\\nT’t”1\\n+t\\ntCreating a GRU cell in TensorFlow is trivial:\\ngru_cell = tf.contrib.rnn.GRUCell(num_units=n_neurons)LSTM or GRU cells \\nare one of the main reasons behind the success of RNNs in recent\\nyears, in particular for applications in \\nnatural language processing\\n (NLP).Natural Language ProcessingMost \\nof the state-of-the-art NLP applications, such as machine translation, automatic\\nsummarization, parsing, sentiment analysis, and more, are now based (at least in\\npart) on RNNs. In this last section, we will take a quick look at what a machine trans…\\nlation model looks like. This topic is very well covered by \\nTensorFlow‡s awesome\\nWord2Vec\\n and Seq2Seq tutorials, so you should definitely check them out.Word EmbeddingsBefore we start, we need to choose a word representation. One option could be to\\nrepresent each word using a one-hot vector. Suppose your vocabulary contains\\n50,000 words, then the nth word would be represented as a 50,000-dimensional vector,\\nfull of 0s except for a 1 at the n\\nth position. However, with such a large vocabulary, this\\nsparse representation would not be efficient at all. Ideally, you want similar words to\\nhave similar representations, making it easy for the model to generalize what it learns\\nabout a word to all similar words. For example, if the model is told that ƒI drink milk⁄\\nis a valid sentence, and if it knows that ƒmilk⁄ is close to ƒwater⁄ but far from ƒshoes,⁄\\nNatural Language Processing | 405\\n9For more details, check out Christopher Olah‡s \\ngreat post\\n, or Sebastian Ruder‡s \\nseries of posts.then it will know that ƒI drink water⁄ is probably a valid sentence as well, while ƒI\\ndrink shoes⁄ is probably not. But how can you come up with such a meaningful rep…\\nresentation?\\nThe most common solution is to represent each word in the vocabulary using a fairly\\nsmall and dense vector (e.g., 150 dimensions), called an embedding\\n, and just let theneural network learn a good embedding for each word during training. At the begin…\\nning of training, embeddings are simply chosen randomly, but during training, back…\\npropagation automatically moves the embeddings around in a way that helps the\\nneural network perform its task. Typically this means that similar words will gradu…\\nally cluster close to one another, and even end up organized in a rather meaningful\\nway. For example, embeddings may end up placed along various axes that represent\\ngender, singular/plural, adjective/noun, and so on. The result can be truly amazing.\\n9In TensorFlow, you first need to create the variable representing the embeddings for\\nevery word in your vocabulary (initialized randomly):\\nvocabulary_size = 50000embedding_size = 150embeddings = tf.Variable(    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))Now suppose you want to feed the sentence ƒI drink milk⁄ to your neural network.\\nYou should first preprocess the sentence and break it into a list of known words. For\\nexample you may remove unnecessary characters, replace unknown words by a pre…\\ndefined token word such as ƒ[UNK]⁄, replace numerical values by ƒ[NUM]⁄, replace\\nURLs by ƒ[URL]⁄, and so on. Once you have a list of known words, you can look up\\neach word‡s integer identifier (from 0 to 49999) in a dictionary, for example [72, 3335,\\n288]. At that point, you are ready to feed these word identifiers to TensorFlow using a\\nplaceholder, and apply the \\nembedding_lookup() function to get the correspondingembeddings:train_inputs = tf.placeholder(tf.int32, shape=[None])  # from ids...embed = tf.nn.embedding_lookup(embeddings, train_inputs)  # ...to embeddingsOnce your model has learned good word embeddings, they can actually be reusedfairly efficiently in any NLP application: after all, ƒmilk⁄ is still close to ƒwater⁄ and far\\nfrom ƒshoes⁄ no matter what your application is. In fact, instead of training your own\\nword embeddings, you may want to download pretrained word embeddings. Just like\\nwhen reusing pretrained layers (see \\nChapter 11\\n), you can choose to freeze the pre…trained embeddings (e.g., creating the \\nembeddings variable using trainable=False)or let backpropagation tweak them for your application. The first option will speed\\nup training, but the second may lead to slightly higher performance.\\n406 | Chapter 14: Recurrent Neural Networks\\n10ƒSequence to Sequence learning with Neural Networks,⁄ I. Sutskever et al. (2014).\\nEmbeddings are also useful for representing categorical attributes\\nthat can take on a large number of different values, especially when\\nthere are complex similarities between values. For example, con…\\nsider professions, hobbies, dishes, species, brands, and so on.You now have almost all the tools you need to implement a machine translation sys…\\ntem. Let‡s look at this now.\\nAn Encoder—Decoder Network for Machine TranslationLet‡s take a look at a \\nsimple machine translation model\\n10 that will translate English\\nsentences to French (see \\nFigure 14-15).Figure 14-15. A simple machine translation model\\nThe English sentences are fed to the encoder, and the decoder outputs the French\\ntranslations. Note that the French translations are also used as inputs to the decoder,\\nbut pushed back by one step. In other words, the decoder is given as input the word\\nthat it \\nshould\\n have output at the previous step (regardless of what it actually output).\\nFor the very first word, it is given a token that represents the beginning of the sen…\\nNatural Language Processing | 407\\ntence (e.g., ƒ<go>⁄). The decoder is expected to end the sentence with an end-of-\\nsequence (EOS) token (e.g., ƒ<eos>⁄).Note that the English sentences are reversed before they are fed to the encoder. For\\nexample ƒI drink milk⁄ is reversed to ƒmilk drink I.⁄ This ensures that the beginning\\nof the English sentence will be fed last to the encoder, which is useful because that‡s\\ngenerally the first thing that the decoder needs to translate.\\nEach word is initially represented by a simple integer identifier (e.g., 288 for the word\\nƒmilk⁄). Next, an embedding lookup returns the word embedding (as explained ear…\\nlier, this is a dense, fairly low-dimensional vector). These word embeddings are what\\nis actually fed to the encoder and the decoder.\\nAt each step, the decoder outputs a score for each word in the output vocabulary (i.e.,\\nFrench), and then the Softmax layer turns these scores into probabilities. For exam…\\nple, at the first step the word ƒJe⁄ may have a probability of 20%, ƒTu⁄ may have a\\nprobability of 1%, and so on. The word with the highest probability is output. This isvery much like a regular classification task, so you can train the model using the \\nsoftmax_cross_entropy_with_logits() function.Note that at inference time (after training), you will not have the target sentence to\\nfeed to the decoder. Instead, simply feed the decoder the word that it output at the\\nprevious step, as shown in \\nFigure 14-16 (this will require an embedding lookup that\\nis not shown on the diagram).Figure 14-16. Feeding the previous output word as input at inference time\\nOkay, now you have the big picture. However, if you go through TensorFlow‡s\\nsequence-to-sequence tutorial and you look at the code in \\nrnn/translate/\\nseq2seq_model.py\\n (in the TensorFlow models\\n), you will notice a few important differ…\\nences:408 | Chapter 14: Recurrent Neural Networks\\n11The bucket sizes used in the tutorial are different.\\n12ƒOn Using Very Large Target Vocabulary for Neural Machine Translation,⁄ S. Jean et al. (2015).\\n13ƒNeural Machine Translation by Jointly Learning to Align and Translate,⁄ D. Bahdanau et al. (2014).\\n14ƒLong Short-Term Memory-Networks for Machine Reading,⁄ J. Cheng (2016).\\n15ƒShow, Attend and Tell: Neural Image Caption Generation with Visual Attention,⁄ K. Xu et al. (2015).\\n‹First, so far we have assumed that all input sequences (to the encoder and to the\\ndecoder) have a constant length. But obviously sentence lengths may vary. There\\nare several ways that this can be handled›for example, using the\\nsequence_length argument to the \\nstatic_rnn() or dynamic_rnn() functions tospecify each sentence‡s length (as discussed earlier). However, another approach\\nis used in the tutorial (presumably for performance reasons): sentences are grou…\\nped into buckets of similar lengths (e.g., a bucket for the 1- to 6-word sentences,\\nanother for the 7- to 12-word sentences, and so on\\n11), and the shorter sentences\\nare padded using a special padding token (e.g., ƒ<pad>⁄). For example ƒI drink\\nmilk⁄ becomes ƒ<pad> <pad> <pad> milk drink I⁄, and its translation becomes\\nƒJe bois du lait <eos> <pad>⁄. Of course, we want to ignore any output past the\\nEOS token. For this, the tutorial‡s implementation uses a \\ntarget_weights vector.\\nFor example, for the target sentence ƒJe bois du lait <eos> <pad>⁄, the weights\\nwould be set to [1.0, 1.0, 1.0, 1.0, 1.0, 0.0] (notice the weight 0.0 that\\ncorresponds to the padding token in the target sentence). Simply multiplying the\\nlosses by the target weights will zero out the losses that correspond to words past\\nEOS tokens.‹Second, when the output vocabulary is large (which is the case here), outputting\\na probability for each and every possible word would be terribly slow. If the tar…\\nget vocabulary contains, say, 50,000 French words, then the decoder would out…\\nput 50,000-dimensional vectors, and then computing the softmax function over\\nsuch a large vector would be very computationally intensive. To avoid this, one\\nsolution is to let the decoder output much smaller vectors, such as 1,000-\\ndimensional vectors, then use a sampling technique to estimate the loss without\\nhaving to compute it over every single word in the target vocabulary. This \\nSam…\\npled \\nSo“max technique was introduced in 2015 by S•bastien Jean et al\\n.12 In Ten…\\nsorFlow you can use the sampled_softmax_loss() function.‹Third, the tutorial‡s implementation uses an \\nattention mechanism\\n that lets the\\ndecoder peek into the input sequence. Attention augmented RNNs are beyond\\nthe scope of this book, but if you are interested there are helpful papers about\\nmachine translation\\n,13 machine reading,14 and image captions\\n15 using attention.\\n‹Finally, the tutorial‡s implementation makes use of the \\ntf.nn.legacy_seq2seqmodule, which provides tools to build various Encoder–Decoder models easily.\\nNatural Language Processing | 409\\nFor example, the \\nembedding_rnn_seq2seq() function creates a simple Encoder–\\nDecoder model that automatically takes care of word embeddings for you, just\\nlike the one represented in \\nFigure 14-15. This code will likely be updated quickly\\nto use the new tf.nn.seq2seq module.You now have all the tools you need to understand the sequence-to-sequence tutor…\\nial‡s implementation. Check it out and train your own English-to-French translator!\\nExercises1.Can you think of a few applications for a sequence-to-sequence RNN? What\\nabout a sequence-to-vector RNN? And a vector-to-sequence RNN?\\n2.Why do people use encoder–decoder RNNs rather than plain sequence-to-\\nsequence RNNs for automatic translation?\\n3.How could you combine a convolutional neural network with an RNN to classify\\nvideos?4.What are the advantages of building an RNN using \\ndynamic_rnn() rather than\\nstatic_rnn()?5.How can you deal with variable-length input sequences? What about variable-\\nlength output sequences?6.What is a common way to distribute training and execution of a deep RNN\\nacross multiple GPUs?\\n7.Embedded Reber grammars\\n were used by Hochreiter and Schmidhuber in their\\npaper about LSTMs. They are artificial grammars that produce strings such as\\nƒBPBTSXXVPSEPE.⁄ Check out Jenny Orr‡s \\nnice introduction\\n to this topic.Choose a particular embedded Reber grammar (such as the one represented on\\nJenny Orr‡s page), then train an RNN to identify whether a string respects that\\ngrammar or not. You will first need to write a function capable of generating a\\ntraining batch containing about 50% strings that respect the grammar, and 50%\\nthat don‡t.\\n8.Tackle the ƒHow much did it rain? II⁄ \\nKaggle competition\\n. This is a time seriesprediction task: you are given snapshots of polarimetric radar values and asked to\\npredict the hourly rain gauge total. Luis Andre Dutra e Silva‡s \\ninterview\\n givessome interesting insights into the techniques he used to reach second place in the\\ncompetition. In particular, he used an RNN composed of two LSTM layers.\\n9.Go through TensorFlow‡s \\nWord2Vec\\n tutorial to create word embeddings, and\\nthen go through the Seq2Seq tutorial to train an English-to-French translation\\nsystem.Solutions to these exercises are available in \\nAppendix A\\n.410 | Chapter 14: Recurrent Neural Networks\\nCHAPTER 15AutoencodersAutoencoders are artificial neural networks capable of learning efficient representa…\\ntions of the input data, called \\ncodings\\n, without any supervision (i.e., the training set is\\nunlabeled). These codings typically have a much lower dimensionality than the input\\ndata, making autoencoders useful for dimensionality reduction (see \\nChapter 8\\n). More\\nimportantly, autoencoders act as powerful feature detectors, and they can be used for\\nunsupervised pretraining of deep neural networks (as we discussed in \\nChapter 11\\n).Lastly, they are capable of randomly generating new data that looks very similar to the\\ntraining data; this is called a \\ngenerative model\\n. For example, you could train an\\nautoencoder on pictures of faces, and it would then be able to generate new faces.\\nSurprisingly, autoencoders work by simply learning to copy their inputs to their out…\\nputs. This may sound like a trivial task, but we will see that constraining the network\\nin various ways can make it rather difficult. For example, you can limit the size of the\\ninternal representation, or you can add noise to the inputs and train the network to\\nrecover the original inputs. These constraints prevent the autoencoder from trivially\\ncopying the inputs directly to the outputs, which forces it to learn efficient ways of\\nrepresenting the data. In short, the codings are byproducts of the autoencoder‡s\\nattempt to learn the identity function under some constraints.\\nIn this chapter we will explain in more depth how autoencoders work, what types of\\nconstraints can be imposed, and how to implement them using TensorFlow, whether\\nit is for dimensionality reduction, feature extraction, unsupervised pretraining, or as\\ngenerative models.\\n4111ƒPerception in chess,⁄ W. Chase and H. Simon (1973).\\nE…cient Data RepresentationsWhich of the following number sequences do you find the easiest to memorize?\\n‹40, 27, 25, 36, 81, 57, 10, 73, 19, 68‹50, 25, 76, 38, 19, 58, 29, 88, 44, 22, 11, 34, 17, 52, 26, 13, 40, 20At first glance, it would seem that the first sequence should be easier, since it is much\\nshorter. However, if you look carefully at the second sequence, you may notice that it\\nfollows two simple rules: even numbers are followed by their half, and odd numbers\\nare followed by their triple plus one (this is a famous sequence known as the hailstone\\nsequence\\n). Once you notice this pattern, the second sequence becomes much easier to\\nmemorize than the first because you only need to memorize the two rules, the first\\nnumber, and the length of the sequence. Note that if you could quickly and easily\\nmemorize very long sequences, you would not care much about the existence of a\\npattern in the second sequence. You would just learn every number by heart, and that\\nwould be that. It is the fact that it is hard to memorize long sequences that makes it\\nuseful to recognize patterns, and hopefully this clarifies why constraining an autoen…\\ncoder during training pushes it to discover and exploit patterns in the data.\\nThe relationship between memory, perception, and pattern matching was \\nfamouslystudied by William Chase and Herbert Simon in the early 1970s\\n.1 They observed that\\nexpert chess players were able to memorize the positions of all the pieces in a game by\\nlooking at the board for just 5 seconds, a task that most people would find impossible.\\nHowever, this was only the case when the pieces were placed in realistic positions\\n(from actual games), not when the pieces were placed randomly. Chess experts don‡t\\nhave a much better memory than you and I, they just see chess patterns more easily\\nthanks to their experience with the game. Noticing patterns helps them store infor…\\nmation efficiently.\\nJust like the chess players in this memory experiment, an autoencoder looks at the\\ninputs, converts them to an efficient internal representation, and then spits out some…\\nthing that (hopefully) looks very close to the inputs. An autoencoder is always com…\\nposed of two parts: an encoder\\n (or recognition network\\n) that converts the inputs to an\\ninternal representation, followed by a \\ndecoder\\n (or generative network\\n) that converts\\nthe internal representation to the outputs (see \\nFigure 15-1).As you can see, an autoencoder typically has the same architecture as a Multi-Layer\\nPerceptron (MLP; see \\nChapter 10\\n), except that the number of neurons in the output\\nlayer must be equal to the number of inputs. In this example, there is just one hidden\\n412 | Chapter 15: Autoencoders\\nlayer composed of two neurons (the encoder), and one output layer composed of\\nthree neurons (the decoder). The outputs are often called the reconstructions\\n since the\\nautoencoder tries to reconstruct the inputs, and the cost function contains a \\nrecon…\\nstruction loss\\n that penalizes the model when the reconstructions are different from the\\ninputs.\\nFigure 15-1. \\n•e chess memory experiment \\n(le“) and a simple autoencoder (right)\\nBecause the internal representation has a lower dimensionality than the input data (it\\nis 2D instead of 3D), the autoencoder is said to be \\nundercomplete\\n. An undercomplete\\nautoencoder cannot trivially copy its inputs to the codings, yet it must find a way to\\noutput a copy of its inputs. It is forced to learn the most important features in the\\ninput data (and drop the unimportant ones).\\nLet‡s see how to implement a very simple undercomplete autoencoder for dimension…\\nality reduction.Performing PCA with an Undercomplete LinearAutoencoderIf the autoencoder uses only linear activations and the cost function is the Mean\\nSquared Error (MSE), then it can be shown that it ends up performing Principal\\nComponent Analysis (see \\nChapter 8\\n).The following code builds a simple linear autoencoder to perform PCA on a 3D data…\\nset, projecting it to 2D:import tensorflow as tffrom tensorflow.contrib.layers import fully_connectedn_inputs = 3  # 3D inputsn_hidden = 2  # 2D codingsPerforming PCA with an Undercomplete Linear Autoencoder | 413\\nn_outputs = n_inputslearning_rate = 0.01X = tf.placeholder(tf.float32, shape=[None, n_inputs])hidden = fully_connected(X, n_hidden, activation_fn=None)outputs = fully_connected(hidden, n_outputs, activation_fn=None)reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))  # MSEoptimizer = tf.train.AdamOptimizer(learning_rate)training_op = optimizer.minimize(reconstruction_loss)init = tf.global_variables_initializer()This code is really not very different from all the MLPs we built in past chapters. The\\ntwo things to note are:‹The number of outputs is equal to the number of inputs.\\n‹To perform simple PCA, we set \\nactivation_fn=None (i.e., all neurons are linear)and the cost function is the MSE. We will see more complex autoencoders\\nshortly.\\nNow let‡s load the dataset, train the model on the training set, and use it to encode the\\ntest set (i.e., project it to 2D):X_train, X_test = [...] # load the datasetn_iterations = 1000codings = hidden  # the output of the hidden layer provides the codingswith tf.Session() as sess:    init.run()    for iteration in range(n_iterations):        training_op.run(feed_dict={X: X_train})  # no labels (unsupervised)    codings_val = codings.eval(feed_dict={X: X_test})Figure 15-2 shows the original 3D dataset (at the left) and the output of the autoen…\\ncoder‡s hidden layer (i.e., the coding layer, at the right). As you can see, the autoen…\\ncoder found the best 2D plane to project the data onto, preserving as much variance\\nin the data as it could (just like PCA).\\n414 | Chapter 15: Autoencoders\\nFigure 15-2. PCA performed by an undercomplete linear autoencoder\\nStacked AutoencodersJust like other neural networks we have discussed, autoencoders can have multiple\\nhidden layers. In this case they are called \\nstacked autoencoders\\n (or \\ndeep autoencoders\\n). Adding more layers helps the autoencoder learn more complex codings. However,\\none must be careful not to make the autoencoder too powerful. Imagine an encoder\\nso powerful that it just learns to map each input to a single arbitrary number (and the\\ndecoder learns the reverse mapping). Obviously such an autoencoder will reconstruct\\nthe training data perfectly, but it will not have learned any useful data representation\\nin the process (and it is unlikely to generalize well to new instances).The architecture of a stacked autoencoder is typically symmetrical with regards to the\\ncentral hidden layer (the coding layer). To put it simply, it looks like a sandwich. For\\nexample, an autoencoder for MNIST (introduced in \\nChapter 3\\n) may have 784 inputs,\\nfollowed by a hidden layer with 300 neurons, then a central hidden layer of 150 neu…\\nrons, then another hidden layer with 300 neurons, and an output layer with 784 neu…\\nrons. This stacked autoencoder is represented in \\nFigure 15-3.Figure 15-3. Stacked autoencoder\\nStacked Autoencoders | 415\\nTensorFlow ImplementationYou \\ncan implement a stacked autoencoder very much like a regular deep MLP. In par…\\nticular, the same techniques we used in \\nChapter 11\\n for training deep nets can beapplied. For example, the following code builds a stacked autoencoder for MNIST,\\nusing He initialization, the ELU activation function, and —\\n2 regularization. The code\\nshould look very familiar, except that there are no labels (no \\ny):n_inputs = 28 * 28  # for MNISTn_hidden1 = 300n_hidden2 = 150  # codingsn_hidden3 = n_hidden1n_outputs = n_inputslearning_rate = 0.01l2_reg = 0.001X = tf.placeholder(tf.float32, shape=[None, n_inputs])with tf.contrib.framework.arg_scope(        [fully_connected],        activation_fn=tf.nn.elu,        weights_initializer=tf.contrib.layers.variance_scaling_initializer(),        weights_regularizer=tf.contrib.layers.l2_regularizer(l2_reg)):    hidden1 = fully_connected(X, n_hidden1)    hidden2 = fully_connected(hidden1, n_hidden2)  # codings    hidden3 = fully_connected(hidden2, n_hidden3)    outputs = fully_connected(hidden3, n_outputs, activation_fn=None)reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))  # MSEreg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)loss = tf.add_n([reconstruction_loss] + reg_losses)optimizer = tf.train.AdamOptimizer(learning_rate)training_op = optimizer.minimize(loss)init = tf.global_variables_initializer()You can then train the model normally. Note that the digit labels (\\ny_batch) areunused:\\nn_epochs = 5batch_size = 150with tf.Session() as sess:    init.run()    for epoch in range(n_epochs):        n_batches = mnist.train.num_examples // batch_size        for iteration in range(n_batches):            X_batch, y_batch = mnist.train.next_batch(batch_size)            sess.run(training_op, feed_dict={X: X_batch})416 | Chapter 15: Autoencoders\\nTying WeightsWhen an autoencoder is neatly symmetrical, like the one we just built, a common\\ntechnique is to tie the weights\\n of the decoder layers to the weights of the encoder lay…\\ners. This halves the number of weights in the model, speeding up training and limit…\\ning the risk of overfitting. Specifically, if the autoencoder has a total of \\nN layers (not\\ncounting the input layer), and \\nWL represents the connection weights of the \\nLth layer\\n(e.g., layer 1 is the first hidden layer, layer \\nN2 is the coding layer, and layer \\nN is theoutput layer), then the decoder layer weights can be defined simply as: \\nWN‘L+1 = WLT(with L = 1, 2, ,N2).Unfortunately, implementing tied weights in TensorFlow using the \\nfully_connected() function is a bit cumbersome; it‡s actually easier to just define the layers man…\\nually. The code ends up significantly more verbose:\\nactivation = tf.nn.eluregularizer = tf.contrib.layers.l2_regularizer(l2_reg)initializer = tf.contrib.layers.variance_scaling_initializer()X = tf.placeholder(tf.float32, shape=[None, n_inputs])weights1_init = initializer([n_inputs, n_hidden1])weights2_init = initializer([n_hidden1, n_hidden2])weights1 = tf.Variable(weights1_init, dtype=tf.float32, name=\"weights1\")weights2 = tf.Variable(weights2_init, dtype=tf.float32, name=\"weights2\")weights3 = tf.transpose(weights2, name=\"weights3\")  # tied weightsweights4 = tf.transpose(weights1, name=\"weights4\")  # tied weightsbiases1 = tf.Variable(tf.zeros(n_hidden1), name=\"biases1\")biases2 = tf.Variable(tf.zeros(n_hidden2), name=\"biases2\")biases3 = tf.Variable(tf.zeros(n_hidden3), name=\"biases3\")biases4 = tf.Variable(tf.zeros(n_outputs), name=\"biases4\")hidden1 = activation(tf.matmul(X, weights1) + biases1)hidden2 = activation(tf.matmul(hidden1, weights2) + biases2)hidden3 = activation(tf.matmul(hidden2, weights3) + biases3)outputs = tf.matmul(hidden3, weights4) + biases4reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))reg_loss = regularizer(weights1) + regularizer(weights2)loss = reconstruction_loss + reg_lossoptimizer = tf.train.AdamOptimizer(learning_rate)training_op = optimizer.minimize(loss)init = tf.global_variables_initializer()This code is fairly straightforward, but there are a few important things to note:\\nStacked Autoencoders | 417\\n‹First, weight3 and \\nweights4 are not variables, they are respectively the transposeof weights2 and weights1 (they are ƒtied⁄ to them).\\n‹Second, since they are not variables, it‡s no use regularizing them: we only regula…\\nrize weights1 and weights2.‹Third, biases are never tied, and never regularized.Training One Autoencoder at a Time\\nRather than training the whole stacked autoencoder in one go like we just did, it is\\noften much faster to train one shallow autoencoder at a time, then stack all of them\\ninto a single stacked autoencoder (hence the name), as shown on \\nFigure 15-4. This isespecially useful for very deep autoencoders.\\nFigure 15-4. Training one autoencoder at a time\\nDuring the first phase of training, the first autoencoder learns to reconstruct the\\ninputs. During the second phase, the second autoencoder learns to reconstruct the\\noutput of the first autoencoder‡s hidden layer. Finally, you just build a big sandwich\\nusing all these autoencoders, as shown in \\nFigure 15-4 (i.e., you first stack the hiddenlayers of each autoencoder, then the output layers in reverse order). This gives you\\nthe final stacked autoencoder. You could easily train more autoencoders this way,\\nbuilding a very deep stacked autoencoder.\\nTo implement this multiphase training algorithm, the simplest approach is to use a\\ndifferent TensorFlow graph for each phase. After training an autoencoder, you just\\nrun the training set through it and capture the output of the hidden layer. This output\\nthen serves as the training set for the next autoencoder. Once all autoencoders have\\nbeen trained this way, you simply copy the weights and biases from each autoencoder\\nand use them to build the stacked autoencoder. Implementing this approach is quite\\n418 | Chapter 15: Autoencoders\\nstraightforward, so we won‡t detail it here, but please check out the code in the\\nJupyter notebooks\\n for an example.\\nAnother approach is to use a single graph containing the whole stacked autoencoder,\\nplus some extra operations to perform each training phase, as shown in \\nFigure 15-5.Figure 15-5. A single graph to train a stacked autoencoder\\nThis deserves a bit of explanation:\\n‹The central column in the graph is the full stacked autoencoder. This part can be\\nused after training.‹The left column is the set of operations needed to run the first phase of training.\\nIt creates an output layer that bypasses hidden layers 2 and 3. This output layer\\nshares the same weights and biases as the stacked autoencoder‡s output layer. On\\ntop of that are the training operations that will aim at making the output as close\\nas possible to the inputs. Thus, this phase will train the weights and biases for the\\nhidden layer 1 and the output layer (i.e., the first autoencoder).\\n‹The right column in the graph is the set of operations needed to run the second\\nphase of training. It adds the training operation that will aim at making the out…\\nput of hidden layer 3 as close as possible to the output of hidden layer 1. Note\\nthat we must freeze hidden layer 1 while running phase 2. This phase will train\\nthe weights and biases for hidden layers 2 and 3 (i.e., the second autoencoder).\\nThe TensorFlow code looks like this:\\n[...] # Build the whole stacked autoencoder normally.      # In this example, the weights are not tied.Stacked Autoencoders | 419\\noptimizer = tf.train.AdamOptimizer(learning_rate)with tf.name_scope(\"phase1\"):    phase1_outputs = tf.matmul(hidden1, weights4) + biases4    phase1_reconstruction_loss = tf.reduce_mean(tf.square(phase1_outputs - X))    phase1_reg_loss = regularizer(weights1) + regularizer(weights4)    phase1_loss = phase1_reconstruction_loss + phase1_reg_loss    phase1_training_op = optimizer.minimize(phase1_loss)with tf.name_scope(\"phase2\"):    phase2_reconstruction_loss = tf.reduce_mean(tf.square(hidden3 - hidden1))    phase2_reg_loss = regularizer(weights2) + regularizer(weights3)    phase2_loss = phase2_reconstruction_loss + phase2_reg_loss    train_vars = [weights2, biases2, weights3, biases3]    phase2_training_op = optimizer.minimize(phase2_loss, var_list=train_vars)The first phase is rather straightforward: we just create an output layer that skips hid…\\nden layers 2 and 3, then build the training operations to minimize the distance\\nbetween the outputs and the inputs (plus some regularization).\\nThe second phase just adds the operations needed to minimize the distance between\\nthe output of hidden layer 3 and hidden layer 1 (also with some regularization). Most\\nimportantly, we provide the list of trainable variables to the \\nminimize() method,making sure to leave out \\nweights1 and \\nbiases1; this effectively freezes hidden layer 1\\nduring phase 2.During the execution phase, all you need to do is run the phase 1 training op for anumber of epochs, then the phase 2 training op for some more epochs.\\nSince hidden layer 1 is frozen during phase 2, its output will always\\nbe the same for any given training instance. To avoid having to\\nrecompute the output of hidden layer 1 at every single epoch, you\\ncan compute it for the whole training set at the end of phase 1, then\\ndirectly feed the cached output of hidden layer 1 during phase 2.\\nThis can give you a nice performance boost.Visualizing the ReconstructionsOne way to ensure that an autoencoder is properly trained is to compare the inputs\\nand the outputs. They must be fairly similar, and the differences should be unimpor…\\ntant details. Let‡s plot two random digits and their reconstructions:\\nn_test_digits = 2X_test = mnist.test.images[:n_test_digits]with tf.Session() as sess:    [...] # Train the Autoencoder    outputs_val = outputs.eval(feed_dict={X: X_test})420 | Chapter 15: Autoencoders\\ndef plot_image(image, shape=[28, 28]):    plt.imshow(image.reshape(shape), cmap=\"Greys\", interpolation=\"nearest\")    plt.axis(\"off\")for digit_index in range(n_test_digits):    plt.subplot(n_test_digits, 2, digit_index * 2 + 1)    plot_image(X_test[digit_index])    plt.subplot(n_test_digits, 2, digit_index * 2 + 2)    plot_image(outputs_val[digit_index])Figure 15-6 shows the resulting images.Figure 15-6. Original digits \\n(le“) and their reconstructions (right)\\nLooks close enough. So the autoencoder has properly learned to reproduce its inputs,\\nbut has it learned useful features? Let‡s take a look.\\nVisualizing FeaturesOnce your autoencoder has learned some features, you may want to take a look at\\nthem. There are various techniques for this. Arguably the simplest technique is to\\nconsider each neuron in every hidden layer, and find the training instances that acti…\\nvate it the most. This is especially useful for the top hidden layers since they often\\ncapture relatively large features that you can easily spot in a group of training instan…\\nces that contain them. For example, if a neuron strongly activates when it sees a cat in\\na picture, it will be pretty obvious that the pictures that activate it the most all contain\\ncats. However, for lower layers, this technique does not work so well, as the features\\nare smaller and more abstract, so it‡s often hard to understand exactly what the neu…\\nron is getting all excited about.Let‡s look at another technique. For each neuron in the first hidden layer, you can cre…\\nate an image where a pixel‡s intensity corresponds to the weight of the connection to\\nthe given neuron. For example, the following code plots the features learned by five\\nneurons in the first hidden layer:\\nwith tf.Session() as sess:    [...] # train autoencoderStacked Autoencoders | 421\\n    weights1_val = weights1.eval()for i in range(5):    plt.subplot(1, 5, i + 1)    plot_image(weights1_val.T[i])You may get low-level features such as the ones shown in \\nFigure 15-7.Figure 15-7. Features learned by \\n†ve neurons from the \\n†rst hidden layer\\nThe first four features seem to correspond to small patches, while the fifth feature\\nseems to look for vertical strokes (note that these features come from the \\nstackeddenoising autoencoder that we will discuss later).\\nAnother technique is to feed the autoencoder a random input image, measure the\\nactivation of the neuron you are interested in, and then perform backpropagation \\ntotweak the image in such a way that the neuron will activate even more. If you iterate\\nseveral times (performing gradient ascent), the image will gradually turn into the\\nmost exciting image (for the neuron). This is a useful technique to visualize the kindsof inputs that a neuron is looking for.\\nFinally, if you are using an autoencoder to perform unsupervised pretraining›for\\nexample, for a classification task›a simple way to verify that the features learned by\\nthe autoencoder are useful is to measure the performance of the classifier.\\nUnsupervised Pretraining Using Stacked AutoencodersAs we discussed in Chapter 11\\n, if you are tackling a complex supervised task but you\\ndo not have a lot of labeled training data, one solution is to find a neural network that\\nperforms a similar task, and then reuse its lower layers. This makes it possible to train\\na high-performance model using only little training data because your neural net…\\nwork won‡t have to learn all the low-level features; it will just reuse the feature detec…\\ntors learned by the existing net.Similarly, if you have a large dataset but most of it is unlabeled, you can first train a\\nstacked autoencoder using all the data, then reuse the lower layers to create a neural\\nnetwork for your actual task, and train it using the labeled data. For example,\\nFigure 15-8 shows how to use a stacked autoencoder to perform unsupervised pre…\\ntraining for a classification neural network. The stacked autoencoder itself is typically\\ntrained one autoencoder at a time, as discussed earlier. When training the classifier, if\\n422 | Chapter 15: Autoencoders\\n2ƒGreedy Layer-Wise Training of Deep Networks,⁄ Y. Bengio et al. (2007).\\nyou really don‡t have much labeled training data, you may want to freeze the pre…\\ntrained layers (at least the lower ones).\\nFigure 15-8. Unsupervised pretraining using autoencoders\\nThis situation is actually quite common, because building a large\\nunlabeled dataset is often cheap (e.g., a simple script can download\\nmillions of images off the internet), but labeling them can only be\\ndone reliably by humans (e.g., classifying images as cute or not).\\nLabeling instances is time-consuming and costly, so it is quite com…\\nmon to have only a few thousand labeled instances.\\nAs we discussed earlier, one of the triggers of the current Deep Learning tsunami is\\nthe discovery in 2006 by Geoffrey Hinton et al. that deep neural networks can be pre…\\ntrained in an unsupervised fashion. They used restricted Boltzmann machines for\\nthat (see \\nAppendix E\\n), but in 2007 Yoshua Bengio et al. showed\\n2 that autoencoders\\nworked just as well.There is nothing special about the TensorFlow implementation: just train an autoen…\\ncoder using all the training data, then reuse its encoder layers to create a new neural\\nnetwork (see Chapter 11\\n for more details on how to reuse pretrained layers, or check\\nout the code examples in the Jupyter notebooks).\\nUp to now, in order to force the autoencoder to learn interesting features, we have\\nlimited the size of the coding layer, making it undercomplete. There are actually\\nmany other kinds of constraints that can be used, including ones that allow the cod…\\nUnsupervised Pretraining Using Stacked Autoencoders | 423\\n3ƒExtracting and Composing Robust Features with Denoising Autoencoders,⁄ P. Vincent et al. (2008).\\n4ƒStacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denois…\\ning Criterion,⁄ P. Vincent et al. (2010).\\ning layer to be just as large as the inputs, or even larger, resulting in an \\novercomplete\\nautoencoder\\n. Let‡s look at some of those approaches now.\\nDenoising AutoencodersAnother way to force the autoencoder to learn useful features is to add noise to its\\ninputs, training it to recover the original, noise-free inputs. This prevents the autoen…\\ncoder from trivially copying its inputs to its outputs, so it ends up having to find pat…\\nterns in the data.\\nThe idea of using autoencoders to remove noise has been around since the 1980s\\n(e.g., it is mentioned in Yann LeCun‡s 1987 master‡s thesis). In a \\n2008 paper\\n,3 Pascal\\nVincent et al. showed that autoencoders could also be used for feature extraction. In a\\n2010 paper\\n,4 Vincent et al. introduced \\nstacked denoising autoencoders\\n.The noise can be pure Gaussian noise added to the inputs, or it can be randomly\\nswitched off inputs, just like in dropout (introduced in \\nChapter 11\\n). Figure 15-9shows both options.Figure 15-9. Denoising autoencoders, with Gaussian noise \\n(le“) or dropout (right)\\n424 | Chapter 15: Autoencoders\\nTensorFlow ImplementationImplementing denoising autoencoders in TensorFlow is not too hard. Let‡s start with\\nGaussian noise. It‡s really just like training a regular autoencoder, except you add\\nnoise to the inputs, and the reconstruction loss is calculated based on the original\\ninputs:\\nX = tf.placeholder(tf.float32, shape=[None, n_inputs])X_noisy = X + tf.random_normal(tf.shape(X))[...]hidden1 = activation(tf.matmul(X_noisy, weights1) + biases1)[...]reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))  # MSE[...]Since the shape of \\nX is only partially defined during the construc…tion phase, we cannot know in advance the shape of the noise that\\nwe must add to \\nX. We cannot call \\nX.get_shape() because this\\nwould just return the partially defined shape of \\nX ([None,n_inputs]), and random_normal() expects a fully defined shape so\\nit would raise an exception. Instead, we call tf.shape(X), whichcreates an operation that will return the shape of \\nX at runtime,\\nwhich will be fully defined at that point.\\nImplementing the dropout version, which is more common, is not much harder:\\nfrom tensorflow.contrib.layers import dropoutkeep_prob = 0.7is_training = tf.placeholder_with_default(False, shape=(), name=•is_training•)X = tf.placeholder(tf.float32, shape=[None, n_inputs])X_drop = dropout(X, keep_prob, is_training=is_training)[...]hidden1 = activation(tf.matmul(X_drop, weights1) + biases1)[...]reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))  # MSE[...]During training we must set \\nis_training to True (as explained in Chapter 11\\n) usingthe feed_dict:sess.run(training_op, feed_dict={X: X_batch, is_training: True})However, during testing it is not necessary to set \\nis_training to False, since we setthat as the default in the call to the \\nplaceholder_with_default() function.Denoising Autoencoders | 425\\nSparse AutoencodersAnother kind of constraint that often leads to good feature extraction is \\nsparsity\\n: byadding an appropriate term to the cost function, the autoencoder is pushed to reduce\\nthe number of active neurons in the coding layer. For example, it may be pushed to\\nhave on average only 5% significantly active neurons in the coding layer. This forces\\nthe autoencoder to represent each input as a combination of a small number of acti…\\nvations. As a result, each neuron in the coding layer typically ends up representing a\\nuseful feature (if you could speak only a few words per month, you would probably\\ntry to make them worth listening to).\\nIn order to favor sparse models, we must first measure the actual sparsity of the cod…\\ning layer at each training iteration. We do so by computing the average activation of\\neach neuron in the coding layer, over the whole training batch. The batch size must\\nnot be too small, or else the mean will not be accurate.\\nOnce we have the mean activation per neuron, we want to penalize the neurons that\\nare too active by adding a sparsity loss\\n to the cost function. For example, if we meas…\\nure that a neuron has an average activation of 0.3, but the target sparsity is 0.1, it must\\nbe penalized to activate less. One approach could be simply adding the squared error\\n(0.3 – 0.1)2 to the cost function, but in practice a better approach is to use the \\nKull…\\nback–Leibler divergence (briefly discussed in Chapter 4\\n), which has much stronger\\ngradients than the Mean Squared Error, as you can see in \\nFigure 15-10.Figure 15-10. Sparsity loss\\n426 | Chapter 15: Autoencoders\\nGiven two discrete probability distributions P and Q, the KL divergence betweenthese distributions, noted DKL(P  Q), can be computed using \\nEquation 15-1\\n.Equation 15-1. Kullback‘Leibler divergence\\nDKLPQ=“iPilogPiQiIn our case, we want to measure the divergence between the target probability \\np that a\\nneuron in the coding layer will activate, and the actual probability \\nq (i.e., the meanactivation over the training batch). So the KL divergence simplifies to \\nEquation 15-2\\n.Equation 15-2. KL divergence between the target sparsity p and the actual sparsity q\\nDKLpq=plogpq+1”\\nplog1”\\np1”\\nqOnce we have computed the sparsity loss for each neuron in the coding layer, we just\\nsum up these losses, and add the result to the cost function. In order to control the\\nrelative importance of the sparsity loss and the reconstruction loss, we can multiply\\nthe sparsity loss by a sparsity weight hyperparameter. If this weight is too high, the\\nmodel will stick closely to the target sparsity, but it may not reconstruct the inputs\\nproperly, making the model useless. Conversely, if it is too low, the model will mostly\\nignore the sparsity objective and it will not learn any interesting features.\\nTensorFlow ImplementationWe now have all we need to implement a sparse autoencoder using TensorFlow:\\ndef kl_divergence(p, q):    return p * tf.log(p / q) + (1 - p) * tf.log((1 - p) / (1 - q))learning_rate = 0.01sparsity_target = 0.1sparsity_weight = 0.2[...] # Build a normal autoencoder (in this example the coding layer is hidden1)optimizer = tf.train.AdamOptimizer(learning_rate)hidden1_mean = tf.reduce_mean(hidden1, axis=0)  # batch meansparsity_loss = tf.reduce_sum(kl_divergence(sparsity_target, hidden1_mean))reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))  # MSEloss = reconstruction_loss + sparsity_weight * sparsity_losstraining_op = optimizer.minimize(loss)An important detail is the fact that the activations of the coding layer must be\\nbetween 0 and 1 (but not equal to 0 or 1), or else the KL divergence will return NaN\\nSparse Autoencoders | 427\\n5ƒAuto-Encoding Variational Bayes,⁄ D. Kingma and M. Welling (2014).\\n(Not a Number). A simple solution is to use the logistic activation function for the\\ncoding layer:\\nhidden1 = tf.nn.sigmoid(tf.matmul(X, weights1) + biases1)One simple trick can speed up convergence: instead of using the MSE, we can choose\\na reconstruction loss that will have larger gradients. Cross entropy is often a good\\nchoice. To use it, we must normalize the inputs to make them take on values from 0\\nto 1, and use the logistic activation function in the output layer so the outputs also\\ntake on values from 0 to 1. TensorFlow‡s \\nsigmoid_cross_entropy_with_logits()function takes care of efficiently applying the logistic (sigmoid) activation function to\\nthe outputs and computing the cross entropy:\\n[...]logits = tf.matmul(hidden1, weights2) + biases2)outputs = tf.nn.sigmoid(logits)reconstruction_loss = tf.reduce_sum(    tf.nn.sigmoid_cross_entropy_with_logits(labels=X, logits=logits))Note that the \\noutputs operation is not needed during training (we use it only when\\nwe want to look at the reconstructions).\\nVariational AutoencodersAnother important category of autoencoders was \\nintroduced in 2014\\n by DiederikKingma and Max Welling,\\n5 and has quickly become one of the most popular types ofautoencoders: \\nvariational autoencoders\\n.They are quite different from all the autoencoders we have discussed so far, in partic…\\nular:\\n‹They are probabilistic autoencoders\\n, meaning that their outputs are partly deter…\\nmined by chance, even after training (as opposed to denoising autoencoders,\\nwhich use randomness only during training).‹Most importantly, they are \\ngenerative autoencoders\\n, meaning that they can gener…\\nate new instances that look like they were sampled from the training set.\\nBoth these properties make them rather similar to RBMs (see \\nAppendix E\\n), but theyare easier to train and the sampling process is much faster (with RBMs you need to\\nwait for the network to stabilize into a ƒthermal equilibrium⁄ before you can sample a\\nnew instance).428 | Chapter 15: Autoencoders\\n6Variational autoencoders are actually more general; the codings are not limited to Gaussian distributions.\\nLet‡s take a look at how they work. \\nFigure 15-11 (left) shows a variational autoen…\\ncoder. You can recognize, of course, the basic structure of all autoencoders, with an\\nencoder followed by a decoder (in this example, they both have two hidden layers),\\nbut there is a twist: instead of directly producing a coding for a given input, the\\nencoder produces a mean coding\\n ﬂ and a standard deviation \\n„. The actual coding isthen sampled randomly from a Gaussian distribution with mean \\nﬂ and standard devi…\\nation \\n„. After that the decoder just decodes the sampled coding normally. The right\\npart of the diagram shows a training instance going through this autoencoder. First,\\nthe encoder produces ﬂ and „, then a coding is sampled randomly (notice that it is\\nnot exactly located at \\nﬂ), and finally this coding is decoded, and the final outputresembles the training instance.Figure 15-11. Variational autoencoder \\n(le“), and an instance going through it (right)\\nAs you can see on the diagram, although the inputs may have a very convoluted dis…\\ntribution, a variational autoencoder tends to produce codings that look as though\\nthey were sampled from a simple Gaussian distribution:\\n6 during training, the costfunction (discussed next) pushes the codings to gradually migrate within the \\ncodingspace (also called the latent space\\n) to occupy a roughly (hyper)spherical region that\\nlooks like a cloud of Gaussian points. One great consequence is that after training a\\nVariational Autoencoders | 429\\n7For more mathematical details, check out the original paper on variational autoencoders, or Carl Doersch‡s\\ngreat tutorial\\n (2016).variational autoencoder, you can very easily generate a new instance: just sample a\\nrandom coding from the Gaussian distribution, decode it, and voilÉ!\\nSo let‡s look at the cost function. It is composed of two parts. The first is the usual \\nreconstruction loss that pushes the autoencoder to reproduce its inputs (we can use\\ncross entropy for this, as discussed earlier). The second is the \\nlatent loss\\n that pushes\\nthe autoencoder to have codings that look as though they were sampled from a simple\\nGaussian distribution, for which we use the KL divergence between the target distri…\\nbution (the Gaussian distribution) and the actual distribution of the codings. The\\nmath is a bit more complex than earlier, in particular because of the Gaussian noise,\\nwhich limits the amount of information that can be transmitted to the coding layer\\n(thus pushing the autoencoder to learn useful features). Luckily, the equations \\nsim…plify to the following code for the latent loss:\\n7eps = 1e-10  # smoothing term to avoid computing log(0) which is NaNlatent_loss = 0.5 * tf.reduce_sum(    tf.square(hidden3_sigma) + tf.square(hidden3_mean)    - 1 - tf.log(eps + tf.square(hidden3_sigma)))One common variant is to train the encoder to output \\n’ = log(„2) rather than \\n„.Wherever we need „ we can just compute \\n„=exp\\n’2. This makes it a bit easier forthe encoder to capture sigmas of different scales, and thus it helps speed up conver…\\ngence. The latent loss ends up a bit simpler:\\nlatent_loss = 0.5 * tf.reduce_sum(    tf.exp(hidden3_gamma) + tf.square(hidden3_mean) - 1 - hidden3_gamma)The following code builds the variational autoencoder shown in \\nFigure 15-11 (left), using the log(„2) variant:\\nn_inputs = 28 * 28  # for MNISTn_hidden1 = 500n_hidden2 = 500n_hidden3 = 20  # codingsn_hidden4 = n_hidden2n_hidden5 = n_hidden1n_outputs = n_inputslearning_rate = 0.001with tf.contrib.framework.arg_scope(        [fully_connected],        activation_fn=tf.nn.elu,        weights_initializer=tf.contrib.layers.variance_scaling_initializer()):    X = tf.placeholder(tf.float32, [None, n_inputs])430 | Chapter 15: Autoencoders\\n    hidden1 = fully_connected(X, n_hidden1)    hidden2 = fully_connected(hidden1, n_hidden2)    hidden3_mean = fully_connected(hidden2, n_hidden3, activation_fn=None)    hidden3_gamma = fully_connected(hidden2, n_hidden3, activation_fn=None)    hidden3_sigma = tf.exp(0.5 * hidden3_gamma)    noise = tf.random_normal(tf.shape(hidden3_sigma), dtype=tf.float32)    hidden3 = hidden3_mean + hidden3_sigma * noise    hidden4 = fully_connected(hidden3, n_hidden4)    hidden5 = fully_connected(hidden4, n_hidden5)    logits = fully_connected(hidden5, n_outputs, activation_fn=None)    outputs = tf.sigmoid(logits)reconstruction_loss = tf.reduce_sum(    tf.nn.sigmoid_cross_entropy_with_logits(labels=X, logits=logits))latent_loss = 0.5 * tf.reduce_sum(    tf.exp(hidden3_gamma) + tf.square(hidden3_mean) - 1 - hidden3_gamma)cost = reconstruction_loss + latent_lossoptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)training_op = optimizer.minimize(cost)init = tf.global_variables_initializer()Generating DigitsNow let‡s use this variational autoencoder to generate images that look like handwrit…\\nten digits. All we need to do is train the model, then sample random codings from a \\nGaussian distribution and decode them.\\nimport numpy as npn_digits = 60n_epochs = 50batch_size = 150with tf.Session() as sess:    init.run()    for epoch in range(n_epochs):        n_batches = mnist.train.num_examples // batch_size        for iteration in range(n_batches):            X_batch, y_batch = mnist.train.next_batch(batch_size)            sess.run(training_op, feed_dict={X: X_batch})    codings_rnd = np.random.normal(size=[n_digits, n_hidden3])    outputs_val = outputs.eval(feed_dict={hidden3: codings_rnd})That‡s it. Now we can see what the ƒhandwritten⁄ digits produced by the autoencoder\\nlook like (see Figure 15-12):for iteration in range(n_digits):    plt.subplot(n_digits, 10, iteration + 1)    plot_image(outputs_val[iteration])Variational Autoencoders | 431\\n8ƒContractive Auto-Encoders: Explicit Invariance During Feature Extraction,⁄ S. Rifai et al. (2011).\\nFigure 15-12. Images of handwritten digits generated by the variational autoencoder\\nA majority of these digits look pretty convincing, while a few are rather ƒcreative.⁄ But\\ndon‡t be too harsh on the autoencoder›it only started learning less than an hour ago.\\nGive it a bit more training time, and those digits will look better and better.\\nOther AutoencodersThe amazing successes of supervised learning in image recognition, speech recogni…\\ntion, text translation, and more have somewhat overshadowed unsupervised learning,\\nbut it is actually booming. New architectures for autoencoders and other unsuper…\\nvised learning algorithms are invented regularly, so much so that we cannot cover\\nthem all in this book. Here is a brief (by no means exhaustive) overview of a few more\\ntypes of autoencoders that you may want to check out:\\nContractive autoencoder (CAE)\\n8The autoencoder is constrained during training so that the derivatives of the cod…\\nings with regards to the inputs are small. In other words, two similar inputs must\\nhave similar codings.\\n432 | Chapter 15: Autoencoders\\n9ƒStacked Convolutional Auto-Encoders for Hierarchical Feature Extraction,⁄ J. Masci et al. (2011).\\n10ƒGSNs: Generative Stochastic Networks,⁄ G. Alain et al. (2015).\\n11ƒWinner-Take-All Autoencoders,⁄ A. Makhzani and B. Frey (2015).\\n12ƒAdversarial Autoencoders,⁄ A. Makhzani et al. (2016).\\nStacked convolutional autoencoders\\n9Autoencoders that learn to extract visual features by reconstructing images pro…\\ncessed through convolutional layers.\\nGenerative stochastic network (GSN)\\n10A generalization of denoising autoencoders, with the added capability to generate\\ndata.\\nWinner-take-all (WTA) autoencoder\\n11During training, after computing the activations of all the neurons in the coding\\nlayer, only the top \\nk% activations for each neuron over the training batch are pre…\\nserved, and the rest are set to zero. Naturally this leads to sparse codings. More…\\nover, a similar WTA approach can be used to produce sparse convolutional\\nautoencoders.\\nAdversarial autoencoders\\n12One network is trained to reproduce its inputs, and at the same time another is\\ntrained to find inputs that the first network is unable to properly reconstruct.\\nThis pushes the first autoencoder to learn robust codings.\\nExercises1.What are the main tasks that autoencoders are used for?\\n2.Suppose you want to train a classifier and you have plenty of unlabeled training\\ndata, but only a few thousand labeled instances. How can autoencoders help?\\nHow would you proceed?\\n3.If an autoencoder perfectly reconstructs the inputs, is it necessarily a good\\nautoencoder? How can you evaluate the performance of an autoencoder?\\n4.What are undercomplete and overcomplete autoencoders? What is the main risk\\nof an excessively undercomplete autoencoder? What about the main risk of an\\novercomplete autoencoder?\\n5.How do you tie weights in a stacked autoencoder? What is the point of doing so?\\n6.What is a common technique to visualize features learned by the lower layer of a\\nstacked autoencoder? What about higher layers?\\n7.What is a generative model? Can you name a type of generative autoencoder?\\nExercises | 433\\n13ƒSemantic Hashing,⁄ R. Salakhutdinov and G. Hinton (2008).\\n8.Let‡s use a denoising autoencoder to pretrain an image classifier:\\n‹You can use MNIST (simplest), or another large set of images such as \\nCIFAR10\\nif you want a bigger challenge. If you choose CIFAR10, you need to write code\\nto load batches of images for training. If you want to skip this part, Tensor…\\nFlow‡s model zoo contains \\ntools to do just that\\n.‹Split the dataset into a training set and a test set. Train a deep denoising\\nautoencoder on the full training set.\\n‹Check that the images are fairly well reconstructed, and visualize the low-level\\nfeatures. Visualize the images that most activate each neuron in the coding\\nlayer.\\n‹Build a classification deep neural network, reusing the lower layers of the\\nautoencoder. Train it using only 10% of the training set. Can you get it to per…\\nform as well as the same classifier trained on the full training set?9.Semantic hashing\\n, introduced in 2008 by Ruslan Salakhutdinov and Geoffrey\\nHinton\\n,13 is a technique used for efficient \\ninformation retrieval\\n: a document (e.g.,\\nan image) is passed through a system, typically a neural network, which outputs afairly low-dimensional binary vector (e.g., 30 bits). Two similar documents are\\nlikely to have identical or very similar hashes. By indexing each document using\\nits hash, it is possible to retrieve many documents similar to a particular docu…\\nment almost instantly, even if there are billions of documents: just compute the\\nhash of the document and look up all documents with that same hash (or hashes\\ndiffering by just one or two bits). Let‡s implement semantic hashing using a\\nslightly tweaked stacked autoencoder:\\n‹Create a stacked autoencoder containing two hidden layers below the coding\\nlayer, and train it on the image dataset you used in the previous exercise. The\\ncoding layer should contain 30 neurons and use the logistic activation function\\nto output values between 0 and 1. After training, to produce the hash of animage, you can simply run it through the autoencoder, take the output of the\\ncoding layer, and round every value to the closest integer (0 or 1).\\n‹One neat trick proposed by Salakhutdinov and Hinton is to add Gaussian\\nnoise (with zero mean) to the inputs of the coding layer, during training only.\\nIn order to preserve a high signal-to-noise ratio, the autoencoder will learn to\\nfeed large values to the coding layer (so that the noise becomes negligible). In\\nturn, this means that the logistic function of the coding layer will likely satu…\\nrate at 0 or 1. As a result, rounding the codings to 0 or 1 won‡t distort them too\\nmuch, and this will improve the reliability of the hashes.\\n434 | Chapter 15: Autoencoders\\n14ƒCNN Based Hashing for Image Retrieval,⁄ J. Gua and J. Li (2015).\\n‹Compute the hash of every image, and see if images with identical hashes look\\nalike. Since MNIST and CIFAR10 are labeled, a more objective way to measure\\nthe performance of the autoencoder for semantic hashing is to ensure that\\nimages with the same hash generally have the same class. One way to do this is\\nto measure the average Gini purity (introduced in \\nChapter 6\\n) of the sets ofimages with identical (or very similar) hashes.\\n‹Try fine-tuning the hyperparameters using cross-validation.\\n‹Note that with a labeled dataset, another approach is to train a convolutional\\nneural network (see Chapter 13\\n) for classification, then use the layer below the\\noutput layer to produce the hashes. See Jinma Gua and Jianmin Li‡s \\n2015paper\\n.14 See if that performs better.\\n10.Train a variational autoencoder on the image dataset used in the previous exerci…\\nses (MNIST or CIFAR10), and make it generate images. Alternatively, you can try\\nto find an unlabeled dataset that you are interested in and see if you can generate\\nnew samples.\\nSolutions to these exercises are available in \\nAppendix A\\n.Exercises | 435\\n1For more details, be sure to check out Richard Sutton and Andrew Barto‡s \\nbook on RL, Reinforcement Learn…\\ning: An Introduction\\n (MIT Press), or David Silver‡s free \\nonline RL course at University College London.\\n2ƒPlaying Atari with Deep Reinforcement Learning,⁄ V. Mnih et al. (2013).\\n3ƒHuman-level control through deep reinforcement learning,⁄ V. Mnih et al. (2015).\\n4Check out the videos of DeepMind‡s system learning to play \\nSpace Invaders\\n, Breakout\\n, and more at \\nhttps://\\ngoo.gl/yTsH6X\\n.CHAPTER 16Reinforcement LearningReinforcement Learning\\n (RL) is one of the most exciting fields of Machine Learning\\ntoday, and also one of the oldest. It has been around since the 1950s, producing many\\ninteresting applications over the years,\\n1 in particular in games (e.g., TD-Gammon\\n, aBackgammon\\n playing program) and in machine control, but seldom making the\\nheadline news. But a revolution took place in 2013 when researchers from an Englishstartup called DeepMind demonstrated a system that could learn to play just about\\nany Atari game from scratch\\n,2 eventually \\noutperforming humans\\n3 in most of them,using only raw pixels as inputs and without any prior knowledge of the rules of the\\ngames.4 This was the first of a series of amazing feats, culminating in March 2016\\nwith the victory of their system AlphaGo against Lee Sedol, the world champion of\\nthe game of Go\\n. No program had ever come close to beating a master of this game, let\\nalone the world champion. Today the whole field of RL is boiling with new ideas, with\\na wide range of applications. DeepMind was bought by Google for over 500 million\\ndollars in 2014.So how did they do it? With hindsight it seems rather simple: they applied the power\\nof Deep Learning to the field of Reinforcement Learning, and it worked beyond their\\nwildest dreams. In this chapter we will first explain what Reinforcement Learning is\\nand what it is good at, and then we will present two of the most important techniques\\nin deep Reinforcement Learning: \\npolicy gradients\\n and deep Q-networks\\n (DQN),\\n437including a discussion of Markov decision processes\\n (MDP). We will use these techni…\\nques to train a model to balance a pole on a moving cart, and another to play Atari\\ngames. The same techniques can be used for a wide variety of tasks, from walkingrobots to self-driving cars.Learning to Optimize RewardsIn Reinforcement Learning, a software \\nagent\\n makes observations\\n and takes actions\\nwithin an environment\\n, and in return it receives rewards\\n. Its objective is to learn to act\\nin a way that will maximize its expected long-term rewards. If you don‡t mind a bit of\\nanthropomorphism, you can think of positive rewards as pleasure, and negative\\nrewards as pain (the term ƒreward⁄ is a bit misleading in this case). In short, the agent\\nacts in the environment and learns by trial and error to maximize its pleasure and\\nminimize its pain.This is quite a broad setting, which can apply to a wide variety of tasks. Here are a few\\nexamples (see \\nFigure 16-1):a.The agent can be the program controlling a walking robot. In this case, the envi…\\nronment is the real world, the agent observes the environment through a set of\\nsensors\\n such as cameras and touch sensors, and its actions consist of sending sig…nals to activate motors. It may be programmed to get positive rewards whenever\\nit approaches the target destination, and negative rewards whenever it wastes\\ntime, goes in the wrong direction, or falls down.b.\\nThe agent can be the program controlling Ms. Pac-Man. In this case, the environ…\\nment is a simulation of the Atari game, the actions are the nine possible joystick\\npositions (upper left, down, center, and so on), the observations are screenshots,\\nand the rewards are just the game points.\\nc.Similarly, the agent can be the program playing a board game such as the game of\\nGo\\n.d.The agent does not have to control a physically (or virtually) moving thing. For\\nexample, it can be a smart thermostat, getting rewards whenever it is close to the\\ntarget temperature and saves energy, and negative rewards when humans need to\\ntweak the temperature, so the agent must learn to anticipate human needs.\\ne.The agent can observe stock market prices and decide how much to buy or sell\\nevery second. Rewards are obviously the monetary gains and losses.\\n438 | Chapter 16: Reinforcement Learning\\n5Images (a), (c), and (d) are reproduced from Wikipedia. (a) and (d) are in the public domain. (c) was created\\nby user Stevertigo and released under Creative Commons BY-SA 2.0\\n. (b) is a screenshot from the Ms. Pac-\\nMan game, copyright Atari (the author believes it to be fair use in this chapter). (e) was reproduced from Pix…\\nabay, released under \\nCreative Commons CC0\\n.Figure 16-1. Reinforcement Learning examples: (a) walking robot, (b) Ms. Pac-Man, (c)\\nGo player, (d) thermostat, (e) automatic trader\\n5Note that there may not be any positive rewards at all; for example, the agent may\\nmove around in a maze, getting a negative reward at every time step, so it better find\\nthe exit as quickly as possible! There are many other examples of tasks where Rein…\\nforcement Learning is well suited, such as self-driving cars, placing ads on a web\\npage, or controlling where an image classification system should focus its attention.\\nLearning to Optimize Rewards | 439\\n6It is often better to give the poor performers a slight chance of survival, to preserve some diversity in the ƒgene\\npool.⁄\\nPolicy SearchThe algorithm used by the software agent to determine its actions is called its \\npolicy\\n. For example, the policy could be a neural network taking observations as inputs and\\noutputting the action to take (see Figure 16-2).Figure 16-2. Reinforcement Learning using a neural network policy\\nThe policy can be any algorithm you can think of, and it does not even have to be\\ndeterministic. For example, consider a robotic vacuum cleaner whose reward is the\\namount of dust it picks up in 30 minutes. Its policy could be to move forward with\\nsome probability p every second, or randomly rotate left or right with probability 1 –\\np. The rotation angle would be a random angle between –r and +r. Since this policy\\ninvolves some randomness, it is called a \\nstochastic policy\\n. The robot will have an\\nerratic trajectory, which guarantees that it will eventually get to any place it can reach\\nand pick up all the dust. The question is: how much dust will it pick up in 30\\nminutes?\\nHow would you train such a robot? There are just two \\npolicy parameters\\n you cantweak: the probability p and the angle range r. One possible learning algorithm couldbe to try out many different values for these parameters, and pick the combination\\nthat performs best (see \\nFigure 16-3). This is an example of \\npolicy search\\n, in this caseusing a brute force approach. However, when the \\npolicy space\\n is too large (which isgenerally the case), finding a good set of parameters this way is like searching for a\\nneedle in a gigantic haystack.\\nAnother way to explore the policy space is to \\nuse genetic algorithms\\n. For example, you\\ncould randomly create a first generation of 100 policies and try them out, then ƒkill⁄\\nthe 80 worst policies6 and make the 20 survivors produce 4 offspring each. An off…\\n440 | Chapter 16: Reinforcement Learning\\n7If there is a single parent, this is called \\nasexual reproduction\\n. With two (or more) parents, it is called \\nsexual\\nreproduction\\n. An offspring‡s genome (in this case a set of policy parameters) is randomly composed of parts of\\nits parents‡ genomes.\\nspring is just a copy of its parent\\n7 plus some random variation. The surviving policies\\nplus their offspring together constitute the second generation. You can continue to\\niterate through generations this way, until you find a good policy.\\nFigure 16-3. Four points in policy space and the agent‹s corresponding behavior\\nYet another approach is to use optimization techniques, by evaluating the gradients of\\nthe rewards with regards to the policy parameters, then tweaking these parameters byfollowing the gradient toward higher rewards (\\ngradient ascent\\n). This approach is\\ncalled policy gradients\\n (PG), which we will discuss in more detail later in this chapter.\\nFor example, going back to the vacuum cleaner robot, you could slightly increase \\npand evaluate whether this increases the amount of dust picked up by the robot in 30\\nminutes; if it does, then increase \\np some more, or else reduce p. We will implement a\\npopular PG algorithm using TensorFlow, but before we do we need to create an envi…\\nronment for the agent to live in, so it‡s time to introduce OpenAI gym.\\nIntroduction to OpenAI GymOne of the challenges of Reinforcement Learning is that in order to train an agent,\\nyou first need to have a working environment. If you want to program an agent that\\nwill learn to play an Atari game, you will need an Atari game simulator. If you want to\\nprogram a walking robot, then the environment is the real world and you can directly\\ntrain your robot in that environment, but this has its limits: if the robot falls off a cliff,\\nyou can‡t just click ƒundo.⁄ You can‡t speed up time either; adding more computing\\nIntroduction to OpenAI Gym | 441\\n8OpenAI is a nonprofit artificial intelligence research company, funded in part by Elon Musk. Its stated goal is\\nto promote and develop friendly AIs that will benefit humanity (rather than exterminate it).\\npower won‡t make the robot move any faster. And it‡s generally too expensive to train\\n1,000 robots in parallel. In short, training is hard and slow in the real world, so yougenerally need a simulated environment\\n at least to bootstrap training.\\nOpenAI gym\\n8 is a toolkit that provides a wide variety of simulated environments\\n(Atari games, board games, 2D and 3D physical simulations, and so on), so you can\\ntrain agents, compare them, or develop new RL algorithms.\\nLet‡s install OpenAI gym. For a minimal OpenAI gym installation, simply use pip:\\n$ pip3 install --upgrade gymNext open up a Python shell or a Jupyter notebook and create your first environment:\\n>>> import gym>>> env = gym.make(\"CartPole-v0\")[2016-10-14 16:03:23,199] Making new env: MsPacman-v0>>> obs = env.reset()>>> obsarray([-0.03799846, -0.03288115,  0.02337094,  0.00720711])>>> env.render()The make() function creates an environment, in this case a CartPole environment.\\nThis is a 2D simulation in which a cart can be accelerated left or right in order to bal…\\nance a pole placed on top of it (see Figure 16-4). After the environment is created, we\\nmust initialize it using the \\nreset() method. This returns the first observation. Obser…\\nvations depend on the type of environment. For the CartPole environment, each\\nobservation is a 1D NumPy array containing four floats: these floats represent the\\ncart‡s horizontal position (\\n0.0 = center), its velocity, the angle of the pole (\\n0.0 = verti…\\ncal), and its angular velocity. Finally, the \\nrender() method displays the environment\\nas shown in Figure 16-4.Figure 16-4. \\n•e CartPole environment\\n442 | Chapter 16: Reinforcement Learning\\nIf you want \\nrender() to return the rendered image as a NumPy array, you can set the\\nmode parameter to rgb_array (note that other environments may support different\\nmodes):>>> img = env.render(mode=\"rgb_array\")>>> img.shape  # height, width, channels (3=RGB)(400, 600, 3)Unfortunately, the CartPole (and a few other environments) ren…\\nders the image to the screen even if you set the mode to\"rgb_array\". The only way to avoid this is to use a fake X server\\nsuch as Xvfb or Xdummy. For example, you can install Xvfb and\\nstart Python using the following command: xvfb-run -s \"-screen 0 1400x900x24\" python. Or use the xvfbwrapper package\\n.Let‡s ask the environment what actions are possible:\\n>>> env.action_spaceDiscrete(2)Discrete(2) means that the possible actions are integers 0 and 1, which represent\\naccelerating left (0) or right (1). Other environments may have more discrete actions,\\nor other kinds of actions (e.g., continuous). Since the pole is leaning toward the right,\\nlet‡s accelerate the cart toward the right:\\n>>> action = 1  # accelerate right>>> obs, reward, done, info = env.step(action)>>> obsarray([-0.03865608,  0.16189797,  0.02351508, -0.27801135])>>> reward1.0>>> doneFalse>>> info{}The step() method executes the given action and returns four values:obsThis is the new observation. The cart is now moving toward the right (\\nobs[1]>0).The pole is still tilted toward the right (\\nobs[2]>0), but its angular velocity is nownegative (\\nobs[3]<0), so it will likely be tilted toward the left after the next step.\\nrewardIn this environment, you get a reward of 1.0 at every step, no matter what you do,\\nso the goal is to keep running as long as possible.Introduction to OpenAI Gym | 443\\ndoneThis value will be True when the episode\\n is over. This will happen when the pole\\ntilts too much. After that, the environment must be reset before it can be used\\nagain.infoThis dictionary may provide extra debug information in other environments.\\nThis data should not be used for training (it would be cheating).\\nLet‡s hardcode a simple policy that accelerates left when the pole is leaning toward the\\nleft and accelerates right when the pole is leaning toward the right. We will run this\\npolicy to see the average rewards it gets over 500 episodes:\\ndef basic_policy(obs):    angle = obs[2]    return 0 if angle < 0 else 1totals = []for episode in range(500):    episode_rewards = 0    obs = env.reset()    for step in range(1000): # 1000 steps max, we don•t want to run forever        action = basic_policy(obs)        obs, reward, done, info = env.step(action)        episode_rewards += reward        if done:            break    totals.append(episode_rewards)This code is hopefully self-explanatory. Let‡s look at the result:\\n>>> import numpy as np>>> np.mean(totals), np.std(totals), np.min(totals), np.max(totals)(42.125999999999998, 9.1237121830974033, 24.0, 68.0)Even with 500 tries, this policy never managed to keep the pole upright for more than\\n68 consecutive steps. Not great. If you look at the simulation in the \\nJupyter note…\\nbooks, you will see that the cart oscillates left and right more and more strongly until\\nthe pole tilts too much. Let‡s see if a neural network can come up with a better policy.\\nNeural Network PoliciesLet‡s create a neural network policy. Just like the policy we hardcoded earlier, this\\nneural network will take an observation as input, and it will output the action to be\\nexecuted. More precisely, it will estimate a probability for each action, and then we\\nwill select an action randomly according to the estimated probabilities (see\\nFigure 16-5). In the case of the CartPole environment, there are just two possible\\nactions (left or right), so we only need one output neuron. It will output the probabil…\\nity p of action 0 (left), and of course the probability of action 1 (right) will be 1 – \\np.444 | Chapter 16: Reinforcement Learning\\nFor example, if it outputs 0.7, then we will pick action 0 with 70% probability, and\\naction 1 with 30% probability.\\nFigure 16-5. Neural network policy\\nYou may wonder why we are picking a random action based on the probability given\\nby the neural network, rather than just picking the action with the highest score. This\\napproach lets the agent find the right balance between \\nexploring\\n new actions andexploiting\\n the actions that are known to work well. Here‡s an analogy: suppose you go\\nto a restaurant for the first time, and all the dishes look equally appealing so you ran…\\ndomly pick one. If it turns out to be good, you can increase the probability to order itnext time, but you shouldn‡t increase that probability up to 100%, or else you will\\nnever try out the other dishes, some of which may be even better than the one you\\ntried.Also note that in this particular environment, the past actions and observations can\\nsafely be ignored, since each observation contains the environment‡s full state. If there\\nwere some hidden state, then you may need to consider past actions and observations\\nas well. For example, if the environment only revealed the position of the cart but not\\nits velocity, you would have to consider not only the current observation but also the\\nprevious observation in order to estimate the current velocity. Another example is\\nwhen the observations are noisy; in that case, you generally want to use the past few\\nobservations to estimate the most likely current state. The CartPole problem is thus as\\nNeural Network Policies | 445\\nsimple as can be; the observations are noise-free and they contain the environment‡s\\nfull state.\\nHere is the code to build this neural network policy using TensorFlow:\\nimport tensorflow as tffrom tensorflow.contrib.layers import fully_connected# 1. Specify the neural network architecturen_inputs = 4  # == env.observation_space.shape[0]n_hidden = 4  # it•s a simple task, we don•t need more hidden neuronsn_outputs = 1 # only outputs the probability of accelerating leftinitializer = tf.contrib.layers.variance_scaling_initializer()# 2. Build the neural networkX = tf.placeholder(tf.float32, shape=[None, n_inputs])hidden = fully_connected(X, n_hidden, activation_fn=tf.nn.elu,                         weights_initializer=initializer)logits = fully_connected(hidden, n_outputs, activation_fn=None,                         weights_initializer=initializer)outputs = tf.nn.sigmoid(logits)# 3. Select a random action based on the estimated probabilitiesp_left_and_right = tf.concat(axis=1, values=[outputs, 1 - outputs])action = tf.multinomial(tf.log(p_left_and_right), num_samples=1)init = tf.global_variables_initializer()Let‡s go through this code:\\n1.After the imports, we define the neural network architecture. The number of\\ninputs is the size of the observation space (which in the case of the CartPole is\\nfour), we just have four hidden units and no need for more, and we have just one\\noutput probability (the probability of going left).2.Next we build the neural network. In this example, it‡s a vanilla Multi-Layer Per…\\nceptron, with a single output. Note that the output layer uses the logistic (sig…\\nmoid) activation function in order to output a probability from 0.0 to 1.0. If there\\nwere more than two possible actions, there would be one output neuron peraction, and you would use the softmax activation function instead.\\n3.Lastly, we call the \\nmultinomial() function to pick a random action. This func…tion independently samples one (or more) integers, given the log probability of\\neach integer. For example, if you call it with the array \\n[np.log(0.5),np.log(0.2), np.log(0.3)] and with num_samples=5, then it will output fiveintegers, each of which will have a 50% probability of being 0, 20% of being 1,\\nand 30% of being 2. In our case we just need one integer representing the action\\nto take. Since the outputs tensor only contains the probability of going left, we\\nmust first concatenate \\n1-outputs to it to have a tensor containing the probability\\n446 | Chapter 16: Reinforcement Learning\\nof both left and right actions. Note that if there were more than two possible\\nactions, the neural network would have to output one probability per action so\\nyou would not need the concatenation step.\\nOkay, we now have a neural network policy that will take observations and output\\nactions. But how do we train it?Evaluating Actions: The Credit Assignment ProblemIf we knew what the best action was at each step, we could train the neural network as\\nusual, by minimizing the cross entropy between the estimated probability and the tar…\\nget probability. It would just be regular supervised learning. However, in Reinforce…\\nment Learning the only guidance the agent gets is through rewards, and rewards are\\ntypically sparse and delayed. For example, if the agent manages to balance the pole\\nfor 100 steps, how can it know which of the 100 actions it took were good, and whichof them were bad? All it knows is that the pole fell after the last action, but surely this\\nlast action is not entirely responsible. This is called the \\ncredit assignment problem\\n:when the agent gets a reward, it is hard for it to know which actions should get credi…\\nted (or blamed) for it. Think of a dog that gets rewarded hours after it behaved well;\\nwill it understand what it is rewarded for?\\nTo tackle this problem, a common strategy is to evaluate an action based on the sum\\nof all the rewards that come after it, usually applying a \\ndiscount rate\\n r at each step. For\\nexample (see \\nFigure 16-6), if an agent decides to go right three times in a row and gets\\n+10 reward after the first step, 0 after the second step, and finally –50 after the third\\nstep, then assuming we use a discount rate \\nr = 0.8, the first action will have a total\\nscore of 10 + r ‰ 0 + r2 ‰ (–50) = –22. If the discount rate is close to 0, then future\\nrewards won‡t count for much compared to immediate rewards. Conversely, if the \\ndiscount rate is close to 1, then rewards far into the future will count almost as much\\nas immediate rewards. Typical discount rates are 0.95 or 0.99. With a discount rate of\\n0.95, rewards 13 steps into the future count roughly for half as much as immediate\\nrewards (since 0.9513 Ÿ 0.5), while with a discount rate of 0.99, rewards 69 steps into\\nthe future count for half as much as immediate rewards. In the CartPole environ…\\nment, actions have fairly short-term effects, so choosing a discount rate of 0.95 seems\\nreasonable.Evaluating Actions: The Credit Assignment Problem | 447\\n9ƒSimple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning,⁄ R. Williams\\n(1992).Figure 16-6. Discounted rewards\\nOf course, a good action may be followed by several bad actions that cause the pole to\\nfall quickly, resulting in the good action getting a low score (similarly, a good actor\\nmay sometimes star in a terrible movie). However, if we play the game enough times,\\non average good actions will get a better score than bad ones. So, to get fairly reliable\\naction scores, we must run many episodes and normalize all the action scores (by\\nsubtracting the mean and dividing by the standard deviation). After that, we can rea…\\nsonably assume that actions with a negative score were bad while actions with a posi…\\ntive score were good. Perfect›now that we have a way to evaluate each action, we are\\nready to train our first agent using policy gradients. Let‡s see how.\\nPolicy GradientsAs discussed earlier, PG algorithms optimize the parameters of a policy by following\\nthe gradients toward higher rewards. One popular class of PG algorithms, called\\nREINFORCE algorithms\\n, was introduced back in 1992\\n9 by Ronald Williams. Here is\\none common variant:\\n1.First, let the neural network policy play the game several times and at each step\\ncompute the gradients that would make the chosen action even more likely, but\\ndon‡t apply these gradients yet.\\n448 | Chapter 16: Reinforcement Learning\\n10We already did something similar in \\nChapter 11\\n when we discussed Gradient Clipping: we first computed the\\ngradients, then we clipped them, and finally we applied the clipped gradients.\\n2.Once you have run several episodes, compute each action‡s score (using the\\nmethod described in the previous paragraph).\\n3.If an action‡s score is positive, it means that the action was good and you want to\\napply the gradients computed earlier to make the action even more likely to be\\nchosen in the future. However, if the score is negative, it means the action was\\nbad and you want to apply the opposite gradients to make this action slightly \\nless\\nlikely in the future. The solution is simply to multiply each gradient vector by the\\ncorresponding action‡s score.\\n4.Finally, compute the mean of all the resulting gradient vectors, and use it to per…\\nform a Gradient Descent step.\\nLet‡s implement this algorithm using TensorFlow. We will train the neural network\\npolicy we built earlier so that it learns to balance the pole on the cart. Let‡s start by\\ncompleting the construction phase we coded earlier to add the target probability, the \\ncost function, and the training operation. Since we are acting as though the chosen\\naction is the best possible action, the target probability must be 1.0 if the chosen\\naction is action 0 (left) and 0.0 if it is action 1 (right):\\ny = 1. - tf.to_float(action)Now that we have a target probability, we can define the cost function (cross entropy) \\nand compute the gradients:\\nlearning_rate = 0.01cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(                    labels=y, logits=logits)optimizer = tf.train.AdamOptimizer(learning_rate)grads_and_vars = optimizer.compute_gradients(cross_entropy)Note that we are calling the optimizer‡s \\ncompute_gradients() method instead of theminimize() method. This is because we want to tweak the gradients before we apply\\nthem.10 The compute_gradients() method returns a list of gradient vector/variable\\npairs (one pair per trainable variable). Let‡s put all the gradients in a list, to make it\\nmore convenient to obtain their values:\\ngradients = [grad for grad, variable in grads_and_vars]Okay, now comes the tricky part. During the execution phase, the algorithm will run\\nthe policy and at each step it will evaluate these gradient tensors and store their val…\\nues. After a number of episodes it will tweak these gradients as explained earlier (i.e.,\\nmultiply them by the action scores and normalize them) and compute the mean of\\nthe tweaked gradients. Next, it will need to feed the resulting gradients back to the\\nPolicy Gradients | 449\\noptimizer so that it can perform an optimization step. This means we need one place…\\nholder per gradient vector. Moreover, we must create the operation that will apply the\\nupdated gradients. For this we will call the optimizer‡s \\napply_gradients() function,which takes a list of gradient vector/variable pairs. Instead of giving it the original\\ngradient vectors, we will give it a list containing the updated gradients (i.e., the ones\\nfed through the gradient placeholders):\\ngradient_placeholders = []grads_and_vars_feed = []for grad, variable in grads_and_vars:    gradient_placeholder = tf.placeholder(tf.float32, shape=grad.get_shape())    gradient_placeholders.append(gradient_placeholder)    grads_and_vars_feed.append((gradient_placeholder, variable))training_op = optimizer.apply_gradients(grads_and_vars_feed)Let‡s step back and take a look at the full construction phase:\\nn_inputs = 4n_hidden = 4n_outputs = 1initializer = tf.contrib.layers.variance_scaling_initializer()learning_rate = 0.01X = tf.placeholder(tf.float32, shape=[None, n_inputs])hidden = fully_connected(X, n_hidden, activation_fn=tf.nn.elu,                         weights_initializer=initializer)logits = fully_connected(hidden, n_outputs, activation_fn=None,                         weights_initializer=initializer)outputs = tf.nn.sigmoid(logits)p_left_and_right = tf.concat(axis=1, values=[outputs, 1 - outputs])action = tf.multinomial(tf.log(p_left_and_right), num_samples=1)y = 1. - tf.to_float(action)cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(                    labels=y, logits=logits)optimizer = tf.train.AdamOptimizer(learning_rate)grads_and_vars = optimizer.compute_gradients(cross_entropy)gradients = [grad for grad, variable in grads_and_vars]gradient_placeholders = []grads_and_vars_feed = []for grad, variable in grads_and_vars:    gradient_placeholder = tf.placeholder(tf.float32, shape=grad.get_shape())    gradient_placeholders.append(gradient_placeholder)    grads_and_vars_feed.append((gradient_placeholder, variable))training_op = optimizer.apply_gradients(grads_and_vars_feed)init = tf.global_variables_initializer()saver = tf.train.Saver()450 | Chapter 16: Reinforcement Learning\\nOn to the execution phase! We will need a couple of functions to compute the total\\ndiscounted rewards, given the raw rewards, and to normalize the results across multi…\\nple episodes:def discount_rewards(rewards, discount_rate):    discounted_rewards = np.empty(len(rewards))    cumulative_rewards = 0    for step in reversed(range(len(rewards))):        cumulative_rewards = rewards[step] + cumulative_rewards * discount_rate        discounted_rewards[step] = cumulative_rewards    return discounted_rewardsdef discount_and_normalize_rewards(all_rewards, discount_rate):    all_discounted_rewards = [discount_rewards(rewards)                              for rewards in all_rewards]    flat_rewards = np.concatenate(all_discounted_rewards)    reward_mean = flat_rewards.mean()    reward_std = flat_rewards.std()    return [(discounted_rewards - reward_mean)/reward_std            for discounted_rewards in all_discounted_rewards]Let‡s check that this works:\\n>>> discount_rewards([10, 0, -50], discount_rate=0.8)array([-22., -40., -50.])>>> discount_and_normalize_rewards([[10, 0, -50], [10, 20]], discount_rate=0.8)[array([-0.28435071, -0.86597718, -1.18910299]), array([ 1.26665318,  1.0727777 ])]The call to discount_rewards() returns exactly what we expect (see \\nFigure 16-6).You can verify that the function \\ndiscount_and_normalize_rewards() does indeedreturn the normalized scores for each action in both episodes. Notice that the first\\nepisode was much worse than the second, so its normalized scores are all negative; all\\nactions from the first episode would be considered bad, and conversely all actions\\nfrom the second episode would be considered good.We now have all we need to train the policy:\\nn_iterations = 250      # number of training iterationsn_max_steps = 1000      # max steps per episoden_games_per_update = 10 # train the policy every 10 episodessave_iterations = 10    # save the model every 10 training iterationsdiscount_rate = 0.95with tf.Session() as sess:    init.run()    for iteration in range(n_iterations):        all_rewards = []    # all sequences of raw rewards for each episode        all_gradients = []  # gradients saved at each step of each episode        for game in range(n_games_per_update):            current_rewards = []   # all raw rewards from the current episode            current_gradients = [] # all gradients from the current episodePolicy Gradients | 451\\n            obs = env.reset()            for step in range(n_max_steps):                action_val, gradients_val = sess.run(                        [action, gradients],                        feed_dict={X: obs.reshape(1, n_inputs)}) # one obs                obs, reward, done, info = env.step(action_val[0][0])                current_rewards.append(reward)                current_gradients.append(gradients_val)                if done:                    break            all_rewards.append(current_rewards)            all_gradients.append(current_gradients)        # At this point we have run the policy for 10 episodes, and we are        # ready for a policy update using the algorithm described earlier.        all_rewards = discount_and_normalize_rewards(all_rewards)        feed_dict = {}        for var_index, grad_placeholder in enumerate(gradient_placeholders):            # multiply the gradients by the action scores, and compute the mean            mean_gradients = np.mean(                [reward * all_gradients[game_index][step][var_index]                    for game_index, rewards in enumerate(all_rewards)                    for step, reward in enumerate(rewards)],                axis=0)            feed_dict[grad_placeholder] = mean_gradients        sess.run(training_op, feed_dict=feed_dict)        if iteration % save_iterations == 0:            saver.save(sess, \"./my_policy_net_pg.ckpt\")Each training iteration starts by running the policy for 10 episodes (with maximum\\n1,000 steps per episode, to avoid running forever). At each step, we also compute the\\ngradients, pretending that the chosen action was the best. After these 10 episodes\\nhave been run, we compute the action scores using the \\ndiscount_and_normalize_rewards() function; we go through each trainable variable, across all episodesand all steps, to multiply each gradient vector by its corresponding action score; and\\nwe compute the mean of the resulting gradients. Finally, we run the training opera…\\ntion, feeding it these mean gradients (one per trainable variable). We also save the\\nmodel every 10 training operations.\\nAnd we‡re done! This code will train the neural network policy, and it will success…\\nfully learn to balance the pole on the cart (you can try it out in the Jupyter note…\\nbooks). Note that there are actually two ways the agent can lose the game: either the\\npole can tilt too much, or the cart can go completely off the screen. With 250 training\\niterations, the policy learns to balance the pole quite well, but it is not yet good\\nenough at avoiding going off the screen. A few hundred more training iterations will\\nfix that.\\n452 | Chapter 16: Reinforcement Learning\\nResearchers try to find algorithms that work well even when the\\nagent initially knows nothing about the environment. However,\\nunless you are writing a paper, you should inject as much prior\\nknowledge as possible into the agent, as it will speed up training\\ndramatically. For example, you could add negative rewards propor…\\ntional to the distance from the center of the screen, and to the pole‡s\\nangle. Also, if you already have a reasonably good policy (e.g.,\\nhardcoded), you may want to train the neural network to imitate it\\nbefore using policy gradients to improve it.\\nDespite its relative simplicity, this algorithm is quite powerful. You can use it to tackle\\nmuch harder problems than balancing a pole on a cart. In fact, AlphaGo was based\\non a similar PG algorithm (plus Monte Carlo Tree Search\\n, which is beyond the scopeof this book).We will now look at another popular family of algorithms. Whereas PG algorithms\\ndirectly try to optimize the policy to increase rewards, the algorithms we will look at\\nnow are less direct: the agent learns to estimate the expected sum of discounted future\\nrewards for each state, or the expected sum of discounted future rewards for each\\naction in each state, then uses this knowledge to decide how to act. To understand\\nthese algorithms, we must first introduce \\nMarkov decision processes\\n (MDP).Markov Decision ProcessesIn the early 20th century, the mathematician Andrey Markov studied stochastic pro…\\ncesses with no memory, called \\nMarkov chains\\n. Such a process has a fixed number of\\nstates, and it randomly evolves from one state to another at each step. The probability\\nfor it to evolve from a state \\ns to a state \\nsis fixed, and it depends only on the pair (s,s),not on past states (the system has no memory).\\nFigure 16-7 shows an example of a Markov chain with four states. Suppose that the\\nprocess starts in state \\ns0, and there is a 70% chance that it will remain in that state at\\nthe next step. Eventually it is bound to leave that state and never come back since no\\nother state points back to \\ns0. If it goes to state \\ns1, it will then most likely go to state \\ns2(90% probability), then immediately back to state \\ns1 (with 100% probability). It may\\nalternate a number of times between these two states, but eventually it will fall into\\nstate \\ns3 and remain there forever (this is a terminal state\\n). Markov chains can have\\nvery different dynamics, and they are heavily used in thermodynamics, chemistry,\\nstatistics, and much more.\\nMarkov Decision Processes | 453\\n11ƒA Markovian Decision Process,⁄ R. Bellman (1957).\\nFigure 16-7. Example of a Markov chain\\nMarkov decision processes were \\nfirst described in the 1950s by Richard Bellman\\n.11They resemble Markov chains but with a twist: at each step, an agent can choose one\\nof several possible actions, and the transition probabilities depend on the chosenaction. Moreover, some state transitions return some reward (positive or negative),\\nand the agent‡s goal is to find a policy that will maximize rewards over time.\\nFor example, the MDP represented in \\nFigure 16-8 has three states and up to three\\npossible discrete actions at each step. If it starts in state \\ns0, the agent can choose\\nbetween actions a0, a1, or a2. If it chooses action a1, it just remains in state \\ns0 with cer…\\ntainty, and without any reward. It can thus decide to stay there forever if it wants. But\\nif it chooses action a0, it has a 70% probability of gaining a reward of +10, andremaining in state \\ns0. It can then try again and again to gain as much reward as possi…\\nble. But at one point it is going to end up instead in state \\ns1. In state \\ns1 it has only twopossible actions: a0 or \\na1. It can choose to stay put by repeatedly choosing action \\na1, orit can choose to move on to state \\ns2 and get a negative reward of –50 (ouch). In state \\ns3it has no other choice than to take action a1, which will most likely lead it back tostate \\ns0, gaining a reward of +40 on the way. You get the picture. By looking at this\\nMDP, can you guess which strategy will gain the most reward over time? In state \\ns0 itis clear that action \\na0 is the best option, and in state \\ns3 the agent has no choice but to\\ntake action a1, but in state \\ns1 it is not obvious whether the agent should stay put (\\na0) orgo through the fire (a2).454 | Chapter 16: Reinforcement Learning\\nFigure 16-8. Example of a Markov decision process\\nBellman found a way to estimate the \\noptimal state value\\n of any state \\ns, noted V*(s),which is the sum of all discounted future rewards the agent can expect on average\\nafter it reaches a state \\ns, assuming it acts optimally. He showed that if the agent acts\\noptimally, then the \\nBellman Optimality Equation\\n applies (see \\nEquation 16-1\\n). Thisrecursive equation says that if the agent acts optimally, then the optimal value of the\\ncurrent state is equal to the reward it will get on average after taking one optimal\\naction, plus the expected optimal value of all possible next states that this action can\\nlead to.\\nEquation 16-1. Bellman Optimality Equation\\nV*s=max\\na“sTs,a,sRs,a,s+’.V*sforall\\ns‹T(s, a, s) is the transition probability from state \\ns to state \\ns, given that the agent\\nchose action a.‹R(s, a, s) is the reward that the agent gets when it goes from state \\ns to state \\ns,given that the agent chose action \\na.‹’ is the discount rate.\\nThis equation leads directly to an algorithm that can precisely estimate the optimal\\nstate value of every possible state: you first initialize all the state value estimates to\\nzero, and then you iteratively update them using the \\nValue Iteration\\n algorithm (seeEquation 16-2\\n). A remarkable result is that, given enough time, these estimates are\\nMarkov Decision Processes | 455\\nguaranteed to converge to the optimal state values, corresponding to the optimal pol…\\nicy.\\nEquation 16-2. Value Iteration algorithm\\nVk+1\\nsmaxa“sTs,a,sRs,a,s+’.Vksforall\\ns‹Vk(s) is the estimated value of state \\ns at the \\nkth iteration of the algorithm.\\nThis algorithm is an example of \\nDynamic Programming\\n, whichbreaks down a complex problem (in this case estimating a poten…\\ntially infinite sum of discounted future rewards) into tractable sub-\\nproblems that can be tackled iteratively (in this case finding the\\naction that maximizes the average reward plus the discounted next\\nstate value).\\nKnowing the optimal state values can be useful, in particular to evaluate a policy, but\\nit does not tell the agent explicitly what to do. Luckily, Bellman found a very similar\\nalgorithm to estimate the optimal \\nstate-action values\\n, generally called Q-Values\\n. Theoptimal Q-Value of the state-action pair (\\ns,a), noted Q*(s,a), is the sum of discounted\\nfuture rewards the agent can expect on average after it reaches the state \\ns and chooses\\naction a, but before it sees the outcome of this action, assuming it acts optimally afterthat action.\\nHere is how it works: once again, you start by initializing all the Q-Value estimates to\\nzero, then you update them using the \\nQ-Value Iteration\\n algorithm (see Equation\\n16-3).Equation 16-3. Q-Value Iteration algorithm\\nQk+1\\ns,a“sTs,a,sRs,a,s+’.max\\naQks,aforall\\ns,aOnce you have the optimal Q-Values, defining the optimal policy, noted \\nŽ*(s), is triv…ial: when the agent is in state \\ns, it should choose the action with the highest Q-Value\\nfor that state: \\nŽ*s=argmax\\naQ*s,a.Let‡s apply this algorithm to the MDP represented in \\nFigure 16-8. First, we need todefine the MDP:nan=np.nan  # represents impossible actionsT = np.array([  # shape=[s, a, s•]        [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],456 | Chapter 16: Reinforcement Learning\\n        [[0.0, 1.0, 0.0], [nan, nan, nan], [0.0, 0.0, 1.0]],        [[nan, nan, nan], [0.8, 0.1, 0.1], [nan, nan, nan]],    ])R = np.array([  # shape=[s, a, s•]        [[10., 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]],        [[10., 0.0, 0.0], [nan, nan, nan], [0.0, 0.0, -50.]],        [[nan, nan, nan], [40., 0.0, 0.0], [nan, nan, nan]],    ])possible_actions = [[0, 1, 2], [0, 2], [1]]Now let‡s run the Q-Value Iteration algorithm:\\nQ = np.full((3, 3), -np.inf)  # -inf for impossible actionsfor state, actions in enumerate(possible_actions):    Q[state, actions] = 0.0  # Initial value = 0.0, for all possible actionslearning_rate = 0.01discount_rate = 0.95n_iterations = 100for iteration in range(n_iterations):    Q_prev = Q.copy()    for s in range(3):        for a in possible_actions[s]:            Q[s, a] = np.sum([                T[s, a, sp] * (R[s, a, sp] + discount_rate * np.max(Q_prev[sp]))                for sp in range(3)            ])The resulting Q-Values look like this:\\n>>> Qarray([[ 21.89498982,  20.80024033,  16.86353093],       [  1.11669335,         -inf,   1.17573546],       [        -inf,  53.86946068,         -inf]])>>> np.argmax(Q, axis=1)  # optimal action for each statearray([0, 2, 1])This gives us the optimal policy for this MDP, when using a discount rate of 0.95: in\\nstate \\ns0 choose action \\na0, in state \\ns1 choose action \\na2 (go through the fire!), and in state\\ns2 choose action a1 (the only possible action). Interestingly, if you reduce the discount\\nrate to 0.9, the optimal policy changes: in state \\ns1 the best action becomes a0 (stay put;\\ndon‡t go through the fire). It makes sense because if you value the present much more\\nthan the future, then the prospect of future rewards is not worth immediate pain.\\nTemporal Di†erence Learning and Q-LearningReinforcement Learning problems with discrete actions can often be modeled as\\nMarkov decision processes, but the agent initially has no idea what the transition\\nprobabilities are (it does not know T(s, a, s)), and it does not know what the rewards\\nare going to be either (it does not know R(s, a, s)). It must experience each state and\\nTemporal Di†erence Learning and Q-Learning | 457\\neach transition at least once to know the rewards, and it must experience them multi…\\nple times if it is to have a reasonable estimate of the transition probabilities.\\nThe Temporal \\nDi›erence Learning\\n (TD Learning) algorithm is very similar to the\\nValue Iteration algorithm, but tweaked to take into account the fact that the agent has\\nonly partial knowledge of the MDP. In general we assume that the agent initially\\nknows only the possible states and actions, and nothing more. The agent uses an\\nexploration policy\\n›for example, a purely random policy›to explore the MDP, and as\\nit progresses the TD Learning algorithm updates the estimates of the state values\\nbased on the transitions and rewards that are actually observed (see \\nEquation 16-4\\n).Equation 16-4. TD Learning algorithm\\nVk+1\\ns1”\\n‰Vks+‰r+’.Vks‹‰ is the learning rate (e.g., 0.01).\\nTD Learning has many similarities with Stochastic Gradient\\nDescent, in particular the fact that it handles one sample at a time.\\nJust like SGD, it can only truly converge if you gradually reduce the\\nlearning rate (otherwise it will keep bouncing around the opti…\\nmum).\\nFor each state \\ns, this algorithm simply keeps track of a running average of the imme…\\ndiate rewards the agent gets upon leaving that state, plus the rewards it expects to get\\nlater (assuming it acts optimally).\\nSimilarly, the Q-Learning algorithm is an adaptation of the Q-Value Iteration algo…\\nrithm to the situation where the transition probabilities and the rewards are initially\\nunknown (see Equation 16-5\\n).Equation 16-5. Q-Learning algorithm\\nQk+1\\ns,a1”\\n‰Qks,a+‰r+’.max\\naQks,aFor each state-action pair (\\ns, a), this algorithm keeps track of a running average of the\\nrewards r the agent gets upon leaving the state \\ns with action a, plus the rewards itexpects to get later. Since the target policy would act optimally, we take the maximum\\nof the Q-Value estimates for the next state.\\nHere is how Q-Learning can be implemented:\\n458 | Chapter 16: Reinforcement Learning\\nimport numpy.random as rndlearning_rate0 = 0.05learning_rate_decay = 0.1n_iterations = 20000s = 0 # start in state 0Q = np.full((3, 3), -np.inf)  # -inf for impossible actionsfor state, actions in enumerate(possible_actions):    Q[state, actions] = 0.0  # Initial value = 0.0, for all possible actionsfor iteration in range(n_iterations):    a = rnd.choice(possible_actions[s])  # choose an action (randomly)    sp = rnd.choice(range(3), p=T[s, a]) # pick next state using T[s, a]    reward = R[s, a, sp]    learning_rate = learning_rate0 / (1 + iteration * learning_rate_decay)    Q[s, a] = learning_rate * Q[s, a] + (1 - learning_rate) * (            reward + discount_rate * np.max(Q[sp])        )    s = sp # move to next stateGiven enough iterations, this algorithm will converge to the optimal Q-Values. This is\\ncalled an o›-policy algorithm because the policy being trained is not the one being\\nexecuted. It is somewhat surprising that this algorithm is capable of learning the opti…\\nmal policy by just watching an agent act randomly (imagine learning to play golf\\nwhen your teacher is a drunken monkey). Can we do better?Exploration PoliciesOf course Q-Learning can work only if the exploration policy explores the MDP\\nthoroughly enough. Although a purely random policy is guaranteed to eventually\\nvisit every state and every transition many times, it may take an extremely long time\\nto do so. Therefore, a better option is to use the \\nı-greedy policy\\n: at each step it acts\\nrandomly with probability Ò, or greedily (choosing the action with the highest Q-Value) with probability 1-\\nÒ. The advantage of the \\nÒ-greedy policy (compared to a\\ncompletely random policy) is that it will spend more and more time exploring the\\ninteresting parts of the environment, as the Q-Value estimates get better and better,\\nwhile still spending some time visiting unknown regions of the MDP. It is quite com…\\nmon to start with a high value for Ò (e.g., 1.0) and then gradually reduce it (e.g., downto 0.05).Alternatively, rather than relying on chance for exploration, another approach is to\\nencourage the exploration policy to try actions that it has not tried much before. This\\ncan be implemented as a bonus added to the Q-Value estimates, as shown in \\nEquation\\n16-6.Temporal Di†erence Learning and Q-Learning | 459\\nEquation 16-6. Q-Learning using an exploration function\\nQs,a1”\\n‰Qs,a+‰r+’.max\\n‰fQs,a,Ns,a‹N(s, a) counts the number of times the action \\nawas chosen in state \\ns.‹f(q, n) is an exploration function\\n, such as f(q, n) = q + K/(1 + n), where K is acuriosity hyperparameter that measures how much the agent is attracted to to the\\nunknown.Approximate Q-LearningThe main problem with Q-Learning is that it does not scale well to large (or even\\nmedium) MDPs with many states and actions. Consider trying to use Q-Learning to\\ntrain an agent to play Ms. Pac-Man. There are over 250 pellets that Ms. Pac-Man can\\neat, each of which can be present or absent (i.e., already eaten). So the number of pos…\\nsible states is greater than 2\\n250 Ÿ 10\\n75 (and that‡s considering the possible states only of\\nthe pellets). This is way more than atoms in the observable universe, so there‡s abso…\\nlutely no way you can keep track of an estimate for every single Q-Value.\\nThe solution is to find a function that approximates the Q-Values using a manageable\\nnumber of parameters. This is called \\nApproximate Q-Learning\\n. For years it was rec…\\nommended to use linear combinations of hand-crafted features extracted from the\\nstate (e.g., distance of the closest ghosts, their directions, and so on) to estimate Q-\\nValues, but DeepMind showed that using deep neural networks can work much bet…\\nter, especially for complex problems, and it does not require any feature engineering.\\nA DNN used to estimate Q-Values is called a \\ndeep Q-network\\n (DQN), and using a\\nDQN for Approximate Q-Learning is called \\nDeep Q-Learning\\n.In the rest of this chapter, we will use Deep Q-Learning to train an agent to play Ms.\\nPac-Man, much like DeepMind did in 2013. The code can easily be tweaked to learn\\nto play the majority of Atari games quite well. It can achieve superhuman skill at most\\naction games, but it is not so good at games with long-running storylines.\\nLearning to Play Ms. Pac-Man Using Deep Q-LearningSince we will be using an Atari environment, we must first install OpenAI gym‡s Atari\\ndependencies. While we‡re at it, we will also install dependencies for other OpenAI\\ngym environments that you may want to play with. On macOS, assuming you have\\ninstalled Homebrew\\n, you need to run:$ brew install cmake boost boost-python sdl2 swig wget460 | Chapter 16: Reinforcement Learning\\nOn Ubuntu, type the following command (replacing \\npython3 with python if you areusing Python 2):$ apt-get install -y python3-numpy python3-dev cmake zlib1g-dev libjpeg-dev\\\\    xvfb libav-tools xorg-dev python3-opengl libboost-all-dev libsdl2-dev swigThen install the extra Python modules:$ pip3 install --upgrade •gym[all]•If everything went well, you should be able to create a Ms. Pac-Man environment:\\n>>> env = gym.make(\"MsPacman-v0\")>>> obs = env.reset()>>> obs.shape  # [height, width, channels](210, 160, 3)>>> env.action_spaceDiscrete(9)As you can see, there are nine discrete actions available, which correspond to the nine\\npossible positions of the joystick (left, right, up, down, center, upper left, and so on),\\nand the observations are simply screenshots of the Atari screen (see \\nFigure 16-9, left),represented as 3D NumPy arrays. These images are a bit large, so we will create a\\nsmall preprocessing function that will crop the image and shrink it down to 88 ‰ 80\\npixels, convert it to grayscale, and improve the contrast of Ms. Pac-Man. This will\\nreduce the amount of computations required by the DQN, and speed up training.\\nmspacman_color = np.array([210, 164, 74]).mean()def preprocess_observation(obs):    img = obs[1:176:2, ::2] # crop and downsize    img = img.mean(axis=2) # to greyscale    img[img==mspacman_color] = 0 # improve contrast    img = (img - 128) / 128 - 1 # normalize from -1. to 1.    return img.reshape(88, 80, 1)The result of preprocessing is shown in Figure 16-9 (right).\\nLearning to Play Ms. Pac-Man Using Deep Q-Learning | 461\\nFigure 16-9. Ms. Pac-Man observation, original \\n(le“) and \\na“er preprocessing (right)\\nNext, let‡s create the DQN. It could just take a state-action pair (\\ns,a) as input, and out…\\nput an estimate of the corresponding Q-Value \\nQ(s,a), but since the actions are dis…crete it is more convenient to use a neural network that takes only a state \\ns as input\\nand outputs one Q-Value estimate per action. The DQN will be composed of three\\nconvolutional layers, followed by two fully connected layers, including the output\\nlayer (see \\nFigure 16-10).Figure 16-10. Deep Q-network to play Ms. Pac-Man\\n462 | Chapter 16: Reinforcement Learning\\nAs we will see, the training algorithm we will use requires two DQNs with the same\\narchitecture (but different parameters): one will be used to drive Ms. Pac-Man during\\ntraining (the actor\\n), and the other will watch the actor and learn from its trials and\\nerrors (the critic\\n). At regular intervals we will copy the critic to the actor. Since we\\nneed two identical DQNs, we will create a \\nq_network() function to build them:from tensorflow.contrib.layers import convolution2d, fully_connectedinput_height = 88input_width = 80input_channels = 1conv_n_maps = [32, 64, 64]conv_kernel_sizes = [(8,8), (4,4), (3,3)]conv_strides = [4, 2, 1]conv_paddings = [\"SAME\"]*3conv_activation = [tf.nn.relu]*3n_hidden_in = 64 * 11 * 10  # conv3 has 64 maps of 11x10 eachn_hidden = 512hidden_activation = tf.nn.relun_outputs = env.action_space.n  # 9 discrete actions are availableinitializer = tf.contrib.layers.variance_scaling_initializer()def q_network(X_state, scope):    prev_layer = X_state    conv_layers = []    with tf.variable_scope(scope) as scope:        for n_maps, kernel_size, stride, padding, activation in zip(                conv_n_maps, conv_kernel_sizes, conv_strides,                conv_paddings, conv_activation):            prev_layer = convolution2d(                prev_layer, num_outputs=n_maps, kernel_size=kernel_size,                stride=stride, padding=padding, activation_fn=activation,                weights_initializer=initializer)            conv_layers.append(prev_layer)        last_conv_layer_flat = tf.reshape(prev_layer, shape=[-1, n_hidden_in])        hidden = fully_connected(            last_conv_layer_flat, n_hidden, activation_fn=hidden_activation,            weights_initializer=initializer)        outputs = fully_connected(            hidden, n_outputs, activation_fn=None,            weights_initializer=initializer)    trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,                                       scope=scope.name)    trainable_vars_by_name = {var.name[len(scope.name):]: var                              for var in trainable_vars}    return outputs, trainable_vars_by_nameThe first part of this code defines the hyperparameters of the DQN architecture.\\nThen the q_network() function creates the DQN, taking the environment‡s state\\nX_state as input, and the name of the variable scope. Note that we will just use one\\nLearning to Play Ms. Pac-Man Using Deep Q-Learning | 463\\nobservation to represent the environment‡s state since there‡s almost no hidden state\\n(except for blinking objects and the ghosts‡ directions).\\nThe trainable_vars_by_name dictionary gathers all the trainable variables of this\\nDQN. It will be useful in a minute when we create operations to copy the critic DQN\\nto the actor DQN. The keys of the dictionary are the names of the variables, stripping\\nthe part of the prefix that just corresponds to the scope‡s name. It looks like this:\\n>>> trainable_vars_by_name{•/Conv/biases:0•: <tensorflow.python.ops.variables.Variable at 0x121cf7b50>, •/Conv/weights:0•: <tensorflow.python.ops.variables.Variable...>, •/Conv_1/biases:0•: <tensorflow.python.ops.variables.Variable...>, •/Conv_1/weights:0•: <tensorflow.python.ops.variables.Variable...>, •/Conv_2/biases:0•: <tensorflow.python.ops.variables.Variable...>, •/Conv_2/weights:0•: <tensorflow.python.ops.variables.Variable...>, •/fully_connected/biases:0•: <tensorflow.python.ops.variables.Variable...>, •/fully_connected/weights:0•: <tensorflow.python.ops.variables.Variable...>, •/fully_connected_1/biases:0•: <tensorflow.python.ops.variables.Variable...>, •/fully_connected_1/weights:0•: <tensorflow.python.ops.variables.Variable...>}Now let‡s create the input placeholder, the two DQNs, and the operation to copy the\\ncritic DQN to the actor DQN:\\nX_state = tf.placeholder(tf.float32, shape=[None, input_height, input_width,                                            input_channels])actor_q_values, actor_vars = q_network(X_state, scope=\"q_networks/actor\")critic_q_values, critic_vars = q_network(X_state, scope=\"q_networks/critic\")copy_ops = [actor_var.assign(critic_vars[var_name])            for var_name, actor_var in actor_vars.items()]copy_critic_to_actor = tf.group(*copy_ops)Let‡s step back for a second: we now have two DQNs that are both capable of taking\\nan environment state (i.e., a preprocessed observation) as input and outputting an\\nestimated Q-Value for each possible action in that state. Plus we have an operation\\ncalled copy_critic_to_actor to copy all the trainable variables of the critic DQN to\\nthe actor DQN. We use TensorFlow‡s \\ntf.group() function to group all the assign…ment operations into a single convenient operation.\\nThe actor DQN can be used to play Ms. Pac-Man (initially very badly). As discussed\\nearlier, you want it to explore the game thoroughly enough, so you generally want to\\ncombine it with an ı-greedy policy\\n or another exploration strategy.\\nBut what about the critic DQN? How will it learn to play the game? The short answer\\nis that it will try to make its Q-Value predictions match the Q-Values estimated by the\\nactor through its experience of the game. Specifically, we will let the actor play for a\\nwhile, storing all its experiences in a replay memory\\n. Each memory will be a 5-tuple\\n(state, action, next state, reward, continue), where the ƒcontinue⁄ item will be equal to\\n0.0 when the game is over, or 1.0 otherwise. Next, at regular intervals we will sample a\\n464 | Chapter 16: Reinforcement Learning\\nbatch of memories from the replay memory, and we will estimate the Q-Values from\\nthese memories. Finally, we will train the critic DQN to predict these Q-Values using\\nregular supervised learning techniques. Once every few training iterations, we will\\ncopy the critic DQN to the actor DQN. And that‡s it! \\nEquation 16-7\\n shows the costfunction used to train the critic DQN:\\nEquation 16-7. Deep Q-Learning cost function\\nJ–critic=1m“i=1\\nmyi”Qsi,ai,–critic2withyi=ri+’.max\\naQsi,a,–actor‹s(i), a(i), r(i) and s(i) are respectively the state, action, reward, and next state of the\\nith memory sampled from the replay memory.\\n‹m is the size of the memory batch.\\n‹–critic and –actor are the critic and the actor‡s parameters.\\n‹Q(s(i),a(i),–critic) is the critic DQN‡s prediction of the i\\nth memorized state-action‡s Q-\\nValue.\\n‹Q(s(i), a, –actor) is the actor DQN‡s prediction of the Q-Value it can expect from\\nthe next state \\ns(i) if it chooses action a.‹y(i) is the target Q-Value for the i\\nth memory. Note that it is equal to the reward\\nactually observed by the actor, plus the actor‡s \\nprediction\\n of what future rewards it\\nshould expect if it were to play optimally (as far as it knows).\\n‹J(–critic) is the cost function used to train the critic DQN. As you can see, it is just\\nthe Mean Squared Error between the target Q-Values \\ny(i) as estimated by the actor\\nDQN, and the critic DQN‡s predictions of these Q-Values.\\nThe replay memory is optional, but highly recommended. Without\\nit, you would train the critic DQN using consecutive experiences\\nthat may be very correlated. This would introduce a lot of bias and\\nslow down the training algorithm‡s convergence. By using a replay\\nmemory, we ensure that the memories fed to the training algorithm\\ncan be fairly uncorrelated.\\nLet‡s add the critic DQN‡s training operations. First, we need to be able to compute its\\npredicted Q-Values for each state-action in the memory batch. Since the DQN out…\\nputs one Q-Value for every possible action, we need to keep only the Q-Value that\\ncorresponds to the action that was actually chosen in this memory. For this, we will\\nconvert the action to a one-hot vector (recall that this is a vector full of 0s except for a\\nLearning to Play Ms. Pac-Man Using Deep Q-Learning | 465\\n1 at the i\\nth index), and multiply it by the Q-Values: this will zero out all Q-Values\\nexcept for the one corresponding to the memorized action. Then just sum over thefirst axis to obtain only the desired Q-Value prediction for each memory.\\nX_action = tf.placeholder(tf.int32, shape=[None])q_value = tf.reduce_sum(critic_q_values * tf.one_hot(X_action, n_outputs),                        axis=1, keep_dims=True)Next let‡s add the training operations, assuming the target Q-Values will be fed\\nthrough a placeholder. We also create a nontrainable variable called \\nglobal_step. The optimizer‡s \\nminimize() operation will take care of incrementing it. Plus we cre…\\nate the usual \\ninit operation and a \\nSaver.y = tf.placeholder(tf.float32, shape=[None, 1])cost = tf.reduce_mean(tf.square(y - q_value))global_step = tf.Variable(0, trainable=False, name=•global_step•)optimizer = tf.train.AdamOptimizer(learning_rate)training_op = optimizer.minimize(cost, global_step=global_step)init = tf.global_variables_initializer()saver = tf.train.Saver()That‡s it for the construction phase. Before we look at the execution phase, we will\\nneed a couple of tools. First, let‡s start by implementing the replay memory. We will\\nuse a deque list since it is very efficient at pushing items to the queue and popping\\nthem out from the end of the list when the maximum memory size is reached. We\\nwill also write a small function to randomly sample a batch of experiences from the\\nreplay memory:\\nfrom collections import dequereplay_memory_size = 10000replay_memory = deque([], maxlen=replay_memory_size)def sample_memories(batch_size):    indices = rnd.permutation(len(replay_memory))[:batch_size]    cols = [[], [], [], [], []] # state, action, reward, next_state, continue    for idx in indices:        memory = replay_memory[idx]        for col, value in zip(cols, memory):            col.append(value)    cols = [np.array(col) for col in cols]    return (cols[0], cols[1], cols[2].reshape(-1, 1), cols[3],            cols[4].reshape(-1, 1))Next, we will need the actor to explore the game. We will use the \\nÒ-greedy policy, and\\ngradually decrease Ò from 1.0 to 0.05, in 50,000 training steps:eps_min = 0.05eps_max = 1.0eps_decay_steps = 50000466 | Chapter 16: Reinforcement Learning\\ndef epsilon_greedy(q_values, step):    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)    if rnd.rand() < epsilon:        return rnd.randint(n_outputs) # random action    else:        return np.argmax(q_values) # optimal actionThat‡s it! We have all we need to start training. The execution phase does not contain\\nanything too complex, but it is a bit long, so take a deep breath. Ready? Let‡s go! First,\\nlet‡s initialize a few variables:\\nn_steps = 100000  # total number of training stepstraining_start = 1000  # start training after 1,000 game iterationstraining_interval = 3  # run a training step every 3 game iterationssave_steps = 50  # save the model every 50 training stepscopy_steps = 25  # copy the critic to the actor every 25 training stepsdiscount_rate = 0.95skip_start = 90  # skip the start of every game (it•s just waiting time)batch_size = 50iteration = 0  # game iterationscheckpoint_path = \"./my_dqn.ckpt\"done = True # env needs to be resetNext, let‡s open the session and run the main training loop:\\nwith tf.Session() as sess:    if os.path.isfile(checkpoint_path):        saver.restore(sess, checkpoint_path)    else:        init.run()    while True:        step = global_step.eval()        if step >= n_steps:            break        iteration += 1        if done: # game over, start again            obs = env.reset()            for skip in range(skip_start): # skip the start of each game                obs, reward, done, info = env.step(0)            state = preprocess_observation(obs)        # Actor evaluates what to do        q_values = actor_q_values.eval(feed_dict={X_state: [state]})        action = epsilon_greedy(q_values, step)        # Actor plays        obs, reward, done, info = env.step(action)        next_state = preprocess_observation(obs)        # Let•s memorize what just happened        replay_memory.append((state, action, reward, next_state, 1.0 - done))        state = next_stateLearning to Play Ms. Pac-Man Using Deep Q-Learning | 467\\n        if iteration < training_start or iteration % training_interval != 0:            continue        # Critic learns        X_state_val, X_action_val, rewards, X_next_state_val, continues = (            sample_memories(batch_size))        next_q_values = actor_q_values.eval(            feed_dict={X_state: X_next_state_val})        max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)        y_val = rewards + continues * discount_rate * max_next_q_values        training_op.run(feed_dict={X_state: X_state_val,                                   X_action: X_action_val, y: y_val})        # Regularly copy critic to actor        if step % copy_steps == 0:            copy_critic_to_actor.run()        # And save regularly        if step % save_steps == 0:            saver.save(sess, checkpoint_path)We start by restoring the models if a checkpoint file exists, or else we just initialize the\\nvariables normally. Then the main loop starts, where \\niteration counts the total\\nnumber of game steps we have gone through since the program started, and \\nstepcounts the total number of training steps since training started (if a checkpoint is\\nrestored, the global step is restored as well). Then the code resets the game (and skipsthe first boring game steps, where nothing happens). Next, the actor evaluates what to\\ndo, and plays the game, and its experience is memorized in replay memory. Then, at\\nregular intervals (after a warmup period), the critic goes through a training step. It\\nsamples a batch of memories and asks the actor to estimate the Q-Values of all actions\\nfor the next state, and it applies \\nEquation 16-7\\n to compute the target Q-Value \\ny_val.The only tricky part here is that we must multiply the next state‡s Q-Values by the\\ncontinues vector to zero out the Q-Values corresponding to memories where the\\ngame was over. Next we run a training operation to improve the critic‡s ability to pre…\\ndict Q-Values. Finally, at regular intervals we copy the critic to the actor, and we save\\nthe model.468 | Chapter 16: Reinforcement Learning\\nUnfortunately, training is very slow: if you use your laptop for\\ntraining, it will take days before Ms. Pac-Man gets any good, and if\\nyou look at the learning curve, measuring the average rewards per\\nepisode, you will notice that it is extremely noisy. At some points\\nthere may be no apparent progress for a very long time until sud…\\ndenly the agent learns to survive a reasonable amount of time. As\\nmentioned earlier, one solution is to inject as much prior knowl…\\nedge as possible into the model (e.g., through preprocessing,\\nrewards, and so on), and you can also try to bootstrap \\nthe model byfirst training it to imitate a basic strategy. In any case, RL still\\nrequires quite a lot of patience and tweaking, but the end result is\\nvery exciting.\\nExercises1.How would you define Reinforcement Learning? How is it different from regular\\nsupervised or unsupervised learning?\\n2.Can you think of three possible applications of RL that were not mentioned in\\nthis chapter? For each of them, what is the environment? What is the agent?\\nWhat are possible actions? What are the rewards?\\n3.What is the discount rate? Can the optimal policy change if you modify the dis…\\ncount rate?\\n4.How do you measure the performance of a Reinforcement Learning agent?\\n5.What is the credit assignment problem? When does it occur? How can you allevi…\\nate it?\\n6.What is the point of using a replay memory?\\n7.What is an off-policy RL algorithm?\\n8.Use Deep Q-Learning to tackle OpenAI gym‡s ƒBypedalWalker-v2.⁄ The Q-\\nnetworks do not need to be very deep for this task.\\n9.Use policy gradients to train an agent to play \\nPong\\n, the famous Atari game (\\nPong-v0 in the OpenAI gym). Beware: an individual observation is insufficient to tell\\nthe direction and speed of the ball. One solution is to pass two observations at a\\ntime to the neural network policy. To reduce dimensionality and speed up train…\\ning, you should definitely preprocess these images (crop, resize, and convert\\nthem to black and white), and possibly merge them into a single image (e.g., by\\noverlaying them).\\n10.If you have about $100 to spare, you can purchase a Raspberry Pi 3 plus some\\ncheap robotics components, install TensorFlow on the Pi, and go wild! For an\\nexample, check out this \\nfun post by Lukas Biewald, or take a look at GoPiGo or\\nBrickPi. Why not try to build a real-life cartpole by training the robot using pol…\\nExercises | 469\\nicy gradients? Or build a robotic spider that learns to walk; give it rewards any\\ntime it gets closer to some objective (you will need sensors to measure the dis…tance to the objective). The only limit is your imagination.\\nSolutions to these exercises are available in \\nAppendix A\\n.Thank You!Before we close the last chapter of this book, I would like to thank you for reading it\\nup to the last paragraph. I truly hope that you had as much pleasure reading this book\\nas I had writing it, and that it will be useful for your projects, big or small.\\nIf you find errors, please send feedback. More generally, I would love to know what\\nyou think, so please don‡t hesitate to contact me via O‡Reilly, or through the \\nageron/\\nhandson-ml\\n GitHub project.\\nGoing forward, my best advice to you is to practice and practice: try going through all\\nthe exercises if you have not done so already, play with the Jupyter notebooks, join\\nKaggle.com or some other ML community, watch ML courses, read papers, attend\\nconferences, meet experts. You may also want to study some topics that we did not\\ncover in this book, including recommender systems, clustering algorithms, anomalydetection algorithms, and genetic algorithms.My greatest hope is that this book will inspire you to build a wonderful ML applica…\\ntion that will benefit all of us! What will it be?\\nAurłlien Głron, November 26th, 2016\\n470 | Chapter 16: Reinforcement Learning\\nAPPENDIX AExercise SolutionsSolutions to the coding exercises are available in the online Jupyter\\nnotebooks at \\nhttps://github.com/ageron/handson-ml\\n.Chapter 1: The Machine Learning Landscape1.Machine Learning is about building systems that can learn from data. Learning\\nmeans getting better at some task, given some performance measure.\\n2.Machine Learning is great for complex problems for which we have no algorith…\\nmic solution, to replace long lists of hand-tuned rules, to build systems that adapt\\nto fluctuating environments, and finally to help humans learn (e.g., data mining).\\n3.A labeled training set is a training set that contains the desired solution (a.k.a. a\\nlabel) for each instance.4.The two most common supervised tasks are regression and classification.\\n5.Common unsupervised tasks include clustering, visualization, dimensionality\\nreduction, and association rule learning.\\n6.Reinforcement Learning is likely to perform best if we want a robot to learn to\\nwalk in various unknown terrains since this is typically the type of problem that\\nReinforcement Learning tackles. It might be possible to express the problem as a\\nsupervised or semisupervised learning problem, but it would be less natural.\\n7.If you don‡t know how to define the groups, then you can use a clustering algo…\\nrithm (unsupervised learning) to segment your customers into clusters of similar\\ncustomers. However, if you know what groups you would like to have, then you\\n471can feed many examples of each group to a classification algorithm (supervised\\nlearning), and it will classify all your customers into these groups.\\n8.Spam detection is a typical supervised learning problem: the algorithm is fed\\nmany emails along with their label (spam or not spam).\\n9.An online learning system can learn incrementally, as opposed to a batch learn…\\ning system. This makes it capable of adapting rapidly to both changing data and\\nautonomous systems, and of training on very large quantities of data.\\n10.Out-of-core algorithms can handle vast quantities of data that cannot fit in a\\ncomputer‡s main memory. An out-of-core learning algorithm chops the data into\\nmini-batches and uses online learning techniques to learn from these mini-\\nbatches.\\n11.An instance-based learning system learns the training data by heart; then, when\\ngiven a new instance, it uses a similarity measure to find the most similar learnedinstances and uses them to make predictions.12.A model has one or more model parameters that determine what it will predict\\ngiven a new instance (e.g., the slope of a linear model). A learning algorithm triesto find optimal values for these parameters such that the model generalizes well\\nto new instances. A hyperparameter is a parameter of the learning algorithm\\nitself, not of the model (e.g., the amount of regularization to apply).\\n13.Model-based learning algorithms search for an optimal value for the model\\nparameters such that the model will generalize well to new instances. We usually\\ntrain such systems by minimizing a cost function that measures how bad the sys…\\ntem is at making predictions on the training data, plus a penalty for model com…\\nplexity if the model is regularized. To make predictions, we feed the new\\ninstance‡s features into the model‡s prediction function, using the parameter val…\\nues found by the learning algorithm.14.Some of the main challenges in Machine Learning are the lack of data, poor data\\nquality, nonrepresentative data, uninformative features, excessively simple mod…\\nels that underfit the training data, and excessively complex models that overfit\\nthe data.\\n15.If a model performs great on the training data but generalizes poorly to new\\ninstances, the model is likely overfitting the training data (or we got extremely\\nlucky on the training data). Possible solutions to overfitting are getting more\\ndata, simplifying the model (selecting a simpler algorithm, reducing the number\\nof parameters or features used, or regularizing the model), or reducing the noise\\nin the training data.\\n16.A test set is used to estimate the generalization error that a model will make on\\nnew instances, before the model is launched in production.\\n472 | Appendix A: Exercise Solutions\\n1If you draw a straight line between any two points on the curve, the line never crosses the curve.\\n17.A validation set is used to compare models. It makes it possible to select the best\\nmodel and tune the hyperparameters.\\n18.If you tune hyperparameters using the test set, you risk overfitting the test set,\\nand the generalization error you measure will be optimistic (you may launch a\\nmodel that performs worse than you expect).\\n19.Cross-validation is a technique that makes it possible to compare models (for\\nmodel selection and hyperparameter tuning) without the need for a separate vali…\\ndation set. This saves precious training data.\\nChapter 2: End-to-End Machine Learning ProjectSee the Jupyter notebooks available at \\nhttps://github.com/ageron/handson-ml\\n.Chapter 3: Classi•cationSee the Jupyter notebooks available at \\nhttps://github.com/ageron/handson-ml\\n.Chapter 4: Training Linear Models1.If you have a training set with millions of features you can use Stochastic Gradi…\\nent Descent or Mini-batch Gradient Descent, and perhaps Batch Gradient\\nDescent if the training set fits in memory. But you cannot use the Normal Equa…\\ntion because the computational complexity grows quickly (more than quadrati…\\ncally) with the number of features.\\n2.If the features in your training set have very different scales, the cost function will\\nhave the shape of an elongated bowl, so the Gradient Descent algorithms will take\\na long time to converge. To solve this you should scale the data before training\\nthe model. Note that the Normal Equation will work just fine without scaling.\\n3.Gradient Descent cannot get stuck in a local minimum when training a Logistic\\nRegression model because the cost function is convex.\\n14.If the optimization problem is convex (such as Linear Regression or Logistic\\nRegression), and assuming the learning rate is not too high, then all Gradient\\nDescent algorithms will approach the global optimum and end up producing\\nfairly similar models. However, unless you gradually reduce the learning rate,\\nStochastic GD and Mini-batch GD will never truly converge; instead, they will\\nkeep jumping back and forth around the global optimum. This means that even\\nExercise Solutions | 473\\n2Moreover, the Normal Equation requires computing the inverse of a matrix, but that matrix is not always\\ninvertible. In contrast, the matrix for Ridge Regression is always invertible.\\nif you let them run for a very long time, these Gradient Descent algorithms will\\nproduce slightly different models.\\n5.If the validation error consistently goes up after every epoch, then one possibility\\nis that the learning rate is too high and the algorithm is diverging. If the training\\nerror also goes up, then this is clearly the problem and you should reduce the\\nlearning rate. However, if the training error is not going up, then your model is\\noverfitting the training set and you should stop training.6.Due to their random nature, neither Stochastic Gradient Descent nor Mini-batch\\nGradient Descent is guaranteed to make progress at every single training itera…\\ntion. So if you immediately stop training when the validation error goes up, you\\nmay stop much too early, before the optimum is reached. A better option is to\\nsave the model at regular intervals, and when it has not improved for a long time\\n(meaning it will probably never beat the record), you can revert to the best saved\\nmodel.7.Stochastic Gradient Descent has the fastest training iteration since it considers\\nonly one training instance at a time, so it is generally the first to reach the vicinity\\nof the global optimum (or Mini-batch GD with a very small mini-batch size).\\nHowever, only Batch Gradient Descent will actually converge, given enough\\ntraining time. As mentioned, Stochastic GD and Mini-batch GD will bounce\\naround the optimum, unless you gradually reduce the learning rate.\\n8.If the validation error is much higher than the training error, this is likely because\\nyour model is overfitting the training set. One way to try to fix this is to reduce\\nthe polynomial degree: a model with fewer degrees of freedom is less likely tooverfit. Another thing you can try is to regularize the model›for example, by\\nadding an —2 penalty (Ridge) or an —1 penalty (Lasso) to the cost function. Thiswill also reduce the degrees of freedom of the model. Lastly, you can try to\\nincrease the size of the training set.9.If both the training error and the validation error are almost equal and fairly\\nhigh, the model is likely underfitting the training set, which means it has a highbias. You should try reducing the regularization hyperparameter \\n‰.10.Let‡s see:\\n‹A model with some regularization typically performs better than a model\\nwithout any regularization, so you should generally prefer Ridge Regression\\nover plain Linear Regression.2‹Lasso Regression uses an —1 penalty, which tends to push the weights down to\\nexactly zero. This leads to sparse models, where all weights are zero except for\\n474 | Appendix A: Exercise Solutions\\nthe most important weights. This is a way to perform feature selection auto…\\nmatically, which is good if you suspect that only a few features actually matter.\\nWhen you are not sure, you should prefer Ridge Regression.‹Elastic Net is generally preferred over Lasso since Lasso may behave erratically\\nin some cases (when several features are strongly correlated or when there are\\nmore features than training instances). However, it does add an extra hyper…\\nparameter to tune. If you just want Lasso without the erratic behavior, you can\\njust use Elastic Net with an \\nl1_ratio close to 1.11.If you want to classify pictures as outdoor/indoor and daytime/nighttime, since\\nthese are not exclusive classes (i.e., all four combinations are possible) you should\\ntrain two Logistic Regression classifiers.12.See the Jupyter notebooks available at \\nhttps://github.com/ageron/handson-ml\\n.Chapter 5: Support Vector Machines1.The fundamental idea behind Support Vector Machines is to fit the widest possi…\\nble ƒstreet⁄ between the classes. In other words, the goal is to have the largest pos…\\nsible margin between the decision boundary that separates the two classes and\\nthe training instances. When performing soft margin classification, the SVM\\nsearches for a compromise between perfectly separating the two classes and hav…\\ning the widest possible street (i.e., a few instances may end up on the street).\\nAnother key idea is to use kernels when training on nonlinear datasets.\\n2.After training an SVM, a support vector\\n is any instance located on the ƒstreet⁄ (see\\nthe previous answer), including its border. The decision boundary is entirely\\ndetermined by the support vectors. Any instance that is \\nnot\\n a support vector (i.e.,off the street) has no influence whatsoever; you could remove them, add more\\ninstances, or move them around, and as long as they stay off the street they won‡t\\naffect the decision boundary. Computing the predictions only involves the sup…\\nport vectors, not the whole training set.3.SVMs try to fit the largest possible ƒstreet⁄ between the classes (see the first\\nanswer), so if the training set is not scaled, the SVM will tend to neglect smallfeatures (see \\nFigure 5-2).4.An SVM classifier can output the distance between the test instance and the deci…sion boundary, and you can use this as a confidence score. However, this score\\ncannot be directly converted into an estimation of the class probability. If you set\\nprobability=True when creating an SVM in Scikit-Learn, then after training it\\nwill calibrate the probabilities using Logistic Regression on the SVM‡s scores\\n(trained by an additional five-fold cross-validation on the training data). This\\nwill add the predict_proba() and predict_log_proba() methods to the SVM.Exercise Solutions | 475\\n3log2 is the binary log, log\\n2(m) = log(m) / log(2).5.This question applies only to linear SVMs since kernelized can only use the dual\\nform. The computational complexity of the primal form of the SVM problem is\\nproportional to the number of training instances \\nm, while the computational\\ncomplexity of the dual form is proportional to a number between \\nm2 and m3. Soif there are millions of instances, you should definitely use the primal form,because the dual form will be much too slow.\\n6.If an SVM classifier trained with an RBF kernel underfits the training set, theremight be too much regularization. To decrease it, you need to increase \\ngamma or \\nC(or both).7.Let‡s call the QP parameters for the hard-margin problem \\nH, f, A\\nand b\\n(seeƒQuadratic Programming⁄ on page \\n159). The QP parameters for the soft-marginproblem have \\nm additional parameters (np = n + 1 + m) and m additional con…straints (\\nnc = 2m). They can be defined like so:‹H is equal to H, plus m columns of 0s on the right and \\nm rows of 0s at the\\nbottom: =000\\n\\n‹f is equal to f\\nwith m additional elements, all equal to the value of the hyper…\\nparameter C.‹b is equal to bwith m additional elements, all equal to 0.\\n‹A is equal to A, with an extra m ‰ m identity matrix \\nIm appended to the right,\\n– Im just below it, and the rest filled with zeros: =m0”mFor the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available\\nat \\nhttps://github.com/ageron/handson-ml\\n.Chapter 6: Decision Trees1.The depth of a well-balanced binary tree containing \\nm leaves is equal to log\\n2(m)3,rounded up. A binary Decision Tree (one that makes only binary decisions, as is\\nthe case of all trees in Scikit-Learn) will end up more or less well balanced at the\\nend of training, with one leaf per training instance if it is trained without restric…tions. Thus, if the training set contains one million instances, the Decision Tree\\nwill have a depth of log\\n2(106) Ÿ\\n 20 (actually a bit more since the tree will generallynot be perfectly well balanced).476 | Appendix A: Exercise Solutions\\n2.A node‡s Gini impurity is generally lower than its parent‡s. This is ensured by the\\nCART training algorithm‡s cost function, which splits each node in a way that\\nminimizes the weighted sum of its children‡s Gini impurities. However, if one\\nchild is smaller than the other, it is possible for it to have a higher Gini impurity\\nthan its parent, as long as this increase is more than compensated for by a\\ndecrease of the other child‡s impurity. For example, consider a node containing\\nfour instances of class A and 1 of class B. Its Gini impurity is \\n1”\\n152”452 = 0.32.Now suppose the dataset is one-dimensional and the instances are lined up in the\\nfollowing order: A, B, A, A, A. You can verify that the algorithm will split this\\nnode after the second instance, producing one child node with instances A, B,\\nand the other child node with instances A, A, A. The first child node‡s Gini\\nimpurity is \\n1”\\n122”122 = 0.5, which is higher than its parent. This is compensated\\nfor by the fact that the other node is pure, so the overall weighted Gini impurity\\nis 25‰ 0.5 + 35‰0\\n = 0.2 , which is lower than the parent‡s Gini impurity.\\n3.If a Decision Tree is overfitting the training set, it may be a good idea to decrease\\nmax_depth, since this will constrain the model, regularizing it.4.Decision Trees don‡t care whether or not the training data is scaled or centered;\\nthat‡s one of the nice things about them. So if a Decision Tree underfits the train…\\ning set, scaling the input features will just be a waste of time.\\n5.The computational complexity of training a Decision Tree is \\nO(n ‰ \\nm log(\\nm)). Soif you multiply the training set size by 10, the training time will be multiplied by\\nK = (n ‰ 10m ‰ log(10m)) / (n ‰ m ‰ log(m)) = 10 ‰ log(10m) / log(m). If m =106, then K Ÿ 11.7, so you can expect the training time to be roughly 11.7 hours.6.Presorting the training set speeds up training only if the dataset is smaller than a\\nfew thousand instances. If it contains 100,000 instances, setting \\npresort=Truewill considerably slow down training.For the solutions to exercises 7 and 8, please see the Jupyter notebooks available at\\nhttps://github.com/ageron/handson-ml\\n.Chapter 7: Ensemble Learning and Random Forests1.If you have trained five different models and they all achieve 95% precision, you\\ncan try combining them into a voting ensemble, which will often give you even\\nbetter results. It works better if the models are very different (e.g., an SVM classi…\\nfier, a Decision Tree classifier, a Logistic Regression classifier, and so on). It is\\neven better if they are trained on different training instances (that‡s the whole\\npoint of bagging and pasting ensembles), but if not it will still work as long as the\\nmodels are very different.\\nExercise Solutions | 477\\n2.A hard voting classifier just counts the votes of each classifier in the ensemble\\nand picks the class that gets the most votes. A soft voting classifier computes the\\naverage estimated class probability for each class and picks the class with the\\nhighest probability. This gives high-confidence votes more weight and often per…\\nforms better, but it works only if every classifier is able to estimate class probabil…\\nities (e.g., for the SVM classifiers in Scikit-Learn you must set\\nprobability=True).3.It is quite possible to speed up training of a bagging ensemble by distributing it\\nacross multiple servers, since each predictor in the ensemble is independent of\\nthe others. The same goes for pasting ensembles and Random Forests, for the\\nsame reason. However, each predictor in a boosting ensemble is built based on\\nthe previous predictor, so training is necessarily sequential, and you will not gain\\nanything by distributing training across multiple servers. Regarding stacking\\nensembles, all the predictors in a given layer are independent of each other, so\\nthey can be trained in parallel on multiple servers. However, the predictors in one\\nlayer can only be trained after the predictors in the previous layer have all been\\ntrained.4.With out-of-bag evaluation, each predictor in a bagging ensemble is evaluated\\nusing instances that it was not trained on (they were held out). This makes it pos…\\nsible to have a fairly unbiased evaluation of the ensemble without the need for an\\nadditional validation set. Thus, you have more instances available for training,\\nand your ensemble can perform slightly better.\\n5.When you are growing a tree in a Random Forest, only a random subset of the\\nfeatures is considered for splitting at each node. This is true as well for Extra-\\nTrees, but they go one step further: rather than searching for the best possible\\nthresholds, like regular Decision Trees do, they use random thresholds for each\\nfeature. This extra randomness acts like a form of regularization: if a Random\\nForest overfits the training data, Extra-Trees might perform better. Moreover,\\nsince Extra-Trees don‡t search for the best possible thresholds, they are much\\nfaster to train than Random Forests. However, they are neither faster nor slower\\nthan Random Forests when making predictions.\\n6.If your AdaBoost ensemble underfits the training data, you can try increasing the\\nnumber of estimators or reducing the regularization hyperparameters of the base\\nestimator. You may also try slightly increasing the learning rate.\\n7.If your Gradient Boosting ensemble overfits the training set, you should try\\ndecreasing the learning rate. You could also use early stopping to find the right\\nnumber of predictors (you probably have too many).\\nFor the solutions to exercises 8 and 9, please see the Jupyter notebooks available at\\nhttps://github.com/ageron/handson-ml\\n.478 | Appendix A: Exercise Solutions\\nChapter 8: Dimensionality Reduction1.Motivations and drawbacks:\\n‹The main motivations for dimensionality reduction are:\\n›To speed up a subsequent training algorithm (in some cases it may even\\nremove noise and redundant features, making the training algorithm per…\\nform better).›To visualize the data and gain insights on the most important features.\\n›Simply to save space (compression).\\n‹The main drawbacks are:\\n›Some information is lost, possibly degrading the performance of subse…\\nquent training algorithms.\\n›It can be computationally intensive.\\n›It adds some complexity to your Machine Learning pipelines.\\n›Transformed features are often hard to interpret.\\n2.The curse of dimensionality refers to the fact that many problems that do not\\nexist in low-dimensional space arise in high-dimensional space. In Machine\\nLearning, one common manifestation is the fact that randomly sampled high-\\ndimensional vectors are generally very sparse, increasing the risk of overfitting\\nand making it very difficult to identify patterns in the data without having plenty\\nof training data.\\n3.Once a dataset‡s dimensionality has been reduced using one of the algorithms we\\ndiscussed, it is almost always impossible to perfectly reverse the operation,\\nbecause some information gets lost during dimensionality reduction. Moreover,\\nwhile some algorithms (such as PCA) have a simple reverse transformation pro…\\ncedure that can reconstruct a dataset relatively similar to the original, other algo…\\nrithms (such as T-SNE) do not.\\n4.PCA can be used to significantly reduce the dimensionality of most datasets, even\\nif they are highly nonlinear, because it can at least get rid of useless dimensions.\\nHowever, if there are no useless dimensions›for example, the Swiss roll›then\\nreducing dimensionality with PCA will lose too much information. You want to\\nunroll the Swiss roll, not squash it.5.That‡s a trick question: it depends on the dataset. Let‡s look at two extreme exam…\\nples. First, suppose the dataset is composed of points that are almost perfectly\\naligned. In this case, PCA can reduce the dataset down to just one dimension\\nwhile still preserving 95% of the variance. Now imagine that the dataset is com…\\nposed of perfectly random points, scattered all around the 1,000 dimensions. In\\nExercise Solutions | 479\\nthis case all 1,000 dimensions are required to preserve 95% of the variance. So the\\nanswer is, it depends on the dataset, and it could be any number between 1 and\\n1,000. Plotting the explained variance as a function of the number of dimensions\\nis one way to get a rough idea of the dataset‡s intrinsic dimensionality.\\n6.Regular PCA is the default, but it works only if the dataset fits in memory. Incre…\\nmental PCA is useful for large datasets that don‡t fit in memory, but it is slower\\nthan regular PCA, so if the dataset fits in memory you should prefer regular\\nPCA. Incremental PCA is also useful for online tasks, when you need to apply\\nPCA on the fly, every time a new instance arrives. Randomized PCA is useful\\nwhen you want to considerably reduce dimensionality and the dataset fits in\\nmemory; in this case, it is much faster than regular PCA. Finally, Kernel PCA is\\nuseful for nonlinear datasets.\\n7.Intuitively, a dimensionality reduction algorithm performs well if it eliminates a\\nlot of dimensions from the dataset without losing too much information. One\\nway to measure this is to apply the reverse transformation and measure the\\nreconstruction error. However, not all dimensionality reduction algorithms pro…\\nvide a reverse transformation. Alternatively, if you are using dimensionality\\nreduction as a preprocessing step before another Machine Learning algorithm\\n(e.g., a Random Forest classifier), then you can simply measure the performance\\nof that second algorithm; if dimensionality reduction did not lose too much\\ninformation, then the algorithm should perform just as well as when using the\\noriginal dataset.\\n8.It can absolutely make sense to chain two different dimensionality reduction\\nalgorithms. A common example is using PCA to quickly get rid of a large num…\\nber of useless dimensions, then applying another much slower dimensionality\\nreduction algorithm, such as LLE. This two-step approach will likely yield the\\nsame performance as using LLE only, but in a fraction of the time.\\nFor the solutions to exercises 9 and 10, please see the Jupyter notebooks available at\\nhttps://github.com/ageron/handson-ml\\n.Chapter 9: Up and Running with TensorFlow1.Main benefits and drawbacks of creating a computation graph rather than\\ndirectly executing the computations:\\n‹Main benefits:\\n›TensorFlow can automatically compute the gradients for you (using\\nreverse-mode autodiff).\\n›TensorFlow can take care of running the operations in parallel in different\\nthreads.480 | Appendix A: Exercise Solutions\\n›It makes it easier to run the same model across different devices.\\n›It simplifies introspection›for example, to view the model in TensorBoard.\\n‹Main drawbacks:\\n›It makes the learning curve steeper.\\n›It makes step-by-step debugging harder.\\n2.Yes, the statement \\na_val = a.eval(session=sess) is indeed equivalent to \\na_val= sess.run(a).3.No, the statement \\na_val, b_val = a.eval(session=sess), b.eval(session=sess) is not equivalent to \\na_val, b_val = sess.run([a, b]). Indeed, thefirst statement runs the graph twice (once to compute \\na, once to compute \\nb),while the second statement runs the graph only once. If any of these operations\\n(or the ops they depend on) have side effects (e.g., a variable is modified, an item\\nis inserted in a queue, or a reader reads a file), then the effects will be different. If\\nthey don‡t have side effects, both statements will return the same result, but the\\nsecond statement will be faster than the first.\\n4.No, you cannot run two graphs in the same session. You would have to merge the\\ngraphs into a single graph first.\\n5.In local TensorFlow, sessions manage variable values, so if you create a graph \\ngcontaining a variable \\nw, then start two threads and open a local session in eachthread, both using the same graph \\ng, then each session will have its own copy of\\nthe variable w. However, in distributed TensorFlow, variable values are stored in\\ncontainers managed by the cluster, so if both sessions connect to the same cluster\\nand use the same container, then they will share the same variable value for \\nw.6.A variable is initialized when you call its initializer, and it is destroyed when the\\nsession ends. In distributed TensorFlow, variables live in containers on the clus…\\nter, so closing a session will not destroy the variable. To destroy a variable, you\\nneed to clear its container.\\n7.Variables and placeholders are extremely different, but beginners often confuse\\nthem:‹A variable is an operation that holds a value. If you run the variable, it returns\\nthat value. Before you can run it, you need to initialize it. You can change the\\nvariable‡s value (for example, by using an assignment operation). It is stateful:\\nthe variable keeps the same value upon successive runs of the graph. It is typi…\\ncally used to hold model parameters but also for other purposes (e.g., to count\\nthe global training step).‹Placeholders technically don‡t do much: they just hold information about the\\ntype and shape of the tensor they represent, but they have no value. In fact, if\\nExercise Solutions | 481\\nyou try to evaluate an operation that depends on a placeholder, you must feed\\nTensorFlow the value of the placeholder (using the \\nfeed_dict argument) or\\nelse you will get an exception. Placeholders are typically used to feed trainingor test data to TensorFlow during the execution phase. They are also useful to\\npass a value to an assignment node, to change the value of a variable (e.g.,\\nmodel weights).\\n8.If you run the graph to evaluate an operation that depends on a placeholder but\\nyou don‡t feed its value, you get an exception. If the operation does not depend\\non the placeholder, then no exception is raised.\\n9.When you run a graph, you can feed the output value of any operation, not just\\nthe value of placeholders. In practice, however, this is rather rare (it can be useful,\\nfor example, when you are caching the output of frozen layers; see \\nChapter 11\\n).10.You can specify a variable‡s initial value when constructing the graph, and it will\\nbe initialized later when you run the variable‡s initializer during the execution\\nphase. If you want to change that variable‡s value to anything you want during the\\nexecution phase, then the simplest option is to create an assignment node (dur…\\ning the graph construction phase) using the \\ntf.assign() function, passing thevariable and a placeholder as parameters. During the execution phase, you canrun the assignment operation and feed the variable‡s new value using the \\nplace…holder.\\nimport tensorflow as tfx = tf.Variable(tf.random_uniform(shape=(), minval=0.0, maxval=1.0))x_new_val = tf.placeholder(shape=(), dtype=tf.float32)x_assign = tf.assign(x, x_new_val)with tf.Session():    x.initializer.run() # random number is sampled *now*    print(x.eval()) # 0.646157 (some random number)    x_assign.eval(feed_dict={x_new_val: 5.0})    print(x.eval()) # 5.011.Reverse-mode autodiff (implemented by TensorFlow) needs to traverse the graph\\nonly twice in order to compute the gradients of the cost function with regards to\\nany number of variables. On the other hand, forward-mode autodiff would need\\nto run once for each variable (so 10 times if we want the gradients with regards to\\n10 different variables). As for symbolic differentiation, it would build a different\\ngraph to compute the gradients, so it would not traverse the original graph at all\\n(except when building the new gradients graph). A highly optimized symbolic\\ndifferentiation system could potentially run the new gradients graph only once to\\ncompute the gradients with regards to all variables, but that new graph may be\\nhorribly complex and inefficient compared to the original graph.\\n482 | Appendix A: Exercise Solutions\\n12.See the Jupyter notebooks available at \\nhttps://github.com/ageron/handson-ml\\n.Chapter 10: Introduction to Arti•cial Neural Networks1.Here is a neural network based on the original artificial neurons that computes \\nA B (where \\n represents the exclusive OR), using the fact that \\nA  B = (\\nA  ° B) (° A  B). There are other solutions›for example, using the fact that \\nA  B =\\n(A  B)  °(A  B), or the fact that \\nA  B = (A  B)  (° A   B), and so on.2.A classical Perceptron will converge only if the dataset is linearly separable, and it\\nwon‡t be able to estimate class probabilities. In contrast, a Logistic Regression\\nclassifier will converge to a good solution even if the dataset is not linearly sepa…\\nrable, and it will output class probabilities. If you change the Perceptron‡s activa…\\ntion function to the logistic activation function (or the softmax activation\\nfunction if there are multiple neurons), and if you train it using Gradient Descent\\n(or some other optimization algorithm minimizing the cost function, typically\\ncross entropy), then it becomes equivalent to a Logistic Regression classifier.\\n3.The logistic activation function was a key ingredient in training the first MLPs\\nbecause its derivative is always nonzero, so Gradient Descent can always roll\\ndown the slope. When the activation function is a step function, Gradient\\nDescent cannot move, as there is no slope at all.\\n4.The step function, the logistic function, the hyperbolic tangent, the rectified lin…\\near unit (see Figure 10-8). See Chapter 11\\n for other examples, such as ELU and\\nvariants of the ReLU.\\n5.Considering the MLP described in the question: suppose you have an MLP com…\\nposed of one input layer with 10 passthrough neurons, followed by one hidden\\nlayer with 50 artificial neurons, and finally one output layer with 3 artificial neu…\\nrons. All artificial neurons use the ReLU activation function.\\nExercise Solutions | 483\\n4When the values to predict can vary by many orders of magnitude, then you may want to predict the loga…\\nrithm of the target value rather than the target value directly. Simply computing the exponential of the neural\\nnetwork‡s output will give you the estimated value (since exp(log \\nv) = v).‹The shape of the input matrix \\nX is m ‰ 10, where m represents the training\\nbatch size.\\n‹The shape of the hidden layer‡s weight vector \\nWh is 10 ‰ 50 and the length ofits bias vector bh is 50.‹The shape of the output layer‡s weight vector \\nWo is 50 ‰ 3, and the length of itsbias vector bo is 3.‹The shape of the network‡s output matrix \\nY is m ‰ 3.‹Y = (X ’ Wh + bh) ’ Wo + bo. Note that when you are adding a bias vector to a\\nmatrix, it is added to every single row in the matrix, which is called \\nbroadcast…\\ning\\n.6.To classify email into spam or ham, you just need one neuron in the output layer\\nof a neural network›for example, indicating the probability that the email is\\nspam. You would typically use the logistic activation function in the output layer\\nwhen estimating a probability. If instead you want to tackle MNIST, you need 10\\nneurons in the output layer, and you must replace the logistic function with the\\nsoftmax activation function, which can handle multiple classes, outputting one\\nprobability per class. Now, if you want your neural network to predict housing\\nprices like in Chapter 2\\n, then you need one output neuron, using no activation\\nfunction at all in the output layer.\\n47.Backpropagation is a technique used to train artificial neural networks. It first\\ncomputes the gradients of the cost function with regards to every model parame…\\nter (all the weights and biases), and then it performs a Gradient Descent step\\nusing these gradients. This backpropagation step is typically performed thou…\\nsands or millions of times, using many training batches, until the model parame…\\nters converge to values that (hopefully) minimize the cost function. To compute\\nthe gradients, backpropagation uses reverse-mode autodiff (although it wasn‡t\\ncalled that when backpropagation was invented, and it has been reinvented sev…\\neral times). Reverse-mode autodiff performs a forward pass through a computa…\\ntion graph, computing every node‡s value for the current training batch, and then\\nit performs a reverse pass, computing all the gradients at once (see \\nAppendix D\\nfor more details). So what‡s the difference? Well, backpropagation refers to the\\nwhole process of training an artificial neural network using multiple backpropa…\\ngation steps, each of which computes gradients and uses them to perform a Gra…\\ndient Descent step. In contrast, reverse-mode autodiff is a simply a technique to\\ncompute gradients efficiently, and it happens to be used by backpropagation.\\n484 | Appendix A: Exercise Solutions\\n5In Chapter 11\\n we discuss many techniques that introduce additional hyperparameters: type of weight initiali…\\nzation, activation function hyperparameters (e.g., amount of leak in leaky ReLU), Gradient Clipping thres…\\nhold, type of optimizer and its hyperparameters (e.g., the momentum hyperparameter when using a\\nMomentumOptimizer), type of regularization for each layer, and the regularization hyperparameters (e.g., drop…\\nout rate when using dropout) and so on.\\n8.Here is a list of all the hyperparameters you can tweak in a basic MLP: the num…\\nber of hidden layers, the number of neurons in each hidden layer, and the activa…\\ntion function used in each hidden layer and in the output layer.\\n5 In general, theReLU activation function (or one of its variants; see \\nChapter 11\\n) is a good default\\nfor the hidden layers. For the output layer, in general you will want the logistic\\nactivation function for binary classification, the softmax activation function for\\nmulticlass classification, or no activation function for regression.\\nIf the MLP overfits the training data, you can try reducing the number of hidden\\nlayers and reducing the number of neurons per hidden layer.\\n9.See the Jupyter notebooks available at \\nhttps://github.com/ageron/handson-ml\\n.Chapter 11: Training Deep Neural Nets1.No, all weights should be sampled independently; they should not all have the\\nsame initial value. One important goal of sampling weights randomly is to break\\nsymmetries: if all the weights have the same initial value, even if that value is not\\nzero, then symmetry is not broken (i.e., all neurons in a given layer are equiva…\\nlent), and backpropagation will be unable to break it. Concretely, this means that\\nall the neurons in any given layer will always have the same weights. It‡s like hav…\\ning just one neuron per layer, and much slower. It is virtually impossible for such\\na configuration to converge to a good solution.\\n2.It is perfectly fine to initialize the bias terms to zero. Some people like to initialize\\nthem just like weights, and that‡s okay too; it does not make much difference.\\n3.A few advantages of the ELU function over the ReLU function are:\\n‹It can take on negative values, so the average output of the neurons in any\\ngiven layer is typically closer to 0 than when using the ReLU activation func…\\ntion (which never outputs negative values). This helps alleviate the vanishing\\ngradients problem.\\n‹It always has a nonzero derivative, which avoids the dying units issue that can\\naffect ReLU units.\\nExercise Solutions | 485\\n‹It is smooth everywhere, whereas the ReLU‡s slope abruptly jumps from 0 to 1\\nat \\nz = 0. Such an abrupt change can slow down Gradient Descent because it\\nwill bounce around z = 0.4.The ELU activation function is a good default. If you need the neural network to\\nbe as fast as possible, you can use one of the leaky ReLU variants instead (e.g., a\\nsimple leaky ReLU using the default hyperparameter value). The simplicity of the\\nReLU activation function makes it many people‡s preferred option, despite the\\nfact that they are generally outperformed by the ELU and leaky ReLU. However,\\nthe ReLU activation function‡s capability of outputting precisely zero can be use…\\nful in some cases (e.g., see Chapter 15\\n). The hyperbolic tangent (tanh) can be use…\\nful in the output layer if you need to output a number between –1 and 1, but\\nnowadays it is not used much in hidden layers. The logistic activation function is\\nalso useful in the output layer when you need to estimate a probability (e.g., for\\nbinary classification), but it is also rarely used in hidden layers (there are excep…\\ntions›for example, for the coding layer of variational autoencoders; see \\nChap…\\nter 15). Finally, the softmax activation function is useful in the output layer to\\noutput probabilities for mutually exclusive classes, but other than that it is rarely\\n(if ever) used in hidden layers.\\n5.If you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using\\na MomentumOptimizer, then the algorithm will likely pick up a lot of speed, hope…fully roughly toward the global minimum, but then it will shoot right past the\\nminimum, due to its momentum. Then it will slow down and come back, accel…\\nerate again, overshoot again, and so on. It may oscillate this way many times\\nbefore converging, so overall it will take much longer to converge than with a\\nsmaller momentum value.6.One way to produce a sparse model (i.e., with most weights equal to zero) is to\\ntrain the model normally, then zero out tiny weights. For more sparsity, you can\\napply —\\n1 regularization during training, which pushes the optimizer toward spar…\\nsity. A third option is to combine —\\n1 regularization with \\ndual averaging\\n, usingTensorFlow‡s \\nFTRLOptimizer class.7.Yes, dropout does slow down training, in general roughly by a factor of two.\\nHowever, it has no impact on inference since it is only turned on during training.\\nFor the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available\\nat \\nhttps://github.com/ageron/handson-ml\\n.486 | Appendix A: Exercise Solutions\\nChapter 12: Distributing TensorFlow Across Devices andServers1.When a TensorFlow process starts, it grabs all the available memory on all GPU\\ndevices that are visible to it, so if you get a \\nCUDA_ERROR_OUT_OF_MEMORY whenstarting your TensorFlow program, it probably means that other processes are\\nrunning that have already grabbed all the memory on at least one visible GPU\\ndevice (most likely it is another TensorFlow process). To fix this problem, a triv…\\nial solution is to stop the other processes and try again. However, if you need all\\nprocesses to run simultaneously, a simple option is to dedicate different devices\\nto each process, by setting the CUDA_VISIBLE_DEVICES environment variable\\nappropriately for each device. Another option is to configure TensorFlow to grab\\nonly part of the GPU memory, instead of all of it, by creating a \\nConfigProto, set…ting its gpu_options.per_process_gpu_memory_fraction to the proportion ofthe total memory that it should grab (e.g., 0.4), and using this \\nConfigProto when\\nopening a session. The last option is to tell TensorFlow to grab memory only\\nwhen it needs it by setting the gpu_options.allow_growth to True. However,\\nthis last option is usually not recommended because any memory that Tensor…\\nFlow grabs is never released, and it is harder to guarantee a repeatable behavior\\n(there may be race conditions depending on which processes start first, how\\nmuch memory they need during training, and so on).\\n2.By pinning an operation on a device, you are telling TensorFlow that this is\\nwhere you would like this operation to be placed. However, some constraints may\\nprevent TensorFlow from honoring your request. For example, the operation\\nmay have no implementation (called a \\nkernel\\n) for that particular type of device.\\nIn this case, TensorFlow will raise an exception by default, but you can configure\\nit to fall back to the CPU instead (this is called so“ placement\\n). Another example\\nis an operation that can modify a variable; this operation and the variable need to\\nbe collocated. So the difference between pinning an operation and placing an\\noperation is that pinning is what you ask TensorFlow (ƒPlease place this opera…\\ntion on GPU #1⁄) while placement is what TensorFlow actually ends up doing\\n(ƒSorry, falling back to the CPU⁄).\\n3.If you are running on a GPU-enabled TensorFlow installation, and you just use\\nthe default placement, then if all operations have a GPU kernel (i.e., a GPU\\nimplementation), yes, they will all be placed on the first GPU. However, if one or\\nmore operations do not have a GPU kernel, then by default TensorFlow will raise\\nan exception. If you configure TensorFlow to fall back to the CPU instead (soft\\nplacement), then all operations will be placed on the first GPU except the ones\\nwithout a GPU kernel and all the operations that must be collocated with them\\n(see the answer to the previous exercise).Exercise Solutions | 487\\n4.Yes, if you pin a variable to \\n\"/gpu:0\", it can be used by operations placed\\non /gpu:1. TensorFlow will automatically take care of adding the appropriate\\noperations to transfer the variable‡s value across devices. The same goes for devi…\\nces located on different servers (as long as they are part of the same cluster).\\n5.Yes, two operations placed on the same device can run in parallel: TensorFlow\\nautomatically takes care of running operations in parallel (on different CPU\\ncores or different GPU threads), as long as no operation depends on another\\noperation‡s output. Moreover, you can start multiple sessions in parallel threads\\n(or processes), and evaluate operations in each thread. Since sessions are inde…\\npendent, TensorFlow will be able to evaluate any operation from one session in\\nparallel with any operation from another session.\\n6.Control dependencies are used when you want to postpone the evaluation of an\\noperation X until after some other operations are run, even though these opera…\\ntions are not required to compute X. This is useful in particular when X would\\noccupy a lot of memory and you only need it later in the computation graph, or if\\nX uses up a lot of I/O (for example, it requires a large variable value located on a\\ndifferent device or server) and you don‡t want it to run at the same time as other\\nI/O-hungry operations, to avoid saturating the bandwidth.\\n7.You‡re in luck! In distributed TensorFlow, the variable values live in containers\\nmanaged by the cluster, so even if you close the session and exit the client pro…\\ngram, the model parameters are still alive and well on the cluster. You simply\\nneed to open a new session to the cluster and save the model (make sure you\\ndon‡t call the variable initializers or restore a previous model, as this would\\ndestroy your precious new model!).For the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available\\nat \\nhttps://github.com/ageron/handson-ml\\n.Chapter 13: Convolutional Neural Networks1.These are the main advantages of a CNN over a fully connected DNN for image\\nclassification:\\n‹Because consecutive layers are only partially connected and because it heavily\\nreuses its weights, a CNN has many fewer parameters than a fully connected\\nDNN, which makes it much faster to train, reduces the risk of overfitting, and\\nrequires much less training data.\\n‹When a CNN has learned a kernel that can detect a particular feature, it can\\ndetect that feature anywhere on the image. In contrast, when a DNN learns a\\nfeature in one location, it can detect it only in that particular location. Since\\nimages typically have very repetitive features, CNNs are able to generalize\\n488 | Appendix A: Exercise Solutions\\nmuch better than DNNs for image processing tasks such as classification, using\\nfewer training examples.\\n‹Finally, a DNN has no prior knowledge of how pixels are organized; it does not\\nknow that nearby pixels are close. A CNN‡s architecture embeds this prior\\nknowledge. Lower layers typically identify features in small areas of the images,\\nwhile higher layers combine the lower-level features into larger features. This\\nworks well with most natural images, giving CNNs a decisive head start com…\\npared to DNNs.\\n2.Let‡s compute how many parameters the CNN has. Since its first convolutional\\nlayer has 3 ‰ 3 kernels, and the input has three channels (red, green, and blue),\\nthen each feature map has 3 ‰ 3 ‰ 3 weights, plus a bias term. That‡s 28 parame…\\nters per feature map. Since this first convolutional layer has 100 feature maps, it\\nhas a total of 2,800 parameters. The second convolutional layer has 3 ‰ 3 kernels,\\nand its input is the set of 100 feature maps of the previous layer, so each feature\\nmap has 3 ‰ 3 ‰ 100 = 900 weights, plus a bias term. Since it has 200 feature\\nmaps, this layer has 901 ‰ 200 = 180,200 parameters. Finally, the third and last\\nconvolutional layer also has 3 ‰ 3 kernels, and its input is the set of 200 feature\\nmaps of the previous layers, so each feature map has 3 ‰ 3 ‰ 200 = 1,800 weights,\\nplus a bias term. Since it has 400 feature maps, this layer has a total of 1,801 ‰ 400\\n= 720,400 parameters. All in all, the CNN has 2,800 + 180,200 + 720,400 =903,400 parameters.Now let‡s compute how much RAM this neural network will require (at least)\\nwhen making a prediction for a single instance. First let‡s compute the feature\\nmap size for each layer. Since we are using a stride of 2 and SAME padding, the\\nhorizontal and vertical size of the feature maps are divided by 2 at each layer\\n(rounding up if necessary), so as the input channels are 200 ‰ 300 pixels, the first\\nlayer‡s feature maps are 100 ‰ 150, the second layer‡s feature maps are 50 ‰ 75,\\nand the third layer‡s feature maps are 25 ‰ 38. Since 32 bits is 4 bytes and the first\\nconvolutional layer has 100 feature maps, this first layer takes up 4 x 100 ‰ 150 ‰\\n100 = 6 million bytes (about 5.7 MB, considering that 1 MB = 1,024 KB and 1 KB\\n= 1,024 bytes). The second layer takes up 4 ‰ 50 ‰ 75 ‰ 200 = 3 million bytes\\n(about 2.9 MB). Finally, the third layer takes up 4 ‰ 25 ‰ 38 ‰ 400 = 1,520,000\\nbytes (about 1.4 MB). However, once a layer has been computed, the memory\\noccupied by the previous layer can be released, so if everything is well optimized,\\nonly 6 + 9 = 15 million bytes (about 14.3 MB) of RAM will be required (when thesecond layer has just been computed, but the memory occupied by the first layer\\nis not released yet). But wait, you also need to add the memory occupied by the\\nCNN‡s parameters. We computed earlier that it has 903,400 parameters, each\\nusing up 4 bytes, so this adds 3,613,600 bytes (about 3.4 MB). The total RAMrequired is (at least) 18,613,600 bytes (about 17.8 MB).\\nExercise Solutions | 489\\nLastly, let‡s compute the minimum amount of RAM required when training the\\nCNN on a mini-batch of 50 images. During training TensorFlow uses backpropa…\\ngation, which requires keeping all values computed during the forward pass until\\nthe reverse pass begins. So we must compute the total RAM required by all layers\\nfor a single instance and multiply that by 50! At that point let‡s start counting in\\nmegabytes rather than bytes. We computed before that the three layers require\\nrespectively 5.7, 2.9, and 1.4 MB for each instance. That‡s a total of 10.0 MB per\\ninstance. So for 50 instances the total RAM is 500 MB. Add to that the RAM\\nrequired by the input images, which is 50 ‰ 4 ‰ 200 ‰ 300 ‰ 3 = 36 million bytes\\n(about 34.3 MB), plus the RAM required for the model parameters, which isabout 3.4 MB (computed earlier), plus some RAM for the gradients (we will\\nneglect them since they can be released gradually as backpropagation goes down\\nthe layers during the reverse pass). We are up to a total of roughly 500.0 + 34.3 +\\n3.4 = 537.7 MB. And that‡s really an optimistic bare minimum.\\n3.If your GPU runs out of memory while training a CNN, here are five things you\\ncould try to solve the problem (other than purchasing a GPU with more RAM):\\n‹Reduce the mini-batch size.\\n‹Reduce dimensionality using a larger stride in one or more layers.\\n‹Remove one or more layers.\\n‹Use 16-bit floats instead of 32-bit floats.\\n‹Distribute the CNN across multiple devices.\\n4.A max pooling layer has no parameters at all, whereas a convolutional layer has\\nquite a few (see the previous questions).5.A local response normalization\\n layer makes the neurons that most strongly acti…\\nvate inhibit neurons at the same location but in neighboring feature maps, which\\nencourages different feature maps to specialize and pushes them apart, forcing\\nthem to explore a wider range of features. It is typically used in the lower layers to\\nhave a larger pool of low-level features that the upper layers can build upon.\\n6.The main innovations in AlexNet compared to LeNet-5 are (1) it is much larger\\nand deeper, and (2) it stacks convolutional layers directly on top of each other,\\ninstead of stacking a pooling layer on top of each convolutional layer. The main\\ninnovation in GoogLeNet is the introduction of \\ninception modules\\n, which make itpossible to have a much deeper net than previous CNN architectures, with fewer\\nparameters. Finally, ResNet‡s main innovation is the introduction of skip connec…\\ntions, which make it possible to go well beyond 100 layers. Arguably, its simplic…\\nity and consistency are also rather innovative.\\nFor the solutions to exercises 7, 8, 9, and 10, please see the Jupyter notebooks avail…\\nable at \\nhttps://github.com/ageron/handson-ml\\n.490 | Appendix A: Exercise Solutions\\nChapter 14: Recurrent Neural Networks1.Here are a few RNN applications:\\n‹For a sequence-to-sequence RNN: predicting the weather (or any other time\\nseries), machine translation (using an encoder–decoder architecture), video\\ncaptioning, speech to text, music generation (or other sequence generation),\\nidentifying the chords of a song.\\n‹For a sequence-to-vector RNN: classifying music samples by music genre, ana…\\nlyzing the sentiment of a book review, predicting what word an aphasic patient\\nis thinking of based on readings from brain implants, predicting the probabil…\\nity that a user will want to watch a movie based on her watch history (this is\\none of many possible implementations of \\ncollaborative \\n†ltering).‹For a vector-to-sequence RNN: image captioning, creating a music playlist\\nbased on an embedding of the current artist, generating a melody based on a\\nset of parameters, locating pedestrians in a picture (e.g., a video frame from a\\nself-driving car‡s camera).\\n2.In general, if you translate a sentence one word at a time, the result will be terri…\\nble. For example, the French sentence ƒJe vous en prie⁄ means ƒYou are welcome,⁄\\nbut if you translate it one word at a time, you get ƒI you in pray.⁄ Huh? It is much\\nbetter to read the whole sentence first and then translate it. A plain sequence-to-\\nsequence RNN would start translating a sentence immediately after reading the\\nfirst word, while an encoder–decoder RNN will first read the whole sentence and\\nthen translate it. That said, one could imagine a plain sequence-to-sequence\\nRNN that would output silence whenever it is unsure about what to say next (just\\nlike human translators do when they must translate a live broadcast).\\n3.To classify videos based on the visual content, one possible architecture could be\\nto take (say) one frame per second, then run each frame through a convolutional\\nneural network, feed the output of the CNN to a sequence-to-vector RNN, andfinally run its output through a softmax layer, giving you all the class probabili…\\nties. For training you would just use cross entropy as the cost function. If you\\nwanted to use the audio for classification as well, you could convert every second\\nof audio to a spectrograph, feed this spectrograph to a CNN, and feed the output\\nof this CNN to the RNN (along with the corresponding output of the otherCNN).4.Building an RNN using dynamic_rnn() rather than \\nstatic_rnn() offers several advantages:\\n‹It is based on a \\nwhile_loop() operation that is able to swap the GPU‡s memory\\nto the CPU‡s memory during backpropagation, avoiding out-of-memory\\nerrors.Exercise Solutions | 491\\n‹It is arguably easier to use, as it can directly take a single tensor as input and\\noutput (covering all time steps), rather than a list of tensors (one per time\\nstep). No need to stack, unstack, or transpose.\\n‹It generates a smaller graph, easier to visualize in TensorBoard.\\n5.To handle variable length input sequences, the simplest option is to set the\\nsequence_length parameter when calling the static_rnn() or dynamic_rnn()functions. Another option is to pad the smaller inputs (e.g., with zeros) to make\\nthem the same size as the largest input (this may be faster than the first option if\\nthe input sequences all have very similar lengths). To handle variable-length out…\\nput sequences, if you know in advance the length of each output sequence, youcan use the sequence_length parameter (for example, consider a sequence-to-\\nsequence RNN that labels every frame in a video with a violence score: the output\\nsequence will be exactly the same length as the input sequence). If you don‡t\\nknow in advance the length of the output sequence, you can use the paddingtrick: always output the same size sequence, but ignore any outputs that come\\nafter the end-of-sequence token (by ignoring them when computing the cost\\nfunction).6.To distribute training and execution of a deep RNN across multiple GPUs, a\\ncommon technique is simply to place each layer on a different GPU (see \\nChap…\\nter 12).For the solutions to exercises 7, 8, and 9, please see the Jupyter notebooks available at\\nhttps://github.com/ageron/handson-ml\\n.Chapter 15: Autoencoders1.Here are some of the main tasks that autoencoders are used for:\\n‹Feature extraction\\n‹Unsupervised pretraining\\n‹Dimensionality reduction‹Generative models\\n‹Anomaly detection (an autoencoder is generally bad at reconstructing outliers)\\n2.If you want to train a classifier and you have plenty of unlabeled training data,\\nbut only a few thousand labeled instances, then you could first train a deepautoencoder on the full dataset (labeled + unlabeled), then reuse its lower half for\\nthe classifier (i.e., reuse the layers up to the codings layer, included) and train the\\nclassifier using the labeled data. If you have little labeled data, you probably want\\nto freeze the reused layers when training the classifier.\\n492 | Appendix A: Exercise Solutions\\n3.The fact that an autoencoder perfectly reconstructs its inputs does not necessarily\\nmean that it is a good autoencoder; perhaps it is simply an overcomplete autoen…\\ncoder that learned to copy its inputs to the codings layer and then to the outputs.\\nIn fact, even if the codings layer contained a single neuron, it would be possible\\nfor a very deep autoencoder to learn to map each training instance to a different\\ncoding (e.g., the first instance could be mapped to 0.001, the second to 0.002, the\\nthird to 0.003, and so on), and it could learn ƒby heart⁄ to reconstruct the right\\ntraining instance for each coding. It would perfectly reconstruct its inputs\\nwithout really learning any useful pattern in the data. In practice such a mapping\\nis unlikely to happen, but it illustrates the fact that perfect reconstructions are not\\na guarantee that the autoencoder learned anything useful. However, if it produces\\nvery bad reconstructions, then it is almost guaranteed to be a bad autoencoder.\\nTo evaluate the performance of an autoencoder, one option is to measure the\\nreconstruction loss (e.g., compute the MSE, the mean square of the outputs\\nminus the inputs). Again, a high reconstruction loss is a good sign that the\\nautoencoder is bad, but a low reconstruction loss is not a guarantee that it is\\ngood. You should also evaluate the autoencoder according to what it will be used\\nfor. For example, if you are using it for unsupervised pretraining of a classifier,\\nthen you should also evaluate the classifier‡s performance.\\n4.An undercomplete autoencoder is one whose codings layer is smaller than the\\ninput and output layers. If it is larger, then it is an overcomplete autoencoder.\\nThe main risk of an excessively undercomplete autoencoder is that it may fail to\\nreconstruct the inputs. The main risk of an overcomplete autoencoder is that it\\nmay just copy the inputs to the outputs, without learning any useful feature.\\n5.To tie the weights of an encoder layer and its corresponding decoder layer, you\\nsimply make the decoder weights equal to the transpose of the encoder weights.\\nThis reduces the number of parameters in the model by half, often making train…\\ning converge faster with less training data, and reducing the risk of overfitting the\\ntraining set.6.To visualize the features learned by the lower layer of a stacked autoencoder, a\\ncommon technique is simply to plot the weights of each neuron, by reshaping\\neach weight vector to the size of an input image (e.g., for MNIST, reshaping a\\nweight vector of shape \\n[784] to [28, 28]). To visualize the features learned by\\nhigher layers, one technique is to display the training instances that most activate\\neach neuron.7.A generative model is a model capable of randomly generating outputs that\\nresemble the training instances. For example, once trained successfully on the\\nMNIST dataset, a generative model can be used to randomly generate realistic\\nimages of digits. The output distribution is typically similar to the training data.\\nFor example, since MNIST contains many images of each digit, the generative\\nmodel would output roughly the same number of images of each digit. Some\\nExercise Solutions | 493\\ngenerative models can be parametrized›for example, to generate only some\\nkinds of outputs. An example of a generative autoencoder is the variational\\nautoencoder.\\nFor the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available\\nat \\nhttps://github.com/ageron/handson-ml\\n.Chapter 16: Reinforcement Learning1.Reinforcement Learning is an area of Machine Learning aimed at creating agents\\ncapable of taking actions in an environment in a way that maximizes rewards\\nover time. There are many differences between RL and regular supervised and\\nunsupervised learning. Here are a few:\\n‹In supervised and unsupervised learning, the goal is generally to find patterns\\nin the data. In Reinforcement Learning, the goal is to find a good policy.\\n‹Unlike in supervised learning, the agent is not explicitly given the ƒright⁄\\nanswer. It must learn by trial and error.\\n‹Unlike in unsupervised learning, there is a form of supervision, through\\nrewards. We do not tell the agent how to perform the task, but we do tell it\\nwhen it is making progress or when it is failing.‹A Reinforcement Learning agent needs to find the right balance between\\nexploring the environment, looking for new ways of getting rewards, and\\nexploiting sources of rewards that it already knows. In contrast, supervised and\\nunsupervised learning systems generally don‡t need to worry about explora…\\ntion; they just feed on the training data they are given.\\n‹In supervised and unsupervised learning, training instances are typically inde…\\npendent (in fact, they are generally shuffled). In Reinforcement Learning, con…\\nsecutive observations are generally \\nnot\\n independent. An agent may remain in\\nthe same region of the environment for a while before it moves on, so consecu…\\ntive observations will be very correlated. In some cases a replay memory is\\nused to ensure that the training algorithm gets fairly independent observa…\\ntions.2.Here are a few possible applications of Reinforcement Learning, other than those\\nmentioned in \\nChapter 16\\n:Music personalization\\nThe environment is a user‡s personalized web radio. The agent is the software\\ndeciding what song to play next for that user. Its possible actions are to play\\nany song in the catalog (it must try to choose a song the user will enjoy) or to\\nplay an advertisement (it must try to choose an ad that the user will be inter…\\n494 | Appendix A: Exercise Solutions\\nested in). It gets a small reward every time the user listens to a song, a larger\\nreward every time the user listens to an ad, a negative reward when the user\\nskips a song or an ad, and a very negative reward if the user leaves.\\nMarketing\\nThe environment is your company‡s marketing department. The agent is the\\nsoftware that defines which customers a mailing campaign should be sent to,\\ngiven their profile and purchase history (for each customer it has two possi…\\nble actions: send or don‡t send). It gets a negative reward for the cost of the\\nmailing campaign, and a positive reward for estimated revenue generated\\nfrom this campaign.\\nProduct delivery\\nLet the agent control a fleet of delivery trucks, deciding what they should\\npick up at the depots, where they should go, what they should drop off, and\\nso on. They would get positive rewards for each product delivered on time,and negative rewards for late deliveries.\\n3.When estimating the value of an action, Reinforcement Learning algorithms typ…\\nically sum all the rewards that this action led to, giving more weight to immediate\\nrewards, and less weight to later rewards (considering that an action has more\\ninfluence on the near future than on the distant future). To model this, a discount\\nrate is typically applied at each time step. For example, with a discount rate of 0.9,\\na reward of 100 that is received two time steps later is counted as only 0.9\\n2 ‰ 100= 81 when you are estimating the value of the action. You can think of the dis…\\ncount rate as a measure of how much the future is valued relative to the present:\\nif it is very close to 1, then the future is valued almost as much as the present. If it\\nis close to 0, then only immediate rewards matter. Of course, this impacts the\\noptimal policy tremendously: if you value the future, you may be willing to put\\nup with a lot of immediate pain for the prospect of eventual rewards, while if you\\ndon‡t value the future, you will just grab any immediate reward you can find,\\nnever investing in the future.\\n4.To measure the performance of a Reinforcement Learning agent, you can simply\\nsum up the rewards it gets. In a simulated environment, you can run many epi…\\nsodes and look at the total rewards it gets on average (and possibly look at the\\nmin, max, standard deviation, and so on).\\n5.The credit assignment problem is the fact that when a Reinforcement Learning\\nagent receives a reward, it has no direct way of knowing which of its previous\\nactions contributed to this reward. It typically occurs when there is a large delay\\nbetween an action and the resulting rewards (e.g., during a game of Atari‡s \\nPong\\n,there may be a few dozen time steps between the moment the agent hits the ball\\nand the moment it wins the point). One way to alleviate it is to provide the agent\\nwith shorter-term rewards, when possible. This usually requires prior knowledge\\nExercise Solutions | 495\\nabout the task. For example, if we want to build an agent that will learn to play\\nchess, instead of giving it a reward only when it wins the game, we could give it areward every time it captures one of the opponent‡s pieces.\\n6.An agent can often remain in the same region of its environment for a while, so\\nall of its experiences will be very similar for that period of time. This can intro…\\nduce some bias in the learning algorithm. It may tune its policy for this region of\\nthe environment, but it will not perform well as soon as it moves out of this\\nregion. To solve this problem, you can use a replay memory; instead of using\\nonly the most immediate experiences for learning, the agent will learn based on a\\nbuffer of its past experiences, recent and not so recent (perhaps this is why we\\ndream at night: to replay our experiences of the day and better learn from them?).\\n7.An off-policy RL algorithm learns the value of the optimal policy (i.e., the sum ofdiscounted rewards that can be expected for each state if the agent acts opti…\\nmally), independently of how the agent actually acts. Q-Learning is a good exam…\\nple of such an algorithm. In contrast, an on-policy algorithm learns the value of\\nthe policy that the agent actually executes, including both exploration and exploi…\\ntation.\\nFor the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available\\nat \\nhttps://github.com/ageron/handson-ml\\n.496 | Appendix A: Exercise Solutions\\nAPPENDIX BMachine Learning Project ChecklistThis checklist can guide you through your Machine Learning projects. There are\\neight main steps:\\n1.Frame the problem and look at the big picture.\\n2.Get the data.\\n3.Explore the data to gain insights.\\n4.Prepare the data to better expose the underlying data patterns to Machine Learn…\\ning algorithms.5.Explore many different models and short-list the best ones.\\n6.Fine-tune your models and combine them into a great solution.\\n7.Present your solution.\\n8.Launch, monitor, and maintain your system.\\nObviously, you should feel free to adapt this checklist to your needs.\\nFrame the Problem and Look at the Big Picture1.Define the objective in business terms.2.How will your solution be used?\\n3.What are the current solutions/workarounds (if any)?\\n4.How should you frame this problem (supervised/unsupervised, online/offline,\\netc.)?5.How should performance be measured?\\n6.Is the performance measure aligned with the business objective?\\n4977.What would be the minimum performance needed to reach the business objec…\\ntive?8.What are comparable problems? Can you reuse experience or tools?\\n9.Is human expertise available?\\n10.How would you solve the problem manually?\\n11.List the assumptions you (or others) have made so far.\\n12.Verify assumptions if possible.\\nGet the DataNote: automate as much as possible so you can easily get fresh data.\\n1.List the data you need and how much you need.\\n2.Find and document where you can get that data.\\n3.Check how much space it will take.\\n4.Check legal obligations, and get authorization if necessary.\\n5.Get access authorizations.\\n6.Create a workspace (with enough storage space).\\n7.Get the data.\\n8.Convert the data to a format you can easily manipulate (without changing the\\ndata itself).\\n9.Ensure sensitive information is deleted or protected (e.g., anonymized).\\n10.Check the size and type of data (time series, sample, geographical, etc.).\\n11.Sample a test set, put it aside, and never look at it (no data snooping!).\\nExplore the DataNote: try to get insights from a field expert for these steps.\\n1.Create a copy of the data for exploration (sampling it down to a manageable size\\nif necessary).\\n2.Create a Jupyter notebook to keep a record of your data exploration.\\n3.Study each attribute and its characteristics:\\n‹Name\\n‹Type (categorical, int/float, bounded/unbounded, text, structured, etc.)\\n498 | Appendix B: Machine Learning Project Checklist\\n‹% of missing values‹Noisiness and type of noise (stochastic, outliers, rounding errors, etc.)\\n‹Possibly useful for the task?\\n‹Type of distribution (Gaussian, uniform, logarithmic, etc.)\\n4.For supervised learning tasks, identify the target attribute(s).\\n5.Visualize the data.\\n6.Study the correlations between attributes.\\n7.Study how you would solve the problem manually.\\n8.Identify the promising transformations you may want to apply.\\n9.Identify extra data that would be useful (go back to \\nƒGet the Data⁄ on page 498\\n).10.Document what you have learned.\\nPrepare the DataNotes:\\n‹Work on copies of the data (keep the original dataset intact).\\n‹Write functions for all data transformations you apply, for five reasons:\\n›So you can easily prepare the data the next time you get a fresh dataset\\n›So you can apply these transformations in future projects\\n›To clean and prepare the test set\\n›To clean and prepare new data instances once your solution is live\\n›To make it easy to treat your preparation choices as hyperparameters\\n1.Data cleaning:\\n‹Fix or remove outliers (optional).‹Fill in missing values (e.g., with zero, mean, medianµ) or drop their rows (or\\ncolumns).2.Feature selection (optional):\\n‹Drop the attributes that provide no useful information for the task.\\n3.Feature engineering, where appropriate:\\n‹Discretize continuous features.\\nMachine Learning Project Checklist | 499\\n‹Decompose features (e.g., categorical, date/time, etc.).\\n‹Add promising transformations of features (e.g., log(x), sqrt(x), x^2, etc.).\\n‹Aggregate features into promising new features.\\n4.Feature scaling: standardize or normalize features.\\nShort-List Promising ModelsNotes:\\n‹If the data is huge, you may want to sample smaller training sets so you can train\\nmany different models in a reasonable time (be aware that this penalizes complex\\nmodels such as large neural nets or Random Forests).\\n‹Once again, try to automate these steps as much as possible.\\n1.Train many quick and dirty models from different categories (e.g., linear, naive\\nBayes, SVM, Random Forests, neural net, etc.) using standard parameters.\\n2.Measure and compare their performance.\\n‹For each model, use \\nN-fold cross-validation and compute the mean and stan…\\ndard deviation of the performance measure on the \\nN folds.3.Analyze the most significant variables for each algorithm.\\n4.Analyze the types of errors the models make.‹What data would a human have used to avoid these errors?\\n5.Have a quick round of feature selection and engineering.\\n6.Have one or two more quick iterations of the five previous steps.\\n7.Short-list the top three to five most promising models, preferring models that\\nmake different types of errors.\\nFine-Tune the SystemNotes:\\n‹You will want to use as much data as possible for this step, especially as you move\\ntoward the end of fine-tuning.‹As always automate what you can.\\n500 | Appendix B: Machine Learning Project Checklist\\n1ƒPractical Bayesian Optimization of Machine Learning Algorithms,⁄ J. Snoek, H. Larochelle, R. Adams (2012).\\n1.Fine-tune the hyperparameters using cross-validation.\\n‹Treat your data transformation choices as hyperparameters, especially when\\nyou are not sure about them (e.g., should I replace missing values with zero orwith the median value? Or just drop the rows?).‹Unless there are very few hyperparameter values to explore, prefer random\\nsearch over grid search. If training is very long, you may prefer a Bayesian\\noptimization approach (e.g., using Gaussian process priors, \\nas described byJasper Snoek, Hugo Larochelle, and Ryan Adams\\n).12.Try Ensemble methods. Combining your best models will often perform better\\nthan running them individually.\\n3.Once you are confident about your final model, measure its performance on the\\ntest set to estimate the generalization error.\\nDon‡t tweak your model after measuring the generalization error:\\nyou would just start overfitting the test set.Present Your Solution1.Document what you have done.\\n2.Create a nice presentation.\\n‹Make sure you highlight the big picture first.\\n3.Explain why your solution achieves the business objective.\\n4.Don‡t forget to present interesting points you noticed along the way.\\n‹Describe what worked and what did not.\\n‹List your assumptions and your system‡s limitations.\\n5.Ensure your key findings are communicated through beautiful visualizations or\\neasy-to-remember statements (e.g., ƒthe median income is the number-one pre…\\ndictor of housing prices⁄).\\nMachine Learning Project Checklist | 501\\nLaunch!1.Get your solution ready for production (plug into production data inputs, write\\nunit tests, etc.).2.Write monitoring code to check your system‡s live performance at regular inter…\\nvals and trigger alerts when it drops.‹Beware of slow degradation too: models tend to ƒrot⁄ as data evolves.\\n‹Measuring performance may require a human pipeline (e.g., via a crowdsourc…\\ning service).\\n‹Also monitor your inputs‡ quality (e.g., a malfunctioning sensor sending ran…\\ndom values, or another team‡s output becoming stale). This is particularly\\nimportant for online learning systems.\\n3.Retrain your models on a regular basis on fresh data (automate as much as possi…\\nble).502 | Appendix B: Machine Learning Project Checklist\\nAPPENDIX CSVM Dual ProblemTo understand \\nduality\\n, you first need to understand the Lagrange multipliers\\n method.The general idea is to transform a constrained optimization objective into an uncon…\\nstrained one, by moving the constraints into the objective function. Let‡s look at a\\nsimple example. Suppose you want to find the values of \\nx and y that minimize the\\nfunction f(x,y) = x2 + 2y, subject to an equality constraint\\n: 3x + 2y + 1 = 0. Using the\\nLagrange multipliers method, we start by defining a new function called the \\nLagran…\\ngian\\n (or Lagrange function\\n): g(x, y, ‰) = f(x, y) – ‰(3x + 2y + 1). Each constraint (in\\nthis case just one) is subtracted from the original objective, multiplied by a new vari…\\nable called a Lagrange multiplier.\\nJoseph-Louis Lagrange showed that if \\nx,y is a solution to the constrained optimiza…tion problem, then there must exist an \\n‰ such that \\nx,y,‰ is a \\nstationary point\\n of the\\nLagrangian (a stationary point is a point where all partial derivatives are equal to\\nzero). In other words, we can compute the partial derivatives of \\ng(x, y, ‰) with regardsto x, y, and ‰; we can find the points where these derivatives are all equal to zero; and\\nthe solutions to the constrained optimization problem (if they exist) must be among\\nthese stationary points.\\nIn this example the partial derivatives are: \\nﬂﬂxgx,y,‰=2\\nx”3\\n‰ﬂﬂygx,y,‰=2”2\\n‰ﬂﬂ‰gx,y,‰=”3\\nx”2\\ny”1\\nWhen all these partial derivatives are equal to 0, we find that\\n2x”3\\n‰=2”2\\n‰=”3\\nx”2\\ny”1=0\\n, from which we can easily find that \\nx=32,y=”\\n114, and ‰=1\\n. This is the only stationary point, and as it respects the con…\\nstraint, it must be the solution to the constrained optimization problem.\\n503However, this method applies only to equality constraints. Fortunately, under some\\nregularity conditions (which are respected by the SVM objectives), this method canbe generalized to inequality constraints\\n as well (e.g., 3x + 2y + 1 Š 0). The generalized\\nLagrangian\\n for the hard margin problem is given by Equation C-1\\n, where the ‰(i) vari…\\nables are called the \\nKarush‘Kuhn‘Tucker\\n (KKT) multipliers, and they must be greater\\nor equal to zero.\\nEquation C-1. Generalized Lagrangian for the hard margin problem\\n,b,‰=12T’”“i=1\\nm‰itiT’i+b”1\\nwith‰iŠ0for\\ni=1,2,\\n,mJust like with the Lagrange multipliers method, you can compute the partial deriva…\\ntives and locate the stationary points. If there is a solution, it will necessarily be\\namong the stationary points \\n,b,‰ that respect the \\nKKT conditions\\n:‹Respect the problem‡s constraints: \\ntiT’i+bŠ1for\\ni=1,2,\\n,m,‹Verify \\n‰iŠ0for\\ni=1,2,\\n,m,‹Either ‰i=0\\n or the ith constraint must be an \\nactive constraint\\n, meaning it must\\nhold by equality: \\ntiT’i+b=1\\n. This condition is called the complemen…\\ntary slackness\\n condition. It implies that either \\n‰i=0\\n or the ith instance lies on the\\nboundary (it is a support vector).\\nNote that the KKT conditions are necessary conditions for a stationary point to be a\\nsolution of the constrained optimization problem. Under some conditions, they are\\nalso sufficient conditions. Luckily, the SVM optimization problem happens to meet\\nthese conditions, so any stationary point that meets the KKT conditions is guaranteed\\nto be a solution to the constrained optimization problem.\\nWe can compute the partial derivatives of the generalized Lagrangian with regards to\\nw and b with Equation C-2\\n.Equation C-2. Partial derivatives of the generalized Lagrangian\\n,b,‰=”“i=1\\nm‰itiiﬂﬂb,b,‰=”\\n“i=1\\nm‰iti504 | Appendix C: SVM Dual Problem\\nWhen these partial derivatives are equal to 0, we have \\nEquation C-3\\n.Equation C-3. Properties of the stationary points\\n=“i=1\\nm‰itii“i=1\\nm‰iti=0\\nIf we plug these results into the definition of the generalized Lagrangian, some terms\\ndisappear and we find \\nEquation C-4\\n.Equation C-4. Dual form of the SVM problem\\n,b,‰=12“i=1\\nm“j=1\\nm‰i‰jtitjiT’j”“i=1\\nm‰iwith‰iŠ0for\\ni=1,2,\\n,mThe goal is now to find the vector ‰ that minimizes this function, with \\n‰iŠ0\\n for all\\ninstances. This constrained optimization problem is the dual problem we were look…\\ning for.\\nOnce you find the optimal ‰, you can compute \\n using the first line of Equation C-3\\n.To compute \\nb, you can use the fact that a support vector verifies \\nt(i)(wT ’ x(i) + b) = 1,so if the kth instance is a support vector (i.e., ‰k > 0), you can use it to compute\\nb=1”\\ntkT’k. However, it is often prefered to compute the average over all\\nsupport vectors to get a more stable and precise value, as in Equation C-5\\n.Equation C-5. Bias term estimation using the dual form\\nb=1ns“i=1\\n‰i>0\\nm1”\\ntiT’iSVM Dual Problem | 505\\nAPPENDIX DAutodi†This appendix explains how TensorFlow‡s autodiff feature works, and how it com…\\npares to other solutions.Suppose you define a function f(x,y) = x2y + \\ny + 2, and you need its partial derivatives\\nﬂfﬂx and ﬂfﬂy, typically to perform Gradient Descent (or some other optimization algo…\\nrithm). Your main options are manual differentiation, symbolic differentiation,\\nnumerical differentiation, forward-mode autodiff, and finally reverse-mode autodiff.\\nTensorFlow implements this last option. Let‡s go through each of these options.\\nManual Di†erentiationThe first approach is to pick up a pencil and a piece of paper and use your calculus\\nknowledge to derive the partial derivatives manually. For the function \\nf(x,y) justdefined, it is not too hard; you just need to use five rules:‹The derivative of a constant is 0.\\n‹The derivative of \\nŒx is Œ (where Œ is a constant).\\n‹The derivative of \\nxÓ is ŒxŒ – 1, so the derivative of \\nx2 is 2x.‹The derivative of a sum of functions is the sum of these functions‡ derivatives.\\n‹The derivative of \\nŒ times a function is Œ times its derivative.\\n507From these rules, you can derive Equation D-1\\n:Equation D-1. Partial derivatives of f(x,y)\\nﬂfﬂx=ﬂx2yﬂx+ﬂyﬂx+ﬂ2ﬂx=yﬂx2ﬂx+0+0=2\\nxy\\nﬂfﬂy=ﬂx2yﬂy+ﬂyﬂy+ﬂ2ﬂy=x2+1+0=\\nx2+1\\nThis approach can become very tedious for more complex functions, and you run the\\nrisk of making mistakes. The good news is that deriving the mathematical equations\\nfor the partial derivatives like we just did can be automated, through a process called\\nsymbolic \\ndi›erentiation.Symbolic Di†erentiationFigure D-1 shows how symbolic differentiation works on an even simpler function,\\ng(x,y) = 5 + xy. The graph for that function is represented on the left. After symbolic\\ndifferentiation, we get the graph on the right, which represents the partial derivative\\nﬂgﬂx=0+\\n0‰\\nx+y‰1\\n=y (we could similarly obtain the partial derivative with\\nregards to y).Figure D-1. Symbolic \\ndi›erentiationThe algorithm starts by getting the partial derivative of the leaf nodes. The constant\\nnode (5) returns the constant 0, since the derivative of a constant is always 0. The\\n508 | Appendix D: \\nAutodi†variable x returns the constant 1 since \\nﬂxﬂx=1\\n, and the variable y returns the constant\\n0 since ﬂyﬂx=0\\n (if we were looking for the partial derivative with regards to \\ny, it wouldbe the reverse).Now we have all we need to move up the graph to the multiplication node in function\\ng. Calculus tells us that the derivative of the product of two functions \\nu and v isﬂu‰vﬂx=ﬂvﬂx‰u+ﬂuﬂx‰u. We can therefore construct a large part of the graph on the\\nright, representing 0 ‰ \\nx + y ‰ 1.Finally, we can go up to the addition node in function \\ng. As mentioned, the derivative\\nof a sum of functions is the sum of these functions‡ derivatives. So we just need to\\ncreate an addition node and connect it to the parts of the graph we have already com…\\nputed. We get the correct partial derivative: \\nﬂgﬂx=0+\\n0‰\\nx+y‰1\\n.However, it can be simplified (a lot). A few trivial pruning steps can be applied to this\\ngraph to get rid of all unnecessary operations, and we get a much smaller graph with\\njust one node: ﬂgﬂx=y.In this case, simplification is fairly easy, but for a more complex function, symbolic\\ndifferentiation can produce a huge graph that may be tough to simplify and lead to\\nsuboptimal performance. Most importantly, symbolic differentiation cannot deal with\\nfunctions defined with arbitrary code›for example, the following function \\ndiscussedin Chapter 9\\n:def my_func(a, b):    z = 0    for i in range(100):        z = a * np.cos(z + i) + z * np.sin(b - i)    return zNumerical Di†erentiationThe simplest solution is to compute an approximation of the derivatives, numerically.\\nRecall that the derivative \\nh(x0) of a function h(x) at a point \\nx0 is the slope of the func…tion at that point, or more precisely \\nEquation D-2\\n.Equation D-2. Derivative of a function h(x) at point x\\n0hx=lim\\nxx0hx”hx0x”x0=lim\\n0hx0+”hx0Autodi† | 509\\nSo if we want to calculate the partial derivative of \\nf(x,y) with regards to x, at \\nx = 3 and\\ny = 4, we can simply compute \\nf(3 + , 4) – f(3, 4) and divide the result by , using avery small value for \\n. That‡s exactly what the following code does:\\ndef f(x, y):    return x**2*y + y + 2def derivative(f, x, y, x_eps, y_eps):    return (f(x + x_eps, y + y_eps) - f(x, y)) / (x_eps + y_eps)df_dx = derivative(f, 3, 4, 0.00001, 0)df_dy = derivative(f, 3, 4, 0, 0.00001)Unfortunately, the result is imprecise (and it gets worse for more complex functions).\\nThe correct results are respectively 24 and 10, but instead we get:>>> print(df_dx)24.000039999805264>>> print(df_dy)10.000000000331966Notice that to compute both partial derivatives, we have to call \\nf() at least three times\\n(we called it four times in the preceding code, but it could be optimized). If therewere 1,000 parameters, we would need to call f() at least 1,001 times. When you are\\ndealing with large neural networks, this makes numerical differentiation way too\\ninefficient.\\nHowever, numerical differentiation is so simple to implement that it is a great tool to\\ncheck that the other methods are implemented correctly. For example, if it disagrees\\nwith your manually derived function, then your function probably contains a mis…\\ntake.Forward-Mode Autodi†Forward-mode \\nautodi› is neither numerical differentiation nor symbolic differentia…\\ntion, but in some ways it is their love child. It relies on \\ndual numbers\\n, which are(weird but fascinating) numbers of the form \\na + b where a and b are real numbers\\nand  is an infinitesimal number such that \\n2 = 0 (but  ł 0). You can think of the\\ndual number 42 + 24\\n as something akin to 42.0000000024 with an infinite num…\\nber of 0s (but of course this is simplified just to give you some idea of what dual num…\\nbers are). A dual number is represented in memory as a pair of floats. For example, 42\\n+ 24 is represented by the pair (42.0, 24.0).\\n510 | Appendix D: \\nAutodi†Dual numbers can be added, multiplied, and so on, as shown in \\nEquation D-3\\n.Equation D-3. A few operations with dual numbers\\nŒa+b=Œa+Œba+b+c+d=a+c+b+da+b‰c+d=ac+ad+bc+bd2=ac+ad+bcMost importantly, it can be shown that \\nh(a + b) = h(a) + b ‰ h(a), so computing\\nh(a + ) gives you both h(a) and the derivative \\nh(a) in just one shot. Figure D-2shows how forward-mode autodiff computes the partial derivative of \\nf(x,y) withregards to x at \\nx = 3 and y = 4. All we need to do is compute \\nf(3 + , 4); this willoutput a dual number whose first component is equal to \\nf(3, 4) and whose secondcomponent is equal to \\nﬂfﬂx3,4\\n.Figure D-2. Forward-mode \\nautodi›Autodi† | 511\\nTo compute \\nﬂfﬂy3,4\\n we would have to go through the graph again, but this time with\\nx = 3 and y = 4 + .So forward-mode autodiff is much more accurate than numerical differentiation, but\\nit suffers from the same major flaw: if there were 1,000 parameters, it would require\\n1,000 passes through the graph to compute all the partial derivatives. This is where\\nreverse-mode autodiff shines: it can compute all of them in just two passes \\nthroughthe graph.\\nReverse-Mode Autodi†Reverse-mode autodiff is the solution implemented by TensorFlow. It first goes\\nthrough the graph in the forward direction (i.e., from the inputs to the output) to\\ncompute the value of each node. Then it does a second pass, this time in the reverse\\ndirection (i.e., from the output to the inputs) to compute all the partial derivatives.\\nFigure D-3 represents the second pass. During the first pass, all the node values were\\ncomputed, starting from \\nx = 3 and y = 4. You can see those values at the bottom right\\nof each node (e.g., x ‰ x = 9). The nodes are labeled n1 to n7 for clarity. The output\\nnode is n7: f(3,4) = n7 = 42.Figure D-3. Reverse-mode \\nautodi›512 | Appendix D: \\nAutodi†The idea is to gradually go down the graph, computing the partial derivative of \\nf(x,y)with regards to each consecutive node, until we reach the variable nodes. For this,\\nreverse-mode autodiff relies heavily on the \\nchain rule\\n, shown in Equation D-4\\n.Equation D-4. Chain rule\\nﬂfﬂx=ﬂfﬂni‰ﬂniﬂxSince n7 is the output node, f = n7 so trivially ﬂfﬂn7=1\\n.Let‡s continue down the graph to \\nn5: how much does \\nf vary when \\nn5 varies? Theanswer is ﬂfﬂn5=ﬂfﬂn7‰ﬂn7ﬂn5. We already know that \\nﬂfﬂn7=1\\n, so all we need is ﬂn7ﬂn5. Sincen7 simply performs the sum \\nn5 + n6, we find that \\nﬂn7ﬂn5=1\\n, so ﬂfﬂn5=1‰1=1\\n.Now we can proceed to node \\nn4: how much does \\nf vary when \\nn4 varies? The answer isﬂfﬂn4=ﬂfﬂn5‰ﬂn5ﬂn4. Since n5 = n4 ‰ n2, we find that \\nﬂn5ﬂn4=n2, so ﬂfﬂn4=1‰\\nn2=4\\n.The process continues until we reach the bottom of the graph. At that point we will\\nhave calculated all the partial derivatives of \\nf(x,y) at the point \\nx = 3 and y = 4. In thisexample, we find \\nﬂfﬂx=24\\n and ﬂfﬂy=10\\n. Sounds about right!\\nReverse-mode autodiff is a very powerful and accurate technique, especially when\\nthere are many inputs and few outputs, since it requires only one forward pass plus\\none reverse pass per output to compute all the partial derivatives for all outputs with\\nregards to all the inputs. Most importantly, it can deal with functions defined by arbi…\\ntrary code. It can also handle functions that are not entirely differentiable, as long as\\nyou ask it to compute the partial derivatives at points that are differentiable.\\nIf you implement a new type of operation in TensorFlow and you\\nwant to make it compatible with autodiff, then you need to provide\\na function that builds a subgraph to compute its partial derivatives\\nwith regards to its inputs. For example, suppose you implement a\\nfunction that computes the square of its input \\nf(x) = x2. In that case\\nyou would need to provide the corresponding derivative function \\nf(x) = 2x. Note that this function does not compute a numerical\\nresult, but instead builds a subgraph that will (later) compute the\\nresult. This is very useful because it means that you can compute\\ngradients of gradients (to compute second-order derivatives, or\\neven higher-order derivatives).\\nAutodi† | 513\\nAPPENDIX EOther Popular ANN ArchitecturesIn this appendix we will give a quick overview of a few historically important neural\\nnetwork architectures that are much less used today than deep Multi-Layer Percep…\\ntrons (Chapter 10\\n), convolutional neural networks (\\nChapter 13\\n), recurrent neural\\nnetworks (Chapter 14\\n), or autoencoders (\\nChapter 15\\n). They are often mentioned in\\nthe literature, and some are still used in many applications, so it is worth knowing\\nabout them. Moreover, we will discuss \\ndeep belief nets\\n (DBNs), which were the state of\\nthe art in Deep Learning until the early 2010s. They are still the subject of very active\\nresearch, so they may well come back with a vengeance in the near future.\\nHop•eld NetworksHop†eld networks\\n were \\nfirst introduced by W. A. Little in 1974, then popularized by J.\\nHopfield in 1982. They are \\nassociative memory\\n networks: you first teach them somepatterns, and then when they see a new pattern they (hopefully) output the closest\\nlearned pattern. This has made them useful in particular for character recognition\\nbefore they were outperformed by other approaches. You first train the network by\\nshowing it examples of character images (each binary pixel maps to one neuron), and\\nthen when you show it a new character image, after a few iterations it outputs the\\nclosest learned character.\\nThey are fully connected graphs (see \\nFigure E-1); that is, every neuron is connected\\nto every other neuron. Note that on the diagram the images are 6 ‰ 6 pixels, so the\\nneural network on the left should contain 36 neurons (and 648 connections), but for\\nvisual clarity a much smaller network is represented.\\n515Figure E-1. \\nHop†eld network\\nThe training algorithm works by using Hebb‡s rule: for each training image, the\\nweight between two neurons is increased if the corresponding pixels are both on or\\nboth off, but decreased if one pixel is on and the other is off.To show a new image to the network, you just activate the neurons that correspond to\\nactive pixels. The network then computes the output of every neuron, and this gives\\nyou a new image. You can then take this new image and repeat the whole process.\\nAfter a while, the network reaches a stable state. Generally, this corresponds to the\\ntraining image that most resembles the input image.\\nA so-called energy function\\n is associated with Hopfield nets. At each iteration, the\\nenergy decreases, so the network is guaranteed to eventually stabilize to a low-energy\\nstate. The training algorithm tweaks the weights in a way that decreases the energy\\nlevel of the training patterns, so the network is likely to stabilize in one of these low-\\nenergy configurations. Unfortunately, some patterns that were not in the training set\\nalso end up with low energy, so the network sometimes stabilizes in a configuration\\nthat was not learned. These are called \\nspurious patterns\\n.Another major flaw with Hopfield nets is that they don‡t scale very well›their mem…\\nory capacity is roughly equal to 14% of the number of neurons. For example, to clas…\\nsify 28 ‰ 28 images, you would need a Hopfield net with 784 fully connected neurons\\nand 306,936 weights. Such a network would only be able to learn about 110 different\\ncharacters (14% of 784). That‡s a lot of parameters for such a small memory.\\nBoltzmann MachinesBoltzmann machines\\n were invented in 1985 by Geoffrey Hinton and Terrence Sejnow…\\nski. Just like Hopfield nets, they are fully connected ANNs, but they are based \\non sto…\\n516 | Appendix E: Other Popular ANN Architectures\\nchastic neurons\\n: instead of using a deterministic step function to decide what value to\\noutput, these neurons output 1 with some probability, and 0 otherwise. The probabil…\\nity function that these ANNs use is based on the Boltzmann distribution (used in\\nstatistical mechanics) hence their name. \\nEquation E-1\\n gives the probability that a par…\\nticular neuron will output a 1.Equation E-1. Probability that the i\\nth\\n neuron will output 1\\npsinextstep\\n=1\\n=„“j=1\\nNwi,jsj+biT‹sj is the jth neuron‡s state (0 or 1).\\n‹wi,j is the connection weight between the i\\nth and jth neurons. Note that \\nwi,i = 0.‹bi is the ith neuron‡s bias term. We can implement this term by adding a bias neu…\\nron to the network.‹N is the number of neurons in the network.\\n‹T is a number called the network‡s \\ntemperature\\n; the higher the temperature, the\\nmore random the output is (i.e., the more the probability approaches 50%).\\n‹„ is the logistic function.Neurons in Boltzmann machines are separated into two groups: \\nvisible units\\n and \\nhid…\\nden units\\n (see Figure E-2). All neurons work in the same stochastic way, but the visi…\\nble units are the ones that receive the inputs and from which outputs are read.\\nFigure E-2. Boltzmann machine\\nOther Popular ANN Architectures | 517\\nBecause of its stochastic nature, a Boltzmann machine will never stabilize into a fixed\\nconfiguration, but instead it will keep switching between many configurations. If it is\\nleft running for a sufficiently long time, the probability of observing a particular con…\\nfiguration will only be a function of the connection weights and bias terms, not of the\\noriginal configuration (similarly, after you shuffle a deck of cards for long enough, the\\nconfiguration of the deck does not depend on the initial state). When the network\\nreaches this state where the original configuration is ƒforgotten,⁄ it is said to be \\ninthermal equilibrium\\n (although its configuration keeps changing all the time). By set…\\nting the network parameters appropriately, letting the network reach thermal equili…\\nbrium, and then observing its state, we can simulate a wide range of probability\\ndistributions. This is called a generative model\\n.Training a Boltzmann machine means finding the parameters that will make the net…\\nwork approximate the training set‡s probability distribution. For example, if there are\\nthree visible neurons and the training set contains 75% (0, 1, 1) triplets, 10% (0, 0, 1)\\ntriplets, and 15% (1, 1, 1) triplets, then after training a Boltzmann machine, you could\\nuse it to generate random binary triplets with about the same probability distribu…\\ntion. For example, about 75% of the time it would output the (0, 1, 1) triplet.\\nSuch a generative model can be used in a variety of ways. For example, if it is trained\\non images, and you provide an incomplete or noisy image to the network, it will\\nautomatically ƒrepair⁄ the image in a reasonable way. You can also use a generative\\nmodel for classification. Just add a few visible neurons to encode the training image‡s\\nclass (e.g., add 10 visible neurons and turn on only the fifth neuron when the trainingimage represents a 5). Then, when given a new image, the network will automatically\\nturn on the appropriate visible neurons, indicating the image‡s class (e.g., it will turn\\non the fifth visible neuron if the image represents a 5).\\nUnfortunately, there is no efficient technique to train Boltzmann machines. However,\\nfairly efficient algorithms have been developed to \\ntrain restricted Boltzmann machines\\n(RBM).Restricted Boltzmann MachinesAn RBM is simply a Boltzmann machine in which there are no connections between\\nvisible units or between hidden units, only between visible and hidden units. For\\nexample, \\nFigure E-3 represents an RBM with three visible units and four hidden\\nunits.518 | Appendix E: Other Popular ANN Architectures\\n1ƒOn Contrastive Divergence Learning,⁄ M. Ô. Carreira-PerpiÕÖn and G. Hinton (2005).\\nFigure E-3. Restricted Boltzmann machine\\nA very efficient training algorithm, called \\nContrastive Divergence\\n, was introduced in\\n2005 by Miguel Ô. Carreira-PerpiÕÖn and Geoffrey Hinton\\n.1 Here is how it works: for\\neach training instance x, the algorithm starts by feeding it to the network by settingthe state of the visible units to \\nx1, x2, , xn. Then you compute the state of the hidden\\nunits by applying the stochastic equation described before (\\nEquation E-1\\n). This givesyou a hidden vector h (where \\nhi is equal to the state of the i\\nth unit). Next you compute\\nthe state of the visible units, by applying the same stochastic equation. This gives you\\na vector ı. Then once again you compute the state of the hidden units, which gives\\nyou a vector ı. Now you can update each connection weight by applying the rule in\\nEquation E-2\\n.Equation E-2. Contrastive divergence weight update\\nwi,jnextstep\\n=wi,j+−\\nT”ııTThe great benefit of this algorithm it that it does not require waiting for the network\\nto reach thermal equilibrium: it just goes forward, backward, and forward again, and\\nthat‡s it. This makes it incomparably more efficient than previous algorithms, and it\\nwas a key ingredient to the first success of Deep Learning based on multiple stacked\\nRBMs.Deep Belief NetsSeveral layers of RBMs can be stacked; the hidden units of the first-level RBM serves\\nas the visible units for the second-layer RBM, and so on. Such an RBM stack is called\\na deep belief net\\n (DBN).Yee-Whye Teh, one of Geoffrey Hinton‡s students, observed that it was possible to\\ntrain DBNs one layer at a time using Contrastive Divergence, starting with the lower\\nOther Popular ANN Architectures | 519\\n2ƒA Fast Learning Algorithm for Deep Belief Nets,⁄ G. Hinton, S. Osindero, Y. Teh (2006).\\nlayers and then gradually moving up to the top layers. This led to the \\ngroundbreakingarticle that kickstarted the Deep Learning tsunami in 2006\\n.2Just like RBMs, DBNs learn to reproduce the probability distribution of their inputs,\\nwithout any supervision. However, they are much better at it, for the same reason that\\ndeep neural networks are more powerful than shallow ones: real-world data is often\\norganized in hierarchical patterns, and DBNs take advantage of that. Their lower lay…\\ners learn low-level features in the input data, while higher layers learn high-level fea…\\ntures.Just like RBMs, DBNs are fundamentally unsupervised, but you can also train them\\nin a supervised manner by adding some visible units to represent the labels. More…\\nover, one great feature of DBNs is that they can be trained in a semisupervised fash…\\nion. Figure E-4 represents such a DBN configured for semisupervised learning.\\nFigure E-4. A deep belief network \\ncon†gured for semisupervised learning\\nFirst, the RBM 1 is trained without supervision. It learns low-level features in the\\ntraining data. Then RBM 2 is trained with RBM 1‡s hidden units as inputs, again\\nwithout supervision: it learns higher-level features (note that RBM 2‡s hidden units\\ninclude only the three rightmost units, not the label units). Several more RBMs could\\nbe stacked this way, but you get the idea. So far, training was 100% unsupervised.\\n520 | Appendix E: Other Popular ANN Architectures\\n3See this video by Geoffrey Hinton for more details and a demo: \\nhttp://goo.gl/7Z5QiS\\n.Lastly, RBM 3 is trained using both RBM 2‡s hidden units as inputs, as well as extra\\nvisible units used to represent the target labels (e.g., a one-hot vector representing the\\ninstance class). It learns to associate high-level features with training labels. This is\\nthe supervised step.\\nAt the end of training, if you feed RBM 1 a new instance, the signal will propagate up\\nto RBM 2, then up to the top of RBM 3, and then back down to the label units; hope…fully, the appropriate label will light up. This is how a DBN can be used for classifica…\\ntion.One great benefit of this semisupervised approach is that you don‡t need much\\nlabeled training data. If the unsupervised RBMs do a good enough job, then only a\\nsmall amount of labeled training instances per class will be necessary. Similarly, a\\nbaby learns to recognize objects without supervision, so when you point to a chair\\nand say ƒchair,⁄ the baby can associate the word ƒchair⁄ with the class of objects it has\\nalready learned to recognize on its own. You don‡t need to point to every single chair\\nand say ƒchair⁄; only a few examples will suffice (just enough so the baby can be sure\\nthat you are indeed referring to the chair, not to its color or one of the chair‡s parts).\\nQuite amazingly, DBNs can also work in reverse. If you activate one of the label units,\\nthe signal will propagate up to the hidden units of RBM 3, then down to RBM 2, and\\nthen RBM 1, and a new instance will be output by the visible units of RBM 1. Thisnew instance will usually look like a regular instance of the class whose label unit youactivated. This generative capability of DBNs is quite powerful. For example, it has\\nbeen used to automatically generate captions for images, and vice versa: first a DBN is\\ntrained (without supervision) to learn features in images, and another DBN is trained\\n(again without supervision) to learn features in sets of captions (e.g., ƒcar⁄ often\\ncomes with ƒautomobile⁄). Then an RBM is stacked on top of both DBNs and trained\\nwith a set of images along with their captions; it learns to associate high-level features\\nin images with high-level features in captions. Next, if you feed the image DBN an\\nimage of a car, the signal will propagate through the network, up to the top-level\\nRBM, and back down to the bottom of the caption DBN, producing a caption. Due to\\nthe stochastic nature of RBMs and DBNs, the caption will keep changing randomly,\\nbut it will generally be appropriate for the image. If you generate a few hundred cap…\\ntions, the most frequently generated ones will likely be a good description of the\\nimage.3Self-Organizing MapsSelf-organizing maps\\n (SOM) are quite different from all the other types of neural net…\\nworks we have discussed so far. They are used to produce a low-dimensional repre…\\nOther Popular ANN Architectures | 521\\nsentation of a high-dimensional dataset, generally for visualization, clustering, or\\nclassification. The neurons are spread across a map (typically 2D for visualization,\\nbut it can be any number of dimensions you want), as shown in \\nFigure E-5, and eachneuron has a weighted connection to every input (note that the diagram shows just\\ntwo inputs, but there are typically a very large number, since the whole point of\\nSOMs is to reduce dimensionality).Figure E-5. Self-organizing maps\\nOnce the network is trained, you can feed it a new instance and this will activate only\\none neuron (i.e., hence one point on the map): the neuron whose weight vector is\\nclosest to the input vector. In general, instances that are nearby in the original input\\nspace will activate neurons that are nearby on the map. This makes SOMs useful for\\nvisualization (in particular, you can easily identify clusters on the map), but also for\\napplications like speech recognition. For example, if each instance represents the\\naudio recording of a person pronouncing a vowel, then different pronunciations of\\nthe vowel ƒa⁄ will activate neurons in the same area of the map, while instances of the\\nvowel ƒe⁄ will activate neurons in another area, and intermediate sounds will gener…\\nally activate intermediate neurons on the map.\\nOne important difference with the other dimensionality reduction\\ntechniques discussed in Chapter 8\\n is that all instances get mapped\\nto a discrete number of points in the low-dimensional space (one\\npoint per neuron). When there are very few neurons, this techni…\\nque is better described as clustering rather than dimensionality\\nreduction.522 | Appendix E: Other Popular ANN Architectures\\n4You can imagine a class of young children with roughly similar skills. One child happens to be slightly better\\nat basketball. This motivates her to practice more, especially with her friends. After a while, this group of\\nfriends gets so good at basketball that other kids cannot compete. But that‡s okay, because the other kids spe…\\ncialize in other topics. After a while, the class is full of little specialized groups.The training algorithm is unsupervised. It works by having all the neurons compete\\nagainst each other. First, all the weights are initialized randomly. Then a training\\ninstance is picked randomly and fed to the network. All neurons compute the dis…\\ntance between their weight vector and the input vector (this is very different from the\\nartificial neurons we have seen so far). The neuron that measures the smallest dis…\\ntance wins and tweaks its weight vector to be even slightly closer to the input vector,\\nmaking it more likely to win future competitions for other inputs similar to this one.\\nIt also recruits its neighboring neurons, and they too update their weight vector to be\\nslightly closer to the input vector (but they don‡t update their weights as much as the\\nwinner neuron). Then the algorithm picks another training instance and repeats the\\nprocess, again and again. This algorithm tends to make nearby neurons graduallyspecialize in similar inputs.\\n4Other Popular ANN Architectures | 523\\nIndexSymbols__call__(), 385Ò-greedy policy, 459, 464Ò-insensitive, 155× 2 test (see chi square test)— 0 norm, 39— 1 and — 2 regularization, \\n303-304— 1 norm, 39, 130, 139, 300, 303— 2 norm, 39, 128-130, 139, 142, 303, 307— k norm, 39— ‚ norm, 39Aaccuracy, 4, 83-84actions, evaluating, \\n447-448activation functions, \\n262-264active constraints, \\n504actors, 463actual class, 85AdaBoost, \\n192-195Adagrad, \\n296-298Adam optimization, \\n293, 298-300adaptive learning rate, \\n297adaptive moment optimization, \\n298agents, \\n438AlexNet architecture, \\n367-368algorithmspreparing data for, \\n59-68AlphaGo, 14, 253, 437, 453Anaconda, 41anomaly detection, 12Apple‡s Siri, \\n253apply_gradients(), \\n286, 450area under the curve (AUC), \\n92arg_scope(), 285array_split(), \\n217artificial neural networks (ANNs), \\n253-274Boltzmann Machines, \\n516-518deep belief networks (DBNs), \\n519-521evolution of, 254Hopfield Networks, \\n515-516hyperparameter fine-tuning, \\n270-272overview, \\n253-255Perceptrons, \\n257-264self-organizing maps, \\n521-523training a DNN with TensorFlow, \\n265-270artificial neuron, 256(see also artificial neural network (ANN))assign(), 237association rule learning, \\n12associative memory networks, \\n515assumptions, checking, \\n40asynchronous updates, \\n348-349asynchrous communication, \\n329-334atrous_conv2d(), \\n376attention mechanism, \\n409attributes, \\n9, 45-48(see also data structure)\\ncombinations of, \\n58-59preprocessed, 48target, 48autodiff, \\n238-239, 507-513forward-mode, \\n510-512manual differentiation, \\n507numerical differentiation, \\n509reverse-mode, 512-513symbolic differentiation, \\n508-509autoencoders, \\n411-435525adversarial, 433contractive, \\n432denoising, 424-425efficient data representations, \\n412generative stochastic network (GSN), \\n433overcomplete, \\n424PCA with undercomplete linear autoen…\\ncoder, 413reconstructions, 413sparse, 426-428stacked, 415-424stacked convolutional, \\n433undercomplete, \\n413variational, \\n428-432visualizing features, \\n421-422winner-take-all (WTA), \\n433automatic differentiating, \\n231autonomous driving systems, \\n379Average Absolute Deviation, \\n39average pooling layer, \\n364avg_pool(), \\n364Bbackpropagation, \\n261-262, 275, 291, 422backpropagation through time (BPTT), \\n389bagging and pasting, 185-188out-of-bag evaluation, \\n187-188in Scikit-Learn, 186-187bandwidth saturation, \\n349-351BasicLSTMCell, 401BasicRNNCell, 397-398Batch Gradient Descent, \\n114-117, 130batch learning, \\n14-15Batch Normalization, \\n282-286, 374operation summary, \\n282with TensorFlow, \\n284-286batch(), \\n341batch_join(), \\n341batch_norm(), \\n284-285Bellman Optimality Equation, \\n455between-graph replication, \\n344bias neurons, 258bias term, 106bias/variance tradeoff, 126biases, 267binary classifiers, \\n82, 134biological neurons, 254-256black box models, 170blending, 200-203Boltzmann Machines, \\n516-518(see also restricted Boltzman machines\\n(RBMs))boosting, 191-200AdaBoost, \\n192-195Gradient Boosting, \\n195-200bootstrap aggregation (see bagging)\\nbootstrapping, \\n72, 185, 442, 469bottleneck layers, \\n369brew, 202CCaffe model zoo, 291call__(), 398CART (Classification and Regression Tree)\\nalgorithm, 170-171, 176categorical attributes, \\n62-64cell wrapper, \\n392chi square test, 174classification versus regression, \\n8, 101classifiersbinary, \\n82error analysis, 96-99evaluating, \\n96MNIST dataset, \\n79-81multiclass, \\n93-96multilabel, \\n100-101multioutput, \\n101-102performance measures, 82-93precision of, 85voting, 181-184clip_by_value(), 286closed-form equation, \\n105, 128, 136cluster specification, \\n324clustering algorithms, 10clusters, 323coding space, 429codings, 411complementary slackness condition, \\n504components_, \\n214computational complexity, \\n110, 153, 172compute_gradients(), \\n286, 449concat(), \\n369config.gpu_options, 318ConfigProto, 317confusion matrix, \\n84-86, 96-99connectionism, 260constrained optimization, \\n158, 503Contrastive Divergence, \\n519526 | Index\\ncontrol dependencies, \\n323conv1d(), \\n376conv2d_transpose(), \\n376conv3d(), \\n376convergence rate, \\n117convex function, \\n113convolution kernels, \\n357, 365, 370convolutional neural networks (CNNs),\\n353-378architectures, 365-376AlexNet, \\n367-368GoogleNet, \\n368-372LeNet5, \\n366-367ResNet, \\n372-375convolutional layer, \\n355-363, 370, 376feature maps, \\n358-360filters, 357memory requirement, \\n362-363evolution of, 354pooling layer, \\n363-365TensorFlow implementation, \\n360-362Coordinator class, \\n338-340correlation coefficient, \\n55-58correlations, finding, \\n55-58cost function, 20, 39in AdaBoost, \\n193in adagrad, 297in artificial neural networks, 264, 267-268in autodiff, \\n238in batch normalization, \\n285cross entropy, \\n367deep Q-Learning, 465in Elastic Net, \\n132in Gradient Descent, \\n105, 111-112, 114,117-119, 200, 275in Logistic Regression, 135-136in PG algorithms, 449in variational autoencoders, \\n430in Lasso Regression, 130-131in Linear Regression, 108, 113in Momentum optimization, \\n294-295in pretrained layers reuse, \\n293in ridge regression, 127-129in RNNs, \\n389, 393stale gradients and, \\n349creative sequences, \\n396credit assignment problem, \\n447-448critics, 463cross entropy, \\n140-141, 264, 428, 449cross-validation, \\n30, 69-71, 83-84CUDA library, \\n315cuDNN library, \\n315curse of dimensionality, 205-207(see also dimensionality reduction)custom transformers, 64-65Ddata, \\n30(see also test data; training data)\\ncreating workspace for, \\n40-43downloading, 43-45finding correlations in, \\n55-58making assumptions about, \\n30preparing for Machine Learning algorithms,\\n59-68test-set creation, \\n49-53working with real data, \\n33data augmentation, \\n309-310data cleaning, \\n60-62data mining, \\n6data parallelism, \\n347-351asynchronous updates, \\n348-349bandwidth saturation, \\n349-351synchronous updates, \\n348TensorFlow implementation, \\n351data pipeline, \\n36data snooping bias, \\n49data structure, \\n45-48data visualization, \\n53-55DataFrame, \\n60dataquest, \\nxvidecay, \\n284decision boundaries, 136-139, 142, 170decision function, 87, 156-157Decision Stumps, \\n195decision threshold, 87Decision Trees, \\n69-70, 167-179, 181binary trees, \\n170class probability estimates, \\n171computational complexity, \\n172decision boundaries, 170GINI impurity, \\n172instability with, 177-178numbers of children, \\n170predictions, 169-171Random Forests (see Random Forests)\\nregression tasks, 175-176regularization hyperparameters, \\n173-174Index | 527\\ntraining and visualizing, 167-169decoder, 412deconvolutional layer, \\n376deep autoencoders (see stacked autoencoders)\\ndeep belief networks (DBNs), \\n13, 519-521Deep Learning, 437(see also Reinforcement Learning; Tensor…\\nFlow)about, xiii, xvilibraries, 230-231deep neural networks (DNNs), \\n261, 275-312(see also Multi-Layer Perceptrons (MLP))\\nfaster optimizers for, 293-302regularization, \\n302-310reusing pretrained layers, \\n286-293training guidelines overview, \\n310training with TensorFlow, \\n265-270training with TF.Learn, \\n264unstable gradients, \\n276vanishing and exploding gradients, \\n275-286Deep Q-Learning, 460-469Ms. Pac Man example, \\n460-469deep Q-network, 460deep RNNs, \\n396-400applying dropout, \\n399distributing across multiple GPUs, \\n397long sequence difficulties, 400truncated backpropagation through time,\\n400DeepMind, 14, 253, 437, 460degrees of freedom, 27, 126denoising autoencoders, \\n424-425depth concat layer, \\n369depth radius, 368depthwise_conv2d(), \\n376dequeue(), 332dequeue_many(), \\n332, 334dequeue_up_to(), 333-334dequeuing data, \\n331describe(), 46device blocks, 327device(), 319dimensionality reduction, 12, 205-225, 411approaches to\\nManifold Learning, \\n210projection, 207-209choosing the right number of dimensions,\\n215curse of dimensionality, 205-207and data visualization, \\n205Isomap, \\n224LLE (Locally Linear Embedding), 221-223Multidimensional Scaling, \\n223-224PCA (Principal Component Analysis),\\n211-218t-Distributed Stochastic Neighbor Embed…\\nding (t-SNE), 224discount rate, \\n447distributed computing, \\n229distributed sessions, 328-329DNNClassifier, 264drop(), 60dropconnect, 307dropna(), 60dropout, 272, 399dropout rate, \\n304dropout(), 306DropoutWrapper, \\n399DRY (Don‡t Repeat Yourself), \\n247Dual Averaging, \\n300dual numbers, \\n510dual problem, 160duality, 503dying ReLUs, \\n279dynamic placements, \\n320dynamic placer, 318Dynamic Programming, \\n456dynamic unrolling through time, 387dynamic_rnn(), 387, 398, 409Eearly stopping, 133-134, 198, 272, 303Elastic Net, \\n132embedded device blocks, 327Embedded Reber grammars, 410embeddings, 405-407embedding_lookup(), 406encoder, 412Encoder–Decoder, \\n383end-of-sequence (EOS) token, 388energy functions, \\n516enqueuing data, \\n330Ensemble Learning, 70, 74, 181-203bagging and pasting, 185-188boosting, 191-200in-graph versus between-graph replication,\\n343-345Random Forests, \\n189-191528 | Index\\n(see also Random Forests)\\nrandom patches and random subspaces, \\n188stacking, 200-202entropy impurity measure, \\n172environments, in reinforcement learning,\\n438-447, 459, 464episodes (in RL), 444, 448-449, 451-452, 469epochs, 118Ò-insensitive, 155equality contraints, \\n504error analysis, 96-99estimators, \\n61Euclidian norm, \\n39eval(), 240evaluating models, \\n29-31explained variance, 215explained variance ratio, \\n214exploding gradients, \\n276(see also gradients, vanishing and explod…\\ning)exploration policies, \\n459exponential decay, \\n284exponential linear unit (ELU), \\n280-281exponential scheduling, \\n301Extra-Trees, \\n190FF-1 score, \\n86-87face-recognition, 100fake X server, \\n443false positive rate (FPR), \\n91-93fan-in, 277, 279fan-out, 277, 279feature detection, \\n411feature engineering, \\n25feature extraction, \\n12feature importance, \\n190-191feature maps, \\n220, 357-360, 374feature scaling, \\n65feature selection, \\n26, 74, 130, 191, 499feature space, \\n218, 220feature vector, \\n39, 107, 156, 237features, \\n9FeatureUnion, \\n66feedforward neural network (FNN), \\n263feed_dict, 240FIFOQueue, \\n330, 333fillna(), 60first-in first-out (FIFO) queues, 330first-order partial derivatives (Jacobians), \\n300fit(), 61, 66, 217fitness function, 20fit_inverse_transform=, \\n221fit_transform(), 61, 66folds, 69, 81, 83-84Follow The Regularized Leader (FTRL), \\n300forget gate, \\n402forward-mode autodiff, \\n510-512framing a problem, 35-37frozen layers, \\n289-290fully_connected(), 267, 278, 284-285, 417Ggame play (see reinforcement learning)\\ngamma value, 152gate controllers, \\n402Gaussian distribution, \\n37, 429, 431Gaussian RBF, \\n151Gaussian RBF kernel, \\n152-153, 163generalization error, \\n29generalized Lagrangian, 504-505generative autoencoders, \\n428generative models, \\n411, 518genetic algorithms, 440geodesic distance, 224get_variable(), 249-250GINI impurity, \\n169, 172global average pooling, \\n372global_step, 466global_variables(), 308global_variables_initializer(), 233Glorot initialization, \\n276-279Google, 230Google Images, 253Google Photos, 13GoogleNet architecture, \\n368-372gpu_options.per_process_gpu_memory_frac…\\ntion, 317gradient ascent, \\n441Gradient Boosted Regression Trees (GBRT),\\n195Gradient Boosting, \\n195-200Gradient Descent (GD), \\n105, 111-121, 164, 275,294, 296algorithm comparisons, \\n119-121automatically computing gradients, \\n238-239Batch GD, \\n114-117, 130defining, 111Index | 529\\nlocal minimum versus global minimum, \\n112manually computing gradients, \\n237Mini-batch GD, \\n119-121, 239-241optimizer, 239Stochastic GD, 117-119, 148with TensorFlow, \\n237-239Gradient Tree Boosting, \\n195GradientDescentOptimizer, \\n268gradients(), \\n238gradients, vanishing and exploding, \\n275-286,400Batch Normalization, \\n282-286Glorot and He initialization, \\n276-279gradient clipping, \\n286nonsaturating activation functions, \\n279-281graphviz, \\n168greedy algorithm, 172grid search, 71-74, 151group(), 464GRU (Gated Recurrent Unit) cell, \\n404-405Hhailstone sequence, 412hard margin classification, \\n146-147hard voting classifiers, 181-184harmonic mean, 86He initialization, \\n276-279Heaviside step function, \\n257HebbØs rule, \\n258, 516Hebbian learning, \\n259hidden layers, \\n261hierarchical clustering, 10hinge loss function, 164histograms, 47-48hold-out sets, 200(see also blenders)Hopfield Networks, \\n515-516hyperbolic tangent (htan activation function),\\n262, 272, 276, 278, 381hyperparameters, \\n28, 65, 72-74, 76, 111, 151,154, 270(see also neural network hyperparameters)\\nhyperplane, \\n157, 210-211, 213, 224hypothesis, \\n39manifold, 210hypothesis boosting (see boosting)\\nhypothesis function, \\n107hypothesis, null, \\n174Iidentity matrix, \\n128, 160ILSVRC ImageNet challenge, \\n365image classification, \\n365impurity measures, \\n169, 172in-graph replication, \\n343inception modules, 369Inception-v4, 375incremental learning, \\n16, 217inequality constraints, \\n504inference, 22, 311, 363, 408info(), 45information gain, \\n173information theory, \\n172init node, 241input gate, \\n402input neurons, \\n258input_put_keep_prob, \\n399instance-based learning, 17, 21InteractiveSession, \\n233intercept term, \\n106Internal Covariate Shift problem, \\n282inter_op_parallelism_threads, \\n322intra_op_parallelism_threads, \\n322inverse_transform(), \\n221in_top_k(), 268irreducible error, 127isolated environment, \\n41-42Isomap, \\n224is_training, 284-285, 399Jjobs, 323join(), 325, 339Jupyter, \\n40, 42, 48KK-fold cross-validation, \\n69-71, 83k-Nearest Neighbors, \\n21, 100Karush–Kuhn–Tucker (KKT) conditions, \\n504keep probability, 306Keras, \\n231Kernel PCA (kPCA), \\n218-221kernel trick, 150, 152, 161-164, 218kernelized SVM, 161-164kernels, 150-153, 321Kullback–Leibler divergence, \\n141, 426530 | Index\\nLl1_l2_regularizer(), 303LabelBinarizer, 66labels, 8, 37Lagrange function, 504-505Lagrange multiplier, \\n503landmarks, 151-152large margin classification, \\n145-146Lasso Regression, 130-132latent loss, \\n430latent space, \\n429law of large numbers, \\n183leaky ReLU, \\n279learning rate, \\n16, 111, 115-118learning rate scheduling, \\n118, 300-302LeNet-5 architecture, \\n355, 366-367Levenshtein distance, \\n153liblinear library, \\n153libsvm library, \\n154Linear Discriminant Analysis (LDA), \\n224linear modelsearly stopping, 133-134Elastic Net, \\n132Lasso Regression, 130-132Linear Regression (see Linear Regression)regression (see Linear Regression)Ridge Regression, 127-129, 132SVM, 145-148Linear Regression, 20, 68, 105-121, 132computational complexity, \\n110Gradient Descent in, \\n111-121learning curves in, \\n123-127Normal Equation, \\n108-110regularizing models (see regularization)\\nusing Stochastic Gradient Descent (SGD),\\n119with TensorFlow, \\n235-236linear SVM classification, \\n145-148linear threshold units (LTUs), \\n257Lipschitz continuous, \\n113LLE (Locally Linear Embedding), 221-223load_sample_images(), \\n360local receptive field, 354local response normalization, \\n368local sessions, 328location invariance, \\n363log loss, 136logging placements, \\n320-320logistic function, 134Logistic Regression, 9, 134-142decision boundaries, 136-139estimating probablities, \\n134-135Softmax Regression model, 139-142training and cost function, 135-136log_device_placement, \\n320LSTM (Long Short-Term Memory) cell,\\n401-405Mmachine control (see reinforcement learning)\\nMachine Learning\\nlarge-scale projects (see TensorFlow)\\nnotations, \\n38-39process example, \\n33-77project checklist, 35, 497-502resources on, xvi-xviiuses for, xiii-xivMachine Learning basics\\nattributes, \\n9challenges, 22-29algorithm problems, 26-28training data problems, \\n25definition, 4features, \\n9overview, \\n3reasons for using, 4-7spam filter example, \\n4-6summary, \\n28testing and validating, \\n29-31types of systems, 7-22batch and online learning, \\n14-17instance-based versus model-basedlearning, 17-22supervised/unsupervised learning, \\n8-14workflow example, \\n18-22machine translation (see natural language pro…\\ncessing (NLP))make(), 442Manhattan norm, \\n39manifold assumption/hypothesis, \\n210Manifold Learning, \\n210, 221(see also LLE (Locally Linear Embedding)MapReduce, \\n37margin violations, \\n147Markov chains, \\n453Markov decision processes, \\n453-457master service, \\n325Matplotlib, \\n40, 48, 91, 97Index | 531\\nmax margin learning, 293max pooling layer, \\n363max-norm regularization, \\n307-308max_norm(), 308max_norm_regularizer(), 308max_pool(), 364Mean Absolute Error (MAE), \\n39-40mean coding, 429Mean Square Error (MSE), \\n107, 237, 426measure of similarity, 17memmap, \\n217memory cells, \\n346, 382MercerØs theorem, \\n163meta learner (see blending)min-max scaling, 65Mini-batch Gradient Descent, \\n119-121, 136,239-241mini-batches, \\n15minimize(), 286, 289, 449, 466min_after_dequeue, 333MNIST dataset, \\n79-81model parallelism, 345-347model parameters, 114, 116, 133, 156, 159, 234,268, 389defining, 19model selection, 19model zoos, 291model-based learning, 18-22modelsanalyzing, 74-75evaluating on test set, \\n75-76moments, \\n298Momentum optimization, \\n294-295Monte Carlo tree search, \\n453Multi-Layer Perceptrons (MLP), \\n253, 260-263,446training with TF.Learn, \\n264multiclass classifiers, \\n93-96Multidimensional Scaling (MDS), \\n223multilabel classifiers, \\n100-101Multinomial Logistic Regression (see Softmax\\nRegression)multinomial(), \\n446multioutput classifiers, \\n101-102MultiRNNCell, \\n398multithreaded readers, \\n338-340multivariate regression, \\n37Nnaive Bayes classifiers, \\n94name scopes, 245natural language processing (NLP), \\n379,405-410encoder-decoder network for machine\\ntranslation, \\n407-410TensorFlow tutorials, \\n405, 408word embeddings, 405-407Nesterov Accelerated Gradient (NAG), \\n295-296Nesterov momentum optimization, \\n295-296network topology, \\n270neural network hyperparameters, \\n270-272activation functions, \\n272neurons per hidden layer, \\n272number of hidden layers, \\n270-271neural network policies, 444-447neuronsbiological, 254-256logical computations with, \\n256neuron_layer(), \\n267next_batch(), \\n269No Free Lunch theorem, \\n30node edges, 244nonlinear dimensionality reduction (NLDR),221(see also Kernel PCA; LLE (Locally Linear\\nEmbedding))nonlinear SVM classification, \\n149-154computational complexity, \\n153Gaussian RBF kernel, \\n152-153with polynomial features, \\n149-150polynomial kernel, 150-151similarity features, adding, \\n151-152nonparametric models, \\n173nonresponse bias, 25nonsaturating activation functions, \\n279-281normal distribution (see Gaussian distribution)\\nNormal Equation, \\n108-110normalization, \\n65normalized exponential, \\n139norms, 39notations, \\n38-39NP-Complete problems, \\n172null hypothesis, \\n174numerical differentiation, \\n509NumPy, \\n40NumPy arrays, \\n63NVidia Compute Capability, \\n314532 | Index\\nnvidia-smi, \\n318n_components, \\n215Oobservation space, \\n446off-policy algorithm, 459offline learning, 14one-hot encoding, 63one-versus-all (OvA) strategy, \\n94, 141, 165one-versus-one (OvO) strategy, \\n94online learning, 15-17online SVMs, 164-165OpenAI Gym, 441-444operation_timeout_in_ms, \\n345Optical Character Recognition (OCR), \\n3optimal state value, \\n455optimizers, 293-302AdaGrad, \\n296-298Adam optimization, \\n293, 298-300Gradient Descent (see Gradient Descent\\noptimizer)learning rate scheduling, \\n300-302Momentum optimization, \\n294-295Nesterov Accelerated Gradient (NAG),\\n295-296RMSProp, 298out-of-bag evaluation, \\n187-188out-of-core learning, 16out-of-memory (OOM) errors, \\n386out-of-sample error, \\n29OutOfRangeError, \\n337, 339output gate, \\n402output layer, \\n261OutputProjectionWrapper, \\n392-395output_put_keep_prob, 399overcomplete autoencoder, \\n424overfitting, 26-28, 49, 147, 152, 173, 176, 272avoiding through regularization, \\n302-310Pp-value, 174PaddingFIFOQueue, \\n334Pandas, \\n40, 44scatter_matrix, \\n56-57parallel distributed computing, \\n313-352data parallelism, \\n347-351in-graph versus between-graph replication,\\n343-345model parallelism, 345-347multiple devices across multiple servers,\\n323-342asynchronous communication using\\nqueues, 329-334loading training data, \\n335-342master and worker services, \\n325opening a session, 325pinning operations across tasks, \\n326sharding variables, 327sharing state across sessions, \\n328-329multiple devices on a single machine,\\n314-323control dependencies, \\n323installation, \\n314-316managing the GPU RAM, 317-318parallel execution, 321-322placing operations on devices, \\n318-321one neural network per device, 342-343parameter efficiency, 271parameter matrix, \\n139parameter server (ps), \\n324parameter space, 114parameter vector, 107, 111, 135, 139parametric models, 173partial derivative, \\n114partial_fit(), 217PearsonØs r, \\n55peephole connections, 403penalties (see rewards, in RL)percentiles, \\n46Perceptron convergence theorem, \\n259Perceptrons, \\n257-264versus Logistic Regression, 260training, 258-259performance measures, 37-40confusion matrix, \\n84-86cross-validation, \\n83-84precision and recall, 86-90ROC (receiver operating characteristic)\\ncurve, \\n91-93performance scheduling, 301permutation(), \\n49PG algorithms, 448photo-hosting services, \\n13pinning operations, \\n326pip, 41Pipeline constructor, 66-68pipelines, 36placeholder nodes, 239Index | 533\\nplacers (see simple placer; dynamic placer)\\npolicy, 440policy gradients, \\n441 (see PG algorithms)policy space, 440polynomial features, adding, \\n149-150polynomial kernel, 150-151, 162Polynomial Regression, \\n106, 121-123learning curves in, \\n123-127pooling kernel, 363pooling layer, \\n363-365power scheduling, 301precision, 85precision and recall, 86-90F-1 score, \\n86-87precision/recall (PR) curve, \\n92precision/recall tradeoff, 87-90predetermined piecewise constant learning\\nrate, \\n301predict(), 62predicted class, 85predictions, 84-86, 156-157, 169-171predictors, 8, 62preloading training data, \\n335PReLU (parametric leaky ReLU), \\n279preprocessed attributes, \\n48pretrained layers reuse, \\n286-293auxiliary task, \\n292-293caching frozen layers, \\n290freezing lower layers, \\n289model zoos, 291other frameworks, 288TensorFlow model, \\n287-288unsupervised pretraining, \\n291-292upper layers, \\n290Pretty Tensor, \\n231primal problem, 160principal component, \\n212Principal Component Analysis (PCA), \\n211-218explained variance ratios, \\n214finding principal components, \\n212-213for compression, \\n216-217Incremental PCA, \\n217-218Kernel PCA (kPCA), \\n218-221projecting down to d dimensions, 213Randomized PCA, \\n218Scikit Learn for, 214variance, preserving, \\n211-212probabilistic autoencoders, \\n428probabilities, estimating, \\n134-135, 171producer functions, 341projection, 207-209propositional logic, 254pruning, 174, 509Pythonisolated environment in, \\n41-42notebooks in, 42-43pickle, 71pip, 41QQ-Learning algorithm, 458-469approximate Q-Learning, \\n460deep Q-Learning, 460-469Q-Value Iteration Algorithm, \\n456Q-Values, \\n456Quadratic Programming (QP) Problems,\\n159-160quantizing, \\n351queries per second (QPS), 343QueueRunner, \\n338-340queues, 329-334closing, 333dequeuing data, \\n331enqueuing data, \\n330first-in first-out (FIFO), 330of tuples, 332PaddingFIFOQueue, \\n334RandomShuffleQueue, \\n333q_network(), 463RRadial Basis Function (RBF), \\n151Random Forests, \\n70-72, 94, 167, 178, 181,189-191Extra-Trees, \\n190feature importance, \\n190-191random initialization, \\n111, 116, 118, 276Random Patches and Random Subspaces, \\n188randomized leaky ReLU (RReLU), \\n279Randomized PCA, \\n218randomized search, 74, 270RandomShuffleQueue, \\n333, 337random_uniform(), 237reader operations, \\n335recall, 85recognition network, 412reconstruction error, 216reconstruction loss, 413, 428, 430534 | Index\\nreconstruction pre-image, 220reconstructions, 413recurrent neural networks (RNNs), \\n379-410deep RNNs, \\n396-400exploration policies, \\n459GRU cell, \\n404-405input and output sequences, \\n382-383LSTM cell, 401-405natural language processing (NLP), \\n405-410in TensorFlow, \\n384-388dynamic unrolling through time, 387static unrolling through time, \\n385-386variable length input sequences, \\n387variable length output sequences, 388training, 389-396backpropagation through time (BPTT),\\n389creative sequences, \\n396sequence classifiers, 389-391time series predictions, 392-396recurrent neurons, \\n380-383memory cells, \\n382reduce_mean(), 268reduce_sum(), 427-428, 430, 466regression, 8Decision Trees, \\n175-176regression modelslinear, 68regression versus classification, \\n101regularization, \\n27-28, 30, 127-134data augmentation, \\n309-310Decision Trees, \\n173-174dropout, 304-307early stopping, 133-134, 303Elastic Net, \\n132Lasso Regression, 130-132max-norm, 307-308Ridge Regression, 127-129shrinkage, 197— 1 and — 2 regularization, \\n303-304REINFORCE algorithms, 448Reinforcement Learning (RL), \\n13-14, 437-470actions, 447-448credit assignment problem, \\n447-448discount rate, \\n447examples of, \\n438Markov decision processes, \\n453-457neural network policies, 444-447OpenAI gym, \\n441-444PG algorithms, 448-453policy search, 440-441Q-Learning algorithm, 458-469rewards, learning to optimize, 438-439Temporal Difference (TD) Learning,\\n457-458ReLU (rectified linear units), \\n246-248ReLU activation, \\n374ReLU function, \\n262, 272, 278-281relu(z), 266render(), 442replay memory, \\n464replica_device_setter(), 327request_stop(), 339reset(), 442reset_default_graph(), \\n234reshape(), \\n395residual errors, 195-196residual learning, 372residual network (ResNet), \\n291, 372-375residual units, 373ResNet, \\n372-375resource containers, \\n328-329restore(), 241restricted Boltzmann machines (RBMs), \\n13,291, 518reuse_variables(), 249reverse-mode autodiff, \\n512-513rewards, in RL, 438-439rgb_array, \\n443Ridge Regression, 127-129, 132RMSProp, 298ROC (receiver operating characteristic) curve,\\n91-93Root Mean Square Error (RMSE), \\n37-40, 107RReLU (randomized leaky ReLU), \\n279run(), 233, 345SSampled Softmax, \\n409sampling bias, \\n24-25, 51sampling noise, \\n24save(), \\n241Saver node, \\n241Scikit Flow, 231Scikit-Learn, 40about, xivbagging and pasting in, 186-187CART algorithm, \\n170-171, 176Index | 535\\ncross-validation, \\n69-71design principles, 61-62imputer, \\n60-62LinearSVR class, 156MinMaxScaler, \\n65min_ and max_ hyperparameters, \\n173PCA implementation, \\n214Perceptron class, \\n259Pipeline constructor, 66-68, 149Randomized PCA, \\n218Ridge Regression with, 129SAMME, 195SGDClassifier, \\n82, 87-88, 94SGDRegressor, 119sklearn.base.BaseEstimator, \\n64, 67, 84sklearn.base.clone(), 83, 133sklearn.base.TransformerMixin, \\n64, 67sklearn.datasets.fetch_california_housing(),\\n236sklearn.datasets.fetch_mldata(), \\n79sklearn.datasets.load_iris(), \\n137, 148, 167,190, 259sklearn.datasets.load_sample_images(),\\n360-361sklearn.datasets.make_moons(), \\n149, 178sklearn.decomposition.IncrementalPCA,\\n217sklearn.decomposition.KernelPCA,\\n218-219, 221sklearn.decomposition.PCA, \\n214sklearn.ensemble.AdaBoostClassifier, \\n195sklearn.ensemble.BaggingClassifier, 186-189sklearn.ensemble.GradientBoostingRegres…\\nsor, 196, 198-199sklearn.ensemble.RandomForestClassifier,\\n92, 95, 184sklearn.ensemble.RandomForestRegressor,\\n70, 72-74, 189-190, 196sklearn.ensemble.VotingClassifier, \\n184sklearn.externals.joblib, 71sklearn.linear_model.ElasticNet, \\n132sklearn.linear_model.Lasso, 132sklearn.linear_model.LinearRegression,20-21, 62, 68, 110, 120, 122, 124-125sklearn.linear_model.LogisticRegression,137, 139, 141, 184, 219sklearn.linear_model.Perceptron, \\n259sklearn.linear_model.Ridge, 129sklearn.linear_model.SGDClassifier, \\n82sklearn.linear_model.SGDRegressor,119-120, 129, 132-133sklearn.manifold.LocallyLinearEmbedding,221-222sklearn.metrics.accuracy_score(), 184, 188,264sklearn.metrics.confusion_matrix(), \\n85, 96sklearn.metrics.f1_score(), 87, 100sklearn.metrics.mean_squared_error(),68-69, 76, 124, 133, 198-199, 221sklearn.metrics.precision_recall_curve(), \\n88sklearn.metrics.precision_score(), 86, 90sklearn.metrics.recall_score(), 86, 90sklearn.metrics.roc_auc_score(), \\n92-93sklearn.metrics.roc_curve(), \\n91-92sklearn.model_selection.cross_val_pre…dict(), 84, 88, 92, 96, 100sklearn.model_selection.cross_val_score(),69-70, 83-84sklearn.model_selection.GridSearchCV,72-74, 77, 96, 179, 219sklearn.model_selection.StratifiedKFold, \\n83sklearn.model_selection.StratifiedShuffleS…\\nplit, 52sklearn.model_selection.train_test_split(),50, 69, 124, 178, 198sklearn.multiclass.OneVsOneClassifier, \\n95sklearn.neighbors.KNeighborsClassifier,\\n100, 102sklearn.neighbors.KNeighborsRegressor, \\n22sklearn.pipeline.FeatureUnion, \\n66sklearn.pipeline.Pipeline, 66, 125, 148-149,219sklearn.preprocessing.Imputer, \\n60, 66sklearn.preprocessing.LabelBinarizer, 64, 66sklearn.preprocessing.LabelEncoder, 62sklearn.preprocessing.OneHotEncoder, \\n63sklearn.preprocessing.PolynomialFeatures,\\n122-123, 125, 128, 149sklearn.preprocessing.StandardScaler,65-66, 96, 114, 128, 146, 148-150, 152,237, 264sklearn.svm.LinearSVC, \\n147-149, 153-154,156, 165sklearn.svm.LinearSVR, 155-156sklearn.svm.SVC, \\n148, 150, 152-154, 156,165, 184sklearn.svm.SVR, 77, 156536 | Index\\nsklearn.tree.DecisionTreeClassifier, \\n173,179, 186-187, 189, 195sklearn.tree.DecisionTreeRegressor, \\n69, 167,175, 195-196sklearn.tree.export_graphviz(), \\n168StandardScaler, 114, 237, 264SVM classification classes, \\n154TF.Learn, \\n231user guide, xviscore(), 62search space, 74, 270second-order partial derivatives (Hessians), \\n300self-organizing maps (SOMs), \\n521-523semantic hashing, \\n434semisupervised learning, \\n13sensitivity, 85, 91sentiment analysis, \\n379separable_conv2d(), \\n376sequences, 379sequence_length, 387-388, 409ShannonØs information theory, \\n172shortcut connections, 372show(), 48show_graph(), \\n245shrinkage, 197shuffle_batch(), \\n341shuffle_batch_join(), \\n341sigmoid function, 134sigmoid_cross_entropy_with_logits(), \\n428similarity function, 151-152simulated annealing, \\n118simulated environments, \\n442(see also OpenAI Gym)Singular Value Decomposition (SVD), \\n213skewed datasets, \\n84skip connections, 310, 372slack variable, 158smoothing terms, 283, 297, 299, 430soft margin classification, \\n146-148soft placements, \\n321soft voting, 184softmax function, 139, 263, 264Softmax Regression, 139-142source ops, 236, 322spam filters, 3-6, 8sparse autoencoders, \\n426-428sparse matrix, \\n63sparse models, 130, 300sparse_softmax_cross_entropy_with_logits(),\\n268sparsity loss, 426specificity, 91speech recognition, 6spurious patterns, \\n516stack(), 385stacked autoencoders, \\n415-424TensorFlow implementation, \\n416training one-at-a-time, \\n418-420tying weights, \\n417-418unsupervised pretraining with, \\n422-424visualizing the reconstructions, 420-421stacked denoising autoencoders, \\n422, 424stacked denoising encoders, 424stacked generalization (see stacking)\\nstacking, 200-202stale gradients, \\n348standard correlation coefficient, \\n55standard deviation, \\n37standardization, \\n65StandardScaler, 66, 237, 264state-action values, \\n456states tensor, \\n388state_is_tuple, \\n398, 401static unrolling through time, \\n385-386static_rnn(), \\n385-386, 409stationary point, \\n503-505statistical mode, \\n185statistical significance, \\n174stemming, 103step functions, 257step(), 443Stochastic Gradient Boosting, \\n199Stochastic Gradient Descent (SGD), \\n117-119,148, 260training, 136Stochastic Gradient Descent (SGD) classifier,\\n82, 129stochastic neurons, 516stochastic policy, 440stratified sampling, \\n51-53, 83stride, 357string kernels, 153string_input_producer(), \\n341strong learners, 182subderivatives, \\n164subgradient vector, \\n131subsample, \\n199, 363Index | 537\\nsupervised learning, \\n8-9Support Vector Machines (SVMs), \\n94, 145-166decision function and predictions, 156-157dual problem, 503-505kernelized SVM, 161-164linear classification, \\n145-148mechanics of, 156-165nonlinear classification, \\n149-154online SVMs, 164-165Quadratic Programming (QP) problems,\\n159-160SVM regression, 154-165the dual problem, 160training objective, 157-159support vectors, 146svd(), 213symbolic differentiation, \\n238, 508-509synchronous updates, \\n348Tt-Distributed Stochastic Neighbor Embedding\\n(t-SNE), 224tail heavy, \\n48target attributes, \\n48target_weights, \\n409tasks, 323Temporal Difference (TD) Learning, \\n457-458tensor processing units (TPUs), \\n315TensorBoard, \\n231TensorFlow, \\n229-252about, xivautodiff, \\n238-239, 507-513Batch Normalization with, \\n284-286construction phase, 234control dependencies, \\n323convenience functions, \\n341convolutional layers, \\n376convolutional neural networks and, \\n360-362data parallelism and, \\n351denoising autoencoders, \\n425-425dropout with, 306dynamic placer, 318execution phase, 234feeding data to the training algorithm,\\n239-241Gradient Descent with, \\n237-239graphs, managing, \\n234initial graph creation and session run,\\n232-234installation, \\n232l1 and l2 regularization with, \\n303learning schedules in, 302Linear Regression with, 235-236max pooling layer in, \\n364max-norm regularization with, \\n307model zoo, 291modularity, 246-248Momentum optimization in, \\n295name scopes, 245neural network policies, 446NLP tutorials, 405, 408node value lifecycle, 235operations (ops), \\n235optimizer, 239overview, \\n229-231parallel distributed computing (see parallel\\ndistributed computing with TensorFlow)\\nPython APIconstruction, 265-269execution, 269using the neural network, 270queues (see queues)reusing pretrained layers, \\n287-288RNNs in, \\n384-388(see also recurrent neural networks\\n(RNNs))\\nsaving and restoring models, \\n241-242sharing variables, 248-251simple placer, \\n318sklearn.metrics.accuracy_score(), 286sparse autoencoders with, \\n427and stacked autoencoders, \\n416TensorBoard, \\n242-245tf.abs(), 303tf.add(), 246, 303-304tf.add_n(), 247-248, 250-251tf.add_to_collection(), 308tf.assign(), 237, 288, 307-308, 482tf.bfloat16, \\n350tf.bool, 284, 306tf.cast(), 268, 391tf.clip_by_norm(), 307-308tf.clip_by_value(), 286tf.concat(), \\n312, 369, 446, 450tf.ConfigProto, 317, 320-321, 345, 487tf.constant(), \\n235-237, 319-320, 323,325-326tf.constant_initializer(), \\n249-251538 | Index\\ntf.container(), \\n328-330, 351-352, 481tf.contrib.framework.arg_scope(), \\n285, 416,430tf.contrib.layers.batch_norm(), \\n284-285tf.contrib.layers.convolution2d(), \\n463tf.contrib.layers.fully_connected(), \\n267tf.contrib.layers.l1_regularizer(), \\n303, 308tf.contrib.layers.l2_regularizer(), \\n303,416-417tf.contrib.layers.variance_scaling_initial…\\nizer(), 278-279, 391, 416-417, 430, 446,450, 463tf.contrib.learn.DNNClassifier, \\n264tf.contrib.learn.infer_real_valued_col…\\numns_from_input(), \\n264tf.contrib.rnn.BasicLSTMCell, \\n401, 403tf.contrib.rnn.BasicRNNCell, \\n385-387, 390,392-393, 395, 397-399, 401tf.contrib.rnn.DropoutWrapper, \\n399tf.contrib.rnn.GRUCell, \\n405tf.contrib.rnn.LSTMCell, \\n403tf.contrib.rnn.MultiRNNCell, \\n397-399tf.contrib.rnn.OutputProjectionWrapper,\\n392-394tf.contrib.rnn.RNNCell, \\n398tf.contrib.rnn.static_rnn(), \\n385-387,409-410, 491-492tf.contrib.slim module, \\n231, 377tf.contrib.slim.nets module (nets), \\n377tf.control_dependencies(), \\n323tf.decode_csv(), 336, 340tf.device(), 319-321, 326-327, 397-398tf.exp(), 430-431tf.FIFOQueue, \\n330, 332-333, 336, 340tf.float32, \\n236, 482tf.get_collection(), 288-289, 304, 308, 416,463tf.get_default_graph(), \\n234, 242tf.get_default_session(), \\n233tf.get_variable(), 249-251, 288, 303-308tf.global_variables(), 308tf.global_variables_initializer(), 233, 237tf.gradients(), \\n238tf.Graph, \\n232, 234, 242, 335, 343tf.GraphKeys.REGULARIZATION_LOS…\\nSES, 304, 416tf.GraphKeys.TRAINABLE_VARIABLES,\\n288-289, 463tf.group(), 464tf.int32, \\n321-332, 337, 387, 390, 406, 466tf.int64, \\n265tf.InteractiveSession, \\n233TF.Learn, \\n264tf.log(), 427, 430, 446, 450tf.matmul(), \\n236-237, 246, 265, 384, 417,420, 425, 427-428tf.matrix_inverse(), \\n236tf.maximum(), \\n246, 248-251, 281tf.multinomial(), \\n446, 450tf.name_scope(), 245, 248-249, 265,267-268, 419-420tf.nn.conv2d(), \\n360-361tf.nn.dynamic_rnn(), 386-387, 390, 392,395, 397-399, 409-410, 491-492tf.nn.elu(), 281, 416-417, 430, 446, 450tf.nn.embedding_lookup(), 406tf.nn.in_top_k(), 268, 391tf.nn.max_pool(), 364-365tf.nn.relu(), 265, 392-393, 395, 463tf.nn.sigmoid_cross_entropy_with_logits(),\\n428, 431, 449-450tf.nn.sparse_soft…max_cross_entropy_with_logits(),\\n267-268, 390tf.one_hot(), 466tf.PaddingFIFOQueue, \\n334tf.placeholder(), 239-240, 482tf.placeholder_with_default(), \\n425tf.RandomShuffleQueue, \\n333, 337-338,340-341tf.random_normal(), 246, 384, 425, 430tf.random_uniform(), 237, 241, 406, 482tf.reduce_mean(), 237, 245, 267-268, 303,390-391, 414, 416, 418, 420, 425, 427,466tf.reduce_sum(), 303, 427-428, 430-431,465-466tf.reset_default_graph(), \\n234tf.reshape(), \\n395, 463tf.RunOptions, \\n345tf.Session, 233, 482tf.shape(), \\n425, 430tf.square(), 237, 245, 393, 414, 416, 418, 420,425, 427, 430-431, 466tf.stack(), 336, 340, 386tf.string, 336, 340tf.summary.FileWriter, \\n242-243tf.summary.scalar(), \\n242Index | 539\\ntf.tanh(), 384tf.TextLineReader, \\n336, 340tf.to_float(), \\n449-450tf.train.AdamOptimizer, \\n293, 299, 390, 393,414, 416-417, 419, 427, 431, 449-450, 466tf.train.ClusterSpec, 324tf.train.Coordinator, \\n338-340tf.train.exponential_decay(), \\n302tf.train.GradientDescentOptimizer, \\n239,268, 286, 293, 295tf.train.MomentumOptimizer, \\n239, 295-296,302, 311, 351, 485-486tf.train.QueueRunner, \\n338-341tf.train.replica_device_setter(), 327-328tf.train.RMSPropOptimizer, 298tf.train.Saver, \\n241-242, 268, 377, 399, 450,466tf.train.Server, \\n324tf.train.start_queue_runners(), 341tf.transpose(), 236-237, 386, 417tf.truncated_normal(), \\n265tf.unstack(), 385-387, 395, 492tf.Variable, \\n232, 482tf.variable_scope(), 249-251, 288, 307-308,328, 391, 463tf.zeros(), 265, 384, 417truncated backpropagation through time,\\n400visualizing graph and training curves,\\n242-245TensorFlow Serving, \\n343tensorflow.contrib, \\n267test set, 29, 49-53, 81testing and validating, \\n29-31text attributes, \\n62-64TextLineReader, \\n336TF-slim, \\n231TF.Learn, \\n231, 264thermal equilibrium, 518thread pools (inter-op/intra-op, in TensorFlow,\\n322threshold variable, 248-251Tikhonov regularization, \\n127time series data, \\n379toarray(), \\n63tolerance hyperparameter, \\n154trainable, 288training data, \\n4insufficient quantities, \\n22irrelevant features, \\n25loading, 335-342nonrepresentative, \\n24overfitting, 26-28poor quality, 25underfitting, 28training instance, 4training models, 20, 105-143learning curves in, \\n123-127Linear Regression, 105, 106-121Logistic Regression, 134-142overview, \\n105-106Polynomial Regression, \\n106, 121-123training objectives, 157-159training set, 4, 29, 53, 60, 68-69cost function of, 135-136shuffling, \\n81transfer learning, 286-293(see also pretrained layers reuse)\\ntransform(), 61, 66transformation pipelines, \\n66-68transformers, 61transformers, custom, 64-65transpose(), 385true negative rate (TNR), \\n91true positive rate (TPR), \\n85, 91truncated backpropagation through time, \\n400tuples, 332tying weights, \\n417Uunderfitting, 28, 68, 152univariate regression, \\n37unstack(), 385unsupervised learning, \\n10-12anomaly detection, 12association rule learning, \\n10, 12clustering, 10dimensionality reduction algorithm, 12visualization algorithms, \\n11unsupervised pretraining, \\n291-292, 422-424upsampling, \\n376utility function, 20Vvalidation set, \\n30Value Iteration, \\n455value_counts(), \\n46vanishing gradients, \\n276540 | Index\\n(see also gradients, vanishing and explod…\\ning)variables, sharing, 248-251variable_scope(), 249-250variancebias/variance tradeoff, 126variance preservation, \\n211-212variance_scaling_initializer(), 278variational autoencoders, \\n428-432VGGNet, \\n375visual cortex, 354visualization, \\n242-245visualization algorithms, \\n11-12voice recognition, 353voting classifiers, 181-184Wwarmup phase, \\n349weak learners, 182weight-tying, \\n417weights, \\n267, 288freezing, 289while_loop(), 387white box models, 170worker, 324worker service, \\n325worker_device, 327workspace directory, \\n40-43XXavier initialization, \\n276-279YYouTube, \\n253Zzero padding, 356, 361Index | 541\\nAbout the AuthorAur•lien G•ron\\n is a Machine Learning consultant. A former Googler, he led the You…\\nTube video classification team from 2013 to 2016. He was also a founder and CTO of\\nWifirst from 2002 to 2012, a leading Wireless ISP in France; and a founder and CTO\\nof Polyconseil in 2001, the firm that now manages the electric car sharing service\\nAutolib‡.\\nBefore this he worked as an engineer in a variety of domains: finance (JP Morgan and\\nSoci•t• G•n•rale), defense (Canada‡s DOD), and healthcare (blood transfusion). He\\npublished a few technical books (on C++, WiFi, and internet architectures), and was\\na Computer Science lecturer in a French engineering school.\\nA few fun facts: he taught his three children to count in binary with their fingers (up\\nto 1023), he studied microbiology and evolutionary genetics before going into soft…\\nware engineering, and his parachute didn‡t open on the second jump.\\nColophonThe animal on the cover of Hands-On Machine Learning with Scikit-Learn and Ten…\\nsorFlow\\n is the far eastern fire salamander (Salamandra infraimmaculata\\n), an amphib…\\nian found in the Middle East. They have black skin featuring large yellow spots on\\ntheir back and head. These spots are a warning coloration meant to keep predators at\\nbay. Full-grown salamanders can be over a foot in length.\\nFar eastern fire salamanders live in subtropical shrubland and forests near rivers or\\nother freshwater bodies. They spend most of their life on land, but lay their eggs in\\nthe water. They subsist mostly on a diet of insects, worms, and small crustaceans, but\\noccasionally eat other salamanders. Males of the species have been known to live up\\nto 23 years, while females can live up to 21 years.Although not yet endangered, the far eastern fire salamander population is in decline.\\nPrimary threats include damming of rivers (which disrupts the salamander‡s breed…\\ning) and pollution. They are also threatened by the recent introduction of predatory\\nfish, such as the mosquitofish. These fish were intended to control the mosquito pop…\\nulation, but they also feed on young salamanders.\\nMany of the animals on O‡Reilly covers are endangered; all of them are important to\\nthe world. To learn more about how you can help, go to \\nanimals.oreilly.com\\n.The cover image is from Wood‹s Illustrated Natural History\\n. The cover fonts are URW\\nTypewriter and Guardian Sans. The text font is Adobe Minion Pro; the heading font\\nis Adobe Myriad Condensed; and the code font is Dalton Maag‡s Ubuntu Mono.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4oJClDe8FWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f= open(\"Hands_on_machine_learning.txt\",\"w+\")\n",
        "f.write(pdfText)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIOoeQH35cl2",
        "colab_type": "code",
        "outputId": "ccc7e530-acc8-41c4-ded0-a3fc62287c64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1MlGcRD5cnP",
        "colab_type": "code",
        "outputId": "fe3251ae-9512-41b0-c7d7-63426f69af87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "# This function will remove all stop words and punctuations in the text and return a list of keywords.\n",
        "def extractKeywords(text):\n",
        "    # Split the text words into tokens\n",
        "    wordTokens = word_tokenize(text)\n",
        "\n",
        "    # Remove blow punctuation in the list.\n",
        "    punctuations = ['(',')',';',':','[',']',',']\n",
        "\n",
        "    # Get all stop words in english.\n",
        "    stopWords = stopwords.words('english')\n",
        "\n",
        "    # Below list comprehension will return only keywords tha are not in stop words and  punctuations\n",
        "    keywords = [word for word in wordTokens if not word in stopWords and not word in punctuations]\n",
        "   \n",
        "    return keywords\n",
        "\n",
        "if __name__ == '__main__': \n",
        "\n",
        "    #pdfFilePath = '/Users/zhaosong/Documents/WorkSpace/e-book/Mastering-Node.js.pdf'\n",
        "   \n",
        "    #pdfText = extractPdfText(pdfFilePath)\n",
        "    print('There are ' + str(pdfText.__len__()) + ' word in the pdf file.')\n",
        "    #print(pdfText)\n",
        "\n",
        "    keywords = extractKeywords(pdfText)\n",
        "    print('There are ' + str(keywords.__len__()) + ' keyword in the pdf file.')\n",
        "    #print(keywords)    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1047879 word in the pdf file.\n",
            "There are 112796 keyword in the pdf file.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0m8MCNT5cjq",
        "colab_type": "code",
        "outputId": "8c8b4e9a-7093-42ae-8ab2-853944e39982",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(keywords)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Aur•lien', 'G•ron', 'Hands-On', 'Machine', 'Learning', 'withScikit-Learn', 'TensorFlowConcepts', 'Tools', 'Techniques', 'Build', 'Intelligent', 'Systems', 'BostonFarnhamSebastopolTokyoBeijingBostonFarnhamSebastopolTokyoBeijing978-1-491-96229-9', 'LSI', 'Hands-On', 'Machine', 'Learning', 'Scikit-Learn', 'TensorFlowby', 'Aur•lien', 'G•ron', 'Copyright', '†', '2017', 'Aur•lien', 'G•ron', '.', 'All', 'rights', 'reserved', '.', 'Printed', 'United', 'States', 'America', '.', 'Published', 'O‡Reilly', 'Media', 'Inc.', '1005', 'Gravenstein', 'Highway', 'North', 'Sebastopol', 'CA', '95472', '.', 'O‡Reilly', 'books', 'may', 'purchased', 'educational', 'business', 'sales', 'promotional', 'use', '.', 'Online', 'editions', 'also', 'available', 'titles', 'http', '//oreilly.com/safari', '.', 'For', 'information', 'contact', 'corporate/insti…', 'tutional', 'sales', 'department', '800-998-9938', 'corporate', '@', 'oreilly.com', '.Editor', 'Nicole', 'Tache', 'Production', 'Editor', 'Nicholas', 'Adams', 'Copyeditor', 'Rachel', 'Monaghan', 'Proofreader', 'Charles', 'RoumeliotisIndexer', 'Wendy', 'Catalano', 'Interior', 'Designer', 'David', 'Futato', 'Cover', 'Designer', 'Randy', 'Comer', 'Illustrator', 'Rebecca', 'DemarestMarch', '2017', 'First', 'EditionRevision', 'History', 'First', 'Edition2017-03-10', 'First', 'Release', 'See', 'http', '//oreilly.com/catalog/errata.csp', '?', 'isbn=9781491962299', 'release', 'details.The', 'O‡Reilly', 'logo', 'registered', 'trademark', 'O‡Reilly', 'Media', 'Inc.', 'Hands-On', 'Machine', 'Learning', 'Scikit-Learn', 'TensorFlow', 'cover', 'image', 'related', 'trade', 'dress', 'trademarks', 'O‡Reilly', 'Media', 'Inc.While', 'publisher', 'author', 'used', 'good', 'faith', 'efforts', 'ensure', 'information', 'instructions', 'contained', 'work', 'accurate', 'publisher', 'author', 'disclaim', 'responsibility', 'errors', 'omissions', 'including', 'without', 'limitation', 'responsibility', 'damages', 'resulting', 'use', 'reliance', 'work', '.', 'Use', 'information', 'instructions', 'contained', 'work', 'risk', '.', 'If', 'code', 'samples', 'technology', 'work', 'contains', 'describes', 'subject', 'open', 'source', 'licenses', 'intellectual', 'property', 'rights', 'others', 'responsibility', 'ensure', 'use', 'thereof', 'complies', 'licenses', 'and/or', 'rights', '.', 'Table', 'ContentsPreface', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'xiii', 'Part', 'I', '.', 'The', 'Fundamentals', 'Machine', 'Learning1.The', 'Machine', 'Learning', 'Landscape', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '3', 'What', 'Is', 'Machine', 'Learning', '?', '4', 'Why', 'Use', 'Machine', 'Learning', '?', '4', 'Types', 'Machine', 'Learning', 'Systems', '7', 'Supervised/Unsupervised', 'Learning', '8', 'Batch', 'Online', 'Learning', '14', 'Instance-Based', 'Versus', 'Model-Based', 'Learning', '17', 'Main', 'Challenges', 'Machine', 'Learning', '22', 'Insufficient', 'Quantity', 'Training', 'Data', '22', 'Nonrepresentative', 'Training', 'Data', '24', 'Poor-Quality', 'Data', '25', 'Irrelevant', 'Features', '25', 'Overfitting', 'Training', 'Data', '26', 'Underfitting', 'Training', 'Data', '28', 'Stepping', 'Back', '28', 'Testing', 'Validating', '29', 'Exercises', '31', '2.End-to-End', 'Machine', 'Learning', 'Project', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '33', 'Working', 'Real', 'Data', '33', 'Look', 'Big', 'Picture', '35', 'Frame', 'Problem', '35', 'Select', 'Performance', 'Measure', '37', 'iiiCheck', 'Assumptions', '40', 'Get', 'Data', '40', 'Create', 'Workspace', '40', 'Download', 'Data', '43', 'Take', 'Quick', 'Look', 'Data', 'Structure', '45', 'Create', 'Test', 'Set', '49', 'Discover', 'Visualize', 'Data', 'Gain', 'Insights', '53', 'Visualizing', 'Geographical', 'Data', '53', 'Looking', 'Correlations', '55', 'Experimenting', 'Attribute', 'Combinations', '58', 'Prepare', 'Data', 'Machine', 'Learning', 'Algorithms', '59', 'Data', 'Cleaning', '60', 'Handling', 'Text', 'Categorical', 'Attributes', '62', 'Custom', 'Transformers', '64', 'Feature', 'Scaling', '65', 'Transformation', 'Pipelines', '66', 'Select', 'Train', 'Model', '68', 'Training', 'Evaluating', 'Training', 'Set', '68', 'Better', 'Evaluation', 'Using', 'Cross-Validation', '69', 'Fine-Tune', 'Your', 'Model', '71', 'Grid', 'Search', '72', 'Randomized', 'Search', '74', 'Ensemble', 'Methods', '74', 'Analyze', 'Best', 'Models', 'Their', 'Errors', '74', 'Evaluate', 'Your', 'System', 'Test', 'Set', '75', 'Launch', 'Monitor', 'Maintain', 'Your', 'System', '76', 'Try', 'It', 'Out', '!', '77', 'Exercises', '77', '3.Classi•cation', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '79', 'MNIST', '79', 'Training', 'Binary', 'Classifier', '82', 'Performance', 'Measures', '82', 'Measuring', 'Accuracy', 'Using', 'Cross-Validation', '83', 'Confusion', 'Matrix', '84', 'Precision', 'Recall', '86', 'Precision/Recall', 'Tradeoff', '87', 'The', 'ROC', 'Curve', '91', 'Multiclass', 'Classification', '93', 'Error', 'Analysis', '96', 'Multilabel', 'Classification', '100', 'Multioutput', 'Classification', '101', 'iv', '|', 'Table', 'Contents', 'Exercises', '102', '4.Training', 'Models', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '105', 'Linear', 'Regression', '106', 'The', 'Normal', 'Equation', '108', 'Computational', 'Complexity', '110', 'Gradient', 'Descent', '111', 'Batch', 'Gradient', 'Descent', '114', 'Stochastic', 'Gradient', 'Descent', '117', 'Mini-batch', 'Gradient', 'Descent', '119', 'Polynomial', 'Regression', '121', 'Learning', 'Curves', '123', 'Regularized', 'Linear', 'Models', '127', 'Ridge', 'Regression', '127', 'Lasso', 'Regression', '130', 'Elastic', 'Net', '132', 'Early', 'Stopping', '133', 'Logistic', 'Regression', '134', 'Estimating', 'Probabilities', '134', 'Training', 'Cost', 'Function', '135', 'Decision', 'Boundaries', '136', 'Softmax', 'Regression', '139', 'Exercises', '142', '5.Support', 'Vector', 'Machines', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '145', 'Linear', 'SVM', 'Classification', '145', 'Soft', 'Margin', 'Classification', '146', 'Nonlinear', 'SVM', 'Classification', '149', 'Polynomial', 'Kernel', '150', 'Adding', 'Similarity', 'Features', '151', 'Gaussian', 'RBF', 'Kernel', '152', 'Computational', 'Complexity', '153', 'SVM', 'Regression', '154', 'Under', 'Hood', '156', 'Decision', 'Function', 'Predictions', '156', 'Training', 'Objective', '157', 'Quadratic', 'Programming', '159', 'The', 'Dual', 'Problem', '160', 'Kernelized', 'SVM', '161', 'Online', 'SVMs', '164', 'Exercises', '165', 'Table', 'Contents', '|', 'v', '6.Decision', 'Trees', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '167', 'Training', 'Visualizing', 'Decision', 'Tree', '167', 'Making', 'Predictions', '169', 'Estimating', 'Class', 'Probabilities', '171', 'The', 'CART', 'Training', 'Algorithm', '171', 'Computational', 'Complexity', '172', 'Gini', 'Impurity', 'Entropy', '?', '172', 'Regularization', 'Hyperparameters', '173', 'Regression', '175', 'Instability', '177', 'Exercises', '178', '7.Ensemble', 'Learning', 'Random', 'Forests', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '181', 'Voting', 'Classifiers', '181', 'Bagging', 'Pasting', '185', 'Bagging', 'Pasting', 'Scikit-Learn', '186', 'Out-of-Bag', 'Evaluation', '187', 'Random', 'Patches', 'Random', 'Subspaces', '188', 'Random', 'Forests', '189', 'Extra-Trees', '190', 'Feature', 'Importance', '190', 'Boosting', '191', 'AdaBoost', '192', 'Gradient', 'Boosting', '195', 'Stacking', '200', 'Exercises', '202', '8.Dimensionality', 'Reduction', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '205', 'The', 'Curse', 'Dimensionality', '206', 'Main', 'Approaches', 'Dimensionality', 'Reduction', '207', 'Projection', '207', 'Manifold', 'Learning', '210', 'PCA', '211', 'Preserving', 'Variance', '211', 'Principal', 'Components', '212', 'Projecting', 'Down', 'Dimensions', '213', 'Using', 'Scikit-Learn', '214', 'Explained', 'Variance', 'Ratio', '214', 'Choosing', 'Right', 'Number', 'Dimensions', '215', 'PCA', 'Compression', '216', 'Incremental', 'PCA', '217', 'Randomized', 'PCA', '218', 'vi', '|', 'Table', 'Contents', 'Kernel', 'PCA', '218', 'Selecting', 'Kernel', 'Tuning', 'Hyperparameters', '219', 'LLE', '221', 'Other', 'Dimensionality', 'Reduction', 'Techniques', '223', 'Exercises', '224', 'Part', 'II', '.', 'Neural', 'Networks', 'Deep', 'Learning9.Up', 'Running', 'TensorFlow', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '229', 'Installation', '232', 'Creating', 'Your', 'First', 'Graph', 'Running', 'It', 'Session', '232', 'Managing', 'Graphs', '234', 'Lifecycle', 'Node', 'Value', '235', 'Linear', 'Regression', 'TensorFlow', '235', 'Implementing', 'Gradient', 'Descent', '237', 'Manually', 'Computing', 'Gradients', '237', 'Using', 'autodiff', '238', 'Using', 'Optimizer', '239', 'Feeding', 'Data', 'Training', 'Algorithm', '239', 'Saving', 'Restoring', 'Models', '241', 'Visualizing', 'Graph', 'Training', 'Curves', 'Using', 'TensorBoard', '242', 'Name', 'Scopes', '245', 'Modularity', '246', 'Sharing', 'Variables', '248', 'Exercises', '251', '10.Introduction', 'Arti•cial', 'Neural', 'Networks', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '253', 'From', 'Biological', 'Artificial', 'Neurons', '254', 'Biological', 'Neurons', '255', 'Logical', 'Computations', 'Neurons', '256', 'The', 'Perceptron', '257', 'Multi-Layer', 'Perceptron', 'Backpropagation', '261', 'Training', 'MLP', 'TensorFlow‡s', 'High-Level', 'API', '264', 'Training', 'DNN', 'Using', 'Plain', 'TensorFlow', '265', 'Construction', 'Phase', '265', 'Execution', 'Phase', '269', 'Using', 'Neural', 'Network', '270', 'Fine-Tuning', 'Neural', 'Network', 'Hyperparameters', '270', 'Number', 'Hidden', 'Layers', '270', 'Number', 'Neurons', 'per', 'Hidden', 'Layer', '272', 'Activation', 'Functions', '272', 'Table', 'Contents', '|', 'vii', 'Exercises', '273', '11.Training', 'Deep', 'Neural', 'Nets', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '275', 'Vanishing/Exploding', 'Gradients', 'Problems', '275', 'Xavier', 'He', 'Initialization', '277', 'Nonsaturating', 'Activation', 'Functions', '279', 'Batch', 'Normalization', '282', 'Gradient', 'Clipping', '286', 'Reusing', 'Pretrained', 'Layers', '286', 'Reusing', 'TensorFlow', 'Model', '287', 'Reusing', 'Models', 'Other', 'Frameworks', '288', 'Freezing', 'Lower', 'Layers', '289', 'Caching', 'Frozen', 'Layers', '290', 'Tweaking', 'Dropping', 'Replacing', 'Upper', 'Layers', '290', 'Model', 'Zoos', '291', 'Unsupervised', 'Pretraining', '291', 'Pretraining', 'Auxiliary', 'Task', '292', 'Faster', 'Optimizers', '293', 'Momentum', 'optimization', '294', 'Nesterov', 'Accelerated', 'Gradient', '295', 'AdaGrad', '296', 'RMSProp', '298', 'Adam', 'Optimization', '298', 'Learning', 'Rate', 'Scheduling', '300', 'Avoiding', 'Overfitting', 'Through', 'Regularization', '302', 'Early', 'Stopping', '303', '—1', '—2', 'Regularization', '303', 'Dropout', '304', 'Max-Norm', 'Regularization', '307', 'Data', 'Augmentation', '309', 'Practical', 'Guidelines', '310', 'Exercises', '311', '12.Distributing', 'TensorFlow', 'Across', 'Devices', 'Servers', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '313', 'Multiple', 'Devices', 'Single', 'Machine', '314', 'Installation', '314', 'Managing', 'GPU', 'RAM', '317', 'Placing', 'Operations', 'Devices', '318', 'Parallel', 'Execution', '321', 'Control', 'Dependencies', '323', 'Multiple', 'Devices', 'Across', 'Multiple', 'Servers', '323', 'Opening', 'Session', '325', 'viii', '|', 'Table', 'Contents', 'The', 'Master', 'Worker', 'Services', '325', 'Pinning', 'Operations', 'Across', 'Tasks', '326', 'Sharding', 'Variables', 'Across', 'Multiple', 'Parameter', 'Servers', '327', 'Sharing', 'State', 'Across', 'Sessions', 'Using', 'Resource', 'Containers', '328', 'Asynchronous', 'Communication', 'Using', 'TensorFlow', 'Queues', '329', 'Loading', 'Data', 'Directly', 'Graph', '335', 'Parallelizing', 'Neural', 'Networks', 'TensorFlow', 'Cluster', '342', 'One', 'Neural', 'Network', 'per', 'Device', '342', 'In-Graph', 'Versus', 'Between-Graph', 'Replication', '343', 'Model', 'Parallelism', '345', 'Data', 'Parallelism', '347', 'Exercises', '352', '13.Convolutional', 'Neural', 'Networks', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '353', 'The', 'Architecture', 'Visual', 'Cortex', '354', 'Convolutional', 'Layer', '355', 'Filters', '357', 'Stacking', 'Multiple', 'Feature', 'Maps', '358', 'TensorFlow', 'Implementation', '360', 'Memory', 'Requirements', '362', 'Pooling', 'Layer', '363', 'CNN', 'Architectures', '365', 'LeNet-5', '366', 'AlexNet', '367', 'GoogLeNet', '368', 'ResNet', '372', 'Exercises', '376', '14.Recurrent', 'Neural', 'Networks', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '379', 'Recurrent', 'Neurons', '380', 'Memory', 'Cells', '382', 'Input', 'Output', 'Sequences', '382', 'Basic', 'RNNs', 'TensorFlow', '384', 'Static', 'Unrolling', 'Through', 'Time', '385', 'Dynamic', 'Unrolling', 'Through', 'Time', '387', 'Handling', 'Variable', 'Length', 'Input', 'Sequences', '387', 'Handling', 'Variable-Length', 'Output', 'Sequences', '388', 'Training', 'RNNs', '389', 'Training', 'Sequence', 'Classifier', '389', 'Training', 'Predict', 'Time', 'Series', '392', 'Creative', 'RNN', '396', 'Deep', 'RNNs', '396', 'Table', 'Contents', '|', 'ix', 'Distributing', 'Deep', 'RNN', 'Across', 'Multiple', 'GPUs', '397', 'Applying', 'Dropout', '399', 'The', 'Difficulty', 'Training', 'Many', 'Time', 'Steps', '400', 'LSTM', 'Cell', '401', 'Peephole', 'Connections', '403', 'GRU', 'Cell', '404', 'Natural', 'Language', 'Processing', '405', 'Word', 'Embeddings', '405', 'An', 'Encoder–Decoder', 'Network', 'Machine', 'Translation', '407', 'Exercises', '410', '15.Autoencoders', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '411', 'Efficient', 'Data', 'Representations', '412', 'Performing', 'PCA', 'Undercomplete', 'Linear', 'Autoencoder', '413', 'Stacked', 'Autoencoders', '415', 'TensorFlow', 'Implementation', '416', 'Tying', 'Weights', '417', 'Training', 'One', 'Autoencoder', 'Time', '418', 'Visualizing', 'Reconstructions', '420', 'Visualizing', 'Features', '421', 'Unsupervised', 'Pretraining', 'Using', 'Stacked', 'Autoencoders', '422', 'Denoising', 'Autoencoders', '424', 'TensorFlow', 'Implementation', '425', 'Sparse', 'Autoencoders', '426', 'TensorFlow', 'Implementation', '427', 'Variational', 'Autoencoders', '428', 'Generating', 'Digits', '431', 'Other', 'Autoencoders', '432', 'Exercises', '433', '16.Reinforcement', 'Learning', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '437', 'Learning', 'Optimize', 'Rewards', '438', 'Policy', 'Search', '440', 'Introduction', 'OpenAI', 'Gym', '441', 'Neural', 'Network', 'Policies', '444', 'Evaluating', 'Actions', 'The', 'Credit', 'Assignment', 'Problem', '447', 'Policy', 'Gradients', '448', 'Markov', 'Decision', 'Processes', '453', 'Temporal', 'Difference', 'Learning', 'Q-Learning', '457', 'Exploration', 'Policies', '459', 'Approximate', 'Q-Learning', '460', 'Learning', 'Play', 'Ms.', 'Pac-Man', 'Using', 'Deep', 'Q-Learning', '460', 'x', '|', 'Table', 'Contents', 'Exercises', '469', 'Thank', 'You', '!', '470', 'A.Exercise', 'Solutions', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '471', 'B.Machine', 'Learning', 'Project', 'Checklist', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '497', 'C.SVM', 'Dual', 'Problem', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '503', 'D.Autodi†', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '507', 'E.Other', 'Popular', 'ANN', 'Architectures', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '515', 'Index', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '525', 'Table', 'Contents', '|', 'xi', '1Available', 'Hinton‡s', 'home', 'page', 'http', '//www.cs.toronto.edu/~hinton/', '.2Despite', 'fact', 'Yann', 'Lecun‡s', 'deep', 'convolutional', 'neural', 'networks', 'worked', 'well', 'image', 'recognition', 'since', '1990s', 'although', 'general', 'purpose.PrefaceThe', 'Machine', 'Learning', 'TsunamiIn', '2006', 'Geoffrey', 'Hinton', 'et', 'al', '.', 'published', 'paper', '1', 'showing', 'train', 'deep', 'neuralnetwork', 'capable', 'recognizing', 'handwritten', 'digits', 'state-of-the-art', 'precision', '>', '98', '%', '.', 'They', 'branded', 'technique', 'ƒDeep', 'Learning.⁄', 'Training', 'deep', 'neural', 'net', 'widely', 'considered', 'impossible', 'time', '2', 'researchers', 'abandonedthe', 'idea', 'since', '1990s', '.', 'This', 'paper', 'revived', 'interest', 'scientific', 'community', 'long', 'many', 'new', 'papers', 'demonstrated', 'Deep', 'Learning', 'possible', 'capable', 'mind-blowing', 'achievements', 'Machine', 'Learning', 'ML', 'technique', 'could', 'hope', 'match', 'help', 'tremendous', 'computing', 'power', 'great', 'amounts', 'data', '.', 'This', 'enthusiasm', 'soon', 'extended', 'many', 'areas', 'Machine', 'Learning', '.', 'Fast-forward', '10', 'years', 'Machine', 'Learning', 'conquered', 'industry', 'heart', 'much', 'magic', 'today‡s', 'high-tech', 'products', 'ranking', 'web', 'search', 'results', 'powering', 'smartphone‡s', 'speech', 'recognition', 'recommending', 'videos', 'beating', 'world', 'champion', 'game', 'Go', '.', 'Before', 'know', 'driving', 'car', '.', 'Machine', 'Learning', 'Your', 'ProjectsSo', 'naturally', 'excited', 'Machine', 'Learning', 'would', 'love', 'join', 'party', '!', 'Perhaps', 'would', 'like', 'give', 'homemade', 'robot', 'brain', '?', 'Make', 'rec…', 'ognize', 'faces', '?', 'Or', 'learn', 'walk', 'around', '?', 'xiiiOr', 'maybe', 'company', 'tons', 'data', 'user', 'logs', 'financial', 'data', 'production', 'data', 'machine', 'sensor', 'data', 'hotline', 'stats', 'HR', 'reports', 'etc', '.', 'likely', 'could', 'unearth', 'hidden', 'gems', 'knew', 'look', 'example', '‹Segment', 'customers', 'find', 'best', 'marketing', 'strategy', 'group', '‹Recommend', 'products', 'client', 'based', 'similar', 'clients', 'bought', '‹Detect', 'transactions', 'likely', 'fraudulent', '‹Predict', 'next', 'year‡s', 'revenue', '‹And', 'moreWhatever', 'reason', 'decided', 'learn', 'Machine', 'Learning', 'implement', 'projects', '.', 'Great', 'idea', '!', 'Objective', 'ApproachThis', 'book', 'assumes', 'know', 'close', 'nothing', 'Machine', 'Learning', '.', 'Its', 'goal', 'give', 'concepts', 'intuitions', 'tools', 'need', 'actually', 'imple…', 'ment', 'programs', 'capable', 'learning', 'data', '.We', 'cover', 'large', 'number', 'techniques', 'simplest', 'commonly', 'used', 'linear', 'regression', 'Deep', 'Learning', 'techniques', 'regu…', 'larly', 'win', 'competitions', '.', 'Rather', 'implementing', 'toy', 'versions', 'algorithm', 'using', 'actual', 'production-ready', 'Python', 'frameworks', '‹Scikit-Learn', 'easy', 'use', 'yet', 'implements', 'many', 'Machine', 'Learning', 'algo…', 'rithms', 'efficiently', 'makes', 'great', 'entry', 'point', 'learn', 'Machine', 'Learning', '.', '‹TensorFlow', 'complex', 'library', 'distributed', 'numerical', 'computation', 'using', 'data', 'flow', 'graphs', '.', 'It', 'makes', 'possible', 'train', 'run', 'large', 'neural', 'net…', 'works', 'efficiently', 'distributing', 'computations', 'across', 'potentially', 'thousands', 'multi-GPU', 'servers', '.', 'TensorFlow', 'created', 'Google', 'supports', 'many', 'large-scale', 'Machine', 'Learning', 'applications', '.', 'It', 'open-sourced', 'Novem…', 'ber', '2015.The', 'book', 'favors', 'hands-on', 'approach', 'growing', 'intuitive', 'understanding', 'Machine', 'Learning', 'concrete', 'working', 'examples', 'little', 'bit', 'theory', '.', 'While', 'read', 'book', 'without', 'picking', 'laptop', 'highly', 'recommend', 'experiment', 'code', 'examples', 'available', 'online', 'Jupyter', 'notebooks', 'https', '//github.com/ageron/handson-ml', '.xiv', '|', 'Preface', 'PrerequisitesThis', 'book', 'assumes', 'Python', 'programming', 'experience', 'familiar', 'Python‡s', 'main', 'scientific', 'libraries', 'particular', 'NumPy', 'Pandas', 'andMatplotlib', '.Also', 'care', 'what‡s', 'hood', 'reasonable', 'under…', 'standing', 'college-level', 'math', 'well', 'calculus', 'linear', 'algebra', 'probabilities', 'sta…', 'tistics', '.If', 'don‡t', 'know', 'Python', 'yet', 'http', '//learnpython.org/', 'great', 'place', 'start', '.', 'The', 'offi…', 'cial', 'tutorial', 'python.org', 'also', 'quite', 'good.If', 'never', 'used', 'Jupyter', 'Chapter', '2', 'guide', 'installation', 'basics', 'great', 'tool', 'toolbox', '.', 'If', 'familiar', 'Python‡s', 'scientific', 'libraries', 'provided', 'Jupyter', 'note…', 'books', 'include', 'tutorials', '.', 'There', 'also', 'quick', 'math', 'tutorial', 'linear', 'algebra', '.', 'RoadmapThis', 'book', 'organized', 'two', 'parts', '.', 'Part', 'I', '•e', 'Fundamentals', 'Machine', 'Learning', 'covers', 'following', 'topics', '‹What', 'Machine', 'Learning', '?', 'What', 'problems', 'try', 'solve', '?', 'What', 'main', 'categories', 'fundamental', 'concepts', 'Machine', 'Learning', 'systems', '?', '‹The', 'main', 'steps', 'typical', 'Machine', 'Learning', 'project', '.', '‹Learning', 'fitting', 'model', 'data', '.', '‹Optimizing', 'cost', 'function.‹Handling', 'cleaning', 'preparing', 'data', '.', '‹Selecting', 'engineering', 'features', '.', '‹Selecting', 'model', 'tuning', 'hyperparameters', 'using', 'cross-validation', '.', '‹The', 'main', 'challenges', 'Machine', 'Learning', 'particular', 'underfitting', 'overfit…', 'ting', 'bias/variance', 'tradeoff', '.', '‹Reducing', 'dimensionality', 'training', 'data', 'fight', 'curse', 'dimension…', 'ality', '.', '‹The', 'common', 'learning', 'algorithms', 'Linear', 'Polynomial', 'Regression', 'Logistic', 'Regression', 'k-Nearest', 'Neighbors', 'Support', 'Vector', 'Machines', 'Decision', 'Trees', 'Random', 'Forests', 'Ensemble', 'methods', '.', 'Preface', '|', 'xv', 'Part', 'II', 'Neural', 'Networks', 'Deep', 'Learning', 'covers', 'following', 'topics', '‹What', 'neural', 'nets', '?', 'What', 'good', '?', '‹Building', 'training', 'neural', 'nets', 'using', 'TensorFlow', '.', '‹The', 'important', 'neural', 'net', 'architectures', 'feedforward', 'neural', 'nets', 'convolu…', 'tional', 'nets', 'recurrent', 'nets', 'long', 'short-term', 'memory', 'LSTM', 'nets', 'autoen…', 'coders.‹Techniques', 'training', 'deep', 'neural', 'nets', '.', '‹Scaling', 'neural', 'networks', 'huge', 'datasets', '.', '‹Reinforcement', 'learning', '.', 'The', 'first', 'part', 'based', 'mostly', 'Scikit-Learn', 'second', 'part', 'uses', 'TensorFlow', '.', 'Don‡t', 'jump', 'deep', 'waters', 'hastily', 'Deep', 'Learning', 'doubt', 'one', 'exciting', 'areas', 'Machine', 'Learning', 'master', 'fundamentals', 'first', '.', 'Moreover', 'problems', 'solved', 'quite', 'well', 'using', 'simpler', 'techniques', 'Random', 'Forests', 'Ensemble', 'methods', 'discussed', 'Part', 'I', '.', 'Deep', 'Learn…ing', 'best', 'suited', 'complex', 'problems', 'image', 'recognition', 'speech', 'recognition', 'natural', 'language', 'processing', 'provided', 'enough', 'data', 'computing', 'power', 'patience', '.', 'Other', 'ResourcesMany', 'resources', 'available', 'learn', 'Machine', 'Learning', '.', 'Andrew', 'Ng‡s', 'MLcourse', 'Coursera', 'Geoffrey', 'Hinton‡s', 'course', 'neural', 'networks', 'DeepLearning', 'amazing', 'although', 'require', 'significant', 'time', 'investment', 'think', 'months', '.', 'There', 'also', 'many', 'interesting', 'websites', 'Machine', 'Learning', 'including', 'course', 'Scikit-Learn‡s', 'exceptional', 'User', 'Guide', '.', 'You', 'may', 'also', 'enjoy', 'Dataquest', 'whichprovides', 'nice', 'interactive', 'tutorials', 'ML', 'blogs', 'listed', 'Quora.Finally', 'Deep', 'Learning', 'website', 'good', 'list', 'resources', 'learn', 'more.Of', 'course', 'also', 'many', 'introductory', 'books', 'Machine', 'Learning', 'particular', '‹Joel', 'Grus', 'Data', 'Science', 'Scratch', 'O‡Reilly', '.', 'This', 'book', 'presents', 'funda…', 'mentals', 'Machine', 'Learning', 'implements', 'main', 'algorithms', 'pure', 'Python', 'scratch', 'name', 'suggests', '.', '‹Stephen', 'Marsland', 'Machine', 'Learning', 'An', 'Algorithmic', 'Perspective', 'Chapman', 'Hall', '.', 'This', 'book', 'great', 'introduction', 'Machine', 'Learning', 'covering', 'wide', 'xvi', '|', 'Preface', 'range', 'topics', 'depth', 'code', 'examples', 'Python', 'also', 'scratch', 'using', 'NumPy', '.', '‹Sebastian', 'Raschka', 'Python', 'Machine', 'Learning', 'Packt', 'Publishing', '.', 'Also', 'great', 'introduction', 'Machine', 'Learning', 'book', 'leverages', 'Python', 'open', 'source', 'libra…', 'ries', 'Pylearn', '2', 'Theano', '.‹Yaser', 'S.', 'Abu-Mostafa', 'Malik', 'Magdon-Ismail', 'Hsuan-Tien', 'Lin', 'Learning', 'Data', 'AMLBook', '.', 'A', 'rather', 'theoretical', 'approach', 'ML', 'book', 'provides', 'deep', 'insights', 'particular', 'bias/variance', 'tradeoff', 'see', 'Chapter', '4', '.‹Stuart', 'Russell', 'Peter', 'Norvig', 'Arti†cial', 'Intelligence', 'A', 'Modern', 'Approach', '3rd', 'Edition', 'Pearson', '.', 'This', 'great', 'huge', 'book', 'covering', 'incredible', 'amount', 'topics', 'including', 'Machine', 'Learning', '.', 'It', 'helps', 'put', 'ML', 'perspective', '.', 'Finally', 'great', 'way', 'learn', 'join', 'ML', 'competition', 'websites', 'Kaggle.comthis', 'allow', 'practice', 'skills', 'real-world', 'problems', 'help', 'andinsights', 'best', 'ML', 'professionals', '.', 'Conventions', 'Used', 'This', 'BookThe', 'following', 'typographical', 'conventions', 'used', 'book', 'Italic', 'Indicates', 'new', 'terms', 'URLs', 'email', 'addresses', 'filenames', 'file', 'extensions', '.', 'Constant', 'widthUsed', 'program', 'listings', 'well', 'within', 'paragraphs', 'refer', 'program', 'ele…', 'ments', 'variable', 'function', 'names', 'databases', 'data', 'types', 'environment', 'variables', 'statements', 'keywords', '.', 'Constant', 'width', 'boldShows', 'commands', 'text', 'typed', 'literally', 'user', '.', 'Constant', 'width', 'italicShows', 'text', 'replaced', 'user-supplied', 'values', 'values', 'deter…', 'mined', 'context', '.', 'This', 'element', 'signifies', 'tip', 'suggestion', '.', 'Preface', '|', 'xvii', 'This', 'element', 'signifies', 'general', 'note', '.', 'This', 'element', 'indicates', 'warning', 'caution', '.', 'Using', 'Code', 'ExamplesSupplemental', 'material', 'code', 'examples', 'exercises', 'etc', '.', 'available', 'download', 'https', '//github.com/ageron/handson-ml', '.This', 'book', 'help', 'get', 'job', 'done', '.', 'In', 'general', 'example', 'code', 'offered', 'book', 'may', 'use', 'programs', 'documentation', '.', 'You', 'need', 'contact', 'us', 'permission', 'unless', 'you‡re', 'reproducing', 'significant', 'portion', 'code', '.', 'For', 'example', 'writing', 'program', 'uses', 'several', 'chunks', 'code', 'book', 'require', 'permission', '.', 'Selling', 'distributing', 'CD-ROM', 'examples', 'O‡Reilly', 'books', 'require', 'permission', '.', 'Answering', 'question', 'citing', 'book', 'quoting', 'example', 'code', 'require', 'permission', '.', 'Incorporating', 'signifi…', 'cant', 'amount', 'example', 'code', 'book', 'product‡s', 'documentation', 'require', 'permission.We', 'appreciate', 'require', 'attribution', '.', 'An', 'attribution', 'usually', 'includes', 'title', 'author', 'publisher', 'ISBN', '.', 'For', 'example', 'ƒ', 'Hands-On', 'Machine', 'Learning', 'Scikit-Learn', 'TensorFlow', 'Aur•lien', 'G•ron', 'O‡Reilly', '.', 'Copyright', '2017', 'Aur•lien', 'G•ron', '978-1-491-96229-9.⁄', 'If', 'feel', 'use', 'code', 'examples', 'falls', 'outside', 'fair', 'use', 'permission', 'given', 'feel', 'free', 'contact', 'us', 'permissions', '@', 'oreilly.com', '.O‡Reilly', 'SafariSafari', 'formerly', 'Safari', 'Books', 'Online', 'membership-based', 'training', 'reference', 'platform', 'enterprise', 'government', 'educators', 'individuals', '.', 'Members', 'access', 'thousands', 'books', 'training', 'videos', 'Learning', 'Paths', 'interac…', 'tive', 'tutorials', 'curated', 'playlists', '250', 'publishers', 'including', 'O‡Reilly', 'Media', 'Harvard', 'Business', 'Review', 'Prentice', 'Hall', 'Professional', 'Addison-Wesley', 'Profes…', 'sional', 'Microsoft', 'Press', 'Sams', 'Que', 'Peachpit', 'Press', 'Adobe', 'Focal', 'Press', 'Cisco', 'Press', 'xviii', '|', 'Preface', 'John', 'Wiley', '&', 'Sons', 'Syngress', 'Morgan', 'Kaufmann', 'IBM', 'Redbooks', 'Packt', 'Adobe', 'Press', 'FT', 'Press', 'Apress', 'Manning', 'New', 'Riders', 'McGraw-Hill', 'Jones', '&', 'Bartlett', 'Course', 'Technology', 'among', 'others', '.', 'For', 'information', 'please', 'visit', 'http', '//oreilly.com/safari', '.How', 'Contact', 'UsPlease', 'address', 'comments', 'questions', 'concerning', 'book', 'publisher', 'O‡Reilly', 'Media', 'Inc.', '1005', 'Gravenstein', 'Highway', 'North', 'Sebastopol', 'CA', '95472800-998-9938', 'United', 'States', 'Canada', '707-829-0515', 'international', 'local', '707-829-0104', 'fax', 'We', 'web', 'page', 'book', 'list', 'errata', 'examples', 'additional', 'information', '.', 'You', 'access', 'page', 'http', '//bit.ly/hands-on-machine-learning-with-scikit-learn-and-tensor‡ow.To', 'comment', 'ask', 'technical', 'questions', 'book', 'send', 'email', 'bookques…', 'tions', '@', 'oreilly.com', '.For', 'information', 'books', 'courses', 'conferences', 'news', 'see', 'web…', 'site', 'http', '//www.oreilly.com', '.Find', 'us', 'Facebook', 'http', '//facebook.com/oreilly', 'Follow', 'us', 'Twitter', 'http', '//twitter.com/oreillymedia', 'Watch', 'us', 'YouTube', 'http', '//www.youtube.com/oreillymedia', 'AcknowledgmentsI', 'would', 'like', 'thank', 'Google', 'colleagues', 'particular', 'YouTube', 'video', 'classifi…', 'cation', 'team', 'teaching', 'much', 'Machine', 'Learning', '.', 'I', 'could', 'never', 'started', 'project', 'without', '.', 'Special', 'thanks', 'personal', 'ML', 'gurus', 'Cl•ment', 'Courbet', 'Julien', 'Dubois', 'Mathias', 'Kende', 'Daniel', 'Kitachewsky', 'James', 'Pack', 'Alexander', 'Pak', 'Anosh', 'Raj', 'Vitor', 'Sessak', 'Wiktor', 'Tomczak', 'Ingrid', 'von', 'Glehn', 'Rich', 'Washington', 'everyone', 'YouTube', 'Paris', '.', 'I', 'incredibly', 'grateful', 'amazing', 'people', 'took', 'time', 'busy', 'lives', 'review', 'book', 'much', 'detail', '.', 'Thanks', 'Pete', 'Warden', 'answering', 'TensorFlow', 'questions', 'reviewing', 'Part', 'II', 'providing', 'many', 'interesting', 'insights', 'course', 'part', 'core', 'TensorFlow', 'team', '.', 'You', 'definitely', 'check', 'Preface', '|', 'xix', 'blog', '!', 'Many', 'thanks', 'Lukas', 'Biewald', 'thorough', 'review', 'Part', 'II', 'leftno', 'stone', 'unturned', 'tested', 'code', 'caught', 'errors', 'made', 'many', 'great', 'suggestions', 'enthusiasm', 'contagious', '.', 'You', 'check', 'blog', 'andhis', 'cool', 'robots', '!', 'Thanks', 'Justin', 'Francis', 'also', 'reviewed', 'Part', 'II', 'thoroughly', 'catching', 'errors', 'providing', 'great', 'insights', 'particular', 'Chapter', '16', '.', 'Check', 'outhis', 'posts', 'TensorFlow', '!', 'Huge', 'thanks', 'well', 'David', 'Andrzejewski', 'reviewed', 'Part', 'I', 'providedincredibly', 'useful', 'feedback', 'identifying', 'unclear', 'sections', 'suggesting', 'improve', '.', 'Check', 'website', '!', 'Thanks', 'Gr•goire', 'Mesnil', 'reviewed', 'Part', 'II', 'contributed', 'interesting', 'practical', 'advice', 'training', 'neural', 'networks', '.', 'Thanks', 'well', 'Eddy', 'Hung', 'Salim', 'S•maoune', 'Karim', 'Matrah', 'Ingrid', 'von', 'Glehn', 'Iain', 'Smears', 'Vincent', 'Guilbeau', 'reviewing', 'Part', 'I', 'making', 'many', 'useful', 'sug…', 'gestions', '.', 'And', 'I', 'also', 'wish', 'thank', 'father-in-law', 'Michel', 'Tessier', 'former', 'mathe…', 'matics', 'teacher', 'great', 'translator', 'Anton', 'Chekhov', 'helping', 'iron', 'mathematics', 'notations', 'book', 'reviewing', 'linear', 'algebra', 'Jupyter', 'notebook', '.', 'And', 'course', 'gigantic', 'ƒthank', 'you⁄', 'dear', 'brother', 'Sylvain', 'reviewed', 'every', 'single', 'chapter', 'tested', 'every', 'line', 'code', 'provided', 'feedback', 'virtually', 'every', 'section', 'encouraged', 'first', 'line', 'last', '.', 'Love', 'bro', '!', 'Many', 'thanks', 'well', 'O‡Reilly‡s', 'fantastic', 'staff', 'particular', 'Nicole', 'Tache', 'gave', 'insightful', 'feedback', 'always', 'cheerful', 'encouraging', 'helpful', '.', 'Thanks', 'well', 'Marie', 'Beaugureau', 'Ben', 'Lorica', 'Mike', 'Loukides', 'Laurel', 'Ruma', 'believing', 'project', 'helping', 'define', 'scope', '.', 'Thanks', 'Matt', 'Hacker', 'Atlas', 'team', 'answering', 'technical', 'questions', 'regarding', 'formatting', 'asciidoc', 'LaTeX', 'thanks', 'Rachel', 'Monaghan', 'Nick', 'Adams', 'production', 'team', 'final', 'review', 'hundreds', 'corrections', '.', 'Last', 'least', 'I', 'infinitely', 'grateful', 'beloved', 'wife', 'Emmanuelle', 'three', 'wonderful', 'kids', 'Alexandre', 'R•mi', 'Gabrielle', 'encouraging', 'workhard', 'book', 'asking', 'many', 'questions', 'said', 'can‡t', 'teach', 'neural', 'networks', 'seven-year-old', '?', 'even', 'bringing', 'cookies', 'coffee', '.', 'What', 'one', 'dream', '?', 'xx', '|', 'Preface', 'PART', 'IThe', 'Fundamentals', 'ofMachine', 'LearningCHAPTER', '1The', 'Machine', 'Learning', 'LandscapeWhen', 'people', 'hear', 'ƒMachine', 'Learning', '⁄', 'picture', 'robot', 'dependable', 'but…', 'ler', 'deadly', 'Terminator', 'depending', 'ask', '.', 'But', 'Machine', 'Learning', 'futuristic', 'fantasy', 'it‡s', 'already', '.', 'In', 'fact', 'around', 'decades', 'specialized', 'applications', 'Optical', 'Character', 'Recognition', 'OCR', '.', 'But', 'thefirst', 'ML', 'application', 'really', 'became', 'mainstream', 'improving', 'lives', 'hundreds', 'millions', 'people', 'took', 'world', 'back', '1990s', 'spam', '†lter.Not', 'exactly', 'self-aware', 'Skynet', 'technically', 'qualify', 'Machine', 'Learning', 'actually', 'learned', 'well', 'seldom', 'need', 'flag', 'email', 'spam', 'any…', '.', 'It', 'followed', 'hundreds', 'ML', 'applications', 'quietly', 'power', 'hun…', 'dreds', 'products', 'features', 'use', 'regularly', 'better', 'recommendations', 'voice', 'search.Where', 'Machine', 'Learning', 'start', 'end', '?', 'What', 'exactly', 'mean', 'machine', 'learn', 'something', '?', 'If', 'I', 'download', 'copy', 'Wikipedia', 'computer', 'really', 'ƒlearned⁄', 'something', '?', 'Is', 'suddenly', 'smarter', '?', 'In', 'chapter', 'start', 'clarifying', 'Machine', 'Learning', 'may', 'want', 'use', '.', 'Then', 'set', 'explore', 'Machine', 'Learning', 'continent', 'take', 'look', 'map', 'learn', 'main', 'regions', 'notable', 'landmarks', 'supervised', 'versus', 'unsupervised', 'learning', 'online', 'versus', 'batch', 'learning', 'instance-', 'based', 'versus', 'model-based', 'learning', '.', 'Then', 'look', 'workflow', 'typical', 'ML', 'project', 'discuss', 'main', 'challenges', 'may', 'face', 'cover', 'evaluate', 'fine-tune', 'Machine', 'Learning', 'system', '.', 'This', 'chapter', 'introduces', 'lot', 'fundamental', 'concepts', 'jargon', 'every', 'data', 'scientist', 'know', 'heart', '.', 'It', 'high-level', 'overview', 'chapter', 'without', 'much', 'code', 'rather', 'simple', 'make', 'sure', 'everything', 'crystal-clear', 'continuing', 'rest', 'book', '.', 'So', 'grab', 'coffee', 'let‡s', 'get', 'started', '!', '3If', 'already', 'know', 'Machine', 'Learning', 'basics', 'may', 'want', 'skip', 'directly', 'Chapter', '2', '.', 'If', 'sure', 'try', 'answer', 'questions', 'listed', 'end', 'chapter', 'moving', '.', 'What', 'Is', 'Machine', 'Learning', '?', 'Machine', 'Learning', 'science', 'art', 'programming', 'computers', 'learn', 'data', '.Here', 'slightly', 'general', 'definition', 'Machine', 'Learning', 'field', 'study', 'gives', 'computers', 'ability', 'learn', 'without', 'explicitly', 'programmed.›Arthur', 'Samuel', '1959And', 'engineering-oriented', 'one', 'A', 'computer', 'program', 'said', 'learn', 'experience', 'E', 'respect', 'task', 'T', 'performance', 'measure', 'P', 'performance', 'T', 'measured', 'P', 'improves', 'experience', 'E.›Tom', 'Mitchell', '1997For', 'example', 'spam', 'filter', 'Machine', 'Learning', 'program', 'learn', 'flag', 'spam', 'given', 'examples', 'spam', 'emails', 'e.g.', 'flagged', 'users', 'examples', 'regular', 'nonspam', 'also', 'called', 'ƒham⁄', 'emails', '.', 'The', 'examples', 'system', 'uses', 'learn', 'called', 'training', 'set', '.', 'Each', 'training', 'example', 'called', 'training', 'instance', 'sample', '.In', 'case', 'task', 'T', 'flag', 'spam', 'new', 'emails', 'experience', 'E', 'training', 'data', 'performance', 'measure', 'P', 'needs', 'defined', 'example', 'use', 'ratio', 'correctly', 'classified', 'emails', '.', 'This', 'particular', 'performance', 'measure', 'called', 'accuracy', 'often', 'used', 'classification', 'tasks', '.', 'If', 'download', 'copy', 'Wikipedia', 'computer', 'lot', 'data', 'suddenly', 'better', 'task', '.', 'Thus', 'Machine', 'Learning', '.', 'Why', 'Use', 'Machine', 'Learning', '?', 'Consider', 'would', 'write', 'spam', 'filter', 'using', 'traditional', 'programming', 'techni…ques', 'Figure', '1-1', ':1.First', 'would', 'look', 'spam', 'typically', 'looks', 'like', '.', 'You', 'might', 'notice', 'words', 'phrases', 'ƒ4U', '⁄', 'ƒcredit', 'card', '⁄', 'ƒfree', '⁄', 'ƒamazing⁄', 'tend', 'come', 'lot', 'subject', '.', 'Perhaps', 'would', 'also', 'notice', 'patterns', 'sender‡s', 'name', 'email‡s', 'body', '.', '4', '|', 'Chapter', '1', 'The', 'Machine', 'Learning', 'Landscape', '2.You', 'would', 'write', 'detection', 'algorithm', 'patterns', 'noticed', 'program', 'would', 'flag', 'emails', 'spam', 'number', 'patterns', 'detected.3.You', 'would', 'test', 'program', 'repeat', 'steps', '1', '2', 'good', 'enough', '.', 'Figure', '1-1', '.', '•e', 'traditional', 'approach', 'Since', 'problem', 'trivial', 'program', 'likely', 'become', 'long', 'list', 'com…plex', 'rules›pretty', 'hard', 'maintain', '.', 'In', 'contrast', 'spam', 'filter', 'based', 'Machine', 'Learning', 'techniques', 'automatically', 'learns', 'words', 'phrases', 'good', 'predictors', 'spam', 'detecting', 'unusually', 'fre…', 'quent', 'patterns', 'words', 'spam', 'examples', 'compared', 'ham', 'examples', 'Figure', '1-2', '.', 'The', 'program', 'much', 'shorter', 'easier', 'maintain', 'likely', 'accurate', '.', 'Figure', '1-2', '.', 'Machine', 'Learning', 'approach', 'Why', 'Use', 'Machine', 'Learning', '?', '|', '5', 'Moreover', 'spammers', 'notice', 'emails', 'containing', 'ƒ4U⁄', 'blocked', 'might', 'start', 'writing', 'ƒFor', 'U⁄', 'instead', '.', 'A', 'spam', 'filter', 'using', 'traditional', 'programming', 'techniques', 'would', 'need', 'updated', 'flag', 'ƒFor', 'U⁄', 'emails', '.', 'If', 'spammers', 'keep', 'work…', 'ing', 'around', 'spam', 'filter', 'need', 'keep', 'writing', 'new', 'rules', 'forever', '.', 'In', 'contrast', 'spam', 'filter', 'based', 'Machine', 'Learning', 'techniques', 'automatically', 'noti…', 'ces', 'ƒFor', 'U⁄', 'become', 'unusually', 'frequent', 'spam', 'flagged', 'users', 'starts', 'flagging', 'without', 'intervention', 'Figure', '1-3', '.Figure', '1-3', '.', 'Automatically', 'adapting', 'change', 'Another', 'area', 'Machine', 'Learning', 'shines', 'problems', 'either', 'com…', 'plex', 'traditional', 'approaches', 'known', 'algorithm', '.', 'For', 'example', 'consider', 'speech', 'recognition', 'say', 'want', 'start', 'simple', 'write', 'program', 'capable', 'dis…', 'tinguishing', 'words', 'ƒone⁄', 'ƒtwo.⁄', 'You', 'might', 'notice', 'word', 'ƒtwo⁄', 'starts', 'high-pitch', 'sound', 'ƒT⁄', 'could', 'hardcode', 'algorithm', 'measures', 'high-pitch', 'sound', 'intensity', 'use', 'distinguish', 'ones', 'twos', '.', 'Obviously', 'technique', 'scale', 'thousands', 'words', 'spoken', 'millions', 'different', 'people', 'noisy', 'environments', 'dozens', 'languages', '.', 'The', 'best', 'solution', 'least', 'today', 'write', 'algorithm', 'learns', 'given', 'many', 'example', 'recordings', 'word.Finally', 'Machine', 'Learning', 'help', 'humans', 'learn', 'Figure', '1-4', 'ML', 'algorithms', 'beinspected', 'see', 'learned', 'although', 'algorithms', 'tricky', '.', 'For', 'instance', 'spam', 'filter', 'trained', 'enough', 'spam', 'easily', 'inspected', 'reveal', 'list', 'words', 'combinations', 'words', 'believes', 'best', 'predictors', 'spam', '.', 'Sometimes', 'reveal', 'unsuspected', 'cor…relations', 'new', 'trends', 'thereby', 'lead', 'better', 'understanding', 'problem', '.', 'Applying', 'ML', 'techniques', 'dig', 'large', 'amounts', 'data', 'help', 'discover', 'patterns', 'immediately', 'apparent', '.', 'This', 'called', 'data', 'mining', '.6', '|', 'Chapter', '1', 'The', 'Machine', 'Learning', 'Landscape', 'Figure', '1-4', '.', 'Machine', 'Learning', 'help', 'humans', 'learn', 'To', 'summarize', 'Machine', 'Learning', 'great', '‹Problems', 'existing', 'solutions', 'require', 'lot', 'hand-tuning', 'long', 'lists', 'ofrules', 'one', 'Machine', 'Learning', 'algorithm', 'often', 'simplify', 'code', 'perform', 'bet…', 'ter', '.', '‹Complex', 'problems', 'good', 'solution', 'using', 'traditional', 'approach', 'best', 'Machine', 'Learning', 'techniques', 'find', 'solution', '.', '‹Fluctuating', 'environments', 'Machine', 'Learning', 'system', 'adapt', 'new', 'data', '.', '‹Getting', 'insights', 'complex', 'problems', 'large', 'amounts', 'data', '.', 'Types', 'Machine', 'Learning', 'SystemsThere', 'many', 'different', 'types', 'Machine', 'Learning', 'systems', 'useful', 'classify', 'broad', 'categories', 'based', '‹Whether', 'trained', 'human', 'supervision', 'supervised', 'unsuper…', 'vised', 'semisupervised', 'Reinforcement', 'Learning', '‹Whether', 'learn', 'incrementally', 'fly', 'online', 'versus', 'batch', 'learning', '‹Whether', 'work', 'simply', 'comparing', 'new', 'data', 'points', 'known', 'data', 'points', 'instead', 'detect', 'patterns', 'training', 'data', 'build', 'predictive', 'model', 'much', 'like', 'scientists', 'instance-based', 'versus', 'model-based', 'learning', 'These', 'criteria', 'exclusive', 'combine', 'way', 'like', '.', 'For', 'example', 'state-of-the-art', 'spam', 'filter', 'may', 'learn', 'fly', 'using', 'deep', 'neural', 'net…', 'Types', 'Machine', 'Learning', 'Systems', '|', '7', '1Fun', 'fact', 'odd-sounding', 'name', 'statistics', 'term', 'introduced', 'Francis', 'Galton', 'studying', 'fact', 'children', 'tall', 'people', 'tend', 'shorter', 'parents', '.', 'Since', 'children', 'shorter', 'called', 'regression', 'mean', '.', 'This', 'name', 'applied', 'methods', 'used', 'analyze', 'correlations', 'variables.work', 'model', 'trained', 'using', 'examples', 'spam', 'ham', 'makes', 'online', 'model-', 'based', 'supervised', 'learning', 'system', '.', 'Let‡s', 'look', 'criteria', 'bit', 'closely', '.', 'Supervised/Unsupervised', 'LearningMachine', 'Learning', 'systems', 'classified', 'according', 'amount', 'type', 'supervision', 'get', 'training', '.', 'There', 'four', 'major', 'categories', 'supervised', 'learning', 'unsupervised', 'learning', 'semisupervised', 'learning', 'Reinforcement', 'Learn…', 'ing.Supervised', 'learningIn', 'supervised', 'learning', 'training', 'data', 'feed', 'algorithm', 'includes', 'desired', 'solutions', 'called', 'labels', 'Figure', '1-5', '.Figure', '1-5', '.', 'A', 'labeled', 'training', 'set', 'supervised', 'learning', 'e.g.', 'spam', 'classi†cation', 'A', 'typical', 'supervised', 'learning', 'task', 'classi†cation', '.', 'The', 'spam', 'filter', 'good', 'example', 'trained', 'many', 'example', 'emails', 'along', 'class', 'spam', 'ham', 'must', 'learn', 'classify', 'new', 'emails', '.', 'Another', 'typical', 'task', 'predict', 'target', 'numeric', 'value', 'price', 'car', 'given', 'set', 'features', 'mileage', 'age', 'brand', 'etc', '.', 'called', 'predictors', '.', 'This', 'sort', 'task', 'called', 'regression', 'Figure', '1-6', '.1', 'To', 'train', 'system', 'need', 'give', 'many', 'examples', 'cars', 'including', 'predictors', 'labels', 'i.e.', 'prices', '.8', '|', 'Chapter', '1', 'The', 'Machine', 'Learning', 'Landscape', '2Some', 'neural', 'network', 'architectures', 'unsupervised', 'autoencoders', 'restricted', 'Boltzmann', 'machines', '.', 'They', 'also', 'semisupervised', 'deep', 'belief', 'networks', 'unsupervised', 'pretraining', '.', 'In', 'Machine', 'Learning', 'attribute', 'data', 'type', 'e.g.', 'ƒMileage⁄', 'feature', 'several', 'meanings', 'depending', 'context', 'generally', 'means', 'attribute', 'plus', 'value', 'e.g.', 'ƒMileage', '=', '15,000⁄', '.', 'Many', 'people', 'use', 'words', 'attribute', 'feature', 'inter…', 'changeably', 'though', '.', 'Figure', '1-6', '.', 'Regression', 'Note', 'regression', 'algorithms', 'used', 'classification', 'well', 'vice', 'versa', '.', 'For', 'example', 'Logistic', 'Regression', 'commonly', 'used', 'classification', 'output', 'value', 'corresponds', 'probability', 'belonging', 'given', 'class', 'e.g.', '20', '%', 'chance', 'spam', '.Here', 'important', 'supervised', 'learning', 'algorithms', 'covered', 'book', '‹k-Nearest', 'Neighbors', '‹Linear', 'Regression‹Logistic', 'Regression‹Support', 'Vector', 'Machines', 'SVMs', '‹Decision', 'Trees', 'Random', 'Forests', '‹Neural', 'networks', '2Types', 'Machine', 'Learning', 'Systems', '|', '9', 'Unsupervised', 'learningIn', 'unsupervised', 'learning', 'might', 'guess', 'training', 'data', 'unlabeled', 'Figure', '1-7', '.', 'The', 'system', 'tries', 'learn', 'without', 'teacher', '.', 'Figure', '1-7', '.', 'An', 'unlabeled', 'training', 'set', 'unsupervised', 'learning', 'Here', 'important', 'unsupervised', 'learning', 'algorithms', 'cover', 'dimensionality', 'reduction', 'Chapter', '8', '‹Clustering›k-Means', '›Hierarchical', 'Cluster', 'Analysis', 'HCA', '›Expectation', 'Maximization', '‹Visualization', 'dimensionality', 'reduction', '›Principal', 'Component', 'Analysis', 'PCA', '›Kernel', 'PCA', '›Locally-Linear', 'Embedding', 'LLE', '›t-distributed', 'Stochastic', 'Neighbor', 'Embedding', 't-SNE', '‹Association', 'rule', 'learning', '›Apriori', '›Eclat', 'For', 'example', 'say', 'lot', 'data', 'blog‡s', 'visitors', '.', 'You', 'may', 'want', 'run', 'clustering', 'algorithm', 'try', 'detect', 'groups', 'similar', 'visitors', 'Figure', '1-8', '.', 'At', 'point', 'tell', 'algorithm', 'group', 'visitor', 'belongs', 'finds', 'connections', 'without', 'help', '.', 'For', 'example', 'might', 'notice', '40', '%', 'visitors', 'males', 'love', 'comic', 'books', 'generally', 'read', 'blog', 'evening', 'while20', '%', 'young', 'sci-fi', 'lovers', 'visit', 'weekends', '.', 'If', 'use', 'ahierarchical', 'clustering', 'algorithm', 'may', 'also', 'subdivide', 'group', 'smaller', 'groups', '.', 'This', 'may', 'help', 'target', 'posts', 'group', '.', '10', '|', 'Chapter', '1', 'The', 'Machine', 'Learning', 'Landscape', '3Notice', 'animals', 'rather', 'well', 'separated', 'vehicles', 'horses', 'close', 'deer', 'far', 'birds', '.', 'Figure', 'reproduced', 'permission', 'Socher', 'Ganjoo', 'Manning', 'Ng', '2013', 'ƒT-SNE', 'visual…', 'ization', 'semantic', 'word', 'space.⁄', 'Figure', '1-8', '.', 'Clustering', 'Visualization', 'algorithms', 'also', 'good', 'examples', 'unsupervised', 'learning', 'algorithms', 'feed', 'lot', 'complex', 'unlabeled', 'data', 'output', '2D', '3D', 'rep…', 'resentation', 'data', 'easily', 'plotted', 'Figure', '1-9', '.', 'These', 'algorithms', 'try', 'preserve', 'much', 'structure', 'e.g.', 'trying', 'keep', 'separate', 'clusters', 'input', 'space', 'overlapping', 'visualization', 'understand', 'data', 'organized', 'perhaps', 'identify', 'unsuspected', 'patterns', '.', 'Figure', '1-9', '.', 'Example', 't-SNE', 'visualization', 'highlighting', 'semantic', 'clusters', '3Types', 'Machine', 'Learning', 'Systems', '|', '11', 'A', 'related', 'task', 'dimensionality', 'reduction', 'goal', 'simplify', 'data', 'without', 'losing', 'much', 'information', '.', 'One', 'way', 'merge', 'several', 'correla…', 'ted', 'features', 'one', '.', 'For', 'example', 'car‡s', 'mileage', 'may', 'correlated', 'age', 'dimensionality', 'reduction', 'algorithm', 'merge', 'one', 'feature', 'rep…', 'resents', 'car‡s', 'wear', 'tear', '.', 'This', 'called', 'feature', 'extraction', '.It', 'often', 'good', 'idea', 'try', 'reduce', 'dimension', 'train…', 'ing', 'data', 'using', 'dimensionality', 'reduction', 'algorithm', 'feed', 'another', 'Machine', 'Learning', 'algorithm', 'super…', 'vised', 'learning', 'algorithm', '.', 'It', 'run', 'much', 'faster', 'data', 'take', 'less', 'disk', 'memory', 'space', 'cases', 'may', 'also', 'per…', 'form', 'better', '.', 'Yet', 'another', 'important', 'unsupervised', 'task', 'anomaly', 'detection', '›for', 'example', 'detect…', 'ing', 'unusual', 'credit', 'card', 'transactions', 'prevent', 'fraud', 'catching', 'manufacturing', 'defects', 'automatically', 'removing', 'outliers', 'dataset', 'feeding', 'another', 'learn…', 'ing', 'algorithm', '.', 'The', 'system', 'trained', 'normal', 'instances', 'sees', 'newinstance', 'tell', 'whether', 'looks', 'like', 'normal', 'one', 'whether', 'likely', 'anom…aly', 'see', 'Figure', '1-10', '.Figure', '1-10', '.', 'Anomaly', 'detection', 'Finally', 'another', 'common', 'unsupervised', 'task', 'association', 'rule', 'learning', 'thegoal', 'dig', 'large', 'amounts', 'data', 'discover', 'interesting', 'relations', 'attributes', '.', 'For', 'example', 'suppose', 'supermarket', '.', 'Running', 'association', 'rule', 'sales', 'logs', 'may', 'reveal', 'people', 'purchase', 'barbecue', 'sauce', 'potato', 'chips', 'also', 'tend', 'buy', 'steak', '.', 'Thus', 'may', 'want', 'place', 'items', 'close', '.', '12', '|', 'Chapter', '1', 'The', 'Machine', 'Learning', 'Landscape', '4That‡s', 'system', 'works', 'perfectly', '.', 'In', 'practice', 'often', 'creates', 'clusters', 'per', 'person', 'sometimes', 'mixes', 'two', 'people', 'look', 'alike', 'need', 'provide', 'labels', 'per', 'person', 'manually', 'clean', 'clusters.Semisupervised', 'learningSome', 'algorithms', 'deal', 'partially', 'labeled', 'training', 'data', 'usually', 'lot', 'unla…', 'beled', 'data', 'little', 'bit', 'labeled', 'data', '.', 'This', 'called', 'semisupervised', 'learning', 'Figure', '1-11', '.Some', 'photo-hosting', 'services', 'Google', 'Photos', 'good', 'examples', '.', 'Once', 'upload', 'family', 'photos', 'service', 'automatically', 'recognizes', 'person', 'A', 'shows', 'photos', '1', '5', '11', 'another', 'person', 'B', 'shows', 'inphotos', '2', '5', '7', '.', 'This', 'unsupervised', 'part', 'algorithm', 'clustering', '.', 'Now', 'system', 'needs', 'tell', 'people', '.', 'Just', 'one', 'label', 'per', 'person', '4and', 'able', 'name', 'everyone', 'every', 'photo', 'useful', 'searching', 'photos', '.', 'Figure', '1-11', '.', 'Semisupervised', 'learning', 'Most', 'semisupervised', 'learning', 'algorithms', 'combinations', 'unsupervised', 'supervised', 'algorithms', '.', 'For', 'example', 'deep', 'belief', 'networks', 'DBNs', 'based', 'unsu…', 'pervised', 'components', 'called', 'restricted', 'Boltzmann', 'machines', 'RBMs', 'stacked', 'top', 'ofone', 'another', '.', 'RBMs', 'trained', 'sequentially', 'unsupervised', 'manner', 'whole', 'system', 'fine-tuned', 'using', 'supervised', 'learning', 'techniques', '.', 'Reinforcement', 'LearningReinforcement', 'Learning', 'different', 'beast', '.', 'The', 'learning', 'system', 'called', 'agent', 'context', 'observe', 'environment', 'select', 'perform', 'actions', 'get', 'rewards', 'return', 'penalties', 'form', 'negative', 'rewards', 'Figure', '1-12', '.', 'It', 'must', 'learn', 'best', 'strategy', 'called', 'policy', 'get', 'mostreward', 'time', '.', 'A', 'policy', 'defines', 'action', 'agent', 'choose', 'given', 'situation', '.', 'Types', 'Machine', 'Learning', 'Systems', '|', '13', 'Figure', '1-12', '.', 'Reinforcement', 'Learning', 'For', 'example', 'many', 'robots', 'implement', 'Reinforcement', 'Learning', 'algorithms', 'learn', 'walk', '.', 'DeepMind‡s', 'AlphaGo', 'program', 'also', 'good', 'example', 'Reinforcement', 'Learning', 'made', 'headlines', 'March', '2016', 'beat', 'world', 'champion', 'Lee', 'Sedol', 'game', 'Go', '.', 'It', 'learned', 'winning', 'policy', 'analyzing', 'millions', 'games', 'playing', 'many', 'games', '.', 'Note', 'learning', 'turned', 'games', 'champion', 'AlphaGo', 'applying', 'policy', 'learned', '.', 'Batch', 'Online', 'LearningAnother', 'criterion', 'used', 'classify', 'Machine', 'Learning', 'systems', 'whether', 'system', 'learn', 'incrementally', 'stream', 'incoming', 'data', '.', 'Batch', 'learningIn', 'batch', 'learning', 'system', 'incapable', 'learning', 'incrementally', 'must', 'trained', 'using', 'available', 'data', '.', 'This', 'generally', 'take', 'lot', 'time', 'computing', 'resources', 'typically', 'done', 'offline', '.', 'First', 'system', 'trained', 'islaunched', 'production', 'runs', 'without', 'learning', 'anymore', 'applies', 'learned', '.', 'This', 'called', 'o—ine', 'learning', '.If', 'want', 'batch', 'learning', 'system', 'know', 'new', 'data', 'new', 'type', 'spam', 'need', 'train', 'new', 'version', 'system', 'scratch', 'full', 'dataset', 'new', 'data', 'also', 'old', 'data', 'stop', 'old', 'system', 'replace', 'new', 'one.Fortunately', 'whole', 'process', 'training', 'evaluating', 'launching', 'Machine', 'Learning', 'system', 'automated', 'fairly', 'easily', 'shown', 'Figure', '1-3', 'even', 'a14', '|', 'Chapter', '1', 'The', 'Machine', 'Learning', 'Landscape', 'batch', 'learning', 'system', 'adapt', 'change', '.', 'Simply', 'update', 'data', 'train', 'new', 'version', 'system', 'scratch', 'often', 'needed', '.', 'This', 'solution', 'simple', 'often', 'works', 'fine', 'training', 'using', 'full', 'set', 'data', 'take', 'many', 'hours', 'would', 'typically', 'train', 'new', 'system', 'every', '24', 'hours', 'even', 'weekly', '.', 'If', 'system', 'needs', 'adapt', 'rapidly', 'changing', 'data', 'e.g.', 'pre…', 'dict', 'stock', 'prices', 'need', 'reactive', 'solution.Also', 'training', 'full', 'set', 'data', 'requires', 'lot', 'computing', 'resources', 'CPU', 'memory', 'space', 'disk', 'space', 'disk', 'I/O', 'network', 'I/O', 'etc.', '.', 'If', 'lot', 'data', 'automate', 'system', 'train', 'scratch', 'every', 'day', 'end', 'costing', 'lot', 'money', '.', 'If', 'amount', 'data', 'huge', 'may', 'even', 'impossible', 'use', 'batch', 'learning', 'algorithm.Finally', 'system', 'needs', 'able', 'learn', 'autonomously', 'limited', 'resources', 'e.g.', 'smartphone', 'application', 'rover', 'Mars', 'carrying', 'around', 'large', 'amounts', 'training', 'data', 'taking', 'lot', 'resources', 'train', 'hours', 'every', 'day', 'showstopper', '.', 'Fortunately', 'better', 'option', 'cases', 'use', 'algorithms', 'capable', 'learning', 'incrementally', '.', 'Online', 'learningIn', 'online', 'learning', 'train', 'system', 'incrementally', 'feeding', 'data', 'instances', 'sequentially', 'either', 'individually', 'small', 'groups', 'called', 'mini-batches', '.', 'Each', 'learningstep', 'fast', 'cheap', 'system', 'learn', 'new', 'data', 'fly', 'arrives', 'see', 'Figure', '1-13', '.Figure', '1-13', '.', 'Online', 'learning', 'Online', 'learning', 'great', 'systems', 'receive', 'data', 'continuous', 'flow', 'e.g.', 'stock', 'prices', 'need', 'adapt', 'change', 'rapidly', 'autonomously', '.', 'It', 'also', 'good', 'option', 'Types', 'Machine', 'Learning', 'Systems', '|', '15', 'limited', 'computing', 'resources', 'online', 'learning', 'system', 'learned', 'new', 'data', 'instances', 'need', 'anymore', 'discard', 'unless', 'want', 'able', 'roll', 'back', 'previous', 'state', 'ƒreplay⁄', 'data', '.', 'This', 'save', 'huge', 'amount', 'space', '.', 'Online', 'learning', 'algorithms', 'also', 'used', 'train', 'systems', 'huge', 'datasets', 'fit', 'one', 'machine‡s', 'main', 'memory', 'called', 'out-of-core', 'learning', '.', 'Thealgorithm', 'loads', 'part', 'data', 'runs', 'training', 'step', 'data', 'repeats', 'process', 'run', 'data', 'see', 'Figure', '1-14', '.This', 'whole', 'process', 'usually', 'done', 'offline', 'i.e.', 'live', 'sys…tem', 'online', 'learning', 'confusing', 'name', '.', 'Think', 'asincremental', 'learning', '.Figure', '1-14', '.', 'Using', 'online', 'learning', 'handle', 'huge', 'datasets', 'One', 'important', 'parameter', 'online', 'learning', 'systems', 'fast', 'adapt', 'changing', 'data', 'called', 'learning', 'rate', '.', 'If', 'set', 'high', 'learning', 'rate', 'system', 'rapidly', 'adapt', 'new', 'data', 'also', 'tend', 'quickly', 'forget', 'old', 'data', 'don‡t', 'want', 'spam', 'filter', 'flag', 'latest', 'kinds', 'spam', 'shown', '.', 'Conversely', 'set', 'low', 'learning', 'rate', 'system', 'inertia', 'learn', 'slowly', 'also', 'less', 'sensitive', 'noise', 'new', 'data', 'sequences', 'nonrepresentative', 'data', 'points', '.', 'A', 'big', 'challenge', 'online', 'learning', 'bad', 'data', 'fed', 'system', 'sys…', 'tem‡s', 'performance', 'gradually', 'decline', '.', 'If', 'talking', 'live', 'system', 'clients', 'notice', '.', 'For', 'example', 'bad', 'data', 'could', 'come', 'malfunctioning', 'sensor', 'robot', 'someone', 'spamming', 'search', 'engine', 'try', 'rank', 'high', 'search', '16', '|', 'Chapter', '1', 'The', 'Machine', 'Learning', 'Landscape', 'results', '.', 'To', 'reduce', 'risk', 'need', 'monitor', 'system', 'closely', 'promptly', 'switch', 'learning', 'possibly', 'revert', 'previously', 'working', 'state', 'detect', 'drop', 'performance', '.', 'You', 'may', 'also', 'want', 'monitor', 'input', 'data', 'react', 'abnormal', 'data', 'e.g.', 'using', 'anomaly', 'detection', 'algorithm', '.', 'Instance-Based', 'Versus', 'Model-Based', 'LearningOne', 'way', 'categorize', 'Machine', 'Learning', 'systems', 'generalize', '.Most', 'Machine', 'Learning', 'tasks', 'making', 'predictions', '.', 'This', 'means', 'given', 'number', 'training', 'examples', 'system', 'needs', 'able', 'generalize', 'examples', 'never', 'seen', '.', 'Having', 'good', 'performance', 'measure', 'training', 'data', 'good', 'insufficient', 'true', 'goal', 'perform', 'well', 'new', 'instances', '.', 'There', 'two', 'main', 'approaches', 'generalization', 'instance-based', 'learning', 'model-based', 'learning.Instance-based', 'learningPossibly', 'trivial', 'form', 'learning', 'simply', 'learn', 'heart', '.', 'If', 'create', 'spam', 'filter', 'way', 'would', 'flag', 'emails', 'identical', 'emails', 'already', 'flagged', 'users›not', 'worst', 'solution', 'certainly', 'best.Instead', 'flagging', 'emails', 'identical', 'known', 'spam', 'emails', 'spam', 'filter', 'could', 'programmed', 'also', 'flag', 'emails', 'similar', 'known', 'spam', 'emails', '.', 'This', 'requires', 'measure', 'similarity', 'two', 'emails', '.', 'A', 'basic', 'simi…', 'larity', 'measure', 'two', 'emails', 'could', 'count', 'number', 'words', 'common', '.', 'The', 'system', 'would', 'flag', 'email', 'spam', 'many', 'words', 'com…', 'mon', 'known', 'spam', 'email.This', 'called', 'instance-based', 'learning', 'system', 'learns', 'examples', 'heart', 'generalizes', 'new', 'cases', 'using', 'similarity', 'measure', 'Figure', '1-15', '.Figure', '1-15', '.', 'Instance-based', 'learning', 'Types', 'Machine', 'Learning', 'Systems', '|', '17', 'Model-based', 'learningAnother', 'way', 'generalize', 'set', 'examples', 'build', 'model', 'exam…', 'ples', 'use', 'model', 'make', 'predictions', '.', 'This', 'called', 'model-based', 'learning', 'Figure', '1-16', '.Figure', '1-16', '.', 'Model-based', 'learning', 'For', 'example', 'suppose', 'want', 'know', 'money', 'makes', 'people', 'happy', 'down…', 'load', 'Better', 'Life', 'Index', 'data', 'OECD‡s', 'website', 'well', 'stats', 'GDP', 'per', 'capita', 'IMF‡s', 'website', '.', 'Then', 'join', 'tables', 'sort', 'GDP', 'per', 'cap…', 'ita', '.', 'Table', '1-1', 'shows', 'excerpt', 'get', '.', 'Table', '1-1', '.', 'Does', 'money', 'make', 'people', 'happier', '?', 'CountryGDP', 'per', 'capita', 'USD', 'Life', 'satisfaction', 'Hungary12,2404.9Korea27,1955.8France37,6756.5Australia50,9627.3United', 'States', '55,8057.2Let‡s', 'plot', 'data', 'random', 'countries', 'Figure', '1-17', '.18', '|', 'Chapter', '1', 'The', 'Machine', 'Learning', 'Landscape', '5By', 'convention', 'Greek', 'letter', '−', 'theta', 'frequently', 'used', 'represent', 'model', 'parameters', '.', 'Figure', '1-17', '.', 'Do', 'see', 'trend', '?', 'There', 'seem', 'trend', '!', 'Although', 'data', 'noisy', 'i.e.', 'partly', 'random', 'looks', 'like', 'life', 'satisfaction', 'goes', 'less', 'linearly', 'country‡s', 'GDP', 'per', 'cap…', 'ita', 'increases', '.', 'So', 'decide', 'model', 'life', 'satisfaction', 'linear', 'function', 'GDP', 'per', 'capita', '.', 'This', 'step', 'called', 'model', 'selection', 'selected', 'linear', 'model', 'life', 'satisfac…', 'tion', 'one', 'attribute', 'GDP', 'per', 'capita', 'Equation', '1-1', '.Equation', '1-1', '.', 'A', 'simple', 'linear', 'model', 'life', '_satisfaction', '=–0+–1‰GDP_per_capita', 'This', 'model', 'two', 'model', 'parameters', '–0', '–1.5', 'By', 'tweaking', 'parameters', 'youcan', 'make', 'model', 'represent', 'linear', 'function', 'shown', 'Figure', '1-18.Figure', '1-18', '.', 'A', 'possible', 'linear', 'models', 'Types', 'Machine', 'Learning', 'Systems', '|', '19', '6The', 'code', 'assumes', 'prepare_country_stats', 'already', 'defined', 'merges', 'GDP', 'life', 'satisfaction', 'data', 'single', 'Pandas', 'dataframe', '.', '7It‡s', 'okay', 'don‡t', 'understand', 'code', 'yet', 'present', 'Scikit-Learn', 'following', 'chapters', '.', 'Before', 'use', 'model', 'need', 'define', 'parameter', 'values', '–0', '–1.How', 'know', 'values', 'make', 'model', 'perform', 'best', '?', 'To', 'answer', 'question', 'need', 'specify', 'performance', 'measure', '.', 'You', 'either', 'define', 'utility', 'function', '†tness', 'function', 'measures', 'good', 'model', 'definea', 'cost', 'function', 'measures', 'bad', '.', 'For', 'linear', 'regression', 'problems', 'people', 'typically', 'use', 'cost', 'function', 'measures', 'distance', 'linear', 'model‡s', 'predictions', 'training', 'examples', 'objective', 'minimize', 'distance', '.', 'This', 'Linear', 'Regression', 'algorithm', 'comes', 'feed', 'trainingexamples', 'finds', 'parameters', 'make', 'linear', 'model', 'fit', 'best', 'data', '.', 'This', 'called', 'training', 'model', '.', 'In', 'case', 'algorithm', 'finds', 'optimal', 'parameter', 'values', '–0', '=', '4.85', '–1', '=', '4.91', '‰', '10–5.Now', 'model', 'fits', 'training', 'data', 'closely', 'possible', 'linear', 'model', 'see', 'Figure', '1-19.Figure', '1-19', '.', '•e', 'linear', 'model', '†ts', 'training', 'data', 'best', 'You', 'finally', 'ready', 'run', 'model', 'make', 'predictions', '.', 'For', 'example', 'say', 'want', 'know', 'happy', 'Cypriots', 'OECD', 'data', 'answer', '.', 'Fortunately', 'use', 'model', 'make', 'good', 'prediction', 'look', 'Cyprus‡s', 'GDP', 'per', 'capita', 'find', '$', '22,587', 'apply', 'model', 'find', 'life', 'satisfac…', 'tion', 'likely', 'somewhere', 'around', '4.85', '+', '22,587', '‰', '4.91', '‰', '10-5', '=', '5.96.To', 'whet', 'appetite', 'Example', '1-1', 'shows', 'Python', 'code', 'loads', 'data', 'pre…', 'pares', 'it,6', 'creates', 'scatterplot', 'visualization', 'trains', 'linear', 'model', 'makes', 'prediction.720', '|', 'Chapter', '1', 'The', 'Machine', 'Learning', 'Landscape', 'Example', '1-1', '.', 'Training', 'running', 'linear', 'model', 'using', 'Scikit-Learn', 'import', 'matplotlibimport', 'matplotlib.pyplot', 'pltimport', 'numpy', 'npimport', 'pandas', 'pdimport', 'sklearn', '#', 'Load', 'dataoecd_bli', '=', 'pd.read_csv', '``', 'oecd_bli_2015.csv', \"''\", 'thousands=•', '•', 'gdp_per_capita', '=', 'pd.read_csv', '``', 'gdp_per_capita.csv', \"''\", 'thousands=•', '•', 'delimiter=•\\\\t•', 'encoding=•latin1•', 'na_values=', \"''\", 'n/a', \"''\", '#', 'Prepare', 'datacountry_stats', '=', 'prepare_country_stats', 'oecd_bli', 'gdp_per_capita', 'X', '=', 'np.c_', 'country_stats', '``', 'GDP', 'per', 'capita', \"''\", '=', 'np.c_', 'country_stats', '``', 'Life', 'satisfaction', \"''\", '#', 'Visualize', 'datacountry_stats.plot', 'kind=•scatter•', 'x=', \"''\", 'GDP', 'per', 'capita', \"''\", 'y=•Life', 'satisfaction•', 'plt.show', '#', 'Select', 'linear', 'modellin_reg_model', '=', 'sklearn.linear_model.LinearRegression', '#', 'Train', 'modellin_reg_model.fit', 'X', '#', 'Make', 'prediction', 'CyprusX_new', '=', '22587', '#', 'Cyprus•', 'GDP', 'per', 'capitaprint', 'lin_reg_model.predict', 'X_new', '#', 'outputs', '5.96242338', 'If', 'used', 'instance-based', 'learning', 'algorithm', 'instead', 'youwould', 'found', 'Slovenia', 'closest', 'GDP', 'per', 'capita', 'Cyprus', '$', '20,732', 'since', 'OECD', 'data', 'tells', 'us', 'Slovenians‡', 'life', 'satisfaction', '5.7', 'would', 'predicted', 'life', 'satisfaction', '5.7', 'Cyprus', '.', 'If', 'zoom', 'bit', 'look', 'two', 'next', 'closest', 'countries', 'find', 'Portugal', 'Spain', 'life', 'satisfactions', '5.1', '6.5', 'respectively', '.', 'Averaging', 'three', 'values', 'get', '5.77', 'pretty', 'close', 'model-based', 'pre…diction', '.', 'This', 'simple', 'algorithm', 'called', 'k-Nearest', 'Neighbors', 'regres…sion', 'example', 'k', '=', '3', '.Replacing', 'Linear', 'Regression', 'model', 'k-Nearest', 'Neighbors', 'regression', 'previous', 'code', 'simple', 'replacing', 'line', 'clf', '=', 'sklearn.linear_model.LinearRegression', 'one', 'clf', '=', 'sklearn.neighbors.KNeighborsRegressor', 'n_neighbors=3', 'Types', 'Machine', 'Learning', 'Systems', '|', '21', 'If', 'went', 'well', 'model', 'make', 'good', 'predictions', '.', 'If', 'may', 'need', 'use', 'attributes', 'employment', 'rate', 'health', 'air', 'pollution', 'etc', '.', 'get', 'better', 'qual…', 'ity', 'training', 'data', 'perhaps', 'select', 'powerful', 'model', 'e.g.', 'Polynomial', 'Regres…', 'sion', 'model', '.In', 'summary', '‹You', 'studied', 'data', '.', '‹You', 'selected', 'model', '.', '‹You', 'trained', 'training', 'data', 'i.e.', 'learning', 'algorithm', 'searched', 'model', 'parameter', 'values', 'minimize', 'cost', 'function', '.', '‹Finally', 'applied', 'model', 'make', 'predictions', 'new', 'cases', 'called', 'inference', 'hoping', 'model', 'generalize', 'well', '.', 'This', 'typical', 'Machine', 'Learning', 'project', 'looks', 'like', '.', 'In', 'Chapter', '2', 'willexperience', 'first-hand', 'going', 'end-to-end', 'project.We', 'covered', 'lot', 'ground', 'far', 'know', 'Machine', 'Learning', 'really', 'useful', 'common', 'categories', 'ML', 'sys…', 'tems', 'typical', 'project', 'workflow', 'looks', 'like', '.', 'Now', 'let‡s', 'look', 'go', 'wrong', 'learning', 'prevent', 'making', 'accurate', 'predictions', '.', 'Main', 'Challenges', 'Machine', 'LearningIn', 'short', 'since', 'main', 'task', 'select', 'learning', 'algorithm', 'train', 'somedata', 'two', 'things', 'go', 'wrong', 'ƒbad', 'algorithm⁄', 'ƒbad', 'data.⁄', 'Let‡s', 'start', 'examples', 'bad', 'data', '.', 'Insu…cient', 'Quantity', 'Training', 'DataFor', 'toddler', 'learn', 'apple', 'takes', 'point', 'apple', 'say', 'ƒapple⁄', 'possibly', 'repeating', 'procedure', 'times', '.', 'Now', 'child', 'able', 'recognize', 'apples', 'sorts', 'colors', 'shapes', '.', 'Genius', '.', 'Machine', 'Learning', 'quite', 'yet', 'takes', 'lot', 'data', 'Machine', 'Learn…', 'ing', 'algorithms', 'work', 'properly', '.', 'Even', 'simple', 'problems', 'typically', 'need', 'thousands', 'examples', 'complex', 'problems', 'image', 'speech', 'recogni…', 'tion', 'may', 'need', 'millions', 'examples', 'unless', 'reuse', 'parts', 'existing', 'model', '.22', '|', 'Chapter', '1', 'The', 'Machine', 'Learning', 'Landscape', '8For', 'example', 'knowing', 'whether', 'write', 'ƒto', '⁄', 'ƒtwo', '⁄', 'ƒtoo⁄', 'depending', 'context', '.', '9Figure', 'reproduced', 'permission', 'Banko', 'Brill', '2001', 'ƒLearning', 'Curves', 'Confusion', 'Set', 'Disam…', 'biguation.⁄', '10ƒThe', 'Unreasonable', 'Effectiveness', 'Data', '⁄', 'Peter', 'Norvig', 'et', 'al', '.', '2009', '.', 'The', 'Unreasonable', 'E†ectiveness', 'DataIn', 'famous', 'paper', 'published', '2001', 'Microsoft', 'researchers', 'Michele', 'Banko', 'EricBrill', 'showed', 'different', 'Machine', 'Learning', 'algorithms', 'including', 'fairly', 'simple', 'ones', 'performed', 'almost', 'identically', 'well', 'complex', 'problem', 'natural', 'language', 'disambiguation', '8', 'given', 'enough', 'data', 'see', 'Figure', '1-20', '.Figure', '1-20', '.', '•e', 'importance', 'data', 'versus', 'algorithms', '9As', 'authors', 'put', 'ƒthese', 'results', 'suggest', 'may', 'want', 'reconsider', 'trade-', 'spending', 'time', 'money', 'algorithm', 'development', 'versus', 'spending', 'corpus', 'development.⁄', 'The', 'idea', 'data', 'matters', 'algorithms', 'complex', 'problems', 'popularized', 'Peter', 'Norvig', 'et', 'al', '.', 'paper', 'titled', 'ƒThe', 'Unreasonable', 'Effectiveness', 'Data⁄', 'published', '2009.10', 'It', 'noted', 'however', 'small-', 'medium-', 'sized', 'datasets', 'still', 'common', 'always', 'easy', 'cheap', 'get', 'extra', 'training', 'data', 'don‡t', 'abandon', 'algorithms', 'yet', '.', 'Main', 'Challenges', 'Machine', 'Learning', '|', '23', 'Nonrepresentative', 'Training', 'DataIn', 'order', 'generalize', 'well', 'crucial', 'training', 'data', 'representative', 'new', 'cases', 'want', 'generalize', '.', 'This', 'true', 'whether', 'use', 'instance-based', 'learning', 'model-based', 'learning.For', 'example', 'set', 'countries', 'used', 'earlier', 'training', 'linear', 'model', 'perfectly', 'representative', 'countries', 'missing', '.', 'Figure', '1-21', 'shows', 'data', 'looks', 'like', 'add', 'missing', 'countries', '.', 'Figure', '1-21', '.', 'A', 'representative', 'training', 'sample', 'If', 'train', 'linear', 'model', 'data', 'get', 'solid', 'line', 'old', 'model', 'represented', 'dotted', 'line', '.', 'As', 'see', 'adding', 'missing', 'countries', 'significantly', 'alter', 'model', 'makes', 'clear', 'simple', 'linear', 'model', 'probably', 'never', 'going', 'work', 'well', '.', 'It', 'seems', 'rich', 'countries', 'happier', 'moderately', 'rich', 'countries', 'fact', 'seem', 'unhappier', 'conversely', 'poor', 'countries', 'seem', 'happier', 'many', 'rich', 'countries', '.', 'By', 'using', 'nonrepresentative', 'training', 'set', 'trained', 'model', 'unlikely', 'make', 'accurate', 'predictions', 'especially', 'poor', 'rich', 'countries', '.', 'It', 'crucial', 'use', 'training', 'set', 'representative', 'cases', 'want', 'general…', 'ize', '.', 'This', 'often', 'harder', 'sounds', 'sample', 'small', 'sampling', 'noise', 'i.e.', 'nonrepresentative', 'data', 'result', 'chance', 'even', 'large', 'samples', 'nonrepresentative', 'sampling', 'method', 'flawed', '.', 'This', 'called', 'sampling', 'bias', '.A', 'Famous', 'Example', 'Sampling', 'BiasPerhaps', 'famous', 'example', 'sampling', 'bias', 'happened', 'US', 'presi…', 'dential', 'election', '1936', 'pitted', 'Landon', 'Roosevelt', 'Literary', 'Digest', 'conducted', 'large', 'poll', 'sending', 'mail', '10', 'million', 'people', '.', 'It', 'got', '2.4', 'million', 'answers', 'predicted', 'high', 'confidence', 'Landon', 'would', 'get', '57', '%', 'votes', '.', '24', '|', 'Chapter', '1', 'The', 'Machine', 'Learning', 'Landscape', 'Instead', 'Roosevelt', '62', '%', 'votes', '.', 'The', 'flaw', 'Literary', 'Digest', '‡s', 'sampling', 'method', '‹First', 'obtain', 'addresses', 'send', 'polls', 'Literary', 'Digest', 'used', 'tele…phone', 'directories', 'lists', 'magazine', 'subscribers', 'club', 'membership', 'lists', 'thelike', '.', 'All', 'lists', 'tend', 'favor', 'wealthier', 'people', 'likely', 'vote', 'Republican', 'hence', 'Landon', '.‹Second', 'less', '25', '%', 'people', 'received', 'poll', 'answered', '.', 'Again', 'thisintroduces', 'sampling', 'bias', 'ruling', 'people', 'don‡t', 'care', 'much', 'poli…', 'tics', 'people', 'don‡t', 'like', 'Literary', 'Digest', 'key', 'groups', '.', 'This', 'spe…cial', 'type', 'sampling', 'bias', 'called', 'nonresponse', 'bias', '.Here', 'another', 'example', 'say', 'want', 'build', 'system', 'recognize', 'funk', 'music', 'vid…', 'eos', '.', 'One', 'way', 'build', 'training', 'set', 'search', 'ƒfunk', 'music⁄', 'YouTube', 'use', 'resulting', 'videos', '.', 'But', 'assumes', 'YouTube‡s', 'search', 'engine', 'returns', 'set', 'videos', 'representative', 'funk', 'music', 'videos', 'YouTube', '.', 'In', 'reality', 'search', 'results', 'likely', 'biased', 'toward', 'popular', 'artists', 'live', 'Brazilyou', 'get', 'lot', 'ƒfunk', 'carioca⁄', 'videos', 'sound', 'nothing', 'like', 'James', 'Brown', '.', 'On', 'hand', 'else', 'get', 'large', 'training', 'set', '?', 'Poor-Quality', 'DataObviously', 'training', 'data', 'full', 'errors', 'outliers', 'noise', 'e.g.', 'due', 'poor-', 'quality', 'measurements', 'make', 'harder', 'system', 'detect', 'underlying', 'patterns', 'system', 'less', 'likely', 'perform', 'well', '.', 'It', 'often', 'well', 'worth', 'effort', 'spend', 'time', 'cleaning', 'training', 'data', '.', 'The', 'truth', 'data', 'scientists', 'spend', 'significant', 'part', 'time', '.', 'For', 'example', '‹If', 'instances', 'clearly', 'outliers', 'may', 'help', 'simply', 'discard', 'try', 'fix', 'errors', 'manually', '.', '‹If', 'instances', 'missing', 'features', 'e.g.', '5', '%', 'customers', 'specify', 'age', 'must', 'decide', 'whether', 'want', 'ignore', 'attribute', 'alto…', 'gether', 'ignore', 'instances', 'fill', 'missing', 'values', 'e.g.', 'median', 'age', 'train', 'one', 'model', 'feature', 'one', 'model', 'without', '.', 'Irrelevant', 'FeaturesAs', 'saying', 'goes', 'garbage', 'garbage', '.', 'Your', 'system', 'capable', 'learn…', 'ing', 'training', 'data', 'contains', 'enough', 'relevant', 'features', 'many', 'irrelevant', 'ones', '.', 'A', 'critical', 'part', 'success', 'Machine', 'Learning', 'project', 'coming', 'good', 'set', 'features', 'train', '.', 'This', 'process', 'called', 'feature', 'engineering', 'involves', 'Main', 'Challenges', 'Machine', 'Learning', '|', '25', '‹Feature', 'selection', 'selecting', 'useful', 'features', 'train', 'among', 'existing', 'features', '.', '‹Feature', 'extraction', 'combining', 'existing', 'features', 'produce', 'useful', 'one', 'saw', 'earlier', 'dimensionality', 'reduction', 'algorithms', 'help', '.', '‹Creating', 'new', 'features', 'gathering', 'new', 'data', '.', 'Now', 'looked', 'many', 'examples', 'bad', 'data', 'let‡s', 'look', 'couple', 'exam…', 'ples', 'bad', 'algorithms.Over•tting', 'Training', 'DataSay', 'visiting', 'foreign', 'country', 'taxi', 'driver', 'rips', '.', 'You', 'might', 'tempted', 'say', 'taxi', 'drivers', 'country', 'thieves', '.', 'Overgeneralizing', 'something', 'humans', 'often', 'unfortunately', 'machines', 'fall', 'trap', 'careful', '.', 'In', 'Machine', 'Learning', 'called', 'over†tting', 'itmeans', 'model', 'performs', 'well', 'training', 'data', 'generalize', 'well.Figure', '1-22', 'shows', 'example', 'high-degree', 'polynomial', 'life', 'satisfaction', 'model', 'strongly', 'overfits', 'training', 'data', '.', 'Even', 'though', 'performs', 'much', 'better', 'training', 'data', 'simple', 'linear', 'model', 'would', 'really', 'trust', 'predictions', '?', 'Figure', '1-22', '.', 'Over†tting', 'training', 'data', 'Complex', 'models', 'deep', 'neural', 'networks', 'detect', 'subtle', 'patterns', 'data', 'training', 'set', 'noisy', 'small', 'introduces', 'sampling', 'noise', 'model', 'likely', 'detect', 'patterns', 'noise', '.', 'Obviously', 'patterns', 'generalize', 'new', 'instances', '.', 'For', 'example', 'say', 'feed', 'life', 'satisfaction', 'model', 'many', 'attributes', 'including', 'uninformative', 'ones', 'country‡s', 'name', '.', 'In', 'case', 'complex', 'model', 'may', 'detect', 'patterns', 'like', 'fact', 'coun…', 'tries', 'training', 'data', 'w', 'name', 'life', 'satisfaction', 'greater', '7', 'New', 'Zealand', '7.3', 'Norway', '7.4', 'Sweden', '7.2', 'Switzerland', '7.5', '.', 'How', 'confident', '26', '|', 'Chapter', '1', 'The', 'Machine', 'Learning', 'Landscape', 'W-satisfaction', 'rule', 'generalizes', 'Rwanda', 'Zimbabwe', '?', 'Obviously', 'pattern', 'occurred', 'training', 'data', 'pure', 'chance', 'model', 'way', 'tell', 'whether', 'pattern', 'real', 'simply', 'result', 'noise', 'data', '.', 'Overfitting', 'happens', 'model', 'complex', 'relative', 'amount', 'noisiness', 'training', 'data', '.', 'The', 'possible', 'solutions', '‹To', 'simplify', 'model', 'selecting', 'one', 'fewer', 'parameters', 'e.g.', 'linear', 'model', 'rather', 'high-degree', 'polynomial', 'model', 'reducing', 'number', 'attributes', 'training', 'data', 'constraining', 'model', '‹To', 'gather', 'training', 'data', '‹To', 'reduce', 'noise', 'training', 'data', 'e.g.', 'fix', 'data', 'errors', 'remove', 'outliers', 'Constraining', 'model', 'make', 'simpler', 'reduce', 'risk', 'overfitting', 'calledregularization', '.', 'For', 'example', 'linear', 'model', 'defined', 'earlier', 'two', 'parameters', '–0', '–1', '.', 'This', 'gives', 'learning', 'algorithm', 'two', 'degrees', 'freedom', 'adapt', 'model', 'training', 'data', 'tweak', 'height', '–0', 'slope', '–1', 'line', '.', 'Ifwe', 'forced', '–1', '=', '0', 'algorithm', 'would', 'one', 'degree', 'freedom', 'would', 'much', 'harder', 'time', 'fitting', 'data', 'properly', 'could', 'move', 'line', 'get', 'close', 'possible', 'training', 'instances', 'would', 'end', 'uparound', 'mean', '.', 'A', 'simple', 'model', 'indeed', '!', 'If', 'allow', 'algorithm', 'modify', '–1but', 'force', 'keep', 'small', 'learning', 'algorithm', 'effectively', 'some…', 'one', 'two', 'degrees', 'freedom', '.', 'It', 'produce', 'simpler', 'model', 'two', 'degrees', 'freedom', 'complex', 'one', '.', 'You', 'want', 'find', 'right', 'balance', 'fitting', 'data', 'perfectly', 'keeping', 'model', 'simple', 'enough', 'ensure', 'generalize', 'well', '.', 'Figure', '1-23', 'shows', 'three', 'models', 'dotted', 'line', 'represents', 'original', 'model', 'trained', 'countries', 'missing', 'dashed', 'line', 'second', 'model', 'trained', 'countries', 'solid', 'line', 'linear', 'model', 'trained', 'data', 'first', 'model', 'regularization', 'constraint', '.', 'You', 'see', 'regularization', 'forced', 'model', 'smaller', 'slope', 'fits', 'bit', 'less', 'training', 'data', 'model', 'trained', 'actually', 'allows', 'generalize', 'better', 'new', 'examples', '.', 'Main', 'Challenges', 'Machine', 'Learning', '|', '27', 'Figure', '1-23', '.', 'Regularization', 'reduces', 'risk', 'over†ttingThe', 'amount', 'regularization', 'apply', 'learning', 'controlled', 'hyper…', 'parameter', '.', 'A', 'hyperparameter', 'parameter', 'learning', 'algorithm', 'model', '.', 'As', 'affected', 'learning', 'algorithm', 'must', 'set', 'prior', 'training', 'remains', 'constant', 'training', '.', 'If', 'set', 'regularization', 'hyper…', 'parameter', 'large', 'value', 'get', 'almost', 'flat', 'model', 'slope', 'close', 'zero', 'learning', 'algorithm', 'almost', 'certainly', 'overfit', 'training', 'data', 'less', 'likely', 'find', 'good', 'solution', '.', 'Tuning', 'hyperparameters', 'important', 'part', 'building', 'Machine', 'Learning', 'system', 'see', 'detailed', 'example', 'next', 'chapter', '.', 'Under•tting', 'Training', 'DataAs', 'might', 'guess', 'under†tting', 'opposite', 'overfitting', 'occurs', 'yourmodel', 'simple', 'learn', 'underlying', 'structure', 'data', '.', 'For', 'example', 'lin…', 'ear', 'model', 'life', 'satisfaction', 'prone', 'underfit', 'reality', 'complex', 'model', 'predictions', 'bound', 'inaccurate', 'even', 'training', 'exam…', 'ples.The', 'main', 'options', 'fix', 'problem', '‹Selecting', 'powerful', 'model', 'parameters‹Feeding', 'better', 'features', 'learning', 'algorithm', 'feature', 'engineering', '‹Reducing', 'constraints', 'model', 'e.g.', 'reducing', 'regularization', 'hyper…', 'parameter', 'Stepping', 'BackBy', 'already', 'know', 'lot', 'Machine', 'Learning', '.', 'However', 'went', 'many', 'concepts', 'may', 'feeling', 'little', 'lost', 'let‡s', 'step', 'back', 'look', 'big', 'picture:28', '|', 'Chapter', '1', 'The', 'Machine', 'Learning', 'Landscape', '‹Machine', 'Learning', 'making', 'machines', 'get', 'better', 'task', 'learning', 'data', 'instead', 'explicitly', 'code', 'rules', '.', '‹There', 'many', 'different', 'types', 'ML', 'systems', 'supervised', 'batch', 'online', 'instance-based', 'model-based', 'on.‹In', 'ML', 'project', 'gather', 'data', 'training', 'set', 'feed', 'training', 'set', 'learning', 'algorithm', '.', 'If', 'algorithm', 'model-based', 'tunes', 'parameters', 'tofit', 'model', 'training', 'set', 'i.e.', 'make', 'good', 'predictions', 'training', 'setitself', 'hopefully', 'able', 'make', 'good', 'predictions', 'new', 'cases', 'well', '.', 'If', 'algorithm', 'instance-based', 'learns', 'examples', 'heart', 'uses', 'similarity', 'measure', 'generalize', 'new', 'instances.‹The', 'system', 'perform', 'well', 'training', 'set', 'small', 'data', 'representative', 'noisy', 'polluted', 'irrelevant', 'features', 'garbage', 'garbage', '.', 'Lastly', 'model', 'needs', 'neither', 'simple', 'case', 'underfit', 'complex', 'case', 'overfit', '.', 'There‡s', 'one', 'last', 'important', 'topic', 'cover', 'trained', 'model', 'don‡t', 'want', 'ƒhope⁄', 'generalizes', 'new', 'cases', '.', 'You', 'want', 'evaluate', 'fine-', 'tune', 'necessary', '.', 'Let‡s', 'see', '.', 'Testing', 'ValidatingThe', 'way', 'know', 'well', 'model', 'generalize', 'new', 'cases', 'actually', 'try', 'new', 'cases', '.', 'One', 'way', 'put', 'model', 'production', 'moni…', 'tor', 'well', 'performs', '.', 'This', 'works', 'well', 'model', 'horribly', 'bad', 'yourusers', 'complain›not', 'best', 'idea', '.', 'A', 'better', 'option', 'split', 'data', 'two', 'sets', 'training', 'set', 'test', 'set', '.', 'Asthese', 'names', 'imply', 'train', 'model', 'using', 'training', 'set', 'test', 'using', 'test', 'set', '.', 'The', 'error', 'rate', 'new', 'cases', 'called', 'generalization', 'error', 'out-of-', 'sample', 'error', 'evaluating', 'model', 'test', 'set', 'get', 'estimation', 'error', '.', 'This', 'value', 'tells', 'well', 'model', 'perform', 'instances', 'never', 'seen', 'before.If', 'training', 'error', 'low', 'i.e.', 'model', 'makes', 'mistakes', 'training', 'set', 'generalization', 'error', 'high', 'means', 'model', 'overfitting', 'train…', 'ing', 'data', '.', 'It', 'common', 'use', '80', '%', 'data', 'training', 'hold', '20', '%', 'testing.Testing', 'Validating', '|', '29', '11ƒThe', 'Lack', 'A', 'Priori', 'Distinctions', 'Between', 'Learning', 'Algorithms', '⁄', 'D.', 'Wolperts', '1996', '.', 'So', 'evaluating', 'model', 'simple', 'enough', 'use', 'test', 'set', '.', 'Now', 'suppose', 'hesi…', 'tating', 'two', 'models', 'say', 'linear', 'model', 'polynomial', 'model', 'decide', '?', 'One', 'option', 'train', 'compare', 'well', 'generalize', 'using', 'test', 'set.Now', 'suppose', 'linear', 'model', 'generalizes', 'better', 'want', 'apply', 'regularization', 'avoid', 'overfitting', '.', 'The', 'question', 'choose', 'value', 'regularization', 'hyperparameter', '?', 'One', 'option', 'train', '100', 'different', 'models', 'using', '100', 'different', 'values', 'hyperparameter', '.', 'Suppose', 'find', 'best', 'hyperparame…', 'ter', 'value', 'produces', 'model', 'lowest', 'generalization', 'error', 'say', '5', '%', 'error', '.', 'So', 'launch', 'model', 'production', 'unfortunately', 'perform', 'well', 'expected', 'produces', '15', '%', 'errors', '.', 'What', 'happened', '?', 'The', 'problem', 'measured', 'generalization', 'error', 'multiple', 'times', 'test', 'set', 'adapted', 'model', 'hyperparameters', 'produce', 'best', 'model', 'set', '.', 'This', 'means', 'model', 'unlikely', 'perform', 'well', 'new', 'data', '.', 'A', 'common', 'solution', 'problem', 'second', 'holdout', 'set', 'called', 'valida…', 'tion', 'set', '.', 'You', 'train', 'multiple', 'models', 'various', 'hyperparameters', 'using', 'training', 'set', 'select', 'model', 'hyperparameters', 'perform', 'best', 'validation', 'set', 'you‡re', 'happy', 'model', 'run', 'single', 'final', 'test', 'test', 'set', 'get', 'estimate', 'generalization', 'error', '.', 'To', 'avoid', 'ƒwasting⁄', 'much', 'training', 'data', 'validation', 'sets', 'common', 'technique', 'use', 'cross-validation', 'training', 'set', 'split', 'complementary', 'subsets', 'model', 'trained', 'different', 'combination', 'subsets', 'validated', 'remaining', 'parts', '.', 'Once', 'model', 'type', 'hyperparameters', 'selected', 'final', 'model', 'trained', 'using', 'hyperparameters', 'full', 'training', 'set', 'generalized', 'error', 'measured', 'test', 'set.No', 'Free', 'Lunch', 'TheoremA', 'model', 'simplified', 'version', 'observations', '.', 'The', 'simplifications', 'meant', 'discard', 'superfluous', 'details', 'unlikely', 'generalize', 'new', 'instances', '.', 'How…', 'ever', 'decide', 'data', 'discard', 'data', 'keep', 'must', 'make', 'assump…', 'tions', '.', 'For', 'example', 'linear', 'model', 'makes', 'assumption', 'data', 'fundamentally', 'linear', 'distance', 'instances', 'straight', 'line', 'noise', 'safely', 'ignored.In', 'famous', '1996', 'paper', ',11', 'David', 'Wolpert', 'demonstrated', 'make', 'absolutely', 'assumption', 'data', 'reason', 'prefer', 'one', 'model', '.', 'This', 'called', 'No', 'Free', 'Lunch', 'NFL', 'theorem', '.', 'For', 'datasets', 'best', '30', '|', 'Chapter', '1', 'The', 'Machine', 'Learning', 'Landscape', 'model', 'linear', 'model', 'datasets', 'neural', 'network', '.', 'There', 'model', 'priori', 'guaranteed', 'work', 'better', 'hence', 'name', 'theorem', '.', 'The', 'way', 'know', 'sure', 'model', 'best', 'evaluate', '.', 'Since', 'possible', 'practice', 'make', 'reasonable', 'assumptions', 'data', 'evaluate', 'reasonable', 'models', '.', 'For', 'example', 'simple', 'tasks', 'may', 'evalu…', 'ate', 'linear', 'models', 'various', 'levels', 'regularization', 'complex', 'problem', 'may', 'evaluate', 'various', 'neural', 'networks', '.', 'ExercisesIn', 'chapter', 'covered', 'important', 'concepts', 'Machine', 'Learning', '.', 'In', 'next', 'chapters', 'dive', 'deeper', 'write', 'code', 'make', 'sure', 'know', 'answer', 'following', 'questions', '1.How', 'would', 'define', 'Machine', 'Learning', '?', '2.Can', 'name', 'four', 'types', 'problems', 'shines', '?', '3.What', 'labeled', 'training', 'set', '?', '4.What', 'two', 'common', 'supervised', 'tasks', '?', '5.Can', 'name', 'four', 'common', 'unsupervised', 'tasks', '?', '6.What', 'type', 'Machine', 'Learning', 'algorithm', 'would', 'use', 'allow', 'robot', 'walk', 'various', 'unknown', 'terrains', '?', '7.What', 'type', 'algorithm', 'would', 'use', 'segment', 'customers', 'multiple', 'groups', '?', '8.Would', 'frame', 'problem', 'spam', 'detection', 'supervised', 'learning', 'prob…', 'lem', 'unsupervised', 'learning', 'problem', '?', '9.What', 'online', 'learning', 'system', '?', '10.What', 'out-of-core', 'learning', '?', '11.What', 'type', 'learning', 'algorithm', 'relies', 'similarity', 'measure', 'make', 'predic…', 'tions', '?', '12.What', 'difference', 'model', 'parameter', 'learning', 'algorithm‡s', 'hyperparameter', '?', '13.What', 'model-based', 'learning', 'algorithms', 'search', '?', 'What', 'common', 'strategy', 'use', 'succeed', '?', 'How', 'make', 'predictions', '?', '14.Can', 'name', 'four', 'main', 'challenges', 'Machine', 'Learning', '?', '15.If', 'model', 'performs', 'great', 'training', 'data', 'generalizes', 'poorly', 'new', 'instances', 'happening', '?', 'Can', 'name', 'three', 'possible', 'solutions', '?', '16.What', 'test', 'set', 'would', 'want', 'use', '?', 'Exercises', '|', '31', '17.What', 'purpose', 'validation', 'set', '?', '18.What', 'go', 'wrong', 'tune', 'hyperparameters', 'using', 'test', 'set', '?', '19.What', 'cross-validation', 'would', 'prefer', 'validation', 'set', '?', 'Solutions', 'exercises', 'available', 'Appendix', 'A', '.32', '|', 'Chapter', '1', 'The', 'Machine', 'Learning', 'Landscape', '1The', 'example', 'project', 'completely', 'fictitious', 'goal', 'illustrate', 'main', 'steps', 'Machine', 'Learning', 'project', 'learn', 'anything', 'real', 'estate', 'business', '.', 'CHAPTER', '2End-to-End', 'Machine', 'Learning', 'ProjectIn', 'chapter', 'go', 'example', 'project', 'end', 'end', 'pretending', 'recently', 'hired', 'data', 'scientist', 'real', 'estate', 'company', '.', '1', 'Here', 'main', 'steps', 'go', 'through:1.Look', 'big', 'picture', '.', '2.Get', 'data', '.', '3.Discover', 'visualize', 'data', 'gain', 'insights', '.', '4.Prepare', 'data', 'Machine', 'Learning', 'algorithms', '.', '5.Select', 'model', 'train', 'it.6.Fine-tune', 'model.7.Present', 'solution', '.', '8.Launch', 'monitor', 'maintain', 'system', '.', 'Working', 'Real', 'DataWhen', 'learning', 'Machine', 'Learning', 'best', 'actually', 'experiment', 'real-world', 'data', 'artificial', 'datasets', '.', 'Fortunately', 'thousands', 'open', 'datasets', 'choose', 'ranging', 'across', 'sorts', 'domains', '.', 'Here', 'places', 'look', 'get', 'data', '‹Popular', 'open', 'data', 'repositories', '332The', 'original', 'dataset', 'appeared', 'R.', 'Kelley', 'Pace', 'Ronald', 'Barry', 'ƒSparse', 'Spatial', 'Autoregressions', '⁄', 'Statistics', '&', 'Probability', 'Letters', '33', '.', '3', '1997', '291–297', '.', '›UC', 'Irvine', 'Machine', 'Learning', 'Repository', '›Kaggle', 'datasets', '›Amazon‡s', 'AWS', 'datasets', '‹Meta', 'portals', 'list', 'open', 'data', 'repositories', '›http', '//dataportals.org/', '›http', '//opendatamonitor.eu/', '›http', '//quandl.com/', '‹Other', 'pages', 'listing', 'many', 'popular', 'open', 'data', 'repositories', '›Wikipedia‡s', 'list', 'Machine', 'Learning', 'datasets', '›Quora.com', 'question›Datasets', 'subreddit', 'In', 'chapter', 'chose', 'California', 'Housing', 'Prices', 'dataset', 'StatLib', 'repos…', 'itory', '2', 'see', 'Figure', '2-1', '.', 'This', 'dataset', 'based', 'data', '1990', 'California', 'cen…', 'sus', '.', 'It', 'exactly', 'recent', 'could', 'still', 'afford', 'nice', 'house', 'Bay', 'Area', 'time', 'many', 'qualities', 'learning', 'pretend', 'recent', 'data', '.', 'We', 'also', 'added', 'categorical', 'attribute', 'removed', 'features', 'teaching', 'purposes', '.', 'Figure', '2-1', '.', 'California', 'housing', 'prices', '34', '|', 'Chapter', '2', 'End-to-End', 'Machine', 'Learning', 'Project', '3A', 'piece', 'information', 'fed', 'Machine', 'Learning', 'system', 'often', 'called', 'signal', 'reference', 'Shannon‡s', 'information', 'theory', 'want', 'high', 'signal/noise', 'ratio', '.', 'Look', 'Big', 'PictureWelcome', 'Machine', 'Learning', 'Housing', 'Corporation', '!', 'The', 'first', 'task', 'asked', 'perform', 'build', 'model', 'housing', 'prices', 'California', 'using', 'California', 'cen…sus', 'data', '.', 'This', 'data', 'metrics', 'population', 'median', 'income', 'median', 'hous…', 'ing', 'price', 'block', 'group', 'California', '.', 'Block', 'groups', 'smallestgeographical', 'unit', 'US', 'Census', 'Bureau', 'publishes', 'sample', 'data', 'block', 'group', 'typically', 'population', '600', '3,000', 'people', '.', 'We', 'call', 'ƒdis…', 'tricts⁄', 'short', '.', 'Your', 'model', 'learn', 'data', 'able', 'predict', 'median', 'housing', 'price', 'district', 'given', 'metrics', '.', 'Since', 'well-organized', 'data', 'scientist', 'first', 'thing', 'pull', 'Machine', 'Learning', 'project', 'checklist', '.', 'You', 'start', 'one', 'Appendix', 'B', 'work', 'reasonably', 'wellfor', 'Machine', 'Learning', 'projects', 'make', 'sure', 'adapt', 'needs', '.', 'In', 'chapter', 'go', 'many', 'checklist', 'items', 'also', 'skip', 'either', 'self-', 'explanatory', 'discussed', 'later', 'chapters', '.', 'Frame', 'ProblemThe', 'first', 'question', 'ask', 'boss', 'exactly', 'business', 'objective', 'building', 'model', 'probably', 'end', 'goal', '.', 'How', 'company', 'expect', 'use', 'benefit', 'model', '?', 'This', 'important', 'determine', 'frame', 'problem', 'algorithms', 'select', 'performance', 'measure', 'use', 'evaluate', 'model', 'much', 'effort', 'spend', 'tweaking', '.', 'Your', 'boss', 'answers', 'model‡s', 'output', 'prediction', 'district‡s', 'median', 'hous…', 'ing', 'price', 'fed', 'another', 'Machine', 'Learning', 'system', 'see', 'Figure', '2-2', 'alongwith', 'many', 'signals', '.3', 'This', 'downstream', 'system', 'determine', 'whether', 'worthinvesting', 'given', 'area', '.', 'Getting', 'right', 'critical', 'directly', 'affects', 'reve…', 'nue', '.', 'Look', 'Big', 'Picture', '|', '35', 'Figure', '2-2', '.', 'A', 'Machine', 'Learning', 'pipeline', 'real', 'estate', 'investments', 'PipelinesA', 'sequence', 'data', 'processing', 'components', 'called', 'data', 'pipeline', '.', 'Pipelines', 'common', 'Machine', 'Learning', 'systems', 'since', 'lot', 'data', 'manipulate', 'many', 'data', 'transformations', 'apply', '.', 'Components', 'typically', 'run', 'asynchronously', '.', 'Each', 'component', 'pulls', 'large', 'amount', 'data', 'processes', 'spits', 'result', 'another', 'data', 'store', 'time', 'later', 'next', 'component', 'pipeline', 'pulls', 'data', 'spits', 'output', '.', 'Each', 'component', 'fairly', 'self-contained', 'interface', 'components', 'simply', 'data', 'store', '.', 'This', 'makes', 'system', 'quite', 'simple', 'grasp', 'help', 'data', 'flow', 'graph', 'different', 'teams', 'focus', 'different', 'components', '.', 'Moreover', 'component', 'breaks', 'downstream', 'components', 'often', 'continue', 'run', 'normally', 'least', 'using', 'last', 'output', 'broken', 'compo…', 'nent', '.', 'This', 'makes', 'architecture', 'quite', 'robust', '.', 'On', 'hand', 'broken', 'component', 'go', 'unnoticed', 'time', 'proper', 'monitoring', 'implemented', '.', 'The', 'data', 'gets', 'stale', 'overall', 'system‡s', 'perfor…', 'mance', 'drops.The', 'next', 'question', 'ask', 'current', 'solution', 'looks', 'like', '.', 'It', 'often', 'give', 'reference', 'performance', 'well', 'insights', 'solve', 'problem', '.', 'Your', 'boss', 'answers', 'district', 'housing', 'prices', 'currently', 'estimated', 'manually', 'experts', 'team', 'gathers', 'up-to-date', 'information', 'district', 'excluding', 'median', 'housing', 'prices', 'use', 'complex', 'rules', 'come', 'estimate', '.', 'This', 'costly', 'time-consuming', 'estimates', 'great', 'typical', 'error', 'rate', '15', '%', '.Okay', 'information', 'ready', 'start', 'designing', 'system', '.', 'First', 'need', 'frame', 'problem', 'supervised', 'unsupervised', 'Reinforce…', 'ment', 'Learning', '?', 'Is', 'classification', 'task', 'regression', 'task', 'something', 'else', '?', 'Should', '36', '|', 'Chapter', '2', 'End-to-End', 'Machine', 'Learning', 'Project', '4The', 'standard', 'deviation', 'generally', 'denoted', '„', 'Greek', 'letter', 'sigma', 'square', 'root', 'variance', 'whichis', 'average', 'squared', 'deviation', 'mean', '.', '5When', 'feature', 'bell-shaped', 'normal', 'distribution', 'also', 'called', 'Gaussian', 'distribution', 'com…', 'mon', 'ƒ68-95-99.7⁄', 'rule', 'applies', '68', '%', 'values', 'fall', 'within', '1„', 'mean', '95', '%', 'within', '2„', '99.7', '%', 'within', '3„.you', 'use', 'batch', 'learning', 'online', 'learning', 'techniques', '?', 'Before', 'read', 'pause', 'try', 'answer', 'questions', '.', 'Have', 'found', 'answers', '?', 'Let‡s', 'see', 'clearly', 'typical', 'supervised', 'learning', 'task', 'since', 'given', 'labeled', 'training', 'examples', 'instance', 'comes', 'expected', 'output', 'i.e.', 'district‡s', 'median', 'housing', 'price', '.', 'Moreover', 'also', 'typical', 'regres…', 'sion', 'task', 'since', 'asked', 'predict', 'value', '.', 'More', 'specifically', 'multivari…', 'ate', 'regression', 'problem', 'since', 'system', 'use', 'multiple', 'features', 'make', 'prediction', 'use', 'district‡s', 'population', 'median', 'income', 'etc.', '.', 'In', 'first', 'chapter', 'predicted', 'life', 'satisfaction', 'based', 'one', 'feature', 'GDP', 'per', 'capita', 'aunivariate', 'regression', 'problem', '.', 'Finally', 'continuous', 'flow', 'data', 'coming', 'system', 'particular', 'need', 'adjust', 'changing', 'data', 'rapidly', 'data', 'small', 'enough', 'fit', 'memory', 'plain', 'batch', 'learning', 'fine', '.', 'If', 'data', 'huge', 'could', 'either', 'split', 'batch', 'learning', 'work', 'across', 'multiple', 'servers', 'using', 'MapReduce', 'technique', 'aswe', 'see', 'later', 'could', 'use', 'online', 'learning', 'techniqueinstead.Select', 'Performance', 'MeasureYour', 'next', 'step', 'select', 'performance', 'measure', '.', 'A', 'typical', 'performance', 'measure', 'forregression', 'problems', 'Root', 'Mean', 'Square', 'Error', 'RMSE', '.', 'It', 'measures', 'standard', 'deviation', '4', 'errors', 'system', 'makes', 'predictions', '.', 'For', 'example', 'RMSE', 'equal', '50,000', 'means', '68', '%', 'system‡s', 'predictions', 'fall', 'within', '$', '50,000', 'actual', 'value', '95', '%', 'predictions', 'fall', 'within', '$', '100,000', 'actualvalue.5', 'Equation', '2-1', 'shows', 'mathematical', 'formula', 'compute', 'RMSE', '.', 'Equation', '2-1', '.', 'Root', 'Mean', 'Square', 'Error', 'RMSE', 'RMSE', 'h=1m', '“', 'i=1', 'mhi', '”', 'yi2Look', 'Big', 'Picture', '|', '37', '6Recall', 'transpose', 'operator', 'flips', 'column', 'vector', 'row', 'vector', 'vice', 'versa', '.', 'NotationsThis', 'equation', 'introduces', 'several', 'common', 'Machine', 'Learning', 'notations', 'use', 'throughout', 'book', '‹m', 'number', 'instances', 'dataset', 'measuring', 'RMSE', '.', '›For', 'example', 'evaluating', 'RMSE', 'validation', 'set', '2,000', 'dis…', 'tricts', '=', '2,000.‹x', 'vector', 'feature', 'values', 'excluding', 'label', 'ith', 'instance', 'inthe', 'dataset', 'label', 'desired', 'output', 'value', 'instance', '.', '›For', 'example', 'first', 'district', 'dataset', 'located', 'longitude', '–118.29', '‘', 'latitude', '33.91', '‘', '1,416', 'inhabitants', 'median', 'income', '$', '38,372', 'median', 'house', 'value', '$', '156,400', 'ignoring', 'features', 'then:1=', '”', '118.29', '33.91', '1,416', '38,372', 'y1=156,400', '‹X', 'matrix', 'containing', 'feature', 'values', 'excluding', 'labels', 'instances', 'dataset', '.', 'There', 'one', 'row', 'per', 'instance', 'ith', 'row', 'equal', 'transpose', 'x', 'noted', 'x', 'T.6›For', 'example', 'first', 'district', 'described', 'matrix', 'X', 'lookslike', '=1T2T1999T2000T=', '”', '118.2933.911,41638,372', '38', '|', 'Chapter', '2', 'End-to-End', 'Machine', 'Learning', 'Project', '‹h', 'system‡s', 'prediction', 'function', 'also', 'called', 'hypothesis', '.', 'When', 'systemis', 'given', 'instance‡s', 'feature', 'vector', 'x', 'outputs', 'predicted', 'value', 'ƒ', '=', 'h', 'x', 'instance', 'ƒ', 'pronounced', 'ƒy-hat⁄', '.', '›For', 'example', 'system', 'predicts', 'median', 'housing', 'price', 'first', 'district', '$', '158,400', 'ƒ', '1', '=', 'h', 'x', '1', '=', '158,400', '.', 'The', 'prediction', 'error', 'thisdistrict', 'ƒ', '1', '–', '1', '=', '2,000.‹RMSE', 'X', 'h', 'cost', 'function', 'measured', 'set', 'examples', 'using', 'hypothesis', 'h.We', 'use', 'lowercase', 'italic', 'font', 'scalar', 'values', 'function', 'names', 'h', 'lowercase', 'bold', 'font', 'vectors', 'x', 'uppercase', 'bold', 'font', 'matrices', 'X', '.Even', 'though', 'RMSE', 'generally', 'preferred', 'performance', 'measure', 'regression', 'tasks', 'contexts', 'may', 'prefer', 'use', 'another', 'function', '.', 'For', 'example', 'suppose', 'many', 'outlier', 'districts', '.', 'In', 'case', 'may', 'consider', 'using', 'Mean', 'Absolute', 'Error', 'also', 'called', 'Average', 'Absolute', 'Deviation', 'see', 'Equation', '2-2', 'Equation', '2-2', '.', 'Mean', 'Absolute', 'Error', 'MAE', 'h=1m', '“', 'i=1', 'mhi', '”', 'yiBoth', 'RMSE', 'MAE', 'ways', 'measure', 'distance', 'two', 'vectors', 'vector', 'predictions', 'vector', 'target', 'values', '.', 'Various', 'distance', 'measures', 'norms', 'possible', '‹Computing', 'root', 'sum', 'squares', 'RMSE', 'corresponds', 'Euclidian', 'norm', 'notion', 'distance', 'familiar', '.', 'It', 'also', 'called', '—', '2norm', 'noted', '’', '2', '’', '.‹Computing', 'sum', 'absolutes', 'MAE', 'corresponds', '—', '1', 'norm', 'noted', '’', '1.It', 'sometimes', 'called', 'Manhattan', 'norm', 'measures', 'distance', 'two', 'points', 'city', 'travel', 'along', 'orthogonal', 'city', 'blocks', '.', '‹More', 'generally', '—', 'k', 'norm', 'vector', 'v', 'containing', 'n', 'elements', 'defined', 'k=v0k+v1k++vnk1k', '.', '—0', 'gives', 'cardinality', 'vector', 'i.e.', 'number', 'elements', '—', '‚', 'gives', 'maximum', 'absolute', 'value', 'vector', '.', '‹The', 'higher', 'norm', 'index', 'focuses', 'large', 'values', 'neglects', 'smallones', '.', 'This', 'RMSE', 'sensitive', 'outliers', 'MAE', '.', 'But', 'Look', 'Big', 'Picture', '|', '39', '7The', 'latest', 'version', 'Python', '3', 'recommended', '.', 'Python', '2.7+', 'work', 'fine', 'deprecated', '.', 'outliers', 'exponentially', 'rare', 'like', 'bell-shaped', 'curve', 'RMSE', 'performs', 'well', 'generally', 'preferred', '.', 'Check', 'AssumptionsLastly', 'good', 'practice', 'list', 'verify', 'assumptions', 'made', 'far', 'others', 'catch', 'serious', 'issues', 'early', '.', 'For', 'example', 'district', 'prices', 'system', 'outputs', 'going', 'fed', 'downstream', 'Machine', 'Learning', 'system', 'assume', 'prices', 'going', 'used', '.', 'But', 'downstream', 'system', 'actually', 'converts', 'prices', 'categories', 'e.g.', 'ƒcheap', '⁄', 'ƒmedium', '⁄', 'ƒexpensive⁄', 'uses', 'categories', 'instead', 'prices', 'them…', 'selves', '?', 'In', 'case', 'getting', 'price', 'perfectly', 'right', 'important', 'sys…', 'tem', 'needs', 'get', 'category', 'right', '.', 'If', 'that‡s', 'problem', 'framed', 'classification', 'task', 'regression', 'task', '.', 'You', 'don‡t', 'want', 'find', 'working', 'regression', 'system', 'months', '.', 'Fortunately', 'talking', 'team', 'charge', 'downstream', 'system', 'confident', 'indeed', 'need', 'actual', 'prices', 'categories', '.', 'Great', '!', 'You‡re', 'set', 'lights', 'green', 'start', 'coding', '!', 'Get', 'DataIt‡s', 'time', 'get', 'hands', 'dirty', '.', 'Don‡t', 'hesitate', 'pick', 'laptop', 'walk', 'following', 'code', 'examples', 'Jupyter', 'notebook', '.', 'The', 'full', 'Jupyter', 'note…', 'book', 'available', 'https', '//github.com/ageron/handson-ml', '.Create', 'WorkspaceFirst', 'need', 'Python', 'installed', '.', 'It', 'probably', 'already', 'installed', 'system', '.', 'If', 'get', 'https', '//www.python.org/', '.7Next', 'need', 'create', 'workspace', 'directory', 'Machine', 'Learning', 'code', 'datasets', '.', 'Open', 'terminal', 'type', 'following', 'commands', '$', 'prompts', '$', 'export', 'ML_PATH=', \"''\", '$', 'HOME/ml', \"''\", '#', 'You', 'change', 'path', 'prefer', '$', 'mkdir', '-p', '$', 'ML_PATHYou', 'need', 'number', 'Python', 'modules', 'Jupyter', 'NumPy', 'Pandas', 'Matplotlib', 'Scikit-Learn', '.', 'If', 'already', 'Jupyter', 'running', 'modules', 'installed', 'safely', 'skip', 'ƒDownload', 'Data⁄', 'page', '43', '.', 'If', 'don‡t', 'yet', 'many', 'ways', 'install', 'dependencies', '.', 'You', 'use', 'sys…', 'tem‡s', 'packaging', 'system', 'e.g.', 'apt-get', 'Ubuntu', 'MacPorts', 'HomeBrew', '40', '|', 'Chapter', '2', 'End-to-End', 'Machine', 'Learning', 'Project', '8We', 'show', 'installation', 'steps', 'using', 'pip', 'bash', 'shell', 'Linux', 'macOS', 'system', '.', 'You', 'may', 'need', 'adapt', 'commands', 'system', '.', 'On', 'Windows', 'recommend', 'installing', 'Anaconda', 'instead', '.', '9You', 'may', 'need', 'administrator', 'rights', 'run', 'command', 'try', 'prefixing', 'sudo.macOS', 'install', 'Scientific', 'Python', 'distribution', 'Anaconda', 'use', 'packag…', 'ing', 'system', 'use', 'Python‡s', 'packaging', 'system', 'pip', 'included', 'default', 'Python', 'binary', 'installers', 'since', 'Python', '2.7.9', '.', '8', 'You', 'check', 'see', 'pip', 'installed', 'typing', 'following', 'command', '$', 'pip3', '--', 'versionpip', '9.0.1', '...', '/lib/python3.5/site-packages', 'python', '3.5', 'You', 'make', 'sure', 'recent', 'version', 'pip', 'installed', 'least', '>', '1.4', 'support', 'binary', 'module', 'installation', 'a.k.a', '.', 'wheels', '.', 'To', 'upgrade', 'pip', 'module', 'type:9', '$', 'pip3', 'install', '--', 'upgrade', 'pipCollecting', 'pip', '...', 'Successfully', 'installed', 'pip-9.0.1Creating', 'Isolated', 'EnvironmentIf', 'would', 'like', 'work', 'isolated', 'environment', 'strongly', 'recom…', 'mended', 'work', 'different', 'projects', 'without', 'conflicting', 'library', 'ver…', 'sions', 'install', 'virtualenv', 'running', 'following', 'pip', 'command', '$', 'pip3', 'install', '--', 'user', '--', 'upgrade', 'virtualenvCollecting', 'virtualenv', '...', 'Successfully', 'installed', 'virtualenvNow', 'create', 'isolated', 'Python', 'environment', 'typing', '$', 'cd', '$', 'ML_PATH', '$', 'virtualenv', 'envUsing', 'base', 'prefix', '•', '...', '•New', 'python', 'executable', '...', '/ml/env/bin/python3.5Also', 'creating', 'executable', '...', '/ml/env/bin/pythonInstalling', 'setuptools', 'pip', 'wheel', '...', 'done.Now', 'every', 'time', 'want', 'activate', 'environment', 'open', 'terminal', 'type', '$', 'cd', '$', 'ML_PATH', '$', 'source', 'env/bin/activateWhile', 'environment', 'active', 'package', 'install', 'using', 'pip', 'installed', 'isolated', 'environment', 'Python', 'access', 'packages', 'also', 'want', 'access', 'system‡s', 'site', 'packages', 'create', 'environment', 'Get', 'Data', '|', '41', '10Note', 'Jupyter', 'handle', 'multiple', 'versions', 'Python', 'even', 'many', 'languages', 'R', 'Octave', '.', 'using', 'virtualenv‡s', '--', 'system-site-packages', 'option', '.', 'Check', 'virtualenv‡s', 'docu…', 'mentation', 'information', '.', 'Now', 'install', 'required', 'modules', 'dependencies', 'using', 'sim…', 'ple', 'pip', 'command', '$', 'pip3', 'install', '--', 'upgrade', 'jupyter', 'matplotlib', 'numpy', 'pandas', 'scipy', 'scikit-learnCollecting', 'jupyter', 'Downloading', 'jupyter-1.0.0-py2.py3-none-any.whlCollecting', 'matplotlib', '...', 'To', 'check', 'installation', 'try', 'import', 'every', 'module', 'like', '$', 'python3', '-c', '``', 'import', 'jupyter', 'matplotlib', 'numpy', 'pandas', 'scipy', 'sklearn', \"''\", 'There', 'output', 'error', '.', 'Now', 'fire', 'Jupyter', 'typing', '$', 'jupyter', 'notebook', 'I', '15:24', 'NotebookApp', 'Serving', 'notebooks', 'local', 'directory', '...', '/ml', 'I', '15:24', 'NotebookApp', '0', 'active', 'kernels', 'I', '15:24', 'NotebookApp', 'The', 'Jupyter', 'Notebook', 'running', 'http', '//localhost:8888/', 'I', '15:24', 'NotebookApp', 'Use', 'Control-C', 'stop', 'server', 'shut', 'allkernels', 'twice', 'skip', 'confirmation', '.A', 'Jupyter', 'server', 'running', 'terminal', 'listening', 'port', '8888', '.', 'You', 'visit', 'server', 'opening', 'web', 'browser', 'http', '//localhost:8888/', 'usually', 'hap…', 'pens', 'automatically', 'server', 'starts', '.', 'You', 'see', 'empty', 'workspace', 'directory', 'containing', 'env', 'directory', 'followed', 'preceding', 'virtualenv', 'instructions', '.Now', 'create', 'new', 'Python', 'notebook', 'clicking', 'New', 'button', 'selecting', 'appropriate', 'Python', 'version', '10', 'see', 'Figure', '2-3', '.This', 'three', 'things', 'first', 'creates', 'new', 'notebook', 'file', 'called', 'Untitled.ipynb', 'inyour', 'workspace', 'second', 'starts', 'Jupyter', 'Python', 'kernel', 'run', 'notebook', 'third', 'opens', 'notebook', 'new', 'tab', '.', 'You', 'start', 'renaming', 'note…', 'book', 'ƒHousing⁄', 'automatically', 'rename', 'file', 'Housing.ipynb', 'click…ing', 'Untitled', 'typing', 'new', 'name', '.', '42', '|', 'Chapter', '2', 'End-to-End', 'Machine', 'Learning', 'Project', 'Figure', '2-3', '.', 'Your', 'workspace', 'Jupyter', 'A', 'notebook', 'contains', 'list', 'cells', '.', 'Each', 'cell', 'contain', 'executable', 'code', 'formatted', 'text', '.', 'Right', 'notebook', 'contains', 'one', 'empty', 'code', 'cell', 'labeled', 'ƒIn', '1', '⁄', '.', 'Try', 'typing', 'print', '``', 'Hello', 'world', '!', \"''\", 'cell', 'click', 'play', 'button', 'see', 'Figure', '2-4', 'press', 'Shift-Enter', '.', 'This', 'sends', 'current', 'cell', 'notebook‡s', 'Python', 'kernel', 'runs', 'returns', 'output', '.', 'The', 'result', 'displayed', 'cell', 'since', 'reached', 'end', 'notebook', 'new', 'cell', 'automatically', 'created', '.', 'Go', 'User', 'Interface', 'Tour', 'Jupyter‡s', 'Help', 'menu', 'learn', 'basics', '.', 'Figure', '2-4', '.', 'Hello', 'world', 'Python', 'notebook', 'Download', 'DataIn', 'typical', 'environments', 'data', 'would', 'available', 'relational', 'database', 'common', 'datastore', 'spread', 'across', 'multiple', 'tables/documents/files', '.', 'To', 'Get', 'Data', '|', '43', '11You', 'might', 'also', 'need', 'check', 'legal', 'constraints', 'private', 'fields', 'never', 'copied', 'unsafe', 'datastores', '.', '12In', 'real', 'project', 'would', 'save', 'code', 'Python', 'file', 'write', 'Jupyter', 'notebook.access', 'would', 'first', 'need', 'get', 'credentials', 'access', 'authorizations', '11', 'familiarize', 'data', 'schema', '.', 'In', 'project', 'however', 'things', 'much', 'simpler', 'download', 'single', 'compressed', 'file', 'housing.tgz', 'contains', 'comma-separated', 'value', 'CSV', 'file', 'called', 'housing.csv', 'data', '.', 'You', 'could', 'use', 'web', 'browser', 'download', 'run', 'tar', 'xzf', 'housing.tgz', 'todecompress', 'file', 'extract', 'CSV', 'file', 'preferable', 'create', 'small', 'func…', 'tion', '.', 'It', 'useful', 'particular', 'data', 'changes', 'regularly', 'allows', 'write', 'small', 'script', 'run', 'whenever', 'need', 'fetch', 'latest', 'data', 'set', 'scheduled', 'job', 'automatically', 'regular', 'intervals', '.', 'Auto…', 'mating', 'process', 'fetching', 'data', 'also', 'useful', 'need', 'install', 'dataset', 'multiple', 'machines', '.', 'Here', 'function', 'fetch', 'data', '12import', 'osimport', 'tarfilefrom', 'six.moves', 'import', 'urllibDOWNLOAD_ROOT', '=', '``', 'https', '//raw.githubusercontent.com/ageron/handson-ml/master/', \"''\", 'HOUSING_PATH', '=', '``', 'datasets/housing', \"''\", 'HOUSING_URL', '=', 'DOWNLOAD_ROOT', '+', 'HOUSING_PATH', '+', '``', '/housing.tgz', \"''\", 'def', 'fetch_housing_data', 'housing_url=HOUSING_URL', 'housing_path=HOUSING_PATH', 'os.path.isdir', 'housing_path', 'os.makedirs', 'housing_path', 'tgz_path', '=', 'os.path.join', 'housing_path', '``', 'housing.tgz', \"''\", 'urllib.request.urlretrieve', 'housing_url', 'tgz_path', 'housing_tgz', '=', 'tarfile.open', 'tgz_path', 'housing_tgz.extractall', 'path=housing_path', 'housing_tgz.close', 'Now', 'call', 'fetch_housing_data', 'creates', 'datasets/housing', 'directory', 'workspace', 'downloads', 'housing.tgz', 'file', 'extracts', 'housing.csv', 'directory', '.', 'Now', 'let‡s', 'load', 'data', 'using', 'Pandas', '.', 'Once', 'write', 'small', 'function', 'load', 'data', 'import', 'pandas', 'pddef', 'load_housing_data', 'housing_path=HOUSING_PATH', 'csv_path', '=', 'os.path.join', 'housing_path', '``', 'housing.csv', \"''\", 'return', 'pd.read_csv', 'csv_path', '44', '|', 'Chapter', '2', 'End-to-End', 'Machine', 'Learning', 'Project', 'This', 'function', 'returns', 'Pandas', 'DataFrame', 'object', 'containing', 'data', '.', 'Take', 'Quick', 'Look', 'Data', 'StructureLet‡s', 'take', 'look', 'top', 'five', 'rows', 'using', 'DataFrame‡s', 'head', 'method', 'seeFigure', '2-5', '.Figure', '2-5', '.', 'Top', '†ve', 'rows', 'dataset', 'Each', 'row', 'represents', 'one', 'district', '.', 'There', '10', 'attributes', 'see', 'first', '6', 'screenshot', 'longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', 'median_house_value', 'andocean_proximity.The', 'info', 'method', 'useful', 'get', 'quick', 'description', 'data', 'particular', 'total', 'number', 'rows', 'attribute‡s', 'type', 'number', 'non-null', 'values', 'see', 'Figure', '2-6', '.Figure', '2-6', '.', 'Housing', 'info', 'Get', 'Data', '|', '45', 'There', '20,640', 'instances', 'dataset', 'means', 'fairly', 'small', 'Machine', 'Learning', 'standards', 'it‡s', 'perfect', 'get', 'started', '.', 'Notice', 'total_bedrooms', 'attribute', '20,433', 'non-null', 'values', 'meaning', '207', 'districts', 'miss…', 'ing', 'feature', '.', 'We', 'need', 'take', 'care', 'later', '.', 'All', 'attributes', 'numerical', 'except', 'ocean_proximity', 'field', '.', 'Its', 'type', 'object', 'itcould', 'hold', 'kind', 'Python', 'object', 'since', 'loaded', 'data', 'CSV', 'file', 'know', 'must', 'text', 'attribute', '.', 'When', 'looked', 'top', 'five', 'rows', 'probably', 'noticed', 'values', 'column', 'repetitive', 'means', 'probably', 'categorical', 'attribute', '.', 'You', 'find', 'categories', 'exist', 'many', 'districts', 'belong', 'category', 'using', 'value_counts', 'method', '>', '>', '>', 'housing', '``', 'ocean_proximity', \"''\", '.value_counts', '<', '1H', 'OCEAN', '9136INLAND', '6551NEAR', 'OCEAN', '2658NEAR', 'BAY', '2290ISLAND', '5Name', 'ocean_proximity', 'dtype', 'int64Let‡s', 'look', 'fields', '.', 'The', 'describe', 'method', 'shows', 'summary', 'numerical', 'attributes', 'Figure', '2-7', '.Figure', '2-7', '.', 'Summary', 'numerical', 'attribute', 'The', 'count', 'mean', 'min', 'max', 'rows', 'self-explanatory', '.', 'Note', 'null', 'values', 'ignored', 'example', 'count', 'total_bedrooms', '20,433', '20,640', '.', 'The', 'stdrow', 'shows', 'standard', 'deviation', 'measures', 'dispersed', 'values', '.', 'The', '25', '%', '50', '%', '75', '%', 'rows', 'show', 'corresponding', 'percentiles', 'percentile', 'indi…', 'cates', 'value', 'given', 'percentage', 'observations', 'group', 'observa…', 'tions', 'falls', '.', 'For', 'example', '25', '%', 'districts', 'housing_median_age', 'lower', 'than46', '|', 'Chapter', '2', 'End-to-End', 'Machine', 'Learning', 'Project', '18', '50', '%', 'lower', '29', '75', '%', 'lower', '37', '.', 'These', 'often', 'called', 'the25th', 'percentile', '1', 'st', 'quartile', 'median', '75th', 'percentile', '3', 'rd', 'quartile', '.Another', 'quick', 'way', 'get', 'feel', 'type', 'data', 'dealing', 'plot', 'histogram', 'numerical', 'attribute', '.', 'A', 'histogram', 'shows', 'number', 'instances', 'vertical', 'axis', 'given', 'value', 'range', 'horizontal', 'axis', '.', 'You', 'either', 'plot', 'one', 'attribute', 'time', 'call', 'hist', 'method', 'thewhole', 'dataset', 'plot', 'histogram', 'numerical', 'attribute', 'see', 'Figure', '2-8', '.', 'For', 'example', 'see', 'slightly', '800', 'districts', 'median_house_value', 'equal', '$', '500,000.', '%', 'matplotlib', 'inline', '#', 'Jupyter', 'notebookimport', 'matplotlib.pyplot', 'plthousing.hist', 'bins=50', 'figsize=', '20,15', 'plt.show', 'Figure', '2-8', '.', 'A', 'histogram', 'numerical', 'attribute', 'Get', 'Data', '|', '47', 'The', 'hist', 'method', 'relies', 'Matplotlib', 'turn', 'relies', 'user-specified', 'graphical', 'backend', 'draw', 'screen', '.', 'So', 'plot', 'anything', 'need', 'specify', 'backend', 'Matplot…', 'lib', 'use', '.', 'The', 'simplest', 'option', 'use', 'Jupyter‡s', 'magic', 'com…', 'mand', '%', 'matplotlib', 'inline', '.', 'This', 'tells', 'Jupyter', 'set', 'Matplotlib', 'uses', 'Jupyter‡s', 'backend', '.', 'Plots', 'rendered', 'within', 'notebook', '.', 'Note', 'calling', 'show', 'optional', 'Jupyter', 'notebook', 'Jupyter', 'automatically', 'display', 'plots', 'cell', 'executed.Notice', 'things', 'histograms', '1.First', 'median', 'income', 'attribute', 'look', 'like', 'expressed', 'US', 'dollars', 'USD', '.', 'After', 'checking', 'team', 'collected', 'data', 'told', 'data', 'scaled', 'capped', '15', 'actually', '15.0001', 'higher', 'median', 'incomes', '0.5', 'actually', '0.4999', 'lower', 'median', 'incomes', '.', 'Working', 'preprocessed', 'attributes', 'common', 'Machine', 'Learning', 'necessarily', 'problem', 'try', 'understand', 'data', 'computed', '.', '2.The', 'housing', 'median', 'age', 'median', 'house', 'value', 'also', 'capped', '.', 'The', 'lat…', 'ter', 'may', 'serious', 'problem', 'since', 'target', 'attribute', 'labels', '.', 'Your', 'Machine', 'Learning', 'algorithms', 'may', 'learn', 'prices', 'never', 'go', 'beyond', 'limit', '.', 'You', 'need', 'check', 'client', 'team', 'team', 'use', 'system‡s', 'out…', 'put', 'see', 'problem', '.', 'If', 'tell', 'need', 'precise', 'pre…', 'dictions', 'even', 'beyond', '$', '500,000', 'mainly', 'two', 'options', 'a.Collect', 'proper', 'labels', 'districts', 'whose', 'labels', 'capped', '.', 'b', '.', 'Remove', 'districts', 'training', 'set', 'also', 'test', 'set', 'sinceyour', 'system', 'evaluated', 'poorly', 'predicts', 'values', 'beyond', '$', '500,000', '.3.These', 'attributes', 'different', 'scales', '.', 'We', 'discuss', 'later', 'chap…', 'ter', 'explore', 'feature', 'scaling', '.', '4.Finally', 'many', 'histograms', 'tail', 'heavy', 'extend', 'much', 'farther', 'right', 'median', 'left', '.', 'This', 'may', 'make', 'bit', 'harder', 'Machine', 'Learning', 'algorithms', 'detect', 'patterns', '.', 'We', 'try', 'transforming', 'attributes', 'later', 'bell-shaped', 'distributions', '.', 'Hopefully', 'better', 'understanding', 'kind', 'data', 'dealingwith.48', '|', 'Chapter', '2', 'End-to-End', 'Machine', 'Learning', 'Project', '13You', 'often', 'see', 'people', 'set', 'random', 'seed', '42', '.', 'This', 'number', 'special', 'property', 'The', 'Answer', 'Ultimate', 'Question', 'Life', 'Universe', 'Everything', '.', 'Wait', '!', 'Before', 'look', 'data', 'need', 'create', 'test', 'set', 'put', 'aside', 'never', 'look', '.', 'Create', 'Test', 'SetIt', 'may', 'sound', 'strange', 'voluntarily', 'set', 'aside', 'part', 'data', 'stage', '.', 'After', 'taken', 'quick', 'glance', 'data', 'surely', 'learn', 'whole', 'lot', 'decide', 'algorithms', 'use', 'right', '?', 'This', 'true', 'brain', 'amazing', 'pattern', 'detection', 'system', 'means', 'highly', 'prone', 'overfitting', 'look', 'test', 'set', 'may', 'stumble', 'upon', 'seemingly', 'interesting', 'pattern', 'test', 'data', 'leads', 'select', 'particular', 'kind', 'Machine', 'Learning', 'model', '.', 'When', 'estimate', 'generalization', 'error', 'using', 'test', 'set', 'estimate', 'optimistic', 'launch', 'system', 'perform', 'well', 'expected', '.', 'This', 'called', 'data', 'snooping', 'bias.Creating', 'test', 'set', 'theoretically', 'quite', 'simple', 'pick', 'instances', 'randomly', 'typically', '20', '%', 'dataset', 'set', 'aside', 'import', 'numpy', 'npdef', 'split_train_test', 'data', 'test_ratio', 'shuffled_indices', '=', 'np.random.permutation', 'len', 'data', 'test_set_size', '=', 'int', 'len', 'data', '*', 'test_ratio', 'test_indices', '=', 'shuffled_indices', 'test_set_size', 'train_indices', '=', 'shuffled_indices', 'test_set_size', 'return', 'data.iloc', 'train_indices', 'data.iloc', 'test_indices', 'You', 'use', 'function', 'like', '>', '>', '>', 'train_set', 'test_set', '=', 'split_train_test', 'housing', '0.2', '>', '>', '>', 'print', 'len', 'train_set', '``', 'train', '+', \"''\", 'len', 'test_set', '``', 'test', \"''\", '16512', 'train', '+', '4128', 'testWell', 'works', 'perfect', 'run', 'program', 'generate', 'different', 'test', 'set', '!', 'Over', 'time', 'Machine', 'Learning', 'algorithms', 'get', 'see', 'whole', 'dataset', 'want', 'avoid', '.', 'One', 'solution', 'save', 'test', 'set', 'first', 'run', 'load', 'subsequent', 'runs', '.', 'Another', 'option', 'set', 'random', 'number', 'generator‡s', 'seed', 'e.g.', 'np.random.seed', '42', '13', 'calling', 'np.random.permutation', 'always', 'generates', 'shuffled', 'indices', '.', 'Get', 'Data', '|', '49', '14The', 'location', 'information', 'actually', 'quite', 'coarse', 'result', 'many', 'districts', 'exact', 'ID', 'end', 'set', 'test', 'train', '.', 'This', 'introduces', 'unfortunate', 'sampling', 'bias', '.', 'But', 'solutions', 'break', 'next', 'time', 'fetch', 'updated', 'dataset', '.', 'A', 'com…', 'mon', 'solution', 'use', 'instance‡s', 'identifier', 'decide', 'whether', 'go', 'test', 'set', 'assuming', 'instances', 'unique', 'immutable', 'identifier', '.', 'For', 'example', 'could', 'compute', 'hash', 'instance‡s', 'identifier', 'keep', 'last', 'byte', 'hash', 'put', 'instance', 'test', 'set', 'value', 'lower', 'equal', 'to51', '~20', '%', '256', '.', 'This', 'ensures', 'test', 'set', 'remain', 'consistent', 'across', 'multiple', 'runs', 'even', 'refresh', 'dataset', '.', 'The', 'new', 'test', 'set', 'contain', '20', '%', 'new', 'instances', 'contain', 'instance', 'previously', 'training', 'set', '.', 'Here', 'possible', 'implementation', 'import', 'hashlibdef', 'test_set_check', 'identifier', 'test_ratio', 'hash', 'return', 'hash', 'np.int64', 'identifier', '.digest', '-1', '<', '256', '*', 'test_ratiodef', 'split_train_test_by_id', 'data', 'test_ratio', 'id_column', 'hash=hashlib.md5', 'ids', '=', 'data', 'id_column', 'in_test_set', '=', 'ids.apply', 'lambda', 'id_', 'test_set_check', 'id_', 'test_ratio', 'hash', 'return', 'data.loc', '~in_test_set', 'data.loc', 'in_test_set', 'Unfortunately', 'housing', 'dataset', 'identifier', 'column', '.', 'The', 'simplest', 'solution', 'use', 'row', 'index', 'ID', 'housing_with_id', '=', 'housing.reset_index', '#', 'adds', '†index†', 'columntrain_set', 'test_set', '=', 'split_train_test_by_id', 'housing_with_id', '0.2', '``', 'index', \"''\", 'If', 'use', 'row', 'index', 'unique', 'identifier', 'need', 'make', 'sure', 'new', 'data', 'gets', 'appended', 'end', 'dataset', 'row', 'ever', 'gets', 'deleted', '.', 'If', 'possible', 'try', 'use', 'stable', 'features', 'build', 'unique', 'identifier', '.', 'For', 'example', 'district‡s', 'latitude', 'longitude', 'guaranteed', 'stable', 'million', 'years', 'could', 'combine', 'ID', 'like', '14housing_with_id', '``', 'id', \"''\", '=', 'housing', '``', 'longitude', \"''\", '*', '1000', '+', 'housing', '``', 'latitude', \"''\", 'train_set', 'test_set', '=', 'split_train_test_by_id', 'housing_with_id', '0.2', '``', 'id', \"''\", 'Scikit-Learn', 'provides', 'functions', 'split', 'datasets', 'multiple', 'subsets', 'various', 'ways', '.', 'The', 'simplest', 'function', 'train_test_split', 'pretty', 'much', 'thing', 'function', 'split_train_test', 'defined', 'earlier', 'couple', 'additional', 'features', '.', 'First', 'random_state', 'parameter', 'allows', 'set', 'random', 'generator', 'seed', 'explained', 'previously', 'second', 'pass', 'multiple', 'datasets', 'identical', 'number', 'rows', 'split', 'indices', 'useful', 'example', 'separate', 'DataFrame', 'labels', '50', '|', 'Chapter', '2', 'End-to-End', 'Machine', 'Learning', 'Project', 'sklearn.model_selection', 'import', 'train_test_splittrain_set', 'test_set', '=', 'train_test_split', 'housing', 'test_size=0.2', 'random_state=42', 'So', 'far', 'considered', 'purely', 'random', 'sampling', 'methods', '.', 'This', 'generally', 'fine', 'dataset', 'large', 'enough', 'especially', 'relative', 'number', 'attributes', 'run', 'risk', 'introducing', 'significant', 'sampling', 'bias', '.', 'When', 'survey', 'company', 'decides', 'call', '1,000', 'people', 'ask', 'questions', 'don‡t', 'pick', '1,000', 'people', 'randomly', 'phone', 'booth', '.', 'They', 'try', 'ensure', '1,000', 'people', 'representative', 'whole', 'population', '.', 'For', 'example', 'US', 'population', 'com…', 'posed', '51.3', '%', 'female', '48.7', '%', 'male', 'well-conducted', 'survey', 'US', 'would', 'try', 'maintain', 'ratio', 'sample', '513', 'female', '487', 'male', '.', 'This', 'called', 'strati…', '†ed', 'sampling', 'population', 'divided', 'homogeneous', 'subgroups', 'called', 'strata', 'right', 'number', 'instances', 'sampled', 'stratum', 'guarantee', 'test', 'set', 'representative', 'overall', 'population', '.', 'If', 'used', 'purely', 'random', 'sam…', 'pling', 'would', '12', '%', 'chance', 'sampling', 'skewed', 'test', 'set', 'either', 'less', '49', '%', 'female', '54', '%', 'female', '.', 'Either', 'way', 'survey', 'results', 'would', 'significantly', 'biased', '.', 'Suppose', 'chatted', 'experts', 'told', 'median', 'income', 'important', 'attribute', 'predict', 'median', 'housing', 'prices', '.', 'You', 'may', 'want', 'ensure', 'test', 'set', 'representative', 'various', 'categories', 'incomes', 'whole', 'dataset', '.', 'Since', 'median', 'income', 'continuous', 'numerical', 'attribute', 'first', 'need', 'create', 'income', 'category', 'attribute', '.', 'Let‡s', 'look', 'median', 'income', 'histogram', 'closely', 'see', 'Figure', '2-9', 'Figure', '2-9', '.', 'Histogram', 'income', 'categories', 'Most', 'median', 'income', 'values', 'clustered', 'around', '2–5', 'tens', 'thousands', 'dollars', 'median', 'incomes', 'go', 'far', 'beyond', '6', '.', 'It', 'important', 'sufficient', 'num…', 'Get', 'Data', '|', '51', 'ber', 'instances', 'dataset', 'stratum', 'else', 'estimate', 'stratum‡s', 'importance', 'may', 'biased', '.', 'This', 'means', 'many', 'strata', 'stratum', 'large', 'enough', '.', 'The', 'following', 'code', 'creates', 'income', 'category', 'attribute', 'dividing', 'median', 'income', '1.5', 'limit', 'number', 'income', 'cate…', 'gories', 'rounding', 'using', 'ceil', 'discrete', 'categories', 'merging', 'categories', 'greater', '5', 'category', '5', 'housing', '``', 'income_cat', \"''\", '=', 'np.ceil', 'housing', '``', 'median_income', \"''\", '/', '1.5', 'housing', '``', 'income_cat', \"''\", '.where', 'housing', '``', 'income_cat', \"''\", '<', '5', '5.0', 'inplace=True', 'Now', 'ready', 'stratified', 'sampling', 'based', 'income', 'category', '.', 'For', 'use', 'Scikit-Learn‡s', 'StratifiedShuffleSplit', 'class', 'sklearn.model_selection', 'import', 'StratifiedShuffleSplitsplit', '=', 'StratifiedShuffleSplit', 'n_splits=1', 'test_size=0.2', 'random_state=42', 'train_index', 'test_index', 'split.split', 'housing', 'housing', '``', 'income_cat', \"''\", 'strat_train_set', '=', 'housing.loc', 'train_index', 'strat_test_set', '=', 'housing.loc', 'test_index', 'Let‡s', 'see', 'worked', 'expected', '.', 'You', 'start', 'looking', 'income', 'category', 'proportions', 'full', 'housing', 'dataset', '>', '>', '>', 'housing', '``', 'income_cat', \"''\", '.value_counts', '/', 'len', 'housing', '3.0', '0.3505812.0', '0.3188474.0', '0.1763085.0', '0.1144381.0', '0.039826Name', 'income_cat', 'dtype', 'float64With', 'similar', 'code', 'measure', 'income', 'category', 'proportions', 'test', 'set', '.', 'Figure', '2-10', 'compares', 'income', 'category', 'proportions', 'overall', 'dataset', 'test', 'set', 'generated', 'stratified', 'sampling', 'test', 'set', 'generated', 'using', 'purely', 'random', 'sampling', '.', 'As', 'see', 'test', 'set', 'generated', 'using', 'stratified', 'sampling', 'income', 'category', 'proportions', 'almost', 'identical', 'full', 'dataset', 'whereas', 'test', 'set', 'generated', 'using', 'purely', 'random', 'sampling', 'quite', 'skewed', '.', 'Figure', '2-10', '.', 'Sampling', 'bias', 'comparison', 'strati†ed', 'versus', 'purely', 'random', 'sampling', '52', '|', 'Chapter', '2', 'End-to-End', 'Machine', 'Learning', 'Project', 'Now', 'remove', 'income_cat', 'attribute', 'data', 'back', 'original', 'state', 'set', 'strat_train_set', 'strat_test_set', 'set.drop', '``', 'income_cat', \"''\", 'axis=1', 'inplace=True', 'We', 'spent', 'quite', 'bit', 'time', 'test', 'set', 'generation', 'good', 'reason', 'often', 'neglected', 'critical', 'part', 'Machine', 'Learning', 'project', '.', 'Moreover', 'many', 'ideas', 'useful', 'later', 'discuss', 'cross-validation', '.', 'Now', 'it‡s', 'time', 'move', 'next', 'stage', 'exploring', 'data', '.', 'Discover', 'Visualize', 'Data', 'Gain', 'InsightsSo', 'far', 'taken', 'quick', 'glance', 'data', 'get', 'general', 'understanding', 'kind', 'data', 'manipulating', '.', 'Now', 'goal', 'go', 'little', 'bit', 'depth', '.', 'First', 'make', 'sure', 'put', 'test', 'set', 'aside', 'exploring', 'train…', 'ing', 'set', '.', 'Also', 'training', 'set', 'large', 'may', 'want', 'sample', 'exploration', 'set', 'make', 'manipulations', 'easy', 'fast', '.', 'In', 'case', 'set', 'quite', 'small', 'work', 'directly', 'full', 'set', '.', 'Let‡s', 'create', 'copy', 'play', 'without', 'harming', 'training', 'set', 'housing', '=', 'strat_train_set.copy', 'Visualizing', 'Geographical', 'DataSince', 'geographical', 'information', 'latitude', 'longitude', 'good', 'idea', 'create', 'scatterplot', 'districts', 'visualize', 'data', 'Figure', '2-11', 'housing.plot', 'kind=', \"''\", 'scatter', \"''\", 'x=', \"''\", 'longitude', \"''\", 'y=', \"''\", 'latitude', \"''\", 'Figure', '2-11', '.', 'A', 'geographical', 'scatterplot', 'data', 'Discover', 'Visualize', 'Data', 'Gain', 'Insights', '|', '53', '15If', 'reading', 'grayscale', 'grab', 'red', 'pen', 'scribble', 'coastline', 'Bay', 'Area', 'San', 'Diego', 'might', 'expect', '.', 'You', 'add', 'patch', 'yellow', 'around', 'Sacramento', 'well', '.', 'This', 'looks', 'like', 'California', 'right', 'hard', 'see', 'particular', 'pattern', '.', 'Setting', 'alpha', 'option', '0.1', 'makes', 'much', 'easier', 'visualize', 'places', 'high', 'density', 'data', 'points', 'Figure', '2-12', 'housing.plot', 'kind=', \"''\", 'scatter', \"''\", 'x=', \"''\", 'longitude', \"''\", 'y=', \"''\", 'latitude', \"''\", 'alpha=0.1', 'Figure', '2-12', '.', 'A', 'better', 'visualization', 'highlighting', 'high-density', 'areas', 'Now', 'that‡s', 'much', 'better', 'clearly', 'see', 'high-density', 'areas', 'namely', 'Bay', 'Area', 'around', 'Los', 'Angeles', 'San', 'Diego', 'plus', 'long', 'line', 'fairly', 'high', 'density', 'Central', 'Valley', 'particular', 'around', 'Sacramento', 'Fresno', '.', 'More', 'generally', 'brains', 'good', 'spotting', 'patterns', 'pictures', 'may', 'need', 'play', 'around', 'visualization', 'parameters', 'make', 'patterns', 'stand', 'out.Now', 'let‡s', 'look', 'housing', 'prices', 'Figure', '2-13', '.', 'The', 'radius', 'circle', 'represents', 'district‡s', 'population', 'option', 'color', 'represents', 'price', 'option', 'c', '.', 'We', 'use', 'predefined', 'color', 'map', 'option', 'cmap', 'called', 'jet', 'ranges', 'blue', 'low', 'values', 'red', 'high', 'prices', ':15housing.plot', 'kind=', \"''\", 'scatter', \"''\", 'x=', \"''\", 'longitude', \"''\", 'y=', \"''\", 'latitude', \"''\", 'alpha=0.4', 's=housing', '``', 'population', \"''\", '/100', 'label=', \"''\", 'population', \"''\", 'c=', \"''\", 'median_house_value', \"''\", 'cmap=plt.get_cmap', '``', 'jet', \"''\", 'colorbar=True', 'plt.legend', '54', '|', 'Chapter', '2', 'End-to-End', 'Machine', 'Learning', 'Project', 'Figure', '2-13', '.', 'California', 'housing', 'prices', 'This', 'image', 'tells', 'housing', 'prices', 'much', 'related', 'location', 'e.g.', 'close', 'ocean', 'population', 'density', 'probably', 'knew', 'already', '.', 'It', 'probably', 'useful', 'use', 'clustering', 'algorithm', 'detect', 'main', 'clusters', 'add', 'new', 'features', 'measure', 'proximity', 'cluster', 'centers', '.', 'The', 'ocean', 'prox…', 'imity', 'attribute', 'may', 'useful', 'well', 'although', 'Northern', 'California', 'housing', 'prices', 'coastal', 'districts', 'high', 'simple', 'rule', '.', 'Looking', 'CorrelationsSince', 'dataset', 'large', 'easily', 'compute', 'standard', 'correlation', 'coe⁄cient', 'also', 'called', 'Pearson‹s', 'r', 'every', 'pair', 'attributes', 'using', 'corr', 'method', 'corr_matrix', '=', 'housing.corr', 'Now', 'let‡s', 'look', 'much', 'attribute', 'correlates', 'median', 'house', 'value', '>', '>', '>', 'corr_matrix', '``', 'median_house_value', \"''\", '.sort_values', 'ascending=False', 'median_house_value', '1.000000median_income', '0.687170total_rooms', '0.135231housing_median_age', '0.114220households', '0.064702Discover', 'Visualize', 'Data', 'Gain', 'Insights', '|', '55', 'total_bedrooms', '0.047865population', '-0.026699longitude', '-0.047279latitude', '-0.142826Name', 'median_house_value', 'dtype', 'float64The', 'correlation', 'coefficient', 'ranges', '–1', '1', '.', 'When', 'close', '1', 'means', 'strong', 'positive', 'correlation', 'example', 'median', 'house', 'value', 'tends', 'go', 'median', 'income', 'goes', '.', 'When', 'coefficient', 'close', '–1', 'means', 'strong', 'negative', 'correlation', 'see', 'small', 'negative', 'correlation', 'latitude', 'median', 'house', 'value', 'i.e.', 'prices', 'slight', 'tendency', 'go', 'go', 'north', '.', 'Finally', 'coefficients', 'close', 'zero', 'mean', 'linear', 'correlation', '.', 'Figure', '2-14', 'shows', 'various', 'plots', 'along', 'correlation', 'coeffi…', 'cient', 'horizontal', 'vertical', 'axes', '.', 'Figure', '2-14', '.', 'Standard', 'correlation', 'coe⁄cient', 'various', 'datasets', 'source', 'Wikipedia', 'public', 'domain', 'image', 'The', 'correlation', 'coefficient', 'measures', 'linear', 'correlations', 'ƒif', 'xgoes', 'generally', 'goes', 'up/down⁄', '.', 'It', 'may', 'completely', 'miss', 'nonlinear', 'relationships', 'e.g.', 'ƒif', 'x', 'close', 'zero', 'gen…', 'erally', 'goes', 'up⁄', '.', 'Note', 'plots', 'bottom', 'row', 'correlation', 'coefficient', 'equal', 'zero', 'despite', 'fact', 'axes', 'clearly', 'independent', 'examples', 'nonlinear', 'rela…', 'tionships', '.', 'Also', 'second', 'row', 'shows', 'examples', 'correla…', 'tion', 'coefficient', 'equal', '1', '–1', 'notice', 'nothing', 'slope', '.', 'For', 'example', 'height', 'inches', 'correla…', 'tion', 'coefficient', '1', 'height', 'feet', 'nanometers', '.', 'Another', 'way', 'check', 'correlation', 'attributes', 'use', 'Pandas‡', 'scatter_matrix', 'function', 'plots', 'every', 'numerical', 'attribute', 'every', 'numerical', 'attribute', '.', 'Since', '11', 'numerical', 'attributes', 'would', 'get', '11', '2', '=', '56', '|', 'Chapter', '2', 'End-to-End', 'Machine', 'Learning', 'Project', '121', 'plots', 'would', 'fit', 'page', 'let‡s', 'focus', 'promising', 'attributes', 'seem', 'correlated', 'median', 'housing', 'value', 'Figure', '2-15', 'pandas.tools.plotting', 'import', 'scatter_matrixattributes', '=', '``', 'median_house_value', \"''\", '``', 'median_income', \"''\", '``', 'total_rooms', \"''\", '``', 'housing_median_age', \"''\", 'scatter_matrix', 'housing', 'attributes', 'figsize=', '12', '8', 'Figure', '2-15', '.', 'Scatter', 'matrix', 'The', 'main', 'diagonal', 'top', 'left', 'bottom', 'right', 'would', 'full', 'straight', 'lines', 'Pandas', 'plotted', 'variable', 'would', 'useful', '.', 'So', 'instead', 'Pandas', 'displays', 'histogram', 'attribute', 'options', 'available', 'see', 'Pandas‡', 'docu…', 'mentation', 'details', '.', 'The', 'promising', 'attribute', 'predict', 'median', 'house', 'value', 'median', 'income', 'let‡s', 'zoom', 'correlation', 'scatterplot', 'Figure', '2-16', 'housing.plot', 'kind=', \"''\", 'scatter', \"''\", 'x=', \"''\", 'median_income', \"''\", 'y=', \"''\", 'median_house_value', \"''\", 'alpha=0.1', 'Discover', 'Visualize', 'Data', 'Gain', 'Insights', '|', '57', 'Figure', '2-16', '.', 'Median', 'income', 'versus', 'median', 'house', 'value', 'This', 'plot', 'reveals', 'things', '.', 'First', 'correlation', 'indeed', 'strong', 'clearly', 'see', 'upward', 'trend', 'points', 'dispersed', '.', 'Second', 'price', 'cap', 'noticed', 'earlier', 'clearly', 'visible', 'horizontal', 'line', '$', '500,000', '.', 'But', 'plot', 'reveals', 'less', 'obvious', 'straight', 'lines', 'horizontal', 'line', 'around', '$', '450,000', 'another', 'around', '$', '350,000', 'perhaps', 'one', 'around', '$', '280,000', '.', 'You', 'may', 'want', 'try', 'removing', 'corresponding', 'districts', 'prevent', 'algorithms', 'learning', 'reproduce', 'data', 'quirks', '.', 'Experimenting', 'Attribute', 'CombinationsHopefully', 'previous', 'sections', 'gave', 'idea', 'ways', 'explore', 'data', 'gain', 'insights', '.', 'You', 'identified', 'data', 'quirks', 'may', 'want', 'clean', 'feeding', 'data', 'Machine', 'Learning', 'algorithm', 'found', 'interesting', 'correlations', 'attributes', 'particular', 'target', 'attribute', '.', 'You', 'also', 'noticed', 'attributes', 'tail-heavy', 'distribution', 'may', 'want', 'trans…', 'form', 'e.g.', 'computing', 'logarithm', '.', 'Of', 'course', 'mileage', 'vary', 'considerably', 'project', 'general', 'ideas', 'similar', '.', 'One', 'last', 'thing', 'may', 'want', 'actually', 'preparing', 'data', 'Machine', 'Learning', 'algorithms', 'try', 'various', 'attribute', 'combinations', '.', 'For', 'example', 'total', 'number', 'rooms', 'district', 'useful', 'don‡t', 'know', 'many', 'households', '.', 'What', 'really', 'want', 'number', 'rooms', 'per', 'household', '.', 'Similarly', 'total', 'number', 'bedrooms', 'useful', 'probably', 'want', 'compare', 'number', 'rooms', '.', 'And', 'population', 'per', 'household', 'also', '58', '|', 'Chapter', '2', 'End-to-End', 'Machine', 'Learning', 'Project', 'seems', 'like', 'interesting', 'attribute', 'combination', 'look', '.', 'Let‡s', 'create', 'new', 'attributes', 'housing', '``', 'rooms_per_household', \"''\", '=', 'housing', '``', 'total_rooms', \"''\", '/housing', '``', 'households', \"''\", 'housing', '``', 'bedrooms_per_room', \"''\", '=', 'housing', '``', 'total_bedrooms', \"''\", '/housing', '``', 'total_rooms', \"''\", 'housing', '``', 'population_per_household', \"''\", '=housing', '``', 'population', \"''\", '/housing', '``', 'households', \"''\", 'And', 'let‡s', 'look', 'correlation', 'matrix', '>', '>', '>', 'corr_matrix', '=', 'housing.corr', '>', '>', '>', 'corr_matrix', '``', 'median_house_value', \"''\", '.sort_values', 'ascending=False', 'median_house_value', '1.000000median_income', '0.687170rooms_per_household', '0.199343total_rooms', '0.135231housing_median_age', '0.114220households', '0.064702total_bedrooms', '0.047865population_per_household', '-0.021984population', '-0.026699longitude', '-0.047279latitude', '-0.142826bedrooms_per_room', '-0.260070Name', 'median_house_value', 'dtype', 'float64Hey', 'bad', '!', 'The', 'new', 'bedrooms_per_room', 'attribute', 'much', 'correlated', 'median', 'house', 'value', 'total', 'number', 'rooms', 'bedrooms', '.', 'Apparently', 'houses', 'lower', 'bedroom/room', 'ratio', 'tend', 'expensive', '.', 'The', 'number', 'rooms', 'per', 'household', 'also', 'informative', 'total', 'number', 'rooms', 'district›obviously', 'larger', 'houses', 'expensive', 'are.This', 'round', 'exploration', 'absolutely', 'thorough', 'point', 'start', 'right', 'foot', 'quickly', 'gain', 'insights', 'help', 'get', 'first', 'rea…', 'sonably', 'good', 'prototype', '.', 'But', 'iterative', 'process', 'get', 'prototype', 'running', 'analyze', 'output', 'gain', 'insights', 'come', 'back', 'exploration', 'step', '.', 'Prepare', 'Data', 'Machine', 'Learning', 'AlgorithmsIt‡s', 'time', 'prepare', 'data', 'Machine', 'Learning', 'algorithms', '.', 'Instead', 'manually', 'write', 'functions', 'several', 'good', 'reasons', '‹This', 'allow', 'reproduce', 'transformations', 'easily', 'dataset', 'e.g.', 'next', 'time', 'get', 'fresh', 'dataset', '.', '‹You', 'gradually', 'build', 'library', 'transformation', 'functions', 'reuse', 'future', 'projects.‹You', 'use', 'functions', 'live', 'system', 'transform', 'new', 'data', 'feeding', 'algorithms.Prepare', 'Data', 'Machine', 'Learning', 'Algorithms', '|', '59', '‹This', 'make', 'possible', 'easily', 'try', 'various', 'transformations', 'see', 'combination', 'transformations', 'works', 'best', '.', 'But', 'first', 'let‡s', 'revert', 'clean', 'training', 'set', 'copying', 'strat_train_set', 'let‡s', 'separate', 'predictors', 'labels', 'since', 'don‡t', 'necessarily', 'want', 'apply', 'transformations', 'predictors', 'target', 'values', 'note', 'drop', 'creates', 'copy', 'data', 'affect', 'strat_train_set', 'housing', '=', 'strat_train_set.drop', '``', 'median_house_value', \"''\", 'axis=1', 'housing_labels', '=', 'strat_train_set', '``', 'median_house_value', \"''\", '.copy', 'Data', 'CleaningMost', 'Machine', 'Learning', 'algorithms', 'work', 'missing', 'features', 'let‡s', 'create', 'functions', 'take', 'care', '.', 'You', 'noticed', 'earlier', 'total_bedroomsattribute', 'missing', 'values', 'let‡s', 'fix', '.', 'You', 'three', 'options', '‹Get', 'rid', 'corresponding', 'districts.‹Get', 'rid', 'whole', 'attribute', '.', '‹Set', 'values', 'value', 'zero', 'mean', 'median', 'etc.', '.', 'You', 'accomplish', 'easily', 'using', 'DataFrame‡s', 'dropna', 'drop', 'fillna', 'methods', 'housing.dropna', 'subset=', '``', 'total_bedrooms', \"''\", '#', 'option', '1housing.drop', '``', 'total_bedrooms', \"''\", 'axis=1', '#', 'option', '2median', '=', 'housing', '``', 'total_bedrooms', \"''\", '.median', 'housing', '``', 'total_bedrooms', \"''\", '.fillna', 'median', '#', 'option', '3If', 'choose', 'option', '3', 'compute', 'median', 'value', 'training', 'set', 'use', 'fill', 'missing', 'values', 'training', 'set', 'also', 'don‡t', 'forget', 'save', 'median', 'value', 'computed', '.', 'You', 'need', 'later', 'replace', 'missing', 'values', 'test', 'set', 'want', 'evaluate', 'system', 'also', 'system', 'goes', 'live', 'replace', 'missing', 'values', 'new', 'data', '.', 'Scikit-Learn', 'provides', 'handy', 'class', 'take', 'care', 'missing', 'values', 'Imputer', '.', 'Here', 'use', '.', 'First', 'need', 'create', 'Imputer', 'instance', 'specifying', 'want', 'replace', 'attribute‡s', 'missing', 'values', 'median', 'attribute', 'sklearn.preprocessing', 'import', 'Imputerimputer', '=', 'Imputer', 'strategy=', \"''\", 'median', \"''\", 'Since', 'median', 'computed', 'numerical', 'attributes', 'need', 'create', 'copy', 'data', 'without', 'text', 'attribute', 'ocean_proximity', 'housing_num', '=', 'housing.drop', '``', 'ocean_proximity', \"''\", 'axis=1', '60', '|', 'Chapter', '2', 'End-to-End', 'Machine', 'Learning', 'Project', '16For', 'details', 'design', 'principles', 'see', 'ƒAPI', 'design', 'machine', 'learning', 'software', 'experiences', 'scikit-learn', 'project', '⁄', 'L.', 'Buitinck', 'G.', 'Louppe', 'M.', 'Blondel', 'F.', 'Pedregosa', 'A.', 'M™ller', 'et', 'al', '.', '2013', '.', 'Now', 'fit', 'imputer', 'instance', 'training', 'data', 'using', 'fit', 'method', 'imputer.fit', 'housing_num', 'The', 'imputer', 'simply', 'computed', 'median', 'attribute', 'stored', 'result', 'statistics_', 'instance', 'variable', '.', 'Only', 'total_bedrooms', 'attribute', 'missing', 'values', 'sure', 'won‡t', 'missing', 'values', 'new', 'data', 'system', 'goes', 'live', 'safer', 'apply', 'imputer', 'numerical', 'attributes', '>', '>', '>', 'imputer.statistics_array', '-118.51', '34.26', '29.', '2119.', '433.', '1164.', '408.', '3.5414', '>', '>', '>', 'housing_num.median', '.valuesarray', '-118.51', '34.26', '29.', '2119.', '433.', '1164.', '408.', '3.5414', 'Now', 'use', 'ƒtrained⁄', 'imputer', 'transform', 'training', 'set', 'replacingmissing', 'values', 'learned', 'medians', 'X', '=', 'imputer.transform', 'housing_num', 'The', 'result', 'plain', 'Numpy', 'array', 'containing', 'transformed', 'features', '.', 'If', 'want', 'put', 'back', 'Pandas', 'DataFrame', 'it‡s', 'simple', 'housing_tr', '=', 'pd.DataFrame', 'X', 'columns=housing_num.columns', 'Scikit-Learn', 'DesignScikit-Learn‡s', 'API', 'remarkably', 'well', 'designed', '.', 'The', 'main', 'design', 'principles', 'are:16‹Consistency', '.', 'All', 'objects', 'share', 'consistent', 'simple', 'interface', '›Estimators', '.', 'Any', 'object', 'estimate', 'parameters', 'based', 'dataset', 'called', 'estimator', 'e.g.', 'imputer', 'estimator', '.', 'The', 'estimation', 'performed', 'fit', 'method', 'takes', 'dataset', 'parameter', 'two', 'supervised', 'learning', 'algorithms', 'second', 'dataset', 'contains', 'labels', '.', 'Any', 'parameter', 'needed', 'guide', 'estimation', 'process', 'con…', 'sidered', 'hyperparameter', 'imputer‡s', 'strategy', 'must', 'set', 'instance', 'variable', 'generally', 'via', 'constructor', 'parameter', '.›Transformers', '.', 'Some', 'estimators', 'imputer', 'also', 'transform', 'adataset', 'called', 'transformers', '.', 'Once', 'API', 'quite', 'simple', 'transformation', 'performed', 'transform', 'method', 'dataset', 'transform', 'parameter', '.', 'It', 'returns', 'transformed', 'dataset', '.', 'This', 'transforma…', 'tion', 'generally', 'relies', 'learned', 'parameters', 'case', 'imputer.All', 'transformers', 'also', 'convenience', 'method', 'called', 'fit_transform', 'Prepare', 'Data', 'Machine', 'Learning', 'Algorithms', '|', '61', '17Some', 'predictors', 'also', 'provide', 'methods', 'measure', 'confidence', 'predictions.that', 'equivalent', 'calling', 'fit', 'transform', 'sometimesfit_transform', 'optimized', 'runs', 'much', 'faster', '.', '›Predictors', '.', 'Finally', 'estimators', 'capable', 'making', 'predictions', 'given', 'dataset', 'called', 'predictors', '.', 'For', 'example', 'LinearRegression', 'model', 'previous', 'chapter', 'predictor', 'predicted', 'life', 'satisfaction', 'given', 'country‡s', 'GDP', 'per', 'capita', '.', 'A', 'predictor', 'predict', 'method', 'takes', 'dataset', 'new', 'instances', 'returns', 'dataset', 'corresponding', 'predictions', '.', 'It', 'also', 'score', 'method', 'measures', 'quality', 'predictions', 'given', 'test', 'set', 'corresponding', 'labels', 'case', 'supervised', 'learning', 'algorithms', '.17‹Inspection', '.', 'All', 'estimator‡s', 'hyperparameters', 'accessible', 'directly', 'via', 'public', 'instance', 'variables', 'e.g.', 'imputer.strategy', 'estimator‡s', 'learned', 'parameters', 'also', 'accessible', 'via', 'public', 'instance', 'variables', 'underscoresuffix', 'e.g.', 'imputer.statistics_', '.‹Nonproliferation', 'classes', '.', 'Datasets', 'represented', 'NumPy', 'arrays', 'SciPy', 'sparse', 'matrices', 'instead', 'homemade', 'classes', '.', 'Hyperparameters', 'regular', 'Python', 'strings', 'numbers', '.', '‹Composition', '.', 'Existing', 'building', 'blocks', 'reused', 'much', 'possible', '.', 'For', 'example', 'easy', 'create', 'Pipeline', 'estimator', 'arbitrary', 'sequence', 'transformers', 'followed', 'final', 'estimator', 'see', '.', '‹Sensible', 'defaults', '.', 'Scikit-Learn', 'provides', 'reasonable', 'default', 'values', 'parameters', 'making', 'easy', 'create', 'baseline', 'working', 'system', 'quickly', '.', 'Handling', 'Text', 'Categorical', 'AttributesEarlier', 'left', 'categorical', 'attribute', 'ocean_proximity', 'text', 'attribute', 'compute', 'median', '.', 'Most', 'Machine', 'Learning', 'algorithms', 'pre…', 'fer', 'work', 'numbers', 'anyway', 'let‡s', 'convert', 'text', 'labels', 'numbers', '.', 'Scikit-Learn', 'provides', 'transformer', 'task', 'called', 'LabelEncoder', '>', '>', '>', 'sklearn.preprocessing', 'import', 'LabelEncoder', '>', '>', '>', 'encoder', '=', 'LabelEncoder', '>', '>', '>', 'housing_cat', '=', 'housing', '``', 'ocean_proximity', \"''\", '>', '>', '>', 'housing_cat_encoded', '=', 'encoder.fit_transform', 'housing_cat', '>', '>', '>', 'housing_cat_encodedarray', '1', '1', '4', '...', '1', '0', '3', '62', '|', 'Chapter', '2', 'End-to-End', 'Machine', 'Learning', 'Project', '18NumPy‡s', 'reshape', 'function', 'allows', 'one', 'dimension', '–1', 'means', 'ƒunspecified⁄', 'value', 'inferred', 'length', 'array', 'remaining', 'dimensions', '.', '19See', 'SciPy‡s', 'documentation', 'details', '.', 'This', 'better', 'use', 'numerical', 'data', 'ML', 'algorithm', '.', 'You', 'look', 'mapping', 'encoder', 'learned', 'using', 'classes_', 'attribute', 'ƒ', '<', '1H', 'OCEAN⁄', 'mapped', '0', 'ƒINLAND⁄', 'mapped', '1', 'etc', '.', '>', '>', '>', 'print', 'encoder.classes_', '•', '<', '1H', 'OCEAN•', '•INLAND•', '•ISLAND•', '•NEAR', 'BAY•', '•NEAR', 'OCEAN•', 'One', 'issue', 'representation', 'ML', 'algorithms', 'assume', 'two', 'nearby', 'values', 'similar', 'two', 'distant', 'values', '.', 'Obviously', 'case', 'example', 'categories', '0', '4', 'similar', 'categories', '0', '1', '.', 'To', 'fix', 'issue', 'common', 'solution', 'create', 'one', 'binary', 'attribute', 'per', 'category', 'one', 'attribute', 'equal', '1', 'category', 'ƒ', '<', '1H', 'OCEAN⁄', '0', 'otherwise', 'another', 'attribute', 'equal', '1', 'category', 'ƒINLAND⁄', '0', 'otherwise', '.', 'This', 'called', 'one-hot', 'encoding', 'one', 'attribute', 'equal', '1', 'hot', 'others', '0', 'cold', '.Scikit-Learn', 'provides', 'OneHotEncoder', 'encoder', 'convert', 'integer', 'categorical', 'values', 'one-hot', 'vectors', '.', 'Let‡s', 'encode', 'categories', 'one-hot', 'vectors', '.', 'Note', 'fit_transform', 'expects', '2D', 'array', 'housing_cat_encoded', '1D', 'array', 'need', 'reshape', '18', '>', '>', '>', 'sklearn.preprocessing', 'import', 'OneHotEncoder', '>', '>', '>', 'encoder', '=', 'OneHotEncoder', '>', '>', '>', 'housing_cat_1hot', '=', 'encoder.fit_transform', 'housing_cat_encoded.reshape', '-1,1', '>', '>', '>', 'housing_cat_1hot', '<', '16513x5', 'sparse', 'matrix', 'type', '•', '<', 'class', '•numpy.float64•', '>', '•', '16513', 'stored', 'elements', 'Compressed', 'Sparse', 'Row', 'format', '>', 'Notice', 'output', 'SciPy', 'sparse', 'matrix', 'instead', 'NumPy', 'array', '.', 'This', 'useful', 'categorical', 'attributes', 'thousands', 'categories', '.', 'After', 'one-', 'hot', 'encoding', 'get', 'matrix', 'thousands', 'columns', 'matrix', 'full', 'zeros', 'except', 'one', '1', 'per', 'row', '.', 'Using', 'tons', 'memory', 'mostly', 'store', 'zeros', 'would', 'wasteful', 'instead', 'sparse', 'matrix', 'stores', 'location', 'nonzero', 'elements', '.', 'You', 'use', 'mostly', 'like', 'normal', '2D', 'array', '19', 'really', 'want', 'con…', 'vert', 'dense', 'NumPy', 'array', 'call', 'toarray', 'method', '>', '>', '>', 'housing_cat_1hot.toarray', 'array', '0.', '1.', '0.', '0.', '0', '.', '0.', '1.', '0.', '0.', '0', '.', '0.', '0.', '0.', '0.', '1', '.', '...', '0.', '1.', '0.', '0.', '0', '.', 'Prepare', 'Data', 'Machine', 'Learning', 'Algorithms', '|', '63', '1.', '0.', '0.', '0.', '0', '.', '0.', '0.', '0.', '1.', '0', '.', 'We', 'apply', 'transformations', 'text', 'categories', 'integer', 'categories', 'integer', 'categories', 'one-hot', 'vectors', 'one', 'shot', 'using', 'LabelBinarizer', 'class', '>', '>', '>', 'sklearn.preprocessing', 'import', 'LabelBinarizer', '>', '>', '>', 'encoder', '=', 'LabelBinarizer', '>', '>', '>', 'housing_cat_1hot', '=', 'encoder.fit_transform', 'housing_cat', '>', '>', '>', 'housing_cat_1hotarray', '0', '1', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1', '...', '0', '1', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1', '0', 'Note', 'returns', 'dense', 'NumPy', 'array', 'default', '.', 'You', 'get', 'sparse', 'matrix', 'instead', 'passing', 'sparse_output=True', 'LabelBinarizer', 'constructor', '.', 'Custom', 'TransformersAlthough', 'Scikit-Learn', 'provides', 'many', 'useful', 'transformers', 'need', 'write', 'tasks', 'custom', 'cleanup', 'operations', 'combining', 'specific', 'attributes', '.', 'You', 'want', 'transformer', 'work', 'seamlessly', 'Scikit-Learn', 'func…', 'tionalities', 'pipelines', 'since', 'Scikit-Learn', 'relies', 'duck', 'typing', 'inher…itance', 'need', 'create', 'class', 'implement', 'three', 'methods', 'fit', 'returning', 'self', 'transform', 'fit_transform', '.', 'You', 'get', 'last', 'one', 'free', 'simply', 'adding', 'TransformerMixin', 'base', 'class', '.', 'Also', 'add', 'BaseEstimator', 'base', 'class', 'avoid', '*args', '**kargs', 'constructor', 'gettwo', 'extra', 'methods', 'get_params', 'set_params', 'useful', 'auto…', 'matic', 'hyperparameter', 'tuning', '.', 'For', 'example', 'small', 'transformer', 'class', 'adds', 'combined', 'attributes', 'discussed', 'earlier', 'sklearn.base', 'import', 'BaseEstimator', 'TransformerMixinrooms_ix', 'bedrooms_ix', 'population_ix', 'household_ix', '=', '3', '4', '5', '6class', 'CombinedAttributesAdder', 'BaseEstimator', 'TransformerMixin', 'def', '__init__', 'self', 'add_bedrooms_per_room', '=', 'True', '#', '*args', '**kargs', 'self.add_bedrooms_per_room', '=', 'add_bedrooms_per_room', 'def', 'fit', 'self', 'X', 'y=None', 'return', 'self', '#', 'nothing', 'else', 'def', 'transform', 'self', 'X', 'y=None', 'rooms_per_household', '=', 'X', 'rooms_ix', '/', 'X', 'household_ix', 'population_per_household', '=', 'X', 'population_ix', '/', 'X', 'household_ix', 'self.add_bedrooms_per_room', 'bedrooms_per_room', '=', 'X', 'bedrooms_ix', '/', 'X', 'rooms_ix', '64', '|', 'Chapter', '2', 'End-to-End', 'Machine', 'Learning', 'Project', 'return', 'np.c_', 'X', 'rooms_per_household', 'population_per_household', 'bedrooms_per_room', 'else', 'return', 'np.c_', 'X', 'rooms_per_household', 'population_per_household', 'attr_adder', '=', 'CombinedAttributesAdder', 'add_bedrooms_per_room=False', 'housing_extra_attribs', '=', 'attr_adder.transform', 'housing.values', 'In', 'example', 'transformer', 'one', 'hyperparameter', 'add_bedrooms_per_room', 'set', 'True', 'default', 'often', 'helpful', 'provide', 'sensible', 'defaults', '.', 'This', 'hyperpara…', 'meter', 'allow', 'easily', 'find', 'whether', 'adding', 'attribute', 'helps', 'Machine', 'Learning', 'algorithms', '.', 'More', 'generally', 'add', 'hyperparameter', 'gate', 'data', 'preparation', 'step', '100', '%', 'sure', '.', 'The', 'automate', 'data', 'preparation', 'steps', 'combinations', 'automatically', 'try', 'making', 'much', 'likely', 'find', 'great', 'combination', 'sav…', 'ing', 'lot', 'time', '.Feature', 'ScalingOne', 'important', 'transformations', 'need', 'apply', 'data', 'feature', 'scaling', '.', 'With', 'exceptions', 'Machine', 'Learning', 'algorithms', 'don‡t', 'perform', 'well', 'input', 'numerical', 'attributes', 'different', 'scales', '.', 'This', 'case', 'hous…', 'ing', 'data', 'total', 'number', 'rooms', 'ranges', '6', '39,320', 'median', 'incomes', 'range', '0', '15', '.', 'Note', 'scaling', 'target', 'values', 'generally', 'required.There', 'two', 'common', 'ways', 'get', 'attributes', 'scale', 'min-max', 'scaling', 'standardization', '.Min-max', 'scaling', 'many', 'people', 'call', 'normalization', 'quite', 'simple', 'values', 'shifted', 'rescaled', 'end', 'ranging', '0', '1', '.', 'We', 'subtract…', 'ing', 'min', 'value', 'dividing', 'max', 'minus', 'min', '.', 'Scikit-Learn', 'provides', 'transformer', 'called', 'MinMaxScaler', '.', 'It', 'feature_range', 'hyperparameter', 'lets', 'change', 'range', 'don‡t', 'want', '0–1', 'reason', '.', 'Standardization', 'quite', 'different', 'first', 'subtracts', 'mean', 'value', 'standardized', 'values', 'always', 'zero', 'mean', 'divides', 'variance', 'result…', 'ing', 'distribution', 'unit', 'variance', '.', 'Unlike', 'min-max', 'scaling', 'standardization', 'bound', 'values', 'specific', 'range', 'may', 'problem', 'algorithms', 'e.g.', 'neural', 'networks', 'often', 'expect', 'input', 'value', 'ranging', '0', '1', '.', 'However', 'standard…', 'ization', 'much', 'less', 'affected', 'outliers', '.', 'For', 'example', 'suppose', 'district', 'median', 'income', 'equal', '100', 'mistake', '.', 'Min-max', 'scaling', 'would', 'crush', 'othervalues', '0–15', '0–0.15', 'whereas', 'standardization', 'would', 'much', 'affec…', 'ted', '.', 'Scikit-Learn', 'provides', 'transformer', 'called', 'StandardScaler', 'standardization', '.', 'Prepare', 'Data', 'Machine', 'Learning', 'Algorithms', '|', '65', 'As', 'transformations', 'important', 'fit', 'scalers', 'training', 'data', 'full', 'dataset', 'including', 'test', 'set', '.', 'Only', 'use', 'transform', 'training', 'set', 'thetest', 'set', 'new', 'data', '.', 'Transformation', 'PipelinesAs', 'see', 'many', 'data', 'transformation', 'steps', 'need', 'executed', 'right', 'order', '.', 'Fortunately', 'Scikit-Learn', 'provides', 'Pipeline', 'class', 'help', 'withsuch', 'sequences', 'transformations', '.', 'Here', 'small', 'pipeline', 'numerical', 'attributes', 'sklearn.pipeline', 'import', 'Pipelinefrom', 'sklearn.preprocessing', 'import', 'StandardScalernum_pipeline', '=', 'Pipeline', '•imputer•', 'Imputer', 'strategy=', \"''\", 'median', \"''\", '•attribs_adder•', 'CombinedAttributesAdder', '•std_scaler•', 'StandardScaler', 'housing_num_tr', '=', 'num_pipeline.fit_transform', 'housing_num', 'The', 'Pipeline', 'constructor', 'takes', 'list', 'name/estimator', 'pairs', 'defining', 'sequence', 'steps', '.', 'All', 'last', 'estimator', 'must', 'transformers', 'i.e.', 'must', 'fit_transform', 'method', '.', 'The', 'names', 'anything', 'like', '.', 'When', 'call', 'pipeline‡s', 'fit', 'method', 'calls', 'fit_transform', 'sequentially', 'transformers', 'passing', 'output', 'call', 'parameter', 'next', 'call', 'reaches', 'final', 'estimator', 'calls', 'fit', 'method.The', 'pipeline', 'exposes', 'methods', 'final', 'estimator', '.', 'In', 'example', 'last', 'estimator', 'StandardScaler', 'transformer', 'pipeline', 'transform', 'method', 'applies', 'transforms', 'data', 'sequence', 'also', 'fit_transform', 'method', 'could', 'used', 'instead', 'calling', 'fit', 'thentransform', '.You', 'pipeline', 'numerical', 'values', 'also', 'need', 'apply', 'LabelBinarizer', 'categorical', 'values', 'join', 'transformations', 'sin…', 'gle', 'pipeline', '?', 'Scikit-Learn', 'provides', 'FeatureUnion', 'class', '.', 'You', 'give', 'list', 'transformers', 'entire', 'transformer', 'pipelines', 'transform', 'method', 'called', 'runs', 'transformer‡s', 'transform', 'method', 'parallel', 'waits', 'fortheir', 'output', 'concatenates', 'returns', 'result', 'course', 'calling', 'fit', 'method', 'calls', 'transformer‡s', 'fit', 'method', '.', 'A', 'full', 'pipeline', 'handlingboth', 'numerical', 'categorical', 'attributes', 'may', 'look', 'like', '66', '|', 'Chapter', '2', 'End-to-End', 'Machine', 'Learning', 'Project', '20But', 'check', 'Pull', 'Request', '#', '3886', 'may', 'introduce', 'ColumnTransformer', 'class', 'making', 'attribute-specific', 'transformations', 'easy', '.', 'You', 'could', 'also', 'run', 'pip3', 'install', 'sklearn-pandas', 'get', 'DataFrameMapper', 'class', 'witha', 'similar', 'objective.from', 'sklearn.pipeline', 'import', 'FeatureUnionnum_attribs', '=', 'list', 'housing_num', 'cat_attribs', '=', '``', 'ocean_proximity', \"''\", 'num_pipeline', '=', 'Pipeline', '•selector•', 'DataFrameSelector', 'num_attribs', '•imputer•', 'Imputer', 'strategy=', \"''\", 'median', \"''\", '•attribs_adder•', 'CombinedAttributesAdder', '•std_scaler•', 'StandardScaler', 'cat_pipeline', '=', 'Pipeline', '•selector•', 'DataFrameSelector', 'cat_attribs', '•label_binarizer•', 'LabelBinarizer', 'full_pipeline', '=', 'FeatureUnion', 'transformer_list=', '``', 'num_pipeline', \"''\", 'num_pipeline', '``', 'cat_pipeline', \"''\", 'cat_pipeline', 'And', 'run', 'whole', 'pipeline', 'simply', '>', '>', '>', 'housing_prepared', '=', 'full_pipeline.fit_transform', 'housing', '>', '>', '>', 'housing_preparedarray', '0.73225807', '-0.67331551', '0.58426443', '...', '0.', '0.', '0', '.', '-0.99102923', '1.63234656', '-0.92655887', '...', '0.', '0.', '0', '.', '...', '>', '>', '>', 'housing_prepared.shape', '16513', '17', 'Each', 'subpipeline', 'starts', 'selector', 'transformer', 'simply', 'transforms', 'data', 'selecting', 'desired', 'attributes', 'numerical', 'categorical', 'dropping', 'rest', 'con…', 'verting', 'resulting', 'DataFrame', 'NumPy', 'array', '.', 'There', 'nothing', 'Scikit-Learn', 'handle', 'Pandas', 'DataFrames', '20', 'need', 'write', 'simple', 'custom', 'transformer', 'forthis', 'task', 'sklearn.base', 'import', 'BaseEstimator', 'TransformerMixinclass', 'DataFrameSelector', 'BaseEstimator', 'TransformerMixin', 'def', '__init__', 'self', 'attribute_names', 'self.attribute_names', '=', 'attribute_names', 'def', 'fit', 'self', 'X', 'y=None', 'return', 'selfPrepare', 'Data', 'Machine', 'Learning', 'Algorithms', '|', '67', 'def', 'transform', 'self', 'X', 'return', 'X', 'self.attribute_names', '.valuesSelect', 'Train', 'ModelAt', 'last', '!', 'You', 'framed', 'problem', 'got', 'data', 'explored', 'sampled', 'training', 'set', 'test', 'set', 'wrote', 'transformation', 'pipelines', 'clean', 'prepare', 'data', 'Machine', 'Learning', 'algorithms', 'automatically', '.', 'You', 'ready', 'select', 'train', 'Machine', 'Learning', 'model', '.', 'Training', 'Evaluating', 'Training', 'SetThe', 'good', 'news', 'thanks', 'previous', 'steps', 'things', 'going', 'much', 'simpler', 'might', 'think', '.', 'Let‡s', 'first', 'train', 'Linear', 'Regression', 'model', 'like', 'previous', 'chapter', 'sklearn.linear_model', 'import', 'LinearRegressionlin_reg', '=', 'LinearRegression', 'lin_reg.fit', 'housing_prepared', 'housing_labels', 'Done', '!', 'You', 'working', 'Linear', 'Regression', 'model', '.', 'Let‡s', 'try', 'instances', 'training', 'set', '>', '>', '>', 'some_data', '=', 'housing.iloc', ':5', '>', '>', '>', 'some_labels', '=', 'housing_labels.iloc', ':5', '>', '>', '>', 'some_data_prepared', '=', 'full_pipeline.transform', 'some_data', '>', '>', '>', 'print', '``', 'Predictions', '\\\\t', \"''\", 'lin_reg.predict', 'some_data_prepared', 'Predictions', '303104', '.', '44800', '.', '308928', '.', '294208', '.', '368704', '.', '>', '>', '>', 'print', '``', 'Labels', '\\\\t\\\\t', \"''\", 'list', 'some_labels', 'Labels', '359400.0', '69700.0', '302100.0', '301300.0', '351900.0', 'It', 'works', 'although', 'predictions', 'exactly', 'accurate', 'e.g.', 'second', 'prediction', '50', '%', '!', '.', 'Let‡s', 'measure', 'regression', 'model‡s', 'RMSE', 'whole', 'training', 'set', 'using', 'Scikit-Learn‡s', 'mean_squared_error', 'function', '>', '>', '>', 'sklearn.metrics', 'import', 'mean_squared_error', '>', '>', '>', 'housing_predictions', '=', 'lin_reg.predict', 'housing_prepared', '>', '>', '>', 'lin_mse', '=', 'mean_squared_error', 'housing_labels', 'housing_predictions', '>', '>', '>', 'lin_rmse', '=', 'np.sqrt', 'lin_mse', '>', '>', '>', 'lin_rmse68628.413493824875Okay', 'better', 'nothing', 'clearly', 'great', 'score', 'districts‡', 'median_housing_values', 'range', '$', '120,000', '$', '265,000', 'typical', 'predic…tion', 'error', '$', '68,628', 'satisfying', '.', 'This', 'example', 'model', 'underfittingthe', 'training', 'data', '.', 'When', 'happens', 'mean', 'features', 'provide', 'enough', 'information', 'make', 'good', 'predictions', 'model', 'powerful', 'enough', '.', 'As', 'saw', 'previous', 'chapter', 'main', 'ways', 'fix', 'underfitting', '68', '|', 'Chapter', '2', 'End-to-End', 'Machine', 'Learning', 'Project', 'select', 'powerful', 'model', 'feed', 'training', 'algorithm', 'better', 'features', 'reduce', 'constraints', 'model', '.', 'This', 'model', 'regularized', 'rules', 'last', 'option', '.', 'You', 'could', 'try', 'add', 'features', 'e.g.', 'log', 'popula…', 'tion', 'first', 'let‡s', 'try', 'complex', 'model', 'see', '.', 'Let‡s', 'train', 'DecisionTreeRegressor', '.', 'This', 'powerful', 'model', 'capable', 'finding', 'complex', 'nonlinear', 'relationships', 'data', 'Decision', 'Trees', 'presented', 'detail', 'Chapter', '6', '.', 'The', 'code', 'look', 'familiar', 'sklearn.tree', 'import', 'DecisionTreeRegressortree_reg', '=', 'DecisionTreeRegressor', 'tree_reg.fit', 'housing_prepared', 'housing_labels', 'Now', 'model', 'trained', 'let‡s', 'evaluate', 'training', 'set', '>', '>', '>', 'housing_predictions', '=', 'tree_reg.predict', 'housing_prepared', '>', '>', '>', 'tree_mse', '=', 'mean_squared_error', 'housing_labels', 'housing_predictions', '>', '>', '>', 'tree_rmse', '=', 'np.sqrt', 'tree_mse', '>', '>', '>', 'tree_rmse0.0Wait', '!', '?', 'No', 'error', '?', 'Could', 'model', 'really', 'absolutely', 'perfect', '?', 'Of', 'course', 'much', 'likely', 'model', 'badly', 'overfit', 'data', '.', 'How', 'sure', '?', 'As', 'saw', 'earlier', 'don‡t', 'want', 'touch', 'test', 'set', 'ready', 'launch', 'model', 'confident', 'need', 'use', 'part', 'training', 'set', 'train…', 'ing', 'part', 'model', 'validation', '.', 'Better', 'Evaluation', 'Using', 'Cross-ValidationOne', 'way', 'evaluate', 'Decision', 'Tree', 'model', 'would', 'use', 'train_test_splitfunction', 'split', 'training', 'set', 'smaller', 'training', 'set', 'validation', 'set', 'train', 'models', 'smaller', 'training', 'set', 'evaluate', 'vali…', 'dation', 'set', '.', 'It‡s', 'bit', 'work', 'nothing', 'difficult', 'would', 'work', 'fairly', 'well', '.', 'A', 'great', 'alternative', 'use', 'Scikit-Learn‡s', 'cross-validation', 'feature', '.', 'The', 'following', 'code', 'performs', 'K-fold', 'cross-validation', 'randomly', 'splits', 'training', 'set', '10', 'distinct', 'subsets', 'called', 'folds', 'trains', 'evaluates', 'Decision', 'Tree', 'model', '10', 'times', 'picking', 'different', 'fold', 'evaluation', 'every', 'time', 'training', '9', 'folds', '.', 'The', 'result', 'array', 'containing', '10', 'evaluation', 'scores', 'sklearn.model_selection', 'import', 'cross_val_scorescores', '=', 'cross_val_score', 'tree_reg', 'housing_prepared', 'housing_labels', 'scoring=', \"''\", 'neg_mean_squared_error', \"''\", 'cv=10', 'rmse_scores', '=', 'np.sqrt', '-scores', 'Select', 'Train', 'Model', '|', '69', 'Scikit-Learn', 'cross-validation', 'features', 'expect', 'utility', 'function', 'greater', 'better', 'rather', 'cost', 'function', 'lower', 'better', 'scoring', 'function', 'actually', 'opposite', 'MSE', 'i.e.', 'neg…ative', 'value', 'preceding', 'code', 'computes', '-scoresbefore', 'calculating', 'square', 'root', '.', 'Let‡s', 'look', 'results', '>', '>', '>', 'def', 'display_scores', 'scores', '...', 'print', '``', 'Scores', \"''\", 'scores', '...', 'print', '``', 'Mean', \"''\", 'scores.mean', '...', 'print', '``', 'Standard', 'deviation', \"''\", 'scores.std', '...', '>', '>', '>', 'display_scores', 'tree_rmse_scores', 'Scores', '74678.4916885', '64766.2398337', '69632.86942005', '69166.67693232', '71486.76507766', '73321.65695983', '71860.04741226', '71086.32691692', '76934.2726093', '69060.93319262', 'Mean', '71199.4280043Standard', 'deviation', '3202.70522793Now', 'Decision', 'Tree', 'doesn‡t', 'look', 'good', 'earlier', '.', 'In', 'fact', 'seems', 'per…', 'form', 'worse', 'Linear', 'Regression', 'model', '!', 'Notice', 'cross-validation', 'allows', 'get', 'estimate', 'performance', 'model', 'also', 'measure', 'precise', 'estimate', 'i.e.', 'standard', 'deviation', '.', 'The', 'Decision', 'Tree', 'score', 'approximately', '71,200', 'generally', 'ﬁ3,200', '.', 'You', 'would', 'information', 'used', 'one', 'validation', 'set', '.', 'But', 'cross-validation', 'comes', 'cost', 'training', 'model', 'several', 'times', 'always', 'possible', '.', 'Let‡s', 'compute', 'scores', 'Linear', 'Regression', 'model', 'sure', '>', '>', '>', 'lin_scores', '=', 'cross_val_score', 'lin_reg', 'housing_prepared', 'housing_labels', '...', 'scoring=', \"''\", 'neg_mean_squared_error', \"''\", 'cv=10', '...', '>', '>', '>', 'lin_rmse_scores', '=', 'np.sqrt', '-lin_scores', '>', '>', '>', 'display_scores', 'lin_rmse_scores', 'Scores', '70423.5893262', '65804.84913139', '66620.84314068', '72510.11362141', '66414.74423281', '71958.89083606', '67624.90198297', '67825.36117664', '72512.36533141', '68028.11688067', 'Mean', '68972.377566Standard', 'deviation', '2493.98819069That‡s', 'right', 'Decision', 'Tree', 'model', 'overfitting', 'badly', 'performs', 'worse', 'Linear', 'Regression', 'model.Let‡s', 'try', 'one', 'last', 'model', 'RandomForestRegressor', '.', 'As', 'see', 'Chap…', 'ter', '7', 'Random', 'Forests', 'work', 'training', 'many', 'Decision', 'Trees', 'random', 'subsets', 'features', 'averaging', 'predictions', '.', 'Building', 'model', 'top', 'many', 'models', 'called', 'Ensemble', 'Learning', 'often', 'great', 'way', 'push', 'ML', 'algo…', 'rithms', 'even', '.', 'We', 'skip', 'code', 'since', 'essentially', 'models:70', '|', 'Chapter', '2', 'End-to-End', 'Machine', 'Learning', 'Project', '>', '>', '>', 'sklearn.ensemble', 'import', 'RandomForestRegressor', '>', '>', '>', 'forest_reg', '=', 'RandomForestRegressor', '>', '>', '>', 'forest_reg.fit', 'housing_prepared', 'housing_labels', '>', '>', '>', '...', '>', '>', '>', 'forest_rmse22542.396440343684', '>', '>', '>', 'display_scores', 'forest_rmse_scores', 'Scores', '53789.2879722', '50256.19806622', '52521.55342602', '53237.44937943', '52428.82176158', '55854.61222549', '52158.02291609', '50093.66125649', '53240.80406125', '52761.50852822', 'Mean', '52634.1919593Standard', 'deviation', '1576.20472269Wow', 'much', 'better', 'Random', 'Forests', 'look', 'promising', '.', 'However', 'note', 'score', 'training', 'set', 'still', 'much', 'lower', 'validation', 'sets', 'meaning', 'model', 'still', 'overfitting', 'training', 'set', '.', 'Possible', 'solutions', 'overfitting', 'simplify', 'model', 'constrain', 'i.e.', 'regularize', 'get', 'lot', 'training', 'data', '.', 'However', 'dive', 'much', 'deeper', 'Random', 'Forests', 'try', 'many', 'models', 'various', 'categories', 'Machine', 'Learning', 'algorithms', 'several', 'Sup…', 'port', 'Vector', 'Machines', 'different', 'kernels', 'possibly', 'neural', 'network', 'etc', '.', 'without', 'spending', 'much', 'time', 'tweaking', 'hyperparameters', '.', 'The', 'goal', 'shortlist', 'two', 'five', 'promising', 'models.You', 'save', 'every', 'model', 'experiment', 'come', 'back', 'easily', 'model', 'want', '.', 'Make', 'sure', 'save', 'hyperparameters', 'trained', 'parameters', 'well', 'cross-validation', 'scores', 'perhaps', 'actual', 'predictions', 'well', '.', 'This', 'allow', 'easily', 'compare', 'scores', 'across', 'model', 'types', 'compare', 'types', 'errors', 'make', '.', 'You', 'easily', 'save', 'Scikit-Learn', 'models', 'using', 'Python‡s', 'pickle', 'module', 'usingsklearn.externals.joblib', 'efficient', 'serializing', 'large', 'NumPy', 'arrays', 'sklearn.externals', 'import', 'joblibjoblib.dump', 'my_model', '``', 'my_model.pkl', \"''\", '#', 'later', '...', 'my_model_loaded', '=', 'joblib.load', '``', 'my_model.pkl', \"''\", 'Fine-Tune', 'Your', 'ModelLet‡s', 'assume', 'shortlist', 'promising', 'models', '.', 'You', 'need', 'fine-tune', '.', 'Let‡s', 'look', 'ways', '.', 'Fine-Tune', 'Your', 'Model', '|', '71', 'Grid', 'SearchOne', 'way', 'would', 'fiddle', 'hyperparameters', 'manually', 'find', 'great', 'combination', 'hyperparameter', 'values', '.', 'This', 'would', 'tedious', 'work', 'may', 'time', 'explore', 'many', 'combinations', '.', 'Instead', 'get', 'Scikit-Learn‡s', 'GridSearchCV', 'search', '.', 'All', 'need', 'todo', 'tell', 'hyperparameters', 'want', 'experiment', 'values', 'try', 'evaluate', 'possible', 'combinations', 'hyperparameter', 'values', 'using', 'cross-validation', '.', 'For', 'example', 'following', 'code', 'searches', 'best', 'combi…', 'nation', 'hyperparameter', 'values', 'RandomForestRegressor', 'sklearn.model_selection', 'import', 'GridSearchCVparam_grid', '=', '{', '•n_estimators•', '3', '10', '30', '•max_features•', '2', '4', '6', '8', '}', '{', '•bootstrap•', 'False', '•n_estimators•', '3', '10', '•max_features•', '2', '3', '4', '}', 'forest_reg', '=', 'RandomForestRegressor', 'grid_search', '=', 'GridSearchCV', 'forest_reg', 'param_grid', 'cv=5', 'scoring=•neg_mean_squared_error•', 'grid_search.fit', 'housing_prepared', 'housing_labels', 'When', 'idea', 'value', 'hyperparameter', 'simple', 'approach', 'try', 'consecutive', 'powers', '10', 'smaller', 'number', 'want', 'fine-grained', 'search', 'shown', 'example', 'n_estimators', 'hyperparameter', '.', 'This', 'param_grid', 'tells', 'Scikit-Learn', 'first', 'evaluate', '3', '‰', '4', '=', '12', 'combinations', 'n_estimators', 'max_features', 'hyperparameter', 'values', 'specified', 'first', 'dict', 'don‡t', 'worry', 'hyperparameters', 'mean', 'explained', 'Chapter', '7', 'try', '2', '‰', '3', '=', '6', 'combinations', 'hyperparameter', 'values', 'second', 'dict', 'time', 'bootstrap', 'hyperparameter', 'set', 'False', 'instead', 'True', 'default', 'value', 'hyperparameter', '.', 'All', 'grid', 'search', 'explore', '12', '+', '6', '=', '18', 'combinations', 'RandomForestRegressor', 'hyperparameter', 'values', 'train', 'model', 'five', 'times', 'since', 'using', 'five-fold', 'cross', 'validation', '.', 'In', 'words', '18', '‰', '5', '=', '90', 'rounds', 'training', '!', 'It', 'may', 'take', 'quite', 'long', 'time', 'done', 'get', 'best', 'combination', 'parameters', 'like', '>', '>', '>', 'grid_search.best_params_', '{', '•max_features•', '6', '•n_estimators•', '30', '}', '72', '|', 'Chapter', '2', 'End-to-End', 'Machine', 'Learning', 'Project', 'Since', '30', 'maximum', 'value', 'n_estimators', 'evalu…', 'ated', 'probably', 'evaluate', 'higher', 'values', 'well', 'since', 'score', 'may', 'continue', 'improve', '.', 'You', 'also', 'get', 'best', 'estimator', 'directly', '>', '>', '>', 'grid_search.best_estimator_RandomForestRegressor', 'bootstrap=True', 'criterion=•mse•', 'max_depth=None', 'max_features=6', 'max_leaf_nodes=None', 'min_samples_leaf=1', 'min_samples_split=2', 'min_weight_fraction_leaf=0.0', 'n_estimators=30', 'n_jobs=1', 'oob_score=False', 'random_state=None', 'verbose=0', 'warm_start=False', 'If', 'GridSearchCV', 'initialized', 'refit=True', 'thedefault', 'finds', 'best', 'estimator', 'using', 'cross-', 'validation', 'retrains', 'whole', 'training', 'set', '.', 'This', 'usually', 'good', 'idea', 'since', 'feeding', 'data', 'likely', 'improve', 'perfor…', 'mance.And', 'course', 'evaluation', 'scores', 'also', 'available', '>', '>', '>', 'cvres', '=', 'grid_search.cv_results_', '...', 'mean_score', 'params', 'zip', 'cvres', '``', 'mean_test_score', \"''\", 'cvres', '``', 'params', \"''\", '...', 'print', 'np.sqrt', '-mean_score', 'params', '...', '64912.0351358', '{', '•max_features•', '2', '•n_estimators•', '3', '}', '55535.2786524', '{', '•max_features•', '2', '•n_estimators•', '10', '}', '52940.2696165', '{', '•max_features•', '2', '•n_estimators•', '30', '}', '60384.0908354', '{', '•max_features•', '4', '•n_estimators•', '3', '}', '52709.9199934', '{', '•max_features•', '4', '•n_estimators•', '10', '}', '50503.5985321', '{', '•max_features•', '4', '•n_estimators•', '30', '}', '59058.1153485', '{', '•max_features•', '6', '•n_estimators•', '3', '}', '52172.0292957', '{', '•max_features•', '6', '•n_estimators•', '10', '}', '49958.9555932', '{', '•max_features•', '6', '•n_estimators•', '30', '}', '59122.260006', '{', '•max_features•', '8', '•n_estimators•', '3', '}', '52441.5896087', '{', '•max_features•', '8', '•n_estimators•', '10', '}', '50041.4899416', '{', '•max_features•', '8', '•n_estimators•', '30', '}', '62371.1221202', '{', '•bootstrap•', 'False', '•max_features•', '2', '•n_estimators•', '3', '}', '54572.2557534', '{', '•bootstrap•', 'False', '•max_features•', '2', '•n_estimators•', '10', '}', '59634.0533132', '{', '•bootstrap•', 'False', '•max_features•', '3', '•n_estimators•', '3', '}', '52456.0883904', '{', '•bootstrap•', 'False', '•max_features•', '3', '•n_estimators•', '10', '}', '58825.665239', '{', '•bootstrap•', 'False', '•max_features•', '4', '•n_estimators•', '3', '}', '52012.9945396', '{', '•bootstrap•', 'False', '•max_features•', '4', '•n_estimators•', '10', '}', 'In', 'example', 'obtain', 'best', 'solution', 'setting', 'max_features', 'hyperpara…', 'meter', '6', 'n_estimators', 'hyperparameter', '30', '.', 'The', 'RMSE', 'score', 'thiscombination', '49,959', 'slightly', 'better', 'score', 'got', 'earlier', 'using', 'Fine-Tune', 'Your', 'Model', '|', '73', 'default', 'hyperparameter', 'values', '52,634', '.', 'Congratulations', 'suc…', 'cessfully', 'fine-tuned', 'best', 'model', '!', 'Don‡t', 'forget', 'treat', 'data', 'preparation', 'steps', 'hyperparameters', '.', 'For', 'example', 'grid', 'search', 'automatically', 'find', 'whether', 'add', 'feature', 'sure', 'e.g.', 'using', 'add_bedrooms_per_room', 'hyperparameter', 'CombinedAttributesAdder', 'transformer', '.', 'It', 'may', 'similarly', 'used', 'automatically', 'find', 'best', 'way', 'handle', 'outliers', 'missing', 'fea…', 'tures', 'feature', 'selection', '.', 'Randomized', 'SearchThe', 'grid', 'search', 'approach', 'fine', 'exploring', 'relatively', 'combinations', 'like', 'previous', 'example', 'hyperparameter', 'search', 'space', 'large', 'isoften', 'preferable', 'use', 'RandomizedSearchCV', 'instead', '.', 'This', 'class', 'used', 'much', 'way', 'GridSearchCV', 'class', 'instead', 'trying', 'possible', 'combi…', 'nations', 'evaluates', 'given', 'number', 'random', 'combinations', 'selecting', 'random', 'value', 'hyperparameter', 'every', 'iteration', '.', 'This', 'approach', 'two', 'main', 'bene…', 'fits', '‹If', 'let', 'randomized', 'search', 'run', 'say', '1,000', 'iterations', 'approach', 'explore', '1,000', 'different', 'values', 'hyperparameter', 'instead', 'val…', 'ues', 'per', 'hyperparameter', 'grid', 'search', 'approach', '.', '‹You', 'control', 'computing', 'budget', 'want', 'allocate', 'hyper…', 'parameter', 'search', 'simply', 'setting', 'number', 'iterations', '.', 'Ensemble', 'MethodsAnother', 'way', 'fine-tune', 'system', 'try', 'combine', 'models', 'perform', 'best', '.', 'The', 'group', 'ƒensemble⁄', 'often', 'perform', 'better', 'best', 'individual', 'model', 'like', 'Random', 'Forests', 'perform', 'better', 'individual', 'Decision', 'Trees', 'rely', 'especially', 'individual', 'models', 'make', 'different', 'types', 'errors', '.', 'We', 'cover', 'topic', 'detail', 'Chapter', '7', '.Analyze', 'Best', 'Models', 'Their', 'ErrorsYou', 'often', 'gain', 'good', 'insights', 'problem', 'inspecting', 'best', 'models', '.', 'For', 'example', 'RandomForestRegressor', 'indicate', 'relative', 'importance', 'attribute', 'making', 'accurate', 'predictions', '>', '>', '>', 'feature_importances', '=', 'grid_search.best_estimator_.feature_importances_', '>', '>', '>', 'feature_importancesarray', '7.14156423e-02', '6.76139189e-02', '4.44260894e-02,74', '|', 'Chapter', '2', 'End-to-End', 'Machine', 'Learning', 'Project', '1.66308583e-02', '1.66076861e-02', '1.82402545e-02', '1.63458761e-02', '3.26497987e-01', '6.04365775e-02', '1.13055290e-01', '7.79324766e-02', '1.12166442e-02', '1.53344918e-01', '8.41308969e-05', '2.68483884e-03', '3.46681181e-03', 'Let‡s', 'display', 'importance', 'scores', 'next', 'corresponding', 'attribute', 'names', '>', '>', '>', 'extra_attribs', '=', '``', 'rooms_per_hhold', \"''\", '``', 'pop_per_hhold', \"''\", '``', 'bedrooms_per_room', \"''\", '>', '>', '>', 'cat_one_hot_attribs', '=', 'list', 'encoder.classes_', '>', '>', '>', 'attributes', '=', 'num_attribs', '+', 'extra_attribs', '+', 'cat_one_hot_attribs', '>', '>', '>', 'sorted', 'zip', 'feature_importances', 'attributes', 'reverse=True', '0.32649798665134971', '•median_income•', '0.15334491760305854', '•INLAND•', '0.11305529021187399', '•pop_per_hhold•', '0.07793247662544775', '•bedrooms_per_room•', '0.071415642259275158', '•longitude•', '0.067613918945568688', '•latitude•', '0.060436577499703222', '•rooms_per_hhold•', '0.04442608939578685', '•housing_median_age•', '0.018240254462909437', '•population•', '0.01663085833886218', '•total_rooms•', '0.016607686091288865', '•total_bedrooms•', '0.016345876147580776', '•households•', '0.011216644219017424', '•', '<', '1H', 'OCEAN•', '0.0034668118081117387', '•NEAR', 'OCEAN•', '0.0026848388432755429', '•NEAR', 'BAY•', '8.4130896890070617e-05', '•ISLAND•', 'With', 'information', 'may', 'want', 'try', 'dropping', 'less', 'useful', 'features', 'e.g.', 'apparently', 'one', 'ocean_proximity', 'category', 'really', 'useful', 'could', 'try', 'dropping', 'others', '.You', 'also', 'look', 'specific', 'errors', 'system', 'makes', 'try', 'under…', 'stand', 'makes', 'could', 'fix', 'problem', 'adding', 'extra', 'features', 'contrary', 'getting', 'rid', 'uninformative', 'ones', 'cleaning', 'outliers', 'etc.', '.', 'Evaluate', 'Your', 'System', 'Test', 'SetAfter', 'tweaking', 'models', 'eventually', 'system', 'performs', 'sufficiently', 'well', '.', 'Now', 'time', 'evaluate', 'final', 'model', 'test', 'set', '.', 'There', 'nothing', 'special', 'process', 'get', 'predictors', 'labels', 'yourtest', 'set', 'run', 'full_pipeline', 'transform', 'data', 'call', 'transform', 'fit_transform', '!', 'evaluate', 'final', 'model', 'test', 'set', 'final_model', '=', 'grid_search.best_estimator_X_test', '=', 'strat_test_set.drop', '``', 'median_house_value', \"''\", 'axis=1', 'y_test', '=', 'strat_test_set', '``', 'median_house_value', \"''\", '.copy', 'X_test_prepared', '=', 'full_pipeline.transform', 'X_test', 'Fine-Tune', 'Your', 'Model', '|', '75', 'final_predictions', '=', 'final_model.predict', 'X_test_prepared', 'final_mse', '=', 'mean_squared_error', 'y_test', 'final_predictions', 'final_rmse', '=', 'np.sqrt', 'final_mse', '#', '=', '>', 'evaluates', '48,209.6The', 'performance', 'usually', 'slightly', 'worse', 'measured', 'using', 'cross-', 'validation', 'lot', 'hyperparameter', 'tuning', 'system', 'ends', 'fine-tuned', 'perform', 'well', 'validation', 'data', 'likely', 'perform', 'well', 'unknown', 'datasets', '.', 'It', 'case', 'example', 'happens', 'must', 'resist', 'temptation', 'tweak', 'hyperparameters', 'make', 'numbers', 'look', 'good', 'test', 'set', 'improvements', 'would', 'unlikely', 'generalize', 'new', 'data', '.', 'Now', 'comes', 'project', 'prelaunch', 'phase', 'need', 'present', 'solution', 'high…', 'lighting', 'learned', 'worked', 'assumptions', 'made', 'system‡s', 'limitations', 'document', 'everything', 'create', 'nice', 'presentations', 'clear', 'visualizations', 'easy-to-remember', 'statements', 'e.g.', 'ƒthe', 'median', 'income', 'number', 'one', 'predictor', 'housing', 'prices⁄', '.', 'Launch', 'Monitor', 'Maintain', 'Your', 'SystemPerfect', 'got', 'approval', 'launch', '!', 'You', 'need', 'get', 'solution', 'ready', 'produc…', 'tion', 'particular', 'plugging', 'production', 'input', 'data', 'sources', 'system', 'writing', 'tests.You', 'also', 'need', 'write', 'monitoring', 'code', 'check', 'system‡s', 'live', 'performance', 'regular', 'intervals', 'trigger', 'alerts', 'drops', '.', 'This', 'important', 'catch', 'sudden', 'breakage', 'also', 'performance', 'degradation', '.', 'This', 'quite', 'common', 'models', 'tend', 'ƒrot⁄', 'data', 'evolves', 'time', 'unless', 'models', 'regularly', 'trained', 'fresh', 'data', '.', 'Evaluating', 'system‡s', 'performance', 'require', 'sampling', 'system‡s', 'predictions', 'evaluating', '.', 'This', 'generally', 'require', 'human', 'analysis', '.', 'These', 'analysts', 'may', 'field', 'experts', 'workers', 'crowdsourcing', 'platform', 'Amazon', 'Mechanical', 'Turk', 'CrowdFlower', '.', 'Either', 'way', 'need', 'plug', 'human', 'evalua…', 'tion', 'pipeline', 'system', '.', 'You', 'also', 'make', 'sure', 'evaluate', 'system‡s', 'input', 'data', 'quality', '.', 'Sometimes', 'performance', 'degrade', 'slightly', 'poor', 'quality', 'signal', 'e.g.', 'malfunc…', 'tioning', 'sensor', 'sending', 'random', 'values', 'another', 'team‡s', 'output', 'becoming', 'stale', 'may', 'take', 'system‡s', 'performance', 'degrades', 'enough', 'trigger', 'alert', '.', 'If', 'monitor', 'system‡s', 'inputs', 'may', 'catch', 'earlier', '.', 'Monitoring', 'inputs', 'particularly', 'important', 'online', 'learning', 'systems', '.', 'Finally', 'generally', 'want', 'train', 'models', 'regular', 'basis', 'using', 'fresh', 'data', '.', 'You', 'automate', 'process', 'much', 'possible', '.', 'If', 'don‡t', '76', '|', 'Chapter', '2', 'End-to-End', 'Machine', 'Learning', 'Project', 'likely', 'refresh', 'model', 'every', 'six', 'months', 'best', 'system‡s', 'perfor…', 'mance', 'may', 'fluctuate', 'severely', 'time', '.', 'If', 'system', 'online', 'learning', 'system', 'make', 'sure', 'save', 'snapshots', 'state', 'regular', 'intervals', 'easily', 'roll', 'back', 'previously', 'working', 'state', '.', 'Try', 'It', 'Out', '!', 'Hopefully', 'chapter', 'gave', 'good', 'idea', 'Machine', 'Learning', 'project', 'looks', 'like', 'showed', 'tools', 'use', 'train', 'great', 'system', '.', 'As', 'see', 'much', 'work', 'data', 'preparation', 'step', 'building', 'monitoring', 'tools', 'setting', 'human', 'evaluation', 'pipelines', 'automating', 'regular', 'model', 'training', '.', 'The', 'Machine', 'Learning', 'algorithms', 'also', 'important', 'course', 'probably', 'preferable', 'comfortable', 'overall', 'process', 'know', 'three', 'four', 'algo…rithms', 'well', 'rather', 'spend', 'time', 'exploring', 'advanced', 'algorithms', 'enough', 'time', 'overall', 'process.So', 'already', 'done', 'good', 'time', 'pick', 'laptop', 'select', 'dataset', 'interested', 'try', 'go', 'whole', 'process', 'A', 'Z', '.', 'A', 'good', 'place', 'start', 'competition', 'website', 'http', '//kaggle.com/', 'youwill', 'dataset', 'play', 'clear', 'goal', 'people', 'share', 'experience', '.', 'ExercisesUsing', 'chapter‡s', 'housing', 'dataset', '1.Try', 'Support', 'Vector', 'Machine', 'regressor', 'sklearn.svm.SVR', 'various', 'hyper…', 'parameters', 'kernel=', \"''\", 'linear', \"''\", 'various', 'values', 'C', 'hyperpara…', 'meter', 'kernel=', \"''\", 'rbf', \"''\", 'various', 'values', 'C', 'gammahyperparameters', '.', 'Don‡t', 'worry', 'hyperparameters', 'mean', '.', 'How', 'best', 'SVR', 'predictor', 'perform', '?', '2.Try', 'replacing', 'GridSearchCV', 'RandomizedSearchCV.3.Try', 'adding', 'transformer', 'preparation', 'pipeline', 'select', 'important', 'attributes', '.', '4.Try', 'creating', 'single', 'pipeline', 'full', 'data', 'preparation', 'plus', 'final', 'prediction.5.Automatically', 'explore', 'preparation', 'options', 'using', 'GridSearchCV.Solutions', 'exercises', 'available', 'online', 'Jupyter', 'notebooks', 'https', '//', 'github.com/ageron/handson-ml', '.Try', 'It', 'Out', '!', '|', '77', '1By', 'default', 'Scikit-Learn', 'caches', 'downloaded', 'datasets', 'directory', 'called', '$', 'HOME/scikit_learn_data', '.CHAPTER', '3Classi•cationIn', 'Chapter', '1', 'mentioned', 'common', 'supervised', 'learning', 'tasks', 'regression', 'predicting', 'values', 'classification', 'predicting', 'classes', '.', 'In', 'Chapter', '2', 'weexplored', 'regression', 'task', 'predicting', 'housing', 'values', 'using', 'various', 'algorithms', 'suchas', 'Linear', 'Regression', 'Decision', 'Trees', 'Random', 'Forests', 'explained', 'detail', 'later', 'chapters', '.', 'Now', 'turn', 'attention', 'classification', 'systems.MNISTIn', 'chapter', 'using', 'MNIST', 'dataset', 'set', '70,000', 'small', 'images', 'digits', 'handwritten', 'high', 'school', 'students', 'employees', 'US', 'Cen…', 'sus', 'Bureau', '.', 'Each', 'image', 'labeled', 'digit', 'represents', '.', 'This', 'set', 'stud…', 'ied', 'much', 'often', 'called', 'ƒHello', 'World⁄', 'Machine', 'Learning', 'whenever', 'people', 'come', 'new', 'classification', 'algorithm', 'curious', 'see', 'perform', 'MNIST', '.', 'Whenever', 'someone', 'learns', 'Machine', 'Learning', 'sooner', 'later', 'tackle', 'MNIST', '.', 'Scikit-Learn', 'provides', 'many', 'helper', 'functions', 'download', 'popular', 'datasets', '.', 'MNIST', 'one', '.', 'The', 'following', 'code', 'fetches', 'MNIST', 'dataset', '1', '>', '>', '>', 'sklearn.datasets', 'import', 'fetch_mldata', '>', '>', '>', 'mnist', '=', 'fetch_mldata', '•MNIST', 'original•', '>', '>', '>', 'mnist', '{', '•COL_NAMES•', '•label•', '•data•', '•DESCR•', '•mldata.org', 'dataset', 'mnist-original•', '•data•', 'array', '0', '0', '0', '...', '0', '0', '0', '0', '0', '0', '...', '0', '0', '0', ',79', '0', '0', '0', '...', '0', '0', '0', '...', '0', '0', '0', '...', '0', '0', '0', '0', '0', '0', '...', '0', '0', '0', '0', '0', '0', '...', '0', '0', '0', 'dtype=uint8', '•target•', 'array', '0.', '0.', '0.', '...', '9.', '9.', '9', '.', '}', 'Datasets', 'loaded', 'Scikit-Learn', 'generally', 'similar', 'dictionary', 'structure', 'includ…', 'ing', '‹A', 'DESCR', 'key', 'describing', 'dataset', '‹A', 'data', 'key', 'containing', 'array', 'one', 'row', 'per', 'instance', 'one', 'column', 'per', 'feature', '‹A', 'target', 'key', 'containing', 'array', 'labels', 'Let‡s', 'look', 'arrays', '>', '>', '>', 'X', '=', 'mnist', '``', 'data', \"''\", 'mnist', '``', 'target', \"''\", '>', '>', '>', 'X.shape', '70000', '784', '>', '>', '>', 'y.shape', '70000', 'There', '70,000', 'images', 'image', '784', 'features', '.', 'This', 'image', '28‰28', 'pixels', 'feature', 'simply', 'represents', 'one', 'pixel‡s', 'intensity', '0', 'white', '255', 'black', '.', 'Let‡s', 'take', 'peek', 'one', 'digit', 'dataset', '.', 'All', 'need', 'grab', 'instance‡s', 'feature', 'vector', 'reshape', '28‰28', 'array', 'display', 'using', 'Matplotlib‡s', 'imshow', 'function', '%', 'matplotlib', 'inlineimport', 'matplotlibimport', 'matplotlib.pyplot', 'pltsome_digit', '=', 'X', '36000', 'some_digit_image', '=', 'some_digit.reshape', '28', '28', 'plt.imshow', 'some_digit_image', 'cmap', '=', 'matplotlib.cm.binary', 'interpolation=', \"''\", 'nearest', \"''\", 'plt.axis', '``', \"''\", 'plt.show', 'This', 'looks', 'like', '5', 'indeed', 'that‡s', 'label', 'tells', 'us', '80', '|', 'Chapter', '3', 'Classi•cation2Shuffling', 'may', 'bad', 'idea', 'contexts›for', 'example', 'working', 'time', 'series', 'data', 'stock', 'market', 'prices', 'weather', 'conditions', '.', 'We', 'explore', 'next', 'chapters', '.', '>', '>', '>', '36000', '5.0Figure', '3-1', 'shows', 'images', 'MNIST', 'dataset', 'give', 'feel', 'complexity', 'classification', 'task', '.', 'Figure', '3-1', '.', 'A', 'digits', 'MNIST', 'dataset', 'But', 'wait', '!', 'You', 'always', 'create', 'test', 'set', 'set', 'aside', 'inspecting', 'data', 'closely', '.', 'The', 'MNIST', 'dataset', 'actually', 'already', 'split', 'training', 'set', 'first', '60,000', 'images', 'test', 'set', 'last', '10,000', 'images', 'X_train', 'X_test', 'y_train', 'y_test', '=', 'X', ':60000', 'X', '60000', ':60000', '60000', 'Let‡s', 'also', 'shuffle', 'training', 'set', 'guarantee', 'cross-validation', 'folds', 'willbe', 'similar', 'don‡t', 'want', 'one', 'fold', 'missing', 'digits', '.', 'Moreover', 'learn…', 'ing', 'algorithms', 'sensitive', 'order', 'training', 'instances', 'performpoorly', 'get', 'many', 'similar', 'instances', 'row', '.', 'Shuffling', 'dataset', 'ensures', 'won‡t', 'happen', '2MNIST', '|', '81', 'import', 'numpy', 'npshuffle_index', '=', 'np.random.permutation', '60000', 'X_train', 'y_train', '=', 'X_train', 'shuffle_index', 'y_train', 'shuffle_index', 'Training', 'Binary', 'Classi•erLet‡s', 'simplify', 'problem', 'try', 'identify', 'one', 'digit›for', 'example', 'number', '5', '.', 'This', 'ƒ5-detector⁄', 'example', 'binary', 'classi†er', 'capable', 'distinguishing', 'two', 'classes', '5', 'not-5', '.', 'Let‡s', 'create', 'target', 'vectors', 'classification', 'task', 'y_train_5', '=', 'y_train', '==', '5', '#', 'True', '5s', 'False', 'digits.y_test_5', '=', 'y_test', '==', '5', 'Okay', 'let‡s', 'pick', 'classifier', 'train', '.', 'A', 'good', 'place', 'start', 'Stochastic', 'Gradient', 'Descent', 'SGD', 'classifier', 'using', 'Scikit-Learn‡s', 'SGDClassifier', 'class', '.', 'This', 'clas…', 'sifier', 'advantage', 'capable', 'handling', 'large', 'datasets', 'efficiently', '.', 'This', 'part', 'SGD', 'deals', 'training', 'instances', 'independently', 'one', 'time', 'also', 'makes', 'SGD', 'well', 'suited', 'online', 'learning', 'see', 'later', '.', 'Let‡s', 'create', 'SGDClassifier', 'train', 'whole', 'training', 'set', 'sklearn.linear_model', 'import', 'SGDClassifiersgd_clf', '=', 'SGDClassifier', 'random_state=42', 'sgd_clf.fit', 'X_train', 'y_train_5', 'The', 'SGDClassifier', 'relies', 'randomness', 'training', 'hencethe', 'name', 'ƒstochastic⁄', '.', 'If', 'want', 'reproducible', 'results', 'set', 'random_state', 'parameter', '.', 'Now', 'use', 'detect', 'images', 'number', '5', '>', '>', '>', 'sgd_clf.predict', 'some_digit', 'array', 'True', 'dtype=bool', 'The', 'classifier', 'guesses', 'image', 'represents', '5', 'True', '.', 'Looks', 'like', 'guessed', 'right', 'particular', 'case', '!', 'Now', 'let‡s', 'evaluate', 'model‡s', 'performance', '.', 'Performance', 'MeasuresEvaluating', 'classifier', 'often', 'significantly', 'trickier', 'evaluating', 'regressor', 'spend', 'large', 'part', 'chapter', 'topic', '.', 'There', 'many', 'performance', 'measures', 'available', 'grab', 'another', 'coffee', 'get', 'ready', 'learn', 'many', 'new', 'concepts', 'acronyms', '!', '82', '|', 'Chapter', '3', 'Classi•cationMeasuring', 'Accuracy', 'Using', 'Cross-ValidationA', 'good', 'way', 'evaluate', 'model', 'use', 'cross-validation', 'Chap…', 'ter', '2.Implementing', 'Cross-ValidationOccasionally', 'need', 'control', 'cross-validation', 'process', 'cross_val_score', 'similar', 'functions', 'provide', '.', 'In', 'cases', 'implement', 'cross-validation', 'actually', 'fairly', 'straightforward', '.', 'The', 'following', 'code', 'roughly', 'thing', 'preceding', 'cross_val_score', 'code', 'prints', 'result', 'sklearn.model_selection', 'import', 'StratifiedKFoldfrom', 'sklearn.base', 'import', 'cloneskfolds', '=', 'StratifiedKFold', 'n_splits=3', 'random_state=42', 'train_index', 'test_index', 'skfolds.split', 'X_train', 'y_train_5', 'clone_clf', '=', 'clone', 'sgd_clf', 'X_train_folds', '=', 'X_train', 'train_index', 'y_train_folds', '=', 'y_train_5', 'train_index', 'X_test_fold', '=', 'X_train', 'test_index', 'y_test_fold', '=', 'y_train_5', 'test_index', 'clone_clf.fit', 'X_train_folds', 'y_train_folds', 'y_pred', '=', 'clone_clf.predict', 'X_test_fold', 'n_correct', '=', 'sum', 'y_pred', '==', 'y_test_fold', 'print', 'n_correct', '/', 'len', 'y_pred', '#', 'prints', '0.9502', '0.96565', '0.96495The', 'StratifiedKFold', 'class', 'performs', 'stratified', 'sampling', 'explained', 'Chapter', '2', 'produce', 'folds', 'contain', 'representative', 'ratio', 'class', '.', 'At', 'iteration', 'code', 'creates', 'clone', 'classifier', 'trains', 'clone', 'training', 'folds', 'makes', 'predictions', 'test', 'fold', '.', 'Then', 'counts', 'number', 'correct', 'predictions', 'outputs', 'ratio', 'correct', 'predictions', '.', 'Let‡s', 'use', 'cross_val_score', 'function', 'evaluate', 'SGDClassifier', 'modelusing', 'K-fold', 'cross-validation', 'three', 'folds', '.', 'Remember', 'K-fold', 'cross-', 'validation', 'means', 'splitting', 'training', 'set', 'K-folds', 'case', 'three', 'mak…', 'ing', 'predictions', 'evaluating', 'fold', 'using', 'model', 'trained', 'remaining', 'folds', 'see', 'Chapter', '2', '>', '>', '>', 'sklearn.model_selection', 'import', 'cross_val_score', '>', '>', '>', 'cross_val_score', 'sgd_clf', 'X_train', 'y_train_5', 'cv=3', 'scoring=', \"''\", 'accuracy', \"''\", 'array', '0.9502', '0.96565', '0.96495', 'Performance', 'Measures', '|', '83', 'Wow', '!', 'Above', '95', '%', 'accuracy', 'ratio', 'correct', 'predictions', 'cross-validation', 'folds', '?', 'This', 'looks', 'amazing', 'doesn‡t', '?', 'Well', 'get', 'excited', 'let‡s', 'look', 'dumb', 'classifier', 'classifies', 'every', 'single', 'image', 'ƒnot-5⁄', 'class', 'sklearn.base', 'import', 'BaseEstimatorclass', 'Never5Classifier', 'BaseEstimator', 'def', 'fit', 'self', 'X', 'y=None', 'pass', 'def', 'predict', 'self', 'X', 'return', 'np.zeros', 'len', 'X', '1', 'dtype=bool', 'Can', 'guess', 'model‡s', 'accuracy', '?', 'Let‡s', 'find', '>', '>', '>', 'never_5_clf', '=', 'Never5Classifier', '>', '>', '>', 'cross_val_score', 'never_5_clf', 'X_train', 'y_train_5', 'cv=3', 'scoring=', \"''\", 'accuracy', \"''\", 'array', '0.909', '0.90715', '0.9128', 'That‡s', 'right', '90', '%', 'accuracy', '!', 'This', 'simply', '10', '%', 'images', '5s', 'always', 'guess', 'image', '5', 'right', '90', '%', 'time', '.', 'Beats', 'Nostradamus', '.', 'This', 'demonstrates', 'accuracy', 'generally', 'preferred', 'performance', 'measure', 'classifiers', 'especially', 'dealing', 'skewed', 'datasets', 'i.e.', 'someclasses', 'much', 'frequent', 'others', '.', 'Confusion', 'MatrixA', 'much', 'better', 'way', 'evaluate', 'performance', 'classifier', 'look', 'confu…', 'sion', 'matrix', '.', 'The', 'general', 'idea', 'count', 'number', 'times', 'instances', 'class', 'A', 'classified', 'class', 'B', '.', 'For', 'example', 'know', 'number', 'times', 'classifier', 'confused', 'images', '5s', '3s', 'would', 'look', '5th', 'row', '3rd', 'column', 'confusionmatrix', '.', 'To', 'compute', 'confusion', 'matrix', 'first', 'need', 'set', 'predictions', 'compared', 'actual', 'targets', '.', 'You', 'could', 'make', 'predictions', 'test', 'set', 'let‡s', 'keep', 'untouched', 'remember', 'want', 'use', 'test', 'set', 'end', 'project', 'classifier', 'ready', 'launch', '.', 'Instead', 'use', 'cross_val_predict', 'function', 'sklearn.model_selection', 'import', 'cross_val_predicty_train_pred', '=', 'cross_val_predict', 'sgd_clf', 'X_train', 'y_train_5', 'cv=3', 'Just', 'like', 'cross_val_score', 'function', 'cross_val_predict', 'performs', 'K-foldcross-validation', 'instead', 'returning', 'evaluation', 'scores', 'returns', 'predic…', 'tions', 'made', 'test', 'fold', '.', 'This', 'means', 'get', 'clean', 'prediction', 'instance', 'training', 'set', 'ƒclean⁄', 'meaning', 'prediction', 'made', 'model', 'never', 'saw', 'data', 'training', '.', '84', '|', 'Chapter', '3', 'Classi•cationNow', 'ready', 'get', 'confusion', 'matrix', 'using', 'confusion_matrix', 'func…tion', '.', 'Just', 'pass', 'target', 'classes', 'y_train_5', 'predicted', 'classes', 'y_train_pred', '>', '>', '>', 'sklearn.metrics', 'import', 'confusion_matrix', '>', '>', '>', 'confusion_matrix', 'y_train_5', 'y_train_pred', 'array', '53272', '1307', '1077', '4344', 'Each', 'row', 'confusion', 'matrix', 'represents', 'actual', 'class', 'column', 'repre…sents', 'predicted', 'class', '.', 'The', 'first', 'row', 'matrix', 'considers', 'non-5', 'images', 'nega…', 'tive', 'class', '53,272', 'correctly', 'classified', 'non-5s', 'called', 'true', 'negatives', 'remaining', '1,307', 'wrongly', 'classified', '5s', 'false', 'positives', '.The', 'second', 'row', 'considers', 'images', '5s', 'positive', 'class', '1,077', 'wronglyclassified', 'non-5s', 'false', 'negatives', 'remaining', '4,344', 'correctly', 'classi…fied', '5s', 'true', 'positives', '.', 'A', 'perfect', 'classifier', 'would', 'true', 'positives', 'true', 'negatives', 'confusion', 'matrix', 'would', 'nonzero', 'values', 'main', 'diago…', 'nal', 'top', 'left', 'bottom', 'right', '>', '>', '>', 'confusion_matrix', 'y_train_5', 'y_train_perfect_predictions', 'array', '54579', '0', '0', '5421', 'The', 'confusion', 'matrix', 'gives', 'lot', 'information', 'sometimes', 'may', 'prefer', 'concise', 'metric', '.', 'An', 'interesting', 'one', 'look', 'accuracy', 'positive', 'pre…', 'dictions', 'called', 'precision', 'classifier', 'Equation', '3-1', '.Equation', '3-1', '.', 'Precision', 'precision=', 'TP', 'TP', '+FP', 'TP', 'number', 'true', 'positives', 'FP', 'number', 'false', 'positives', '.', 'A', 'trivial', 'way', 'perfect', 'precision', 'make', 'one', 'single', 'positive', 'prediction', 'ensure', 'correct', 'precision', '=', '1/1', '=', '100', '%', '.', 'This', 'would', 'useful', 'since', 'classifier', 'would', 'ignore', 'one', 'positive', 'instance', '.', 'So', 'precision', 'typically', 'usedalong', 'another', 'metric', 'named', 'recall', 'also', 'called', 'sensitivity', 'true', 'positive', 'rate', 'TPR', 'ratio', 'positive', 'instances', 'correctly', 'detected', 'classifier', 'Equation', '3-2', '.Equation', '3-2', '.', 'Recall', 'recall=', 'TP', 'TP', '+FN', 'FN', 'course', 'number', 'false', 'negatives', '.', 'Performance', 'Measures', '|', '85', 'If', 'confused', 'confusion', 'matrix', 'Figure', '3-2', 'may', 'help', '.', 'Figure', '3-2', '.', 'An', 'illustrated', 'confusion', 'matrix', 'Precision', 'RecallScikit-Learn', 'provides', 'several', 'functions', 'compute', 'classifier', 'metrics', 'including', 'preci…sion', 'recall', '>', '>', '>', 'sklearn.metrics', 'import', 'precision_score', 'recall_score', '>', '>', '>', 'precision_score', 'y_train_5', 'y_pred', '#', '==', '4344', '/', '4344', '+', '1307', '0.76871350203503808', '>', '>', '>', 'recall_score', 'y_train_5', 'y_train_pred', '#', '==', '4344', '/', '4344', '+', '1077', '0.79136690647482011Now', '5-detector', 'look', 'shiny', 'looked', 'accuracy', '.', 'When', 'claims', 'image', 'represents', '5', 'correct', '77', '%', 'time', '.', 'Moreover', 'detects', '79', '%', '5s.It', 'often', 'convenient', 'combine', 'precision', 'recall', 'single', 'metric', 'called', 'F1score', 'particular', 'need', 'simple', 'way', 'compare', 'two', 'classifiers', '.', 'The', 'F', '1', 'score', 'harmonic', 'mean', 'precision', 'recall', 'Equation', '3-3', '.', 'Whereas', 'regular', 'meantreats', 'values', 'equally', 'harmonic', 'mean', 'gives', 'much', 'weight', 'low', 'values', '.', 'As', 'result', 'classifier', 'get', 'high', 'F1', 'score', 'recall', 'precision', 'arehigh.Equation', '3-3', '.', 'F', '1', 'score', 'F1=21precision+1recall=2‰', 'precision‰recall', 'precision+recall', '=TP', 'TP', '+FN', '+FP', '286', '|', 'Chapter', '3', 'Classi•cationTo', 'compute', 'F', '1', 'score', 'simply', 'call', 'f1_score', 'function', '>', '>', '>', 'sklearn.metrics', 'import', 'f1_score', '>', '>', '>', 'f1_score', 'y_train_5', 'y_pred', '0.78468208092485547The', 'F1', 'score', 'favors', 'classifiers', 'similar', 'precision', 'recall', '.', 'This', 'always', 'want', 'contexts', 'mostly', 'care', 'precision', 'con…', 'texts', 'really', 'care', 'recall', '.', 'For', 'example', 'trained', 'classifier', 'detect', 'vid…', 'eos', 'safe', 'kids', 'would', 'probably', 'prefer', 'classifier', 'rejects', 'many', 'good', 'videos', 'low', 'recall', 'keeps', 'safe', 'ones', 'high', 'precision', 'rather', 'clas…', 'sifier', 'much', 'higher', 'recall', 'lets', 'really', 'bad', 'videos', 'show', 'product', 'cases', 'may', 'even', 'want', 'add', 'human', 'pipeline', 'check', 'clas…', 'sifier‡s', 'video', 'selection', '.', 'On', 'hand', 'suppose', 'train', 'classifier', 'detect', 'shoplifters', 'surveillance', 'images', 'probably', 'fine', 'classifier', '30', '%', 'precision', 'long', '99', '%', 'recall', 'sure', 'security', 'guards', 'get', 'falsealerts', 'almost', 'shoplifters', 'get', 'caught', '.', 'Unfortunately', 'can‡t', 'ways', 'increasing', 'precision', 'reduces', 'recall', 'vice', 'versa', '.', 'This', 'called', 'precision/recall', 'tradeo›.Precision/Recall', 'Tradeo†To', 'understand', 'tradeoff', 'let‡s', 'look', 'SGDClassifier', 'makes', 'classifica…tion', 'decisions', '.', 'For', 'instance', 'computes', 'score', 'based', 'decision', 'function', 'score', 'greater', 'threshold', 'assigns', 'instance', 'positive', 'class', 'else', 'assigns', 'negative', 'class', '.', 'Figure', '3-3', 'shows', 'digits', 'positioned', 'lowest', 'score', 'left', 'highest', 'score', 'right', '.', 'Suppose', 'deci…', 'sion', 'threshold', 'positioned', 'central', 'arrow', 'two', '5s', 'find', '4', 'true', 'positives', 'actual', '5s', 'right', 'threshold', 'one', 'false', 'positive', 'actually', '6', '.', 'Therefore', 'threshold', 'precision', '80', '%', '4', '5', '.', 'But', '6', 'actual', '5s', 'classifier', 'detects', '4', 'recall', '67', '%', '4', '6', '.', 'Now', 'raise', 'threshold', 'move', 'arrow', 'right', 'false', 'positive', '6', 'becomes', 'true', 'negative', 'thereby', 'increasing', 'precision', '100', '%', 'case', 'one', 'true', 'positive', 'becomes', 'false', 'negative', 'decreasing', 'recall', '50', '%', '.', 'Conversely', 'lowering', 'threshold', 'increases', 'recall', 'reduces', 'precision.Performance', 'Measures', '|', '87', 'Figure', '3-3', '.', 'Decision', 'threshold', 'precision/recall', 'tradeo›Scikit-Learn', 'let', 'set', 'threshold', 'directly', 'give', 'access', 'decision', 'scores', 'uses', 'make', 'predictions', '.', 'Instead', 'calling', 'classifier‡s', 'predict', 'method', 'call', 'decision_function', 'method', 'returns', 'ascore', 'instance', 'make', 'predictions', 'based', 'scores', 'using', 'threshold', 'want', '>', '>', '>', 'y_scores', '=', 'sgd_clf.decision_function', 'some_digit', '>', '>', '>', 'y_scoresarray', '161855.74572176', '>', '>', '>', 'threshold', '=', '0', '>', '>', '>', 'y_some_digit_pred', '=', 'y_scores', '>', 'threshold', 'array', 'True', 'dtype=bool', 'The', 'SGDClassifier', 'uses', 'threshold', 'equal', '0', 'previous', 'code', 'returns', 'result', 'predict', 'method', 'i.e.', 'True', '.', 'Let‡s', 'raise', 'threshold', '>', '>', '>', 'threshold', '=', '200000', '>', '>', '>', 'y_some_digit_pred', '=', 'y_scores', '>', 'threshold', '>', '>', '>', 'y_some_digit_predarray', 'False', 'dtype=bool', 'This', 'confirms', 'raising', 'threshold', 'decreases', 'recall', '.', 'The', 'image', 'actually', 'repre…', 'sents', '5', 'classifier', 'detects', 'threshold', '0', 'misses', 'threshold', 'increased', '200,000.So', 'decide', 'threshold', 'use', '?', 'For', 'first', 'need', 'get', 'scores', 'instances', 'training', 'set', 'using', 'cross_val_predict', 'functionagain', 'time', 'specifying', 'want', 'return', 'decision', 'scores', 'instead', 'predictions', 'y_scores', '=', 'cross_val_predict', 'sgd_clf', 'X_train', 'y_train_5', 'cv=3', 'method=', \"''\", 'decision_function', \"''\", 'Now', 'scores', 'compute', 'precision', 'recall', 'possible', 'thresh…', 'olds', 'using', 'precision_recall_curve', 'function:88', '|', 'Chapter', '3', 'Classi•cationfrom', 'sklearn.metrics', 'import', 'precision_recall_curveprecisions', 'recalls', 'thresholds', '=', 'precision_recall_curve', 'y_train_5', 'y_scores', 'Finally', 'plot', 'precision', 'recall', 'functions', 'threshold', 'value', 'using', 'Matplotlib', 'Figure', '3-4', 'def', 'plot_precision_recall_vs_threshold', 'precisions', 'recalls', 'thresholds', 'plt.plot', 'thresholds', 'precisions', '-1', '``', 'b', '--', \"''\", 'label=', \"''\", 'Precision', \"''\", 'plt.plot', 'thresholds', 'recalls', '-1', '``', 'g-', \"''\", 'label=', \"''\", 'Recall', \"''\", 'plt.xlabel', '``', 'Threshold', \"''\", 'plt.legend', 'loc=', \"''\", 'upper', 'left', \"''\", 'plt.ylim', '0', '1', 'plot_precision_recall_vs_threshold', 'precisions', 'recalls', 'thresholds', 'plt.show', 'Figure', '3-4', '.', 'Precision', 'recall', 'versus', 'decision', 'threshold', 'You', 'may', 'wonder', 'precision', 'curve', 'bumpier', 'recall', 'curve', 'Figure', '3-4', '.', 'The', 'reason', 'precision', 'may', 'sometimes', 'go', 'raise', 'threshold', 'although', 'general', 'goup', '.', 'To', 'understand', 'look', 'back', 'Figure', '3-3', 'notice', 'happens', 'start', 'central', 'threshold', 'move', 'one', 'digit', 'right', 'precision', 'goes', '4/5', '80', '%', '3/4', '75', '%', '.', 'On', 'hand', 'recall', 'go', 'thres…hold', 'increased', 'explains', 'curve', 'looks', 'smooth', '.', 'Now', 'simply', 'select', 'threshold', 'value', 'gives', 'best', 'precision/recall', 'tradeoff', 'task', '.', 'Another', 'way', 'select', 'good', 'precision/recall', 'tradeoff', 'plot', 'precision', 'directly', 'recall', 'shown', 'Figure', '3-5.Performance', 'Measures', '|', '89', 'Figure', '3-5', '.', 'Precision', 'versus', 'recall', 'You', 'see', 'precision', 'really', 'starts', 'fall', 'sharply', 'around', '80', '%', 'recall', '.', 'You', 'probably', 'want', 'select', 'precision/recall', 'tradeoff', 'drop›for', 'example', 'around', '60', '%', 'recall', '.', 'But', 'course', 'choice', 'depends', 'project', '.', 'So', 'let‡s', 'suppose', 'decide', 'aim', '90', '%', 'precision', '.', 'You', 'look', 'first', 'plot', 'zooming', 'bit', 'find', 'need', 'use', 'threshold', '70,000', '.', 'To', 'make', 'predictions', 'training', 'set', 'instead', 'calling', 'classifier‡s', 'predict', 'method', 'run', 'code', 'y_train_pred_90', '=', 'y_scores', '>', '70000', 'Let‡s', 'check', 'predictions‡', 'precision', 'recall', '>', '>', '>', 'precision_score', 'y_train_5', 'y_train_pred_90', '0.8998702983138781', '>', '>', '>', 'recall_score', 'y_train_5', 'y_train_pred_90', '0.63991883416343853Great', '90', '%', 'precision', 'classifier', 'close', 'enough', '!', 'As', 'see', 'fairly', 'easy', 'create', 'classifier', 'virtually', 'precision', 'want', 'set', 'high', 'enough', 'threshold', 'you‡re', 'done', '.', 'Hmm', 'fast', '.', 'A', 'high-precision', 'classifier', 'useful', 'recall', 'low', '!', 'If', 'someone', 'says', 'ƒlet‡s', 'reach', '99', '%', 'precision', '⁄', 'ask', 'ƒat', 'recall', '?', '⁄', '90', '|', 'Chapter', '3', 'Classi•cationThe', 'ROC', 'CurveThe', 'receiver', 'operating', 'characteristic', 'ROC', 'curve', 'another', 'common', 'tool', 'used', 'binary', 'classifiers', '.', 'It', 'similar', 'precision/recall', 'curve', 'instead', 'plot…', 'ting', 'precision', 'versus', 'recall', 'ROC', 'curve', 'plots', 'true', 'positive', 'rate', 'another', 'namefor', 'recall', 'false', 'positive', 'rate', '.', 'The', 'FPR', 'ratio', 'negative', 'instances', 'incorrectly', 'classified', 'positive', '.', 'It', 'equal', 'one', 'minus', 'true', 'negative', 'rate', 'ratio', 'negative', 'instances', 'correctly', 'classified', 'negative', '.', 'The', 'TNR', 'also', 'called', 'speci†city', '.', 'Hence', 'ROC', 'curve', 'plots', 'sensitivity', 'recall', 'versus1', '–', 'speci†city.To', 'plot', 'ROC', 'curve', 'first', 'need', 'compute', 'TPR', 'FPR', 'various', 'thres…', 'hold', 'values', 'using', 'roc_curve', 'function', 'sklearn.metrics', 'import', 'roc_curvefpr', 'tpr', 'thresholds', '=', 'roc_curve', 'y_train_5', 'y_scores', 'Then', 'plot', 'FPR', 'TPR', 'using', 'Matplotlib', '.', 'This', 'code', 'produces', 'plot', 'Figure', '3-6', 'def', 'plot_roc_curve', 'fpr', 'tpr', 'label=None', 'plt.plot', 'fpr', 'tpr', 'linewidth=2', 'label=label', 'plt.plot', '0', '1', '0', '1', '•k', '--', '•', 'plt.axis', '0', '1', '0', '1', 'plt.xlabel', '•False', 'Positive', 'Rate•', 'plt.ylabel', '•True', 'Positive', 'Rate•', 'plot_roc_curve', 'fpr', 'tpr', 'plt.show', 'Figure', '3-6', '.', 'ROC', 'curve', 'Performance', 'Measures', '|', '91', 'Once', 'tradeoff', 'higher', 'recall', 'TPR', 'false', 'positives', 'FPR', 'classifier', 'produces', '.', 'The', 'dotted', 'line', 'represents', 'ROC', 'curve', 'purely', 'random', 'classifier', 'good', 'classifier', 'stays', 'far', 'away', 'line', 'possible', 'toward', 'top-left', 'corner', '.One', 'way', 'compare', 'classifiers', 'measure', 'area', 'curve', 'AUC', '.', 'A', 'per…fect', 'classifier', 'ROC', 'AUC', 'equal', '1', 'whereas', 'purely', 'random', 'classifier', 'willhave', 'ROC', 'AUC', 'equal', '0.5', '.', 'Scikit-Learn', 'provides', 'function', 'compute', 'ROC', 'AUC', '>', '>', '>', 'sklearn.metrics', 'import', 'roc_auc_score', '>', '>', '>', 'roc_auc_score', 'y_train_5', 'y_scores', '0.97061072797174941Since', 'ROC', 'curve', 'similar', 'precision/recall', 'PR', 'curve', 'may', 'wonder', 'decide', 'one', 'use', '.', 'As', 'rule', 'thumb', 'prefer', 'PR', 'curve', 'whenever', 'positive', 'class', 'rare', 'care', 'false', 'positives', 'thanthe', 'false', 'negatives', 'ROC', 'curve', 'otherwise', '.', 'For', 'example', 'looking', 'previous', 'ROC', 'curve', 'ROC', 'AUC', 'score', 'may', 'think', 'classifier', 'really', 'good', '.', 'But', 'mostly', 'positives', '5s', 'compared', 'negatives', 'non-5s', '.', 'In', 'contrast', 'PR', 'curve', 'makes', 'clear', 'classifier', 'room', 'improvement', 'curve', 'could', 'closer', 'top-', 'right', 'corner', '.', 'Let‡s', 'train', 'RandomForestClassifier', 'compare', 'ROC', 'curve', 'ROC', 'AUC', 'score', 'SGDClassifier', '.', 'First', 'need', 'get', 'scores', 'instance', 'thetraining', 'set', '.', 'But', 'due', 'way', 'works', 'see', 'Chapter', '7', 'RandomForestClassifier', 'class', 'decision_function', 'method', '.', 'Instead', 'predict_proba', 'method', '.', 'Scikit-Learn', 'classifiers', 'generally', 'one', '.', 'The', 'predict_proba', 'method', 'returns', 'array', 'containing', 'row', 'per', 'instance', 'col…', 'umn', 'per', 'class', 'containing', 'probability', 'given', 'instance', 'belongs', 'given', 'class', 'e.g.', '70', '%', 'chance', 'image', 'represents', '5', 'sklearn.ensemble', 'import', 'RandomForestClassifierforest_clf', '=', 'RandomForestClassifier', 'random_state=42', 'y_probas_forest', '=', 'cross_val_predict', 'forest_clf', 'X_train', 'y_train_5', 'cv=3', 'method=', \"''\", 'predict_proba', \"''\", 'But', 'plot', 'ROC', 'curve', 'need', 'scores', 'probabilities', '.', 'A', 'simple', 'solution', 'use', 'positive', 'class‡s', 'probability', 'score', 'y_scores_forest', '=', 'y_probas_forest', '1', '#', 'score', '=', 'proba', 'positive', 'classfpr_forest', 'tpr_forest', 'thresholds_forest', '=', 'roc_curve', 'y_train_5', 'y_scores_forest', '92', '|', 'Chapter', '3', 'Classi•cationNow', 'ready', 'plot', 'ROC', 'curve', '.', 'It', 'useful', 'plot', 'first', 'ROC', 'curve', 'well', 'see', 'compare', 'Figure', '3-7', 'plt.plot', 'fpr', 'tpr', '``', 'b', \"''\", 'label=', \"''\", 'SGD', \"''\", 'plot_roc_curve', 'fpr_forest', 'tpr_forest', '``', 'Random', 'Forest', \"''\", 'plt.legend', 'loc=', \"''\", 'bottom', 'right', \"''\", 'plt.show', 'Figure', '3-7', '.', 'Comparing', 'ROC', 'curves', 'As', 'see', 'Figure', '3-7', 'RandomForestClassifier‡s', 'ROC', 'curve', 'looks', 'much', 'better', 'SGDClassifier‡s', 'comes', 'much', 'closer', 'top-left', 'corner', '.', 'As', 'result', 'ROC', 'AUC', 'score', 'also', 'significantly', 'better', '>', '>', '>', 'roc_auc_score', 'y_train_5', 'y_scores_forest', '0.99312433660038291Try', 'measuring', 'precision', 'recall', 'scores', 'find', '98.5', '%', 'precision', '82.8', '%', 'recall', '.', 'Not', 'bad', '!', 'Hopefully', 'know', 'train', 'binary', 'classifiers', 'choose', 'appropriate', 'met…', 'ric', 'task', 'evaluate', 'classifiers', 'using', 'cross-validation', 'select', 'precision/', 'recall', 'tradeoff', 'fits', 'needs', 'compare', 'various', 'models', 'using', 'ROC', 'curves', 'ROC', 'AUC', 'scores', '.', 'Now', 'let‡s', 'try', 'detect', '5s', '.', 'Multiclass', 'Classi•cationWhereas', 'binary', 'classifiers', 'distinguish', 'two', 'classes', 'multiclass', 'classi†ers', 'alsocalled', 'multinomial', 'classi†ers', 'distinguish', 'two', 'classes.Multiclass', 'Classi•cation', '|', '93', 'Some', 'algorithms', 'Random', 'Forest', 'classifiers', 'naive', 'Bayes', 'classifiers', 'capable', 'handling', 'multiple', 'classes', 'directly', '.', 'Others', 'Support', 'Vector', 'Machine', 'classifiers', 'Linear', 'classifiers', 'strictly', 'binary', 'classifiers', '.', 'However', 'vari…', 'ous', 'strategies', 'use', 'perform', 'multiclass', 'classification', 'using', 'multiple', 'binary', 'classifiers', '.', 'For', 'example', 'one', 'way', 'create', 'system', 'classify', 'digit', 'images', '10', 'classes', '0', '9', 'train', '10', 'binary', 'classifiers', 'one', 'digit', '0-detector', '1-detector', '2-detector', '.', 'Then', 'want', 'classify', 'image', 'get', 'decision', 'score', 'classifier', 'image', 'select', 'class', 'whose', 'classifier', 'outputs', 'highest', 'score', '.', 'This', 'called', 'one-versus-all', 'OvA', 'strategy', 'also', 'called', 'one-versus-the-rest', '.Another', 'strategy', 'train', 'binary', 'classifier', 'every', 'pair', 'digits', 'one', 'distin…', 'guish', '0s', '1s', 'another', 'distinguish', '0s', '2s', 'another', '1s', '2s', 'on.This', 'called', 'one-versus-one', 'OvO', 'strategy', '.', 'If', 'N', 'classes', 'need', 'totrain', 'N', '‰', 'N', '–', '1', '/', '2', 'classifiers', '.', 'For', 'MNIST', 'problem', 'means', 'training', '45', 'binary', 'classifiers', '!', 'When', 'want', 'classify', 'image', 'run', 'image', '45', 'classifiers', 'see', 'class', 'wins', 'duels', '.', 'The', 'main', 'advan…tage', 'OvO', 'classifier', 'needs', 'trained', 'part', 'training', 'set', 'two', 'classes', 'must', 'distinguish', '.', 'Some', 'algorithms', 'Support', 'Vector', 'Machine', 'classifiers', 'scale', 'poorly', 'size', 'training', 'set', 'algorithms', 'OvO', 'preferred', 'since', 'faster', 'train', 'many', 'classifiers', 'small', 'training', 'sets', 'training', 'classifiers', 'large', 'training', 'sets', '.', 'For', 'binary', 'classification', 'algorithms', 'however', 'OvA', 'preferred', '.', 'Scikit-Learn', 'detects', 'try', 'use', 'binary', 'classification', 'algorithm', 'multi…', 'class', 'classification', 'task', 'automatically', 'runs', 'OvA', 'except', 'SVM', 'classifiers', 'uses', 'OvO', '.', 'Let‡s', 'try', 'SGDClassifier', '>', '>', '>', 'sgd_clf.fit', 'X_train', 'y_train', '#', 'y_train', 'y_train_5', '>', '>', '>', 'sgd_clf.predict', 'some_digit', 'array', '5', '.', 'That', 'easy', '!', 'This', 'code', 'trains', 'SGDClassifier', 'training', 'set', 'using', 'origi…', 'nal', 'target', 'classes', '0', '9', 'y_train', 'instead', '5-versus-all', 'target', 'classes', 'y_train_5', '.', 'Then', 'makes', 'prediction', 'correct', 'one', 'case', '.', 'Under', 'hood', 'Scikit-Learn', 'actually', 'trained', '10', 'binary', 'classifiers', 'got', 'decision', 'scores', 'image', 'selected', 'class', 'highest', 'score.To', 'see', 'indeed', 'case', 'call', 'decision_function', 'method.Instead', 'returning', 'one', 'score', 'per', 'instance', 'returns', '10', 'scores', 'one', 'perclass', '>', '>', '>', 'some_digit_scores', '=', 'sgd_clf.decision_function', 'some_digit', '>', '>', '>', 'some_digit_scores94', '|', 'Chapter', '3', 'Classi•cationarray', '-311402.62954431', '-363517.28355739', '-446449.5306454', '-183226.61023518', '-414337.15339485', '161855.74572176', '-452576.39616343', '-471957.14962573', '-518542.33997148', '-536774.63961222', 'The', 'highest', 'score', 'indeed', 'one', 'corresponding', 'class', '5', '>', '>', '>', 'np.argmax', 'some_digit_scores', '5', '>', '>', '>', 'sgd_clf.classes_array', '0.', '1.', '2.', '3.', '4.', '5.', '6.', '7.', '8.', '9', '.', '>', '>', '>', 'sgd_clf.classes', '5', '5.0When', 'classifier', 'trained', 'stores', 'list', 'target', 'classes', 'itsclasses_', 'attribute', 'ordered', 'value', '.', 'In', 'case', 'index', 'class', 'classes_', 'array', 'conveniently', 'matches', 'class', 'e.g.', 'class', 'index', '5', 'happens', 'class', '5', 'general', 'won‡t', 'lucky', '.', 'If', 'want', 'force', 'ScikitLearn', 'use', 'one-versus-one', 'one-versus-all', 'use', 'OneVsOneClassifier', 'OneVsRestClassifier', 'classes', '.', 'Simply', 'create', 'instance', 'pass', 'binary', 'classifier', 'constructor', '.', 'For', 'example', 'code', 'creates', 'multi…', 'class', 'classifier', 'using', 'OvO', 'strategy', 'based', 'SGDClassifier', '>', '>', '>', 'sklearn.multiclass', 'import', 'OneVsOneClassifier', '>', '>', '>', 'ovo_clf', '=', 'OneVsOneClassifier', 'SGDClassifier', 'random_state=42', '>', '>', '>', 'ovo_clf.fit', 'X_train', 'y_train', '>', '>', '>', 'ovo_clf.predict', 'some_digit', 'array', '5', '.', '>', '>', '>', 'len', 'ovo_clf.estimators_', '45Training', 'RandomForestClassifier', 'easy', '>', '>', '>', 'forest_clf.fit', 'X_train', 'y_train', '>', '>', '>', 'forest_clf.predict', 'some_digit', 'array', '5', '.', 'This', 'time', 'Scikit-Learn', 'run', 'OvA', 'OvO', 'Random', 'Forest', 'classifiers', 'directly', 'classify', 'instances', 'multiple', 'classes', '.', 'You', 'call', 'predict_proba', 'get', 'list', 'probabilities', 'classifier', 'assigned', 'instance', 'class', '>', '>', '>', 'forest_clf.predict_proba', 'some_digit', 'array', '0.1', '0.', '0.', '0.1', '0.', '0.8', '0.', '0.', '0.', '0', '.', 'You', 'see', 'classifier', 'fairly', 'confident', 'prediction', '0.8', '5', 'thindex', 'array', 'means', 'model', 'estimates', '80', '%', 'probability', 'image', 'Multiclass', 'Classi•cation', '|', '95', 'represents', '5', '.', 'It', 'also', 'thinks', 'image', 'could', 'instead', '0', '3', '10', '%', 'chance', '.Now', 'course', 'want', 'evaluate', 'classifiers', '.', 'As', 'usual', 'want', 'use', 'cross-', 'validation', '.', 'Let‡s', 'evaluate', 'SGDClassifier‡s', 'accuracy', 'using', 'cross_val_score', 'function', '>', '>', '>', 'cross_val_score', 'sgd_clf', 'X_train', 'y_train', 'cv=3', 'scoring=', \"''\", 'accuracy', \"''\", 'array', '0.84063187', '0.84899245', '0.86652998', 'It', 'gets', '84', '%', 'test', 'folds', '.', 'If', 'used', 'random', 'classifier', 'would', 'get', '10', '%', 'accuracy', 'bad', 'score', 'still', 'much', 'better', '.', 'For', 'exam…', 'ple', 'simply', 'scaling', 'inputs', 'discussed', 'Chapter', '2', 'increases', 'accuracy', 'above90', '%', '>', '>', '>', 'sklearn.preprocessing', 'import', 'StandardScaler', '>', '>', '>', 'scaler', '=', 'StandardScaler', '>', '>', '>', 'X_train_scaled', '=', 'scaler.fit_transform', 'X_train.astype', 'np.float64', '>', '>', '>', 'cross_val_score', 'sgd_clf', 'X_train_scaled', 'y_train', 'cv=3', 'scoring=', \"''\", 'accuracy', \"''\", 'array', '0.91011798', '0.90874544', '0.906636', 'Error', 'AnalysisOf', 'course', 'real', 'project', 'would', 'follow', 'steps', 'Machine', 'Learning', 'project', 'checklist', 'see', 'Appendix', 'B', 'exploring', 'data', 'preparation', 'options', 'try…', 'ing', 'multiple', 'models', 'shortlisting', 'best', 'ones', 'fine-tuning', 'hyperpara…', 'meters', 'using', 'GridSearchCV', 'automating', 'much', 'possible', 'previous', 'chapter', '.', 'Here', 'assume', 'found', 'promising', 'model', 'want', 'find', 'ways', 'improve', '.', 'One', 'way', 'analyze', 'types', 'errors', 'makes.First', 'look', 'confusion', 'matrix', '.', 'You', 'need', 'make', 'predictions', 'using', 'cross_val_predict', 'function', 'call', 'confusion_matrix', 'function', 'likeyou', 'earlier', '>', '>', '>', 'y_train_pred', '=', 'cross_val_predict', 'sgd_clf', 'X_train_scaled', 'y_train', 'cv=3', '>', '>', '>', 'conf_mx', '=', 'confusion_matrix', 'y_train', 'y_train_pred', '>', '>', '>', 'conf_mxarray', '5725', '3', '24', '9', '10', '49', '50', '10', '39', '4', '2', '6493', '43', '25', '7', '40', '5', '10', '109', '8', '51', '41', '5321', '104', '89', '26', '87', '60', '166', '13', '47', '46', '141', '5342', '1', '231', '40', '50', '141', '92', '19', '29', '41', '10', '5366', '9', '56', '37', '86', '189', '73', '45', '36', '193', '64', '4582', '111', '30', '193', '94', '29', '34', '44', '2', '42', '85', '5627', '10', '45', '0', '25', '24', '74', '32', '54', '12', '6', '5787', '15', '236', '52', '161', '73', '156', '10', '163', '61', '25', '5027', '123', '43', '35', '26', '92', '178', '28', '2', '223', '82', '5240', '96', '|', 'Chapter', '3', 'Classi•cationThat‡s', 'lot', 'numbers', '.', 'It‡s', 'often', 'convenient', 'look', 'image', 'representation', 'confusion', 'matrix', 'using', 'Matplotlib‡s', 'matshow', 'function', 'plt.matshow', 'conf_mx', 'cmap=plt.cm.gray', 'plt.show', 'This', 'confusion', 'matrix', 'looks', 'fairly', 'good', 'since', 'images', 'main', 'diagonal', 'means', 'classified', 'correctly', '.', 'The', '5s', 'look', 'slightly', 'darker', 'digits', 'could', 'mean', 'fewer', 'images', '5s', 'dataset', 'classifier', 'perform', 'well', '5s', 'digits', '.', 'In', 'fact', 'verify', 'case', '.', 'Let‡s', 'focus', 'plot', 'errors', '.', 'First', 'need', 'divide', 'value', 'confusion', 'matrix', 'number', 'images', 'corresponding', 'class', 'compare', 'error', 'rates', 'instead', 'absolute', 'number', 'errors', 'would', 'make', 'abundant', 'classes', 'look', 'unfairly', 'bad', 'row_sums', '=', 'conf_mx.sum', 'axis=1', 'keepdims=True', 'norm_conf_mx', '=', 'conf_mx', '/', 'row_sumsNow', 'let‡s', 'fill', 'diagonal', 'zeros', 'keep', 'errors', 'let‡s', 'plot', 'result', 'np.fill_diagonal', 'norm_conf_mx', '0', 'plt.matshow', 'norm_conf_mx', 'cmap=plt.cm.gray', 'plt.show', 'Error', 'Analysis', '|', '97', 'Now', 'clearly', 'see', 'kinds', 'errors', 'classifier', 'makes', '.', 'Remember', 'rows', 'represent', 'actual', 'classes', 'columns', 'represent', 'predicted', 'classes', '.', 'The', 'columns', 'classes', '8', '9', 'quite', 'bright', 'tells', 'many', 'images', 'get', 'misclassified', '8s', '9s', '.', 'Similarly', 'rows', 'classes', '8', '9', 'also', 'quite', 'bright', 'telling', '8s', '9s', 'often', 'confused', 'digits', '.', 'Conversely', 'rows', 'pretty', 'dark', 'row', '1', 'means', '1s', 'classified', 'correctly', 'confused', '8s', 'that‡s', '.', 'Notice', 'errors', 'perfectly', 'symmetrical', 'example', '5s', 'misclassified', '8s', 'reverse', '.', 'Analyzing', 'confusion', 'matrix', 'often', 'give', 'insights', 'ways', 'improve', 'classifier', '.', 'Looking', 'plot', 'seems', 'efforts', 'spent', 'improving', 'classification', '8s', '9s', 'well', 'fixing', 'specific', '3/5', 'confusion', '.', 'For', 'example', 'could', 'try', 'gather', 'training', 'data', 'digits', '.', 'Or', 'could', 'engineer', 'new', 'features', 'would', 'help', 'classifier›for', 'example', 'writing', 'algorithm', 'count', 'number', 'closed', 'loops', 'e.g.', '8', 'two', '6', 'one', '5', 'none', '.', 'Or', 'could', 'preprocess', 'images', 'e.g.', 'using', 'Scikit-Image', 'Pillow', 'OpenCV', 'make', 'patterns', 'stand', 'closed', 'loops', '.', 'Analyzing', 'individual', 'errors', 'also', 'good', 'way', 'gain', 'insights', 'classifier', 'failing', 'difficult', 'time-consuming', '.', 'For', 'example', 'let‡s', 'plot', 'examples', '3s', '5s', 'cl_a', 'cl_b', '=', '3', '5X_aa', '=', 'X_train', 'y_train', '==', 'cl_a', '&', 'y_train_pred', '==', 'cl_a', 'X_ab', '=', 'X_train', 'y_train', '==', 'cl_a', '&', 'y_train_pred', '==', 'cl_b', 'X_ba', '=', 'X_train', 'y_train', '==', 'cl_b', '&', 'y_train_pred', '==', 'cl_a', 'X_bb', '=', 'X_train', 'y_train', '==', 'cl_b', '&', 'y_train_pred', '==', 'cl_b', 'plt.figure', 'figsize=', '8,8', 'plt.subplot', '221', 'plot_digits', 'X_aa', ':25', 'images_per_row=5', 'plt.subplot', '222', 'plot_digits', 'X_ab', ':25', 'images_per_row=5', '98', '|', 'Chapter', '3', 'Classi•cation3But', 'remember', 'brain', 'fantastic', 'pattern', 'recognition', 'system', 'visual', 'system', 'lot', 'complex', 'preprocessing', 'information', 'reaches', 'consciousness', 'fact', 'feels', 'simple', 'mean', '.', 'plt.subplot', '223', 'plot_digits', 'X_ba', ':25', 'images_per_row=5', 'plt.subplot', '224', 'plot_digits', 'X_bb', ':25', 'images_per_row=5', 'plt.show', 'The', 'two', '5‰5', 'blocks', 'left', 'show', 'digits', 'classified', '3s', 'two', '5‰5', 'blocks', 'onthe', 'right', 'show', 'images', 'classified', '5s', '.', 'Some', 'digits', 'classifier', 'gets', 'wrong', 'i.e.', 'bottom-left', 'top-right', 'blocks', 'badly', 'written', 'even', 'human', 'would', 'trouble', 'classifying', 'e.g.', '5', '8', 'th', 'row', '1st', 'column', 'trulylooks', 'like', '3', '.', 'However', 'misclassified', 'images', 'seem', 'like', 'obvious', 'errors', 'us', 'it‡s', 'hard', 'understand', 'classifier', 'made', 'mistakes', '.', '3', 'The', 'reason', 'isthat', 'used', 'simple', 'SGDClassifier', 'linear', 'model', '.', 'All', 'assign', 'aweight', 'per', 'class', 'pixel', 'sees', 'new', 'image', 'sums', 'weigh…', 'ted', 'pixel', 'intensities', 'get', 'score', 'class', '.', 'So', 'since', '3s', '5s', 'differ', 'pixels', 'model', 'easily', 'confuse', 'them.The', 'main', 'difference', '3s', '5s', 'position', 'small', 'line', 'joins', 'top', 'line', 'bottom', 'arc', '.', 'If', 'draw', '3', 'junction', 'slightly', 'shifted', 'left', 'classifier', 'might', 'classify', '5', 'vice', 'versa', '.', 'In', 'words', 'classifier', 'quite', 'sensitive', 'image', 'shifting', 'rotation', '.', 'So', 'one', 'way', 'reduce', '3/5', 'confusion', 'would', 'preprocess', 'images', 'ensure', 'well', 'centered', 'rotated', '.', 'This', 'probably', 'help', 'reduce', 'errors', 'well', '.', 'Error', 'Analysis', '|', '99', 'Multilabel', 'Classi•cationUntil', 'instance', 'always', 'assigned', 'one', 'class', '.', 'In', 'cases', 'may', 'want', 'classifier', 'output', 'multiple', 'classes', 'instance', '.', 'For', 'example', 'consider', 'face-recognition', 'classifier', 'recognizes', 'several', 'people', 'picture', '?', 'Of', 'course', 'attach', 'one', 'label', 'per', 'person', 'recognizes', '.', 'Say', 'classifier', 'trained', 'recognize', 'three', 'faces', 'Alice', 'Bob', 'Charlie', 'shown', 'picture', 'Alice', 'Charlie', 'output', '1', '0', '1', 'meaningƒAlice', 'yes', 'Bob', 'Charlie', 'yes⁄', '.', 'Such', 'classification', 'system', 'outputs', 'multiple', 'binary', 'labels', 'called', 'multilabel', 'classi†cation', 'system.We', 'won‡t', 'go', 'face', 'recognition', 'yet', 'let‡s', 'look', 'simpler', 'example', 'illustration', 'purposes', 'sklearn.neighbors', 'import', 'KNeighborsClassifiery_train_large', '=', 'y_train', '>', '=', '7', 'y_train_odd', '=', 'y_train', '%', '2', '==', '1', 'y_multilabel', '=', 'np.c_', 'y_train_large', 'y_train_odd', 'knn_clf', '=', 'KNeighborsClassifier', 'knn_clf.fit', 'X_train', 'y_multilabel', 'This', 'code', 'creates', 'y_multilabel', 'array', 'containing', 'two', 'target', 'labels', 'digit', 'image', 'first', 'indicates', 'whether', 'digit', 'large', '7', '8', '9', 'second', 'indicates', 'whether', 'odd', '.', 'The', 'next', 'lines', 'create', 'KNeighborsClassifier', 'instance', 'supports', 'multilabel', 'classification', 'classifiers', 'train', 'using', 'multiple', 'targets', 'array', '.', 'Now', 'make', 'prediction', 'notice', 'outputs', 'two', 'labels', '>', '>', '>', 'knn_clf.predict', 'some_digit', 'array', 'False', 'True', 'dtype=bool', 'And', 'gets', 'right', '!', 'The', 'digit', '5', 'indeed', 'large', 'False', 'odd', 'True', '.There', 'many', 'ways', 'evaluate', 'multilabel', 'classifier', 'selecting', 'right', 'metric', 'really', 'depends', 'project', '.', 'For', 'example', 'one', 'approach', 'measure', 'F', '1', 'score', 'individual', 'label', 'binary', 'classifier', 'metric', 'discussed', 'earlier', 'simply', 'compute', 'average', 'score', '.', 'This', 'code', 'computes', 'average', 'F', '1', 'score', 'across', 'alllabels', '>', '>', '>', 'y_train_knn_pred', '=', 'cross_val_predict', 'knn_clf', 'X_train', 'y_train', 'cv=3', '>', '>', '>', 'f1_score', 'y_train', 'y_train_knn_pred', 'average=', \"''\", 'macro', \"''\", '0.96845540180280221This', 'assumes', 'labels', 'equally', 'important', 'may', 'case', '.', 'In', 'par…', 'ticular', 'many', 'pictures', 'Alice', 'Bob', 'Charlie', 'may', 'want', 'give', 'weight', 'classifier‡s', 'score', 'pictures', 'Alice', '.', 'One', 'simple', 'option', '100', '|', 'Chapter', '3', 'Classi•cation4Scikit-Learn', 'offers', 'averaging', 'options', 'multilabel', 'classifier', 'metrics', 'see', 'documentation', 'details.to', 'give', 'label', 'weight', 'equal', 'support', 'i.e.', 'number', 'instances', 'target', 'label', '.', 'To', 'simply', 'set', 'average=', \"''\", 'weighted', \"''\", 'preceding', 'code.4Multioutput', 'Classi•cationThe', 'last', 'type', 'classification', 'task', 'going', 'discuss', 'called', 'multioutput-', 'multiclass', 'classi†cation', 'simply', 'multioutput', 'classi†cation', '.', 'It', 'simply', 'generaliza…', 'tion', 'multilabel', 'classification', 'label', 'multiclass', 'i.e.', 'two', 'possible', 'values', '.To', 'illustrate', 'let‡s', 'build', 'system', 'removes', 'noise', 'images', '.', 'It', 'take', 'input', 'noisy', 'digit', 'image', 'hopefully', 'output', 'clean', 'digit', 'image', 'repre…', 'sented', 'array', 'pixel', 'intensities', 'like', 'MNIST', 'images', '.', 'Notice', 'classifier‡s', 'output', 'multilabel', 'one', 'label', 'per', 'pixel', 'label', 'multiple', 'values', 'pixel', 'intensity', 'ranges', '0', '255', '.', 'It', 'thus', 'example', 'multioutput', 'classification', 'system', '.', 'The', 'line', 'classification', 'regression', 'sometimes', 'blurry', 'example', '.', 'Arguably', 'predicting', 'pixel', 'intensity', 'akin', 'regression', 'classification', '.', 'Moreover', 'multioutput', 'systems', 'limited', 'classification', 'tasks', 'could', 'even', 'system', 'outputs', 'multiple', 'labels', 'per', 'instance', 'including', 'class', 'labels', 'value', 'labels.Let‡s', 'start', 'creating', 'training', 'test', 'sets', 'taking', 'MNIST', 'images', 'adding', 'noise', 'pixel', 'intensities', 'using', 'NumPy‡s', 'randint', 'function', '.', 'The', 'targetimages', 'original', 'images', 'noise', '=', 'rnd.randint', '0', '100', 'len', 'X_train', '784', 'noise', '=', 'rnd.randint', '0', '100', 'len', 'X_test', '784', 'X_train_mod', '=', 'X_train', '+', 'noiseX_test_mod', '=', 'X_test', '+', 'noisey_train_mod', '=', 'X_trainy_test_mod', '=', 'X_testLet‡s', 'take', 'peek', 'image', 'test', 'set', 'yes', 'we‡re', 'snooping', 'test', 'data', 'frowning', 'right', 'Multioutput', 'Classi•cation', '|', '101', '5You', 'use', 'shift', 'function', 'scipy.ndimage.interpolation', 'module', '.', 'For', 'example', 'shift', 'image', '2', '1', 'cval=0', 'shifts', 'image', '2', 'pixels', '1', 'pixel', 'right', '.', 'On', 'left', 'noisy', 'input', 'image', 'right', 'clean', 'target', 'image', '.', 'Now', 'let‡s', 'train', 'classifier', 'make', 'clean', 'image', 'knn_clf.fit', 'X_train_mod', 'y_train_mod', 'clean_digit', '=', 'knn_clf.predict', 'X_test_mod', 'some_index', 'plot_digit', 'clean_digit', 'Looks', 'close', 'enough', 'target', '!', 'This', 'concludes', 'tour', 'classification', '.', 'Hopefully', 'know', 'select', 'good', 'metrics', 'classification', 'tasks', 'pick', 'appropriate', 'precision/recall', 'tradeoff', 'compare', 'classifiers', 'generally', 'build', 'good', 'classification', 'systems', 'variety', 'tasks', '.', 'Exercises1.Try', 'build', 'classifier', 'MNIST', 'dataset', 'achieves', '97', '%', 'accuracy', 'test', 'set', '.', 'Hint', 'KNeighborsClassifier', 'works', 'quite', 'well', 'task', 'need', 'find', 'good', 'hyperparameter', 'values', 'try', 'grid', 'search', 'weights', 'n_neighbors', 'hyperparameters', '.', '2.Write', 'function', 'shift', 'MNIST', 'image', 'direction', 'left', 'right', 'one', 'pixel.5', 'Then', 'image', 'training', 'set', 'create', 'four', 'shif…', 'ted', 'copies', 'one', 'per', 'direction', 'add', 'training', 'set', '.', 'Finally', 'train', 'best', 'model', 'expanded', 'training', 'set', 'measure', 'accuracy', 'test', 'set.You', 'observe', 'model', 'performs', 'even', 'better', '!', 'This', 'technique', '102', '|', 'Chapter', '3', 'Classi•cationartificially', 'growing', 'training', 'set', 'called', 'data', 'augmentation', 'training', 'set', 'expansion', '.3.Tackle', 'Titanic', 'dataset', '.', 'A', 'great', 'place', 'start', 'Kaggle.4.Build', 'spam', 'classifier', 'challenging', 'exercise', '‹Download', 'examples', 'spam', 'ham', 'Apache', 'SpamAssassin‡s', 'public', 'datasets', '.‹Unzip', 'datasets', 'familiarize', 'data', 'format', '.', '‹Split', 'datasets', 'training', 'set', 'test', 'set', '.', '‹Write', 'data', 'preparation', 'pipeline', 'convert', 'email', 'feature', 'vector', '.', 'Your', 'preparation', 'pipeline', 'transform', 'email', 'sparse', 'vector', 'indicating', 'presence', 'absence', 'possible', 'word', '.', 'For', 'example', 'emails', 'ever', 'contain', 'four', 'words', 'ƒHello', '⁄', 'ƒhow', '⁄', 'ƒare', '⁄', 'ƒyou', '⁄', 'email', 'ƒHello', 'Hello', 'Hello', 'you⁄', 'would', 'converted', 'vector', '1', '0', '0', '1', 'meaning', 'ƒHello⁄', 'present', 'ƒhow⁄', 'absent', 'ƒare⁄', 'absent', 'ƒyou⁄', 'present', '3', '0', '0', '2', 'prefer', 'count', 'number', 'occurrences', 'word.‹You', 'may', 'want', 'add', 'hyperparameters', 'preparation', 'pipeline', 'control', 'whether', 'strip', 'email', 'headers', 'convert', 'email', 'lowercase', 'remove', 'punctuation', 'replace', 'URLs', 'ƒURL', '⁄', 'replace', 'numbers', 'ƒNUMBER', '⁄', 'even', 'perform', 'stemming', 'i.e.', 'trim', 'word', 'endings', 'arePython', 'libraries', 'available', '.', '‹Then', 'try', 'several', 'classifiers', 'see', 'build', 'great', 'spam', 'classifier', 'high', 'recall', 'high', 'precision.Solutions', 'exercises', 'available', 'online', 'Jupyter', 'notebooks', 'https', '//', 'github.com/ageron/handson-ml', '.Exercises', '|', '103', 'CHAPTER', '4Training', 'ModelsSo', 'far', 'treated', 'Machine', 'Learning', 'models', 'training', 'algorithms', 'mostly', 'like', 'black', 'boxes', '.', 'If', 'went', 'exercises', 'previous', 'chapters', 'may', 'surprised', 'much', 'get', 'done', 'without', 'knowing', 'any…', 'thing', 'what‡s', 'hood', 'optimized', 'regression', 'system', 'improved', 'digit', 'image', 'classifier', 'even', 'built', 'spam', 'classifier', 'scratch›all', 'without', 'knowing', 'actually', 'work', '.', 'Indeed', 'many', 'situations', 'don‡t', 'really', 'need', 'know', 'implementation', 'details', '.', 'However', 'good', 'understanding', 'things', 'work', 'help', 'quickly', 'home', 'appropriate', 'model', 'right', 'training', 'algorithm', 'use', 'good', 'set', 'hyperparameters', 'task', '.', 'Understanding', 'what‡s', 'hood', 'also', 'help', 'debug', 'issues', 'perform', 'error', 'analysis', 'efficiently', '.', 'Lastly', 'top…', 'ics', 'discussed', 'chapter', 'essential', 'understanding', 'building', 'training', 'neural', 'networks', 'discussed', 'Part', 'II', 'book', '.In', 'chapter', 'start', 'looking', 'Linear', 'Regression', 'model', 'one', 'simplest', 'models', '.', 'We', 'discuss', 'two', 'different', 'ways', 'train', '‹Using', 'direct', 'ƒclosed-form⁄', 'equation', 'directly', 'computes', 'model', 'parame…', 'ters', 'best', 'fit', 'model', 'training', 'set', 'i.e.', 'model', 'parameters', 'minimize', 'cost', 'function', 'training', 'set', '.‹Using', 'iterative', 'optimization', 'approach', 'called', 'Gradient', 'Descent', 'GD', 'gradually', 'tweaks', 'model', 'parameters', 'minimize', 'cost', 'function', 'thetraining', 'set', 'eventually', 'converging', 'set', 'parameters', 'first', 'method', '.', 'We', 'look', 'variants', 'Gradient', 'Descent', 'use', 'study', 'neural', 'networks', 'Part', 'II', 'Batch', 'GD', 'Mini-batch', 'GD', 'Stochastic', 'GD', '.', '105Next', 'look', 'Polynomial', 'Regression', 'complex', 'model', 'fit', 'non…', 'linear', 'datasets', '.', 'Since', 'model', 'parameters', 'Linear', 'Regression', 'prone', 'overfitting', 'training', 'data', 'look', 'detect', 'whether', 'case', 'using', 'learning', 'curves', 'look', 'several', 'regulari…', 'zation', 'techniques', 'reduce', 'risk', 'overfitting', 'training', 'set', '.', 'Finally', 'look', 'two', 'models', 'commonly', 'used', 'classification', 'tasks', 'Logistic', 'Regression', 'Softmax', 'Regression.There', 'quite', 'math', 'equations', 'chapter', 'using', 'basic', 'notions', 'linear', 'algebra', 'calculus', '.', 'To', 'understand', 'equa…', 'tions', 'need', 'know', 'vectors', 'matrices', 'transpose', 'dot', 'product', 'matrix', 'inverse', 'partial', 'derivatives', '.', 'If', 'unfamiliar', 'concepts', 'please', 'go', 'linear', 'algebra', 'calculus', 'intro…', 'ductory', 'tutorials', 'available', 'Jupyter', 'notebooks', 'online', 'sup…', 'plemental', 'material', '.', 'For', 'truly', 'allergic', 'mathematics', 'still', 'go', 'chapter', 'simply', 'skip', 'equations', 'hopefully', 'text', 'sufficient', 'help', 'understand', 'concepts.Linear', 'RegressionIn', 'Chapter', '1', 'looked', 'simple', 'regression', 'model', 'life', 'satisfaction', 'life_satisfac…', 'tion', '=', '–0', '+', '–1', '‰', 'GDP_per_capita', '.This', 'model', 'linear', 'function', 'input', 'feature', 'GDP_per_capita', '.', '–0', '–1', 'model‡s', 'parameters', '.', 'More', 'generally', 'linear', 'model', 'makes', 'prediction', 'simply', 'computing', 'weighted', 'sum', 'input', 'features', 'plus', 'constant', 'called', 'bias', 'term', 'also', 'called', 'intercept', 'term', 'shown', 'Equation', '4-1', '.Equation', '4-1', '.', 'Linear', 'Regression', 'model', 'prediction', 'y=–0+–1x1+–2x2++–nxn‹ƒ', 'predicted', 'value.‹n', 'number', 'features', '.', '‹xi', 'ith', 'feature', 'value', '.', '‹–j', 'jth', 'model', 'parameter', 'including', 'bias', 'term', '–0', 'feature', 'weights', '–1', '–2', '–n', '.106', '|', 'Chapter', '4', 'Training', 'Models', '1It', 'often', 'case', 'learning', 'algorithm', 'try', 'optimize', 'different', 'function', 'performance', 'measure', 'used', 'evaluate', 'final', 'model', '.', 'This', 'generally', 'function', 'easier', 'compute', 'useful', 'differentiation', 'properties', 'performance', 'measure', 'lacks', 'want', 'constrain', 'model', 'training', 'see', 'discuss', 'regularization', '.', 'This', 'written', 'much', 'concisely', 'using', 'vectorized', 'form', 'shown', 'Equa…tion', '4-2.Equation', '4-2', '.', 'Linear', 'Regression', 'model', 'prediction', 'vectorized', 'form', 'y=h–=–T', '’', '‹–', 'model‡s', 'parameter', 'vector', 'containing', 'bias', 'term', '–0', 'feature', 'weights', '–1', '–n.‹–T', 'transpose', '–', 'row', 'vector', 'instead', 'column', 'vector', '.‹x', 'instance‡s', 'feature', 'vector', 'containing', 'x0', 'xn', 'x0', 'always', 'equal', '1', '.', '‹–T', '’', 'x', 'dot', 'product', '–T', 'x.‹h–', 'hypothesis', 'function', 'using', 'model', 'parameters', '–.Okay', 'that‡s', 'Linear', 'Regression', 'model', 'train', '?', 'Well', 'recall', 'training', 'model', 'means', 'setting', 'parameters', 'model', 'best', 'fits', 'training', 'set', '.', 'For', 'purpose', 'first', 'need', 'measure', 'well', 'poorly', 'model', 'fits', 'training', 'data', '.', 'In', 'Chapter', '2', 'saw', 'common', 'performance', 'measure', 'regression', 'model', 'Root', 'Mean', 'Square', 'Error', 'RMSE', 'Equation', '2-1', '.', 'There…fore', 'train', 'Linear', 'Regression', 'model', 'need', 'find', 'value', '–', 'minimi…', 'zes', 'RMSE', '.', 'In', 'practice', 'simpler', 'minimize', 'Mean', 'Square', 'Error', 'MSE', 'RMSE', 'leads', 'result', 'value', 'minimizes', 'function', 'also', 'minimizes', 'square', 'root', '.1The', 'MSE', 'Linear', 'Regression', 'hypothesis', 'h–', 'training', 'set', 'X', 'calculated', 'using', 'Equation', '4-3', '.Equation', '4-3', '.', 'MSE', 'cost', 'function', 'Linear', 'Regression', 'model', 'MSE', 'h–=1m', '“', 'i=1', 'm–T', '’', '”', 'yi2Most', 'notations', 'presented', 'Chapter', '2', 'see', 'ƒNotations⁄', 'page', '38', '.The', 'difference', 'write', 'h–', 'instead', 'h', 'order', 'make', 'clear', 'model', 'parametrized', 'vector', '–', '.', 'To', 'simplify', 'notations', 'write', 'MSE', '–', 'instead', 'MSE', 'X', 'h–', '.Linear', 'Regression', '|', '107', '2The', 'demonstration', 'returns', 'value', '–', 'minimizes', 'cost', 'function', 'outside', 'scope', 'book.The', 'Normal', 'EquationTo', 'find', 'value', '–', 'minimizes', 'cost', 'function', 'closed-form', 'solution', '›in', 'words', 'mathematical', 'equation', 'gives', 'result', 'directly', '.', 'This', 'called', 'Normal', 'Equation', 'Equation', '4-4', '.2Equation', '4-4', '.', 'Normal', 'Equation', '–=T', '’', '”', '1', '’', 'T', '’', '‹–', 'value', '–', 'minimizes', 'cost', 'function', '.', '‹y', 'vector', 'target', 'values', 'containing', '1', '.Let‡s', 'generate', 'linear-looking', 'data', 'test', 'equation', 'Figure', '4-1', 'import', 'numpy', 'npX', '=', '2', '*', 'np.random.rand', '100', '1', '=', '4', '+', '3', '*', 'X', '+', 'np.random.randn', '100', '1', 'Figure', '4-1', '.', 'Randomly', 'generated', 'linear', 'dataset', '108', '|', 'Chapter', '4', 'Training', 'Models', 'Now', 'let‡s', 'compute', '–', 'using', 'Normal', 'Equation', '.', 'We', 'use', 'inv', 'function', 'NumPy‡s', 'Linear', 'Algebra', 'module', 'np.linalg', 'compute', 'inverse', 'matrix', 'dot', 'method', 'matrix', 'multiplication', 'X_b', '=', 'np.c_', 'np.ones', '100', '1', 'X', '#', 'add', 'x0', '=', '1', 'instancetheta_best', '=', 'np.linalg.inv', 'X_b.T.dot', 'X_b', '.dot', 'X_b.T', '.dot', 'The', 'actual', 'function', 'used', 'generate', 'data', '=', '4', '+', '3x0', '+', 'Gaussian', 'noise', '.', 'Let‡s', 'see', 'equation', 'found', '>', '>', '>', 'theta_bestarray', '4.21509616', '2.77011339', 'We', 'would', 'hoped', '–0', '=', '4', '–1', '=', '3', 'instead', '–0', '=', '3.865', '–1', '=', '3.139', '.', 'Closeenough', 'noise', 'made', 'impossible', 'recover', 'exact', 'parameters', 'origi…', 'nal', 'function.Now', 'make', 'predictions', 'using', '–', '>', '>', '>', 'X_new', '=', 'np.array', '0', '2', '>', '>', '>', 'X_new_b', '=', 'np.c_', 'np.ones', '2', '1', 'X_new', '#', 'add', 'x0', '=', '1', 'instance', '>', '>', '>', 'y_predict', '=', 'X_new_b.dot', 'theta_best', '>', '>', '>', 'y_predictarray', '4.21509616', '9.75532293', 'Let‡s', 'plot', 'model‡s', 'predictions', 'Figure', '4-2', 'plt.plot', 'X_new', 'y_predict', '``', 'r-', \"''\", 'plt.plot', 'X', '``', 'b', '.', '``', 'plt.axis', '0', '2', '0', '15', 'plt.show', 'Figure', '4-2', '.', 'Linear', 'Regression', 'model', 'predictions', 'Linear', 'Regression', '|', '109', '3Note', 'Scikit-Learn', 'separates', 'bias', 'term', 'intercept_', 'feature', 'weights', 'coef_', '.The', 'equivalent', 'code', 'using', 'Scikit-Learn', 'looks', 'like', '3', '>', '>', '>', 'sklearn.linear_model', 'import', 'LinearRegression', '>', '>', '>', 'lin_reg', '=', 'LinearRegression', '>', '>', '>', 'lin_reg.fit', 'X', '>', '>', '>', 'lin_reg.intercept_', 'lin_reg.coef_', 'array', '4.21509616', 'array', '2.77011339', '>', '>', '>', 'lin_reg.predict', 'X_new', 'array', '4.21509616', '9.75532293', 'Computational', 'ComplexityThe', 'Normal', 'Equation', 'computes', 'inverse', 'XT', '’', 'X', 'n', '‰', 'n', 'matrix', 'n', 'number', 'features', '.', 'The', 'computational', 'complexity', 'inverting', 'matrix', 'typically', 'O', 'n2.4', 'O', 'n3', 'depending', 'implementation', '.', 'In', 'words', 'double', 'number', 'features', 'multiply', 'computation', 'time', 'roughly', '22.4', '=', '5.3', '23', '=', '8.The', 'Normal', 'Equation', 'gets', 'slow', 'number', 'features', 'grows', 'large', 'e.g.', '100,000', '.On', 'positive', 'side', 'equation', 'linear', 'regards', 'number', 'instances', 'training', 'set', 'O', 'handles', 'large', 'training', 'sets', 'efficiently', 'provided', 'fit', 'memory', '.', 'Also', 'trained', 'Linear', 'Regression', 'model', 'using', 'Normal', 'Equa…', 'tion', 'algorithm', 'predictions', 'fast', 'computational', 'complexity', 'linear', 'regards', 'number', 'instances', 'want', 'make', 'predictions', 'number', 'features', '.', 'In', 'words', 'making', 'predictions', 'twice', 'many', 'instances', 'twice', 'many', 'features', 'take', 'roughly', 'twice', 'much', 'time', '.', 'Now', 'look', 'different', 'ways', 'train', 'Linear', 'Regression', 'model', 'better', 'suited', 'cases', 'large', 'number', 'features', 'many', 'training', 'instances', 'fit', 'memory', '.', '110', '|', 'Chapter', '4', 'Training', 'Models', 'Gradient', 'DescentGradient', 'Descent', 'generic', 'optimization', 'algorithm', 'capable', 'finding', 'optimal', 'solutions', 'wide', 'range', 'problems', '.', 'The', 'general', 'idea', 'Gradient', 'Descent', 'tweak', 'parameters', 'iteratively', 'order', 'minimize', 'cost', 'function', '.', 'Suppose', 'lost', 'mountains', 'dense', 'fog', 'feel', 'slope', 'ground', 'feet', '.', 'A', 'good', 'strategy', 'get', 'bottom', 'valley', 'quickly', 'go', 'downhill', 'direction', 'steepest', 'slope', '.', 'This', 'exactly', 'Gradient', 'Descent', 'measures', 'local', 'gradient', 'error', 'function', 'regards', 'parameter', 'vector', '–', 'goes', 'direction', 'descending', 'gradient', '.', 'Once', 'gra…', 'dient', 'zero', 'reached', 'minimum', '!', 'Concretely', 'start', 'filling', '–', 'random', 'values', 'called', 'random', 'initializa…', 'tion', 'improve', 'gradually', 'taking', 'one', 'baby', 'step', 'time', 'step', 'attempting', 'decrease', 'cost', 'function', 'e.g.', 'MSE', 'algorithm', 'converges', 'minimum', 'see', 'Figure', '4-3', '.Figure', '4-3', '.', 'Gradient', 'Descent', 'An', 'important', 'parameter', 'Gradient', 'Descent', 'size', 'steps', 'determined', 'learning', 'rate', 'hyperparameter', '.', 'If', 'learning', 'rate', 'small', 'algorithm', 'go', 'many', 'iterations', 'converge', 'take', 'long', 'time', 'see', 'Figure', '4-4', '.Gradient', 'Descent', '|', '111', 'Figure', '4-4', '.', 'Learning', 'rate', 'small', 'On', 'hand', 'learning', 'rate', 'high', 'might', 'jump', 'across', 'valley', 'end', 'side', 'possibly', 'even', 'higher', '.', 'Thismight', 'make', 'algorithm', 'diverge', 'larger', 'larger', 'values', 'failing', 'find', 'good', 'solution', 'see', 'Figure', '4-5', '.Figure', '4-5', '.', 'Learning', 'rate', 'large', 'Finally', 'cost', 'functions', 'look', 'like', 'nice', 'regular', 'bowls', '.', 'There', 'may', 'holes', 'ridges', 'plateaus', 'sorts', 'irregular', 'terrains', 'making', 'convergence', 'minimum', 'difficult', '.', 'Figure', '4-6', 'shows', 'two', 'main', 'challenges', 'Gradient', 'Descent', 'ran…', 'dom', 'initialization', 'starts', 'algorithm', 'left', 'converge', 'local', 'mini…', 'mum', 'good', 'global', 'minimum', '.', 'If', 'starts', 'right', 'take', 'long', 'time', 'cross', 'plateau', 'stop', 'early', 'never', 'reach', 'global', 'minimum', '.', '112', '|', 'Chapter', '4', 'Training', 'Models', '4Technically', 'speaking', 'derivative', 'Lipschitz', 'continuous', '.5Since', 'feature', '1', 'smaller', 'takes', 'larger', 'change', '–1', 'affect', 'cost', 'function', 'bowl', 'elongated', 'along', '–1', 'axis.Figure', '4-6', '.', 'Gradient', 'Descent', 'pitfalls', 'Fortunately', 'MSE', 'cost', 'function', 'Linear', 'Regression', 'model', 'happens', 'convex', 'function', 'means', 'pick', 'two', 'points', 'curve', 'line', 'segment', 'joining', 'never', 'crosses', 'curve', '.', 'This', 'implies', 'local', 'minima', 'one', 'global', 'minimum', '.', 'It', 'also', 'continuous', 'function', 'slope', 'never', 'changes', 'abruptly', '.', '4', 'These', 'two', 'facts', 'great', 'consequence', 'Gradient', 'Descent', 'guaranteed', 'approach', 'arbitrarily', 'close', 'global', 'minimum', 'wait', 'long', 'enough', 'learning', 'rate', 'high', '.', 'In', 'fact', 'cost', 'function', 'shape', 'bowl', 'elongated', 'bowl', 'features', 'different', 'scales', '.', 'Figure', '4-7', 'shows', 'Gradient', 'Descent', 'train…', 'ing', 'set', 'features', '1', '2', 'scale', 'left', 'training', 'set', 'feature', '1', 'much', 'smaller', 'values', 'feature', '2', 'right', '.', '5Figure', '4-7', '.', 'Gradient', 'Descent', 'without', 'feature', 'scaling', 'Gradient', 'Descent', '|', '113', 'As', 'see', 'left', 'Gradient', 'Descent', 'algorithm', 'goes', 'straight', 'toward', 'minimum', 'thereby', 'reaching', 'quickly', 'whereas', 'right', 'first', 'goes', 'direction', 'almost', 'orthogonal', 'direction', 'global', 'minimum', 'ends', 'long', 'march', 'almost', 'flat', 'valley', '.', 'It', 'eventually', 'reach', 'minimum', 'take', 'long', 'time.When', 'using', 'Gradient', 'Descent', 'ensure', 'features', 'similar', 'scale', 'e.g.', 'using', 'Scikit-Learn‡s', 'StandardScalerclass', 'else', 'take', 'much', 'longer', 'converge', '.', 'This', 'diagram', 'also', 'illustrates', 'fact', 'training', 'model', 'means', 'searching', 'combination', 'model', 'parameters', 'minimizes', 'cost', 'function', 'training', 'set', '.', 'It', 'search', 'model‡s', 'parameter', 'space', 'parameters', 'model', 'dimensions', 'space', 'harder', 'search', 'searching', 'nee…dle', '300-dimensional', 'haystack', 'much', 'trickier', 'three', 'dimensions', '.', 'Fortu…', 'nately', 'since', 'cost', 'function', 'convex', 'case', 'Linear', 'Regression', 'needle', 'simply', 'bottom', 'bowl', '.', 'Batch', 'Gradient', 'DescentTo', 'implement', 'Gradient', 'Descent', 'need', 'compute', 'gradient', 'cost', 'func…', 'tion', 'regards', 'model', 'parameter', '–j', '.', 'In', 'words', 'need', 'calculate', 'much', 'cost', 'function', 'change', 'change', '–j', 'little', 'bit', '.', 'This', 'called', 'partial', 'derivative', '.', 'It', 'like', 'asking', 'ƒwhat', 'slope', 'mountain', 'feet', 'I', 'face', 'east', '?', '⁄', 'asking', 'question', 'facing', 'north', 'dimensions', 'imagine', 'universe', 'three', 'dimensions', '.', 'Equa…tion', '4-5', 'computes', 'partial', 'derivative', 'cost', 'function', 'regards', 'parame…', 'ter', '–j', 'noted', 'ﬂﬂ–jMSE–.Equation', '4-5', '.', 'Partial', 'derivatives', 'cost', 'function', 'ﬂﬂ–jMSE–=2m', '“', 'i=1', 'm–T', '’', '”', 'yixjiInstead', 'computing', 'gradients', 'individually', 'use', 'Equation', '4-6', 'com…pute', 'one', 'go', '.', 'The', 'gradient', 'vector', 'noted', '–MSE', '–', 'contains', 'partial', 'derivatives', 'cost', 'function', 'one', 'model', 'parameter', '.', '114', '|', 'Chapter', '4', 'Training', 'Models', '6Eta', '−', '7th', 'letter', 'Greek', 'alphabet.Equation', '4-6', '.', 'Gradient', 'vector', 'cost', 'function', '–MSE–=ﬂﬂ–0MSE–ﬂﬂ–1MSE–ﬂﬂ–nMSE–=2mT', '’', '’', '–', '”', 'Notice', 'formula', 'involves', 'calculations', 'full', 'training', 'set', 'X', 'Gradient', 'Descent', 'step', '!', 'This', 'algorithm', 'called', 'Batch', 'Gradient', 'Descent', 'uses', 'whole', 'batch', 'training', 'data', 'every', 'step', '.', 'As', 'result', 'terribly', 'slow', 'large', 'train…', 'ing', 'sets', 'see', 'much', 'faster', 'Gradient', 'Descent', 'algorithms', 'shortly', '.', 'However', 'Gradient', 'Descent', 'scales', 'well', 'number', 'features', 'training', 'Linear', 'Regression', 'model', 'hun…', 'dreds', 'thousands', 'features', 'much', 'faster', 'using', 'Gradient', 'Descent', 'using', 'Normal', 'Equation', '.', 'Once', 'gradient', 'vector', 'points', 'uphill', 'go', 'opposite', 'direc…', 'tion', 'go', 'downhill', '.', 'This', 'means', 'subtracting', '–MSE', '–', '–', '.', 'This', 'learning', 'rate', '−', 'comes', 'play', '6', 'multiply', 'gradient', 'vector', '−', 'determine', 'thesize', 'downhill', 'step', 'Equation', '4-7', '.Equation', '4-7', '.', 'Gradient', 'Descent', 'step', '–nextstep', '=–', '”', '−–MSE–Let‡s', 'look', 'quick', 'implementation', 'algorithm', 'eta', '=', '0.1', '#', 'learning', 'raten_iterations', '=', '1000m', '=', '100theta', '=', 'np.random.randn', '2,1', '#', 'random', 'initializationfor', 'iteration', 'range', 'n_iterations', 'gradients', '=', '2/m', '*', 'X_b.T.dot', 'X_b.dot', 'theta', '-', 'theta', '=', 'theta', '-', 'eta', '*', 'gradientsGradient', 'Descent', '|', '115', 'That', 'wasn‡t', 'hard', '!', 'Let‡s', 'look', 'resulting', 'theta', '>', '>', '>', 'thetaarray', '4.21509616', '2.77011339', 'Hey', 'that‡s', 'exactly', 'Normal', 'Equation', 'found', '!', 'Gradient', 'Descent', 'worked', 'per…', 'fectly', '.', 'But', 'used', 'different', 'learning', 'rate', 'eta', '?', 'Figure', '4-8', 'shows', 'thefirst', '10', 'steps', 'Gradient', 'Descent', 'using', 'three', 'different', 'learning', 'rates', 'dashed', 'line', 'represents', 'starting', 'point', '.', 'Figure', '4-8', '.', 'Gradient', 'Descent', 'various', 'learning', 'rates', 'On', 'left', 'learning', 'rate', 'low', 'algorithm', 'eventually', 'reach', 'solu…', 'tion', 'take', 'long', 'time', '.', 'In', 'middle', 'learning', 'rate', 'looks', 'pretty', 'good', 'iterations', 'already', 'converged', 'solution', '.', 'On', 'right', 'learn…', 'ing', 'rate', 'high', 'algorithm', 'diverges', 'jumping', 'place', 'actually', 'getting', 'away', 'solution', 'every', 'step', '.', 'To', 'find', 'good', 'learning', 'rate', 'use', 'grid', 'search', 'see', 'Chapter', '2', '.', 'However', 'may', 'want', 'limit', 'number', 'iterations', 'grid', 'search', 'eliminate', 'models', 'take', 'long', 'converge', '.', 'You', 'may', 'wonder', 'set', 'number', 'iterations', '.', 'If', 'low', 'still', 'far', 'away', 'optimal', 'solution', 'algorithm', 'stops', 'high', 'waste', 'time', 'model', 'parameters', 'change', 'anymore', '.', 'A', 'simple', 'solu…', 'tion', 'set', 'large', 'number', 'iterations', 'interrupt', 'algorithm', 'gradient', 'vector', 'becomes', 'tiny›that', 'norm', 'becomes', 'smaller', 'tiny', 'number', 'called', 'tolerance', '›because', 'happens', 'Gradient', 'Descent', 'almost', 'reached', 'minimum', '.', '116', '|', 'Chapter', '4', 'Training', 'Models', '7Out-of-core', 'algorithms', 'discussed', 'Chapter', '1', '.Convergence', 'RateWhen', 'cost', 'function', 'convex', 'slope', 'change', 'abruptly', 'case', 'MSE', 'cost', 'function', 'shown', 'Batch', 'Gradient', 'Descent', 'fixed', 'learning', 'rate', 'convergence', 'rate', 'O1iterations', '.', 'In', 'words', 'dividethe', 'tolerance', '10', 'precise', 'solution', 'algorithm', 'run', '10', 'times', 'iterations', '.', 'Stochastic', 'Gradient', 'DescentThe', 'main', 'problem', 'Batch', 'Gradient', 'Descent', 'fact', 'uses', 'whole', 'training', 'set', 'compute', 'gradients', 'every', 'step', 'makes', 'slow', 'training', 'set', 'large', '.', 'At', 'opposite', 'extreme', 'Stochastic', 'Gradient', 'Descent', 'justpicks', 'random', 'instance', 'training', 'set', 'every', 'step', 'computes', 'gradients', 'based', 'single', 'instance', '.', 'Obviously', 'makes', 'algorithm', 'much', 'faster', 'since', 'little', 'data', 'manipulate', 'every', 'iteration', '.', 'It', 'also', 'makes', 'possible', 'train', 'huge', 'training', 'sets', 'since', 'one', 'instance', 'needs', 'memory', 'iteration', 'SGD', 'implemented', 'out-of-core', 'algorithm', '.', '7', 'On', 'hand', 'due', 'stochastic', 'i.e.', 'random', 'nature', 'algorithm', 'much', 'less', 'regular', 'Batch', 'Gradient', 'Descent', 'instead', 'gently', 'decreasing', 'reaches', 'minimum', 'cost', 'function', 'bounce', 'decreasing', 'aver…', 'age', '.', 'Over', 'time', 'end', 'close', 'minimum', 'gets', 'continue', 'bounce', 'around', 'never', 'settling', 'see', 'Figure', '4-9', '.', 'So', 'algo…rithm', 'stops', 'final', 'parameter', 'values', 'good', 'optimal.Figure', '4-9', '.', 'Stochastic', 'Gradient', 'Descent', 'Gradient', 'Descent', '|', '117', 'When', 'cost', 'function', 'irregular', 'Figure', '4-6', 'actually', 'help', 'thealgorithm', 'jump', 'local', 'minima', 'Stochastic', 'Gradient', 'Descent', 'better', 'chance', 'finding', 'global', 'minimum', 'Batch', 'Gradient', 'Descent', '.', 'Therefore', 'randomness', 'good', 'escape', 'local', 'optima', 'bad', 'means', 'algorithm', 'never', 'settle', 'minimum', '.', 'One', 'solution', 'dilemma', 'gradually', 'reduce', 'learning', 'rate', '.', 'The', 'steps', 'start', 'large', 'helps', 'make', 'quick', 'progress', 'escape', 'local', 'minima', 'get', 'smaller', 'smaller', 'allowing', 'algorithm', 'settle', 'global', 'minimum', '.', 'This', 'process', 'called', 'simulated', 'annealing', 'resembles', 'process', 'annealing', 'metallurgy', 'molten', 'metal', 'slowly', 'cooled', '.', 'The', 'function', 'determines', 'learning', 'rate', 'iteration', 'called', 'learning', 'schedule', '.', 'If', 'learning', 'rate', 'reduced', 'quickly', 'may', 'get', 'stuck', 'local', 'minimum', 'even', 'end', 'frozen', 'halfway', 'minimum', '.', 'If', 'learning', 'rate', 'reduced', 'slowly', 'may', 'jump', 'around', 'minimum', 'long', 'time', 'end', 'suboptimal', 'solution', 'halt', 'training', 'early', '.', 'This', 'code', 'implements', 'Stochastic', 'Gradient', 'Descent', 'using', 'simple', 'learning', 'schedule', 'n_epochs', '=', '50t0', 't1', '=', '5', '50', '#', 'learning', 'schedule', 'hyperparametersdef', 'learning_schedule', 'return', 't0', '/', '+', 't1', 'theta', '=', 'np.random.randn', '2,1', '#', 'random', 'initializationfor', 'epoch', 'range', 'n_epochs', 'range', 'random_index', '=', 'np.random.randint', 'xi', '=', 'X_b', 'random_index', 'random_index+1', 'yi', '=', 'random_index', 'random_index+1', 'gradients', '=', '2', '*', 'xi.T.dot', 'xi.dot', 'theta', '-', 'yi', 'eta', '=', 'learning_schedule', 'epoch', '*', '+', 'theta', '=', 'theta', '-', 'eta', '*', 'gradientsBy', 'convention', 'iterate', 'rounds', 'iterations', 'round', 'called', 'epoch', '.', 'While', 'Batch', 'Gradient', 'Descent', 'code', 'iterated', '1,000', 'times', 'whole', 'train…', 'ing', 'set', 'code', 'goes', 'training', 'set', '50', 'times', 'reaches', 'fairly', 'goodsolution', '>', '>', '>', 'thetaarray', '4.21076011', '2.74856079', 'Figure', '4-10', 'shows', 'first', '10', 'steps', 'training', 'notice', 'irregular', 'steps', '.118', '|', 'Chapter', '4', 'Training', 'Models', 'Figure', '4-10', '.', 'Stochastic', 'Gradient', 'Descent', '†rst', '10', 'steps', 'Note', 'since', 'instances', 'picked', 'randomly', 'instances', 'may', 'picked', 'several', 'times', 'per', 'epoch', 'others', 'may', 'picked', '.', 'If', 'want', 'sure', 'algorithm', 'goes', 'every', 'instance', 'epoch', 'another', 'approach', 'shuffle', 'training', 'set', 'go', 'instance', 'instance', 'shuffle', '.', 'However', 'generally', 'converges', 'slowly', '.', 'To', 'perform', 'Linear', 'Regression', 'using', 'SGD', 'Scikit-Learn', 'use', 'SGDRegressor', 'class', 'defaults', 'optimizing', 'squared', 'error', 'cost', 'function', '.', 'The', 'fol…', 'lowing', 'code', 'runs', '50', 'epochs', 'starting', 'learning', 'rate', '0.1', 'eta0=0.1', 'using', 'thedefault', 'learning', 'schedule', 'different', 'preceding', 'one', 'use', 'regularization', 'penalty=None', 'details', 'shortly', 'sklearn.linear_model', 'import', 'SGDRegressorsgd_reg', '=', 'SGDRegressor', 'n_iter=50', 'penalty=None', 'eta0=0.1', 'sgd_reg.fit', 'X', 'y.ravel', 'Once', 'find', 'solution', 'close', 'one', 'returned', 'Normal', 'Equa…', 'tion', '>', '>', '>', 'sgd_reg.intercept_', 'sgd_reg.coef_', 'array', '4.18380366', 'array', '2.74205299', 'Mini-batch', 'Gradient', 'DescentThe', 'last', 'Gradient', 'Descent', 'algorithm', 'look', 'called', 'Mini-batch', 'Gradient', 'Descent', '.', 'It', 'quite', 'simple', 'understand', 'know', 'Batch', 'Stochastic', 'Gradi…', 'ent', 'Descent', 'step', 'instead', 'computing', 'gradients', 'based', 'full', 'train…', 'ing', 'set', 'Batch', 'GD', 'based', 'one', 'instance', 'Stochastic', 'GD', 'Mini-', 'Gradient', 'Descent', '|', '119', '8While', 'Normal', 'Equation', 'perform', 'Linear', 'Regression', 'Gradient', 'Descent', 'algorithms', 'used', 'train', 'many', 'models', 'see', '.', 'batch', 'GD', 'computes', 'gradients', 'small', 'random', 'sets', 'instances', 'called', 'mini-', 'batches', '.', 'The', 'main', 'advantage', 'Mini-batch', 'GD', 'Stochastic', 'GD', 'get', 'performance', 'boost', 'hardware', 'optimization', 'matrix', 'operations', 'especially', 'using', 'GPUs', '.', 'The', 'algorithm‡s', 'progress', 'parameter', 'space', 'less', 'erratic', 'SGD', 'especially', 'fairly', 'large', 'mini-batches', '.', 'As', 'result', 'Mini-batch', 'GD', 'end', 'walking', 'around', 'bit', 'closer', 'minimum', 'SGD', '.', 'But', 'hand', 'may', 'harder', 'escape', 'local', 'minima', 'case', 'problems', 'suffer', 'local', 'minima', 'unlike', 'Linear', 'Regression', 'saw', 'earlier', '.', 'Figure', '4-11', 'shows', 'thepaths', 'taken', 'three', 'Gradient', 'Descent', 'algorithms', 'parameter', 'space', 'training', '.', 'They', 'end', 'near', 'minimum', 'Batch', 'GD‡s', 'path', 'actually', 'stops', 'minimum', 'Stochastic', 'GD', 'Mini-batch', 'GD', 'continue', 'walk', 'around', '.', 'However', 'don‡t', 'forget', 'Batch', 'GD', 'takes', 'lot', 'time', 'take', 'step', 'Stochas…', 'tic', 'GD', 'Mini-batch', 'GD', 'would', 'also', 'reach', 'minimum', 'used', 'good', 'learn…', 'ing', 'schedule.Figure', '4-11', '.', 'Gradient', 'Descent', 'paths', 'parameter', 'space', 'Let‡s', 'compare', 'algorithms', 'we‡ve', 'discussed', 'far', 'Linear', 'Regression', '8', 'recall', 'number', 'training', 'instances', 'n', 'number', 'features', 'see', 'Table', '4-1', '.Table', '4-1', '.', 'Comparison', 'algorithms', 'Linear', 'Regression', 'AlgorithmLarge', 'mOut-of-core', 'support', 'Large', 'nHyperparamsScaling', 'required', 'Scikit-LearnNormal', 'Equation', 'FastNoSlow0NoLinearRegressionBatch', 'GD', 'SlowNoFast2Yesn/a120', '|', 'Chapter', '4', 'Training', 'Models', '9A', 'quadratic', 'equation', 'form', '=', 'ax2', '+', 'bx', '+', 'c.AlgorithmLarge', 'mOut-of-core', 'support', 'Large', 'nHyperparamsScaling', 'required', 'Scikit-LearnStochastic', 'GD', 'FastYesFast•2YesSGDRegressorMini-batch', 'GD', 'FastYesFast•2Yesn/aThere', 'almost', 'difference', 'training', 'algorithmsend', 'similar', 'models', 'make', 'predictions', 'exactly', 'way', '.', 'Polynomial', 'RegressionWhat', 'data', 'actually', 'complex', 'simple', 'straight', 'line', '?', 'Surprisingly', 'actually', 'use', 'linear', 'model', 'fit', 'nonlinear', 'data', '.', 'A', 'simple', 'way', 'add', 'powers', 'feature', 'new', 'features', 'train', 'linear', 'model', 'extended', 'set', 'features', '.', 'This', 'technique', 'called', 'Polynomial', 'Regression', '.Let‡s', 'look', 'example', '.', 'First', 'let‡s', 'generate', 'nonlinear', 'data', 'based', 'simple', 'quadratic', 'equation', '9', 'plus', 'noise', 'see', 'Figure', '4-12', '=', '100X', '=', '6', '*', 'np.random.rand', '1', '-', '3y', '=', '0.5', '*', 'X**2', '+', 'X', '+', '2', '+', 'np.random.randn', '1', 'Figure', '4-12', '.', 'Generated', 'nonlinear', 'noisy', 'dataset', 'Polynomial', 'Regression', '|', '121', 'Clearly', 'straight', 'line', 'never', 'fit', 'data', 'properly', '.', 'So', 'let‡s', 'use', 'Scikit-Learn‡s', 'PolynomialFeatures', 'class', 'transform', 'training', 'data', 'adding', 'square', '2', 'nd-degreepolynomial', 'feature', 'training', 'set', 'new', 'features', 'case', 'one', 'feature', '>', '>', '>', 'sklearn.preprocessing', 'import', 'PolynomialFeatures', '>', '>', '>', 'poly_features', '=', 'PolynomialFeatures', 'degree=2', 'include_bias=False', '>', '>', '>', 'X_poly', '=', 'poly_features.fit_transform', 'X', '>', '>', '>', 'X', '0', 'array', '-0.75275929', '>', '>', '>', 'X_poly', '0', 'array', '-0.75275929', '0.56664654', 'X_poly', 'contains', 'original', 'feature', 'X', 'plus', 'square', 'feature', '.', 'Now', 'fit', 'LinearRegression', 'model', 'extended', 'training', 'data', 'Figure', '4-13', '>', '>', '>', 'lin_reg', '=', 'LinearRegression', '>', '>', '>', 'lin_reg.fit', 'X_poly', '>', '>', '>', 'lin_reg.intercept_', 'lin_reg.coef_', 'array', '1.78134581', 'array', '0.93366893', '0.56456263', 'Figure', '4-13', '.', 'Polynomial', 'Regression', 'model', 'predictions', 'Not', 'bad', 'model', 'estimates', 'y=0.56', 'x12+0.93', 'x1+1.78', 'fact', 'originalfunction', 'y=0.5', 'x12+1.0', 'x1+2.0+Gaussiannoise', '.Note', 'multiple', 'features', 'Polynomial', 'Regression', 'capable', 'find…', 'ing', 'relationships', 'features', 'something', 'plain', 'Linear', 'Regression', 'model', '.', 'This', 'made', 'possible', 'fact', 'PolynomialFeatures', 'alsoadds', 'combinations', 'features', 'given', 'degree', '.', 'For', 'example', '122', '|', 'Chapter', '4', 'Training', 'Models', 'two', 'features', 'b', 'PolynomialFeatures', 'degree=3', 'would', 'add', 'thefeatures', 'a2', 'a3', 'b2', 'b3', 'also', 'combinations', 'ab', 'a2b', 'ab', '2.PolynomialFeatures', 'degree=d', 'transforms', 'array', 'containing', 'nfeatures', 'array', 'containing', 'n+d', '!', '!', 'n', '!', 'features', 'n', '!', 'thefactorial', 'n', 'equal', '1', '‰', '2', '‰', '3', '‰', '‰', 'n.', 'Beware', 'combinato…', 'rial', 'explosion', 'number', 'features', '!', 'Learning', 'CurvesIf', 'perform', 'high-degree', 'Polynomial', 'Regression', 'likely', 'fit', 'training', 'data', 'much', 'better', 'plain', 'Linear', 'Regression', '.', 'For', 'example', 'Figure', '4-14', 'applies', '300-degree', 'polynomial', 'model', 'preceding', 'training', 'data', 'compares', 'result', 'pure', 'linear', 'model', 'quadratic', 'model', '2', 'nd-degree', 'polynomial', '.Notice', '300-degree', 'polynomial', 'model', 'wiggles', 'around', 'get', 'close', 'possi…', 'ble', 'training', 'instances.Figure', '4-14', '.', 'High-degree', 'Polynomial', 'Regression', 'Of', 'course', 'high-degree', 'Polynomial', 'Regression', 'model', 'severely', 'overfitting', 'training', 'data', 'linear', 'model', 'underfitting', '.', 'The', 'model', 'generalize', 'best', 'case', 'quadratic', 'model', '.', 'It', 'makes', 'sense', 'since', 'data', 'generated', 'using', 'quadratic', 'model', 'general', 'won‡t', 'know', 'function', 'generated', 'data', 'decide', 'complex', 'model', '?', 'How', 'tell', 'model', 'overfitting', 'underfitting', 'data', '?', 'Learning', 'Curves', '|', '123', 'In', 'Chapter', '2', 'used', 'cross-validation', 'get', 'estimate', 'model‡s', 'generalization', 'performance', '.', 'If', 'model', 'performs', 'well', 'training', 'data', 'generalizes', 'poorly', 'according', 'cross-validation', 'metrics', 'model', 'overfitting', '.', 'If', 'per…', 'forms', 'poorly', 'underfitting', '.', 'This', 'one', 'way', 'tell', 'model', 'simple', 'complex', '.', 'Another', 'way', 'look', 'learning', 'curves', 'plots', 'model‡s', 'perfor…', 'mance', 'training', 'set', 'validation', 'set', 'function', 'training', 'set', 'size', '.', 'To', 'generate', 'plots', 'simply', 'train', 'model', 'several', 'times', 'different', 'sized', 'subsets', 'training', 'set', '.', 'The', 'following', 'code', 'defines', 'function', 'plots', 'learning', 'curves', 'model', 'given', 'training', 'data', 'sklearn.metrics', 'import', 'mean_squared_errorfrom', 'sklearn.model_selection', 'import', 'train_test_splitdef', 'plot_learning_curves', 'model', 'X', 'X_train', 'X_val', 'y_train', 'y_val', '=', 'train_test_split', 'X', 'test_size=0.2', 'train_errors', 'val_errors', '=', 'range', '1', 'len', 'X_train', 'model.fit', 'X_train', 'y_train', 'y_train_predict', '=', 'model.predict', 'X_train', 'y_val_predict', '=', 'model.predict', 'X_val', 'train_errors.append', 'mean_squared_error', 'y_train_predict', 'y_train', 'val_errors.append', 'mean_squared_error', 'y_val_predict', 'y_val', 'plt.plot', 'np.sqrt', 'train_errors', '``', 'r-+', \"''\", 'linewidth=2', 'label=', \"''\", 'train', \"''\", 'plt.plot', 'np.sqrt', 'val_errors', '``', 'b-', \"''\", 'linewidth=3', 'label=', \"''\", 'val', \"''\", 'Let‡s', 'look', 'learning', 'curves', 'plain', 'Linear', 'Regression', 'model', 'straight', 'line', 'Figure', '4-15', 'lin_reg', '=', 'LinearRegression', 'plot_learning_curves', 'lin_reg', 'X', 'Figure', '4-15', '.', 'Learning', 'curves', '124', '|', 'Chapter', '4', 'Training', 'Models', 'This', 'deserves', 'bit', 'explanation', '.', 'First', 'let‡s', 'look', 'performance', 'training', 'data', 'one', 'two', 'instances', 'training', 'set', 'model', 'fit', 'perfectly', 'curve', 'starts', 'zero', '.', 'But', 'new', 'instances', 'added', 'training', 'set', 'becomes', 'impossible', 'model', 'fit', 'training', 'data', 'per…', 'fectly', 'data', 'noisy', 'linear', '.', 'So', 'error', 'training', 'data', 'goes', 'reaches', 'plateau', 'point', 'adding', 'new', 'instan…', 'ces', 'training', 'set', 'doesn‡t', 'make', 'average', 'error', 'much', 'better', 'worse', '.', 'Now', 'let‡s', 'look', 'performance', 'model', 'validation', 'data', '.', 'When', 'model', 'trained', 'training', 'instances', 'incapable', 'generalizing', 'properly', 'validation', 'error', 'initially', 'quite', 'big', '.', 'Then', 'model', 'shown', 'training', 'examples', 'learns', 'thus', 'validation', 'error', 'slowly', 'goes', '.', 'However', 'straight', 'line', 'good', 'job', 'modeling', 'data', 'error', 'ends', 'plateau', 'close', 'curve', '.', 'These', 'learning', 'curves', 'typical', 'underfitting', 'model', '.', 'Both', 'curves', 'reached', 'plateau', 'close', 'fairly', 'high', '.', 'If', 'model', 'underfitting', 'training', 'data', 'adding', 'train…', 'ing', 'examples', 'help', '.', 'You', 'need', 'use', 'complex', 'model', 'come', 'better', 'features', '.', 'Now', 'let‡s', 'look', 'learning', 'curves', '10', 'th-degree', 'polynomial', 'model', 'samedata', 'Figure', '4-16', 'sklearn.pipeline', 'import', 'Pipelinepolynomial_regression', '=', 'Pipeline', '``', 'poly_features', \"''\", 'PolynomialFeatures', 'degree=10', 'include_bias=False', '``', 'sgd_reg', \"''\", 'LinearRegression', 'plot_learning_curves', 'polynomial_regression', 'X', 'These', 'learning', 'curves', 'look', 'bit', 'like', 'previous', 'ones', 'two', 'impor…', 'tant', 'differences', '‹The', 'error', 'training', 'data', 'much', 'lower', 'Linear', 'Regression', 'model.‹There', 'gap', 'curves', '.', 'This', 'means', 'model', 'performs', 'signifi…', 'cantly', 'better', 'training', 'data', 'validation', 'data', 'hall…', 'mark', 'overfitting', 'model', '.', 'However', 'used', 'much', 'larger', 'training', 'set', 'two', 'curves', 'would', 'continue', 'get', 'closer', '.', 'Learning', 'Curves', '|', '125', '10This', 'notion', 'bias', 'confused', 'bias', 'term', 'linear', 'models.Figure', '4-16', '.', 'Learning', 'curves', 'polynomial', 'model', 'One', 'way', 'improve', 'overfitting', 'model', 'feed', 'training', 'data', 'validation', 'error', 'reaches', 'training', 'error', '.', 'The', 'Bias/Variance', 'Tradeo†An', 'important', 'theoretical', 'result', 'statistics', 'Machine', 'Learning', 'fact', 'model‡s', 'generalization', 'error', 'expressed', 'sum', 'three', 'different', 'errors', 'Bias', 'This', 'part', 'generalization', 'error', 'due', 'wrong', 'assumptions', 'assum…', 'ing', 'data', 'linear', 'actually', 'quadratic', '.', 'A', 'high-bias', 'model', 'likely', 'underfit', 'training', 'data', '.', '10Variance', 'This', 'part', 'due', 'model‡s', 'excessive', 'sensitivity', 'small', 'variations', 'training', 'data', '.', 'A', 'model', 'many', 'degrees', 'freedom', 'high-degree', 'pol…', 'ynomial', 'model', 'likely', 'high', 'variance', 'thus', 'overfit', 'training', 'data', '.', '126', '|', 'Chapter', '4', 'Training', 'Models', 'Irreducible', 'error', 'This', 'part', 'due', 'noisiness', 'data', '.', 'The', 'way', 'reduce', 'part', 'error', 'clean', 'data', 'e.g.', 'fix', 'data', 'sources', 'broken', 'sensors', 'detect', 'remove', 'outliers', '.Increasing', 'model‡s', 'complexity', 'typically', 'increase', 'variance', 'reduce', 'bias', '.', 'Conversely', 'reducing', 'model‡s', 'complexity', 'increases', 'bias', 'reduces', 'variance', '.', 'This', 'called', 'tradeoff', '.', 'Regularized', 'Linear', 'ModelsAs', 'saw', 'Chapters', '1', '2', 'good', 'way', 'reduce', 'overfitting', 'regularize', 'model', 'i.e.', 'constrain', 'fewer', 'degrees', 'freedom', 'harder', 'befor', 'overfit', 'data', '.', 'For', 'example', 'simple', 'way', 'regularize', 'polynomial', 'model', 'reduce', 'number', 'polynomial', 'degrees', '.', 'For', 'linear', 'model', 'regularization', 'typically', 'achieved', 'constraining', 'weights', 'model', '.', 'We', 'look', 'Ridge', 'Regression', 'Lasso', 'Regression', 'Elastic', 'Net', 'implement', 'three', 'different', 'ways', 'constrain', 'weights', '.', 'Ridge', 'RegressionRidge', 'Regression', 'also', 'called', 'Tikhonov', 'regularization', 'regularized', 'version', 'Lin…ear', 'Regression', 'regularization', 'term', 'equal', '‰', '“', 'i=1', 'n–i2', 'added', 'cost', 'function', '.', 'This', 'forces', 'learning', 'algorithm', 'fit', 'data', 'also', 'keep', 'model', 'weights', 'small', 'possible', '.', 'Note', 'regularization', 'term', 'added', 'cost', 'function', 'training', '.', 'Once', 'model', 'trained', 'want', 'evaluate', 'model‡s', 'performance', 'using', 'unregularized', 'performance', 'measure', '.', 'It', 'quite', 'common', 'cost', 'function', 'used', 'training', 'different', 'performance', 'measure', 'used', 'testing', '.', 'Apart', 'regularization', 'another', 'reason', 'might', 'different', 'good', 'training', 'cost', 'function', 'optimization-', 'friendly', 'derivatives', 'performance', 'measure', 'used', 'test…', 'ing', 'close', 'possible', 'final', 'objective', '.', 'A', 'goodexample', 'classifier', 'trained', 'using', 'cost', 'function', 'log', 'loss', 'discussed', 'moment', 'evaluated', 'using', 'precision/', 'recall.The', 'hyperparameter', '‰', 'controls', 'much', 'want', 'regularize', 'model', '.', 'If', '‰', '=', '0then', 'Ridge', 'Regression', 'Linear', 'Regression', '.', 'If', '‰', 'large', 'weights', 'end', 'Regularized', 'Linear', 'Models', '|', '127', '11It', 'common', 'use', 'notation', 'J', '–', 'cost', 'functions', 'don‡t', 'short', 'name', 'often', 'use', 'notation', 'throughout', 'rest', 'book', '.', 'The', 'context', 'make', 'clear', 'cost', 'function', 'dis…', 'cussed.12Norms', 'discussed', 'Chapter', '2', '.13A', 'square', 'matrix', 'full', '0s', 'except', '1s', 'main', 'diagonal', 'top-left', 'bottom-right', '.', 'close', 'zero', 'result', 'flat', 'line', 'going', 'data‡s', 'mean', '.', 'Equa…tion', '4-8', 'presents', 'Ridge', 'Regression', 'cost', 'function', '.', '11Equation', '4-8', '.', 'Ridge', 'Regression', 'cost', 'function', 'J–=MSE', '–+‰12', '“', 'i=1', 'n–i2Note', 'bias', 'term', '–0', 'regularized', 'sum', 'starts', '=', '1', '0', '.', 'If', 'wedefine', 'w', 'vector', 'feature', 'weights', '–1', '–n', 'regularization', 'term', 'simply', 'equal', 'Ł', 'w', '2', '2', '’', '2', 'represents', '—', '2', 'norm', 'weight', 'vector', '.', '12For', 'Gradient', 'Descent', 'add', '‰w', 'MSE', 'gradient', 'vector', 'Equation', '4-6', '.It', 'important', 'scale', 'data', 'e.g.', 'using', 'StandardScaler', 'performing', 'Ridge', 'Regression', 'sensitive', 'scale', 'ofthe', 'input', 'features', '.', 'This', 'true', 'regularized', 'models', '.', 'Figure', '4-17', 'shows', 'several', 'Ridge', 'models', 'trained', 'linear', 'data', 'using', 'different', '‰value', '.', 'On', 'left', 'plain', 'Ridge', 'models', 'used', 'leading', 'linear', 'predictions', '.', 'On', 'theright', 'data', 'first', 'expanded', 'using', 'PolynomialFeatures', 'degree=10', 'isscaled', 'using', 'StandardScaler', 'finally', 'Ridge', 'models', 'applied', 'result…', 'ing', 'features', 'Polynomial', 'Regression', 'Ridge', 'regularization', '.', 'Note', 'increasing', '‰', 'leads', 'flatter', 'i.e.', 'less', 'extreme', 'reasonable', 'predictions', 'reduces', 'model‡s', 'variance', 'increases', 'bias', '.', 'As', 'Linear', 'Regression', 'perform', 'Ridge', 'Regression', 'either', 'computing', 'closed-form', 'equation', 'performing', 'Gradient', 'Descent', '.', 'The', 'pros', 'cons', '.', 'Equation', '4-9', 'shows', 'closed-form', 'solution', 'A', 'n', '‰', 'n', 'identity', 'matrix', '13', 'except', '0', 'top-left', 'cell', 'corresponding', 'bias', 'term', '.128', '|', 'Chapter', '4', 'Training', 'Models', '14Alternatively', 'use', 'Ridge', 'class', '``', 'sag', \"''\", 'solver', '.', 'Stochastic', 'Average', 'GD', 'variant', 'SGD', '.', 'For', 'details', 'see', 'presentation', 'ƒMinimizing', 'Finite', 'Sums', 'Stochastic', 'Average', 'Gradient', 'Algo…', 'rithm⁄', 'Mark', 'Schmidt', 'et', 'al', '.', 'University', 'British', 'Columbia', '.', 'Figure', '4-17', '.', 'Ridge', 'Regression', 'Equation', '4-9', '.', 'Ridge', 'Regression', 'closed-form', 'solution', '–=T', '’', '+‰', '”', '1', '’', 'T', '’', 'Here', 'perform', 'Ridge', 'Regression', 'Scikit-Learn', 'using', 'closed-form', 'solu…', 'tion', 'variant', 'Equation', '4-9', 'using', 'matrix', 'factorization', 'technique', 'Andr•-Louis', 'Cholesky', '>', '>', '>', 'sklearn.linear_model', 'import', 'Ridge', '>', '>', '>', 'ridge_reg', '=', 'Ridge', 'alpha=1', 'solver=', \"''\", 'cholesky', \"''\", '>', '>', '>', 'ridge_reg.fit', 'X', '>', '>', '>', 'ridge_reg.predict', '1.5', 'array', '1.55071465', 'And', 'using', 'Stochastic', 'Gradient', 'Descent', '14', '>', '>', '>', 'sgd_reg', '=', 'SGDRegressor', 'penalty=', \"''\", 'l2', \"''\", '>', '>', '>', 'sgd_reg.fit', 'X', 'y.ravel', '>', '>', '>', 'sgd_reg.predict', '1.5', 'array', '1.13500145', 'The', 'penalty', 'hyperparameter', 'sets', 'type', 'regularization', 'term', 'use', '.', 'Specifying', \"''\", 'l2', \"''\", 'indicates', 'want', 'SGD', 'add', 'regularization', 'term', 'cost', 'function', 'equal', 'half', 'square', '—2', 'norm', 'weight', 'vector', 'simply', 'RidgeRegression.Regularized', 'Linear', 'Models', '|', '129', 'Lasso', 'RegressionLeast', 'Absolute', 'Shrinkage', 'Selection', 'Operator', 'Regression', 'simply', 'called', 'Lasso', 'Regression', 'another', 'regularized', 'version', 'Linear', 'Regression', 'like', 'RidgeRegression', 'adds', 'regularization', 'term', 'cost', 'function', 'uses', '—', '1', 'normof', 'weight', 'vector', 'instead', 'half', 'square', '—', '2', 'norm', 'see', 'Equation', '4-10', '.Equation', '4-10', '.', 'Lasso', 'Regression', 'cost', 'function', 'J–=MSE', '–+‰', '“', 'i=1', 'n–iFigure', '4-18', 'shows', 'thing', 'Figure', '4-17', 'replaces', 'Ridge', 'models', 'withLasso', 'models', 'uses', 'smaller', '‰', 'values.Figure', '4-18', '.', 'Lasso', 'Regression', 'An', 'important', 'characteristic', 'Lasso', 'Regression', 'tends', 'completely', 'elimi…', 'nate', 'weights', 'least', 'important', 'features', 'i.e.', 'set', 'zero', '.', 'For', 'example', 'dashed', 'line', 'right', 'plot', 'Figure', '4-18', '‰', '=', '10', '-7', 'looks', 'quadratic', 'almost', 'linear', 'weights', 'high-degree', 'polynomial', 'features', 'equal', 'zero', '.', 'In', 'words', 'Lasso', 'Regression', 'automatically', 'performs', 'feature', 'selection', 'outputs', 'asparse', 'model', 'i.e.', 'nonzero', 'feature', 'weights', '.', 'You', 'get', 'sense', 'case', 'looking', 'Figure', '4-19', 'top-leftplot', 'background', 'contours', 'ellipses', 'represent', 'unregularized', 'MSE', 'cost', 'func…', 'tion', '‰', '=', '0', 'white', 'circles', 'show', 'Batch', 'Gradient', 'Descent', 'path', 'cost', 'function', '.', 'The', 'foreground', 'contours', 'diamonds', 'represent', '—', '1', 'penalty', 'triangles', 'show', 'BGD', 'path', 'penalty', '‰', 'Œ', '‚', '.', 'Notice', 'path', 'first', '130', '|', 'Chapter', '4', 'Training', 'Models', '15You', 'think', 'subgradient', 'vector', 'nondifferentiable', 'point', 'intermediate', 'vector', 'gra…', 'dient', 'vectors', 'around', 'point', '.', 'reaches', '–1', '=', '0', 'rolls', 'gutter', 'reaches', '–2', '=', '0', '.', 'On', 'top-right', 'plot', 'contours', 'represent', 'cost', 'function', 'plus', '—', '1', 'penalty', '‰', '=', '0.5', '.', 'Theglobal', 'minimum', '–2', '=', '0', 'axis', '.', 'BGD', 'first', 'reaches', '–2', '=', '0', 'rolls', 'thegutter', 'reaches', 'global', 'minimum', '.', 'The', 'two', 'bottom', 'plots', 'show', 'thing', 'uses', '—2', 'penalty', 'instead', '.', 'The', 'regularized', 'minimum', 'closer', '–', '=', '0', 'thanthe', 'unregularized', 'minimum', 'weights', 'get', 'fully', 'eliminated', '.', 'Figure', '4-19', '.', 'Lasso', 'versus', 'Ridge', 'regularization', 'On', 'Lasso', 'cost', 'function', 'BGD', 'path', 'tends', 'bounce', 'across', 'gutter', 'toward', 'end', '.', 'This', 'slope', 'changes', 'abruptly', '–2', '=', '0', '.', 'You', 'need', 'gradually', 'reduce', 'learning', 'rate', 'order', 'actually', 'converge', 'global', 'minimum', '.', 'The', 'Lasso', 'cost', 'function', 'differentiable', '–i', '=', '0', '=', '1', '2', 'n', 'Gradient', 'Descent', 'still', 'works', 'fine', 'use', 'subgradient', 'vector', 'g15', 'instead', '–i', '=', '0.Equation', '4-11', 'shows', 'subgradient', 'vector', 'equation', 'use', 'Gradient', 'Descent', 'Lasso', 'cost', 'function.Regularized', 'Linear', 'Models', '|', '131', 'Equation', '4-11', '.', 'Lasso', 'Regression', 'subgradient', 'vector', 'g–', 'J=–MSE–+‰sign–1sign–2sign–nwheresign', '–i=', '”', '1if', '–i', '<', '0', '0if', '–i=0', '+1if', '–i', '>', '0', 'Here', 'small', 'Scikit-Learn', 'example', 'using', 'Lasso', 'class', '.', 'Note', 'could', 'instead', 'use', 'SGDRegressor', 'penalty=', \"''\", 'l1', \"''\", '.', '>', '>', '>', 'sklearn.linear_model', 'import', 'Lasso', '>', '>', '>', 'lasso_reg', '=', 'Lasso', 'alpha=0.1', '>', '>', '>', 'lasso_reg.fit', 'X', '>', '>', '>', 'lasso_reg.predict', '1.5', 'array', '1.53788174', 'Elastic', 'NetElastic', 'Net', 'middle', 'ground', 'Ridge', 'Regression', 'Lasso', 'Regression', '.', 'The', 'regularization', 'term', 'simple', 'mix', 'Ridge', 'Lasso‡s', 'regularization', 'terms', 'control', 'mix', 'ratio', 'r.', 'When', 'r', '=', '0', 'Elastic', 'Net', 'equivalent', 'Ridge', 'Regression', 'r', '=', '1', 'equivalent', 'Lasso', 'Regression', 'see', 'Equation', '4-12', '.Equation', '4-12', '.', 'Elastic', 'Net', 'cost', 'function', 'J–=MSE', '–+r‰', '“', 'i=1', 'n–i+1', '”', 'r2‰', '“', 'i=1', 'n–i2So', 'use', 'Linear', 'Regression', 'Ridge', 'Lasso', 'Elastic', 'Net', '?', 'It', 'almost', 'always', 'preferable', 'least', 'little', 'bit', 'regularization', 'generally', 'avoid', 'plain', 'Linear', 'Regression', '.', 'Ridge', 'good', 'default', 'suspect', 'features', 'actually', 'useful', 'prefer', 'Lasso', 'Elastic', 'Net', 'since', 'tend', 'reduce', 'useless', 'features‡', 'weights', 'zero', 'discussed', '.', 'In', 'general', 'Elastic', 'Net', 'preferred', 'Lasso', 'since', 'Lasso', 'may', 'behave', 'erratically', 'num…', 'ber', 'features', 'greater', 'number', 'training', 'instances', 'several', 'fea…', 'tures', 'strongly', 'correlated', '.', 'Here', 'short', 'example', 'using', 'Scikit-Learn‡s', 'ElasticNet', 'l1_ratio', 'corresponds', 'tothe', 'mix', 'ratio', 'r', '>', '>', '>', 'sklearn.linear_model', 'import', 'ElasticNet', '>', '>', '>', 'elastic_net', '=', 'ElasticNet', 'alpha=0.1', 'l1_ratio=0.5', '>', '>', '>', 'elastic_net.fit', 'X', '>', '>', '>', 'elastic_net.predict', '1.5', 'array', '1.54333232', '132', '|', 'Chapter', '4', 'Training', 'Models', 'Early', 'StoppingA', 'different', 'way', 'regularize', 'iterative', 'learning', 'algorithms', 'Gradient', 'Descent', 'stop', 'training', 'soon', 'validation', 'error', 'reaches', 'minimum', '.', 'This', 'called', 'early', 'stopping', '.', 'Figure', '4-20', 'shows', 'complex', 'model', 'case', 'high-degree', 'Polynomial', 'Regression', 'model', 'trained', 'using', 'Batch', 'Gradient', 'Descent', '.', 'As', 'epochs', 'go', 'algorithm', 'learns', 'prediction', 'error', 'RMSE', 'training', 'set', 'naturally', 'goes', 'prediction', 'error', 'validation', 'set', '.', 'However', 'validation', 'error', 'stops', 'decreasing', 'actually', 'starts', 'go', 'back', '.', 'This', 'indicates', 'model', 'started', 'overfit', 'training', 'data', '.', 'With', 'early', 'stop…', 'ping', 'stop', 'training', 'soon', 'validation', 'error', 'reaches', 'minimum', '.', 'It', 'simple', 'efficient', 'regularization', 'technique', 'Geoffrey', 'Hinton', 'called', 'ƒbeautiful', 'free', 'lunch.⁄', 'Figure', '4-20', '.', 'Early', 'stopping', 'regularization', 'With', 'Stochastic', 'Mini-batch', 'Gradient', 'Descent', 'curves', 'smooth', 'may', 'hard', 'know', 'whether', 'reached', 'minimum', '.', 'One', 'solution', 'stop', 'validation', 'error', 'minimum', 'time', 'confident', 'model', 'better', 'roll', 'back', 'model', 'parameters', 'point', 'validation', 'error', 'minimum', '.', 'Here', 'basic', 'implementation', 'early', 'stopping', 'sklearn.base', 'import', 'cloneRegularized', 'Linear', 'Models', '|', '133', 'sgd_reg', '=', 'SGDRegressor', 'n_iter=1', 'warm_start=True', 'penalty=None', 'learning_rate=', \"''\", 'constant', \"''\", 'eta0=0.0005', 'minimum_val_error', '=', 'float', '``', 'inf', \"''\", 'best_epoch', '=', 'Nonebest_model', '=', 'Nonefor', 'epoch', 'range', '1000', 'sgd_reg.fit', 'X_train_poly_scaled', 'y_train', '#', 'continues', 'left', 'y_val_predict', '=', 'sgd_reg.predict', 'X_val_poly_scaled', 'val_error', '=', 'mean_squared_error', 'y_val_predict', 'y_val', 'val_error', '<', 'minimum_val_error', 'minimum_val_error', '=', 'val_error', 'best_epoch', '=', 'epoch', 'best_model', '=', 'clone', 'sgd_reg', 'Note', 'warm_start=True', 'fit', 'method', 'called', 'continues', 'training', 'left', 'instead', 'restarting', 'scratch', '.', 'Logistic', 'RegressionAs', 'discussed', 'Chapter', '1', 'regression', 'algorithms', 'used', 'classifica…tion', 'well', 'vice', 'versa', '.', 'Logistic', 'Regression', 'also', 'called', 'Logit', 'Regression', 'com…monly', 'used', 'estimate', 'probability', 'instance', 'belongs', 'particular', 'class', 'e.g.', 'probability', 'email', 'spam', '?', '.', 'If', 'estimated', 'probability', 'greater', '50', '%', 'model', 'predicts', 'instance', 'belongs', 'class', 'called', 'positive', 'class', 'labeled', 'ƒ1⁄', 'else', 'predicts', 'i.e.', 'belongs', 'negative', 'class', 'labeled', 'ƒ0⁄', '.', 'This', 'makes', 'binary', 'classifier', '.', 'Estimating', 'ProbabilitiesSo', 'work', '?', 'Just', 'like', 'Linear', 'Regression', 'model', 'Logistic', 'Regression', 'model', 'computes', 'weighted', 'sum', 'input', 'features', 'plus', 'bias', 'term', 'instead', 'outputting', 'result', 'directly', 'like', 'Linear', 'Regression', 'model', 'outputs', 'thelogistic', 'result', 'see', 'Equation', '4-13', '.Equation', '4-13', '.', 'Logistic', 'Regression', 'model', 'estimated', 'probability', 'vectorized', 'form', 'p=h–=„–T', '’', 'The', 'logistic›also', 'called', 'logit', 'noted', '„', '’', '›is', 'sigmoid', 'function', 'i.e.', 'S-shaped', 'outputs', 'number', '0', '1', '.', 'It', 'defined', 'shown', 'Equation', '4-14', 'andFigure', '4-21.134', '|', 'Chapter', '4', 'Training', 'Models', 'Equation', '4-14', '.', 'Logistic', 'function', '„t=11+exp', '”', 'tFigure', '4-21', '.', 'Logistic', 'function', 'Once', 'Logistic', 'Regression', 'model', 'estimated', 'probability', 'p', '=', 'h–', 'x', 'instance', 'x', 'belongs', 'positive', 'class', 'make', 'prediction', 'ƒ', 'easily', 'see', 'Equa…tion', '4-15', '.Equation', '4-15', '.', 'Logistic', 'Regression', 'model', 'prediction', 'y=0if', 'p', '<', '0.5', '1if', 'pŠ0.5', '.', 'Notice', '„', '<', '0.5', '<', '0', '„', 'Š', '0.5', 'Š', '0', 'Logistic', 'Regressionmodel', 'predicts', '1', '–T', '’', 'x', 'positive', '0', 'negative', '.', 'Training', 'Cost', 'FunctionGood', 'know', 'Logistic', 'Regression', 'model', 'estimates', 'probabilities', 'makes', 'predictions', '.', 'But', 'trained', '?', 'The', 'objective', 'training', 'set', 'param…', 'eter', 'vector', '–', 'model', 'estimates', 'high', 'probabilities', 'positive', 'instances', '=1', 'low', 'probabilities', 'negative', 'instances', '=', '0', '.', 'This', 'idea', 'captured', 'cost', 'function', 'shown', 'Equation', '4-16', 'single', 'training', 'instance', 'x.Equation', '4-16', '.', 'Cost', 'function', 'single', 'training', 'instance', 'c–=', '”', 'log', 'pify=1', '”', 'log', '1', '”', 'pify=0', '.', 'This', 'cost', 'function', 'makes', 'sense', '–', 'log', 'grows', 'large', 'approaches', '0', 'cost', 'large', 'model', 'estimates', 'probability', 'close', '0', 'positive', 'Logistic', 'Regression', '|', '135', 'instance', 'also', 'large', 'model', 'estimates', 'probability', 'close', '1', 'negative', 'instance', '.', 'On', 'hand', '–', 'log', 'close', '0', 'close', '1', 'cost', 'close', '0', 'estimated', 'probability', 'close', '0', 'negative', 'instance', 'close', '1', 'positive', 'instance', 'precisely', 'want', '.', 'The', 'cost', 'function', 'whole', 'training', 'set', 'simply', 'average', 'cost', 'train…', 'ing', 'instances', '.', 'It', 'written', 'single', 'expression', 'verify', 'easily', 'called', 'log', 'loss', 'shown', 'Equation', '4-17', '.Equation', '4-17', '.', 'Logistic', 'Regression', 'cost', 'function', 'log', 'loss', 'J–=', '”', '1m', '“', 'i=1', 'myilog', 'pi+1', '”', 'yilog', '1', '”', 'piThe', 'bad', 'news', 'known', 'closed-form', 'equation', 'compute', 'value', '–', 'minimizes', 'cost', 'function', 'equivalent', 'Normal', 'Equation', '.', 'But', 'good', 'news', 'cost', 'function', 'convex', 'Gradient', 'Descent', 'optimization', 'algorithm', 'guaranteed', 'find', 'global', 'minimum', 'learn…', 'ing', 'rate', 'large', 'wait', 'long', 'enough', '.', 'The', 'partial', 'derivatives', 'cost', 'function', 'regards', 'jth', 'model', 'parameter', '–j', 'given', 'Equation', '4-18', '.Equation', '4-18', '.', 'Logistic', 'cost', 'function', 'partial', 'derivatives', 'ﬂﬂ–jJ–=1m', '“', 'i=1', 'm„–T', '’', '”', 'yixjiThis', 'equation', 'looks', 'much', 'like', 'Equation', '4-5', 'instance', 'computes', 'prediction', 'error', 'multiplies', 'j', 'th', 'feature', 'value', 'computes', 'average', 'training', 'instances', '.', 'Once', 'gradient', 'vector', 'containing', 'partial', 'derivatives', 'use', 'Batch', 'Gradient', 'Descent', 'algorithm', '.', 'That‡s', 'know', 'train', 'Logistic', 'Regression', 'model', '.', 'For', 'Stochastic', 'GD', 'would', 'course', 'take', 'one', 'instance', 'time', 'Mini-batch', 'GD', 'would', 'use', 'mini-batch', 'time', '.', 'Decision', 'BoundariesLet‡s', 'use', 'iris', 'dataset', 'illustrate', 'Logistic', 'Regression', '.', 'This', 'famous', 'dataset', 'contains', 'sepal', 'petal', 'length', 'width', '150', 'iris', 'flowers', 'three', 'different', 'species', 'Iris-Setosa', 'Iris-Versicolor', 'Iris-Virginica', 'see', 'Figure', '4-22', '.136', '|', 'Chapter', '4', 'Training', 'Models', '16Photos', 'reproduced', 'corresponding', 'Wikipedia', 'pages', '.', 'Iris-Virginica', 'photo', 'Frank', 'Mayfield', 'Crea…tive', 'Commons', 'BY-SA', '2.0', 'Iris-Versicolor', 'photo', 'D.', 'Gordon', 'E.', 'Robertson', 'Creative', 'Commons', 'BY-SA', '3.0', 'Iris-Setosa', 'photo', 'public', 'domain.Figure', '4-22', '.', 'Flowers', 'three', 'iris', 'plant', 'species', '16Let‡s', 'try', 'build', 'classifier', 'detect', 'Iris-Virginica', 'type', 'based', 'petal', 'width', 'feature', '.', 'First', 'let‡s', 'load', 'data', '>', '>', '>', 'sklearn', 'import', 'datasets', '>', '>', '>', 'iris', '=', 'datasets.load_iris', '>', '>', '>', 'list', 'iris.keys', '•data•', '•target_names•', '•feature_names•', '•target•', '•DESCR•', '>', '>', '>', 'X', '=', 'iris', '``', 'data', \"''\", '3', '#', 'petal', 'width', '>', '>', '>', '=', 'iris', '``', 'target', \"''\", '==', '2', '.astype', 'np.int', '#', '1', 'Iris-Virginica', 'else', '0Now', 'let‡s', 'train', 'Logistic', 'Regression', 'model', 'sklearn.linear_model', 'import', 'LogisticRegressionlog_reg', '=', 'LogisticRegression', 'log_reg.fit', 'X', 'Let‡s', 'look', 'model‡s', 'estimated', 'probabilities', 'flowers', 'petal', 'widths', 'varying', '0', '3', 'cm', 'Figure', '4-23', 'X_new', '=', 'np.linspace', '0', '3', '1000', '.reshape', '-1', '1', 'y_proba', '=', 'log_reg.predict_proba', 'X_new', 'plt.plot', 'X_new', 'y_proba', '1', '``', 'g-', \"''\", 'label=', \"''\", 'Iris-Virginica', \"''\", 'plt.plot', 'X_new', 'y_proba', '0', '``', 'b', '--', \"''\", 'label=', \"''\", 'Not', 'Iris-Virginica', \"''\", '#', '+', 'Matplotlib', 'code', 'make', 'image', 'look', 'prettyLogistic', 'Regression', '|', '137', '17It', 'set', 'points', 'x', '–0', '+', '–1x1', '+', '–2x2', '=', '0', 'defines', 'straight', 'line', '.', 'Figure', '4-23', '.', 'Estimated', 'probabilities', 'decision', 'boundary', 'The', 'petal', 'width', 'Iris-Virginica', 'flowers', 'represented', 'triangles', 'ranges', '1.4', 'cm', '2.5', 'cm', 'iris', 'flowers', 'represented', 'squares', 'generally', 'smaller', 'petal', 'width', 'ranging', '0.1', 'cm', '1.8', 'cm', '.', 'Notice', 'bit', 'over…', 'lap', '.', 'Above', '2', 'cm', 'classifier', 'highly', 'confident', 'flower', 'Iris-', 'Virginica', 'outputs', 'high', 'probability', 'class', '1', 'cm', 'highly', 'confident', 'Iris-Virginica', 'high', 'probability', 'ƒNot', 'Iris-Virginica⁄', 'class', '.', 'In', 'extremes', 'classifier', 'unsure', '.', 'However', 'ask', 'predict', 'class', 'using', 'predict', 'method', 'rather', 'predict_proba', 'method', 'return', 'whichever', 'class', 'likely', '.', 'Therefore', 'decision', 'boundary', 'around', '1.6', 'cm', 'probabilities', 'equal', '50', '%', 'petal', 'width', 'higher', '1.6', 'cm', 'classifier', 'predict', 'flower', 'Iris-', 'Virginica', 'else', 'predict', 'even', 'confident', '>', '>', '>', 'log_reg.predict', '1.7', '1.5', 'array', '1', '0', 'Figure', '4-24', 'shows', 'dataset', 'time', 'displaying', 'two', 'features', 'petal', 'width', 'length', '.', 'Once', 'trained', 'Logistic', 'Regression', 'classifier', 'estimate', 'probabil…', 'ity', 'new', 'flower', 'Iris-Virginica', 'based', 'two', 'features', '.', 'The', 'dashed', 'line', 'represents', 'points', 'model', 'estimates', '50', '%', 'probability', 'model‡s', 'decision', 'boundary', '.', 'Note', 'linear', 'boundary', '.', '17', 'Each', 'parallel', 'line', 'represents', 'points', 'model', 'outputs', 'specific', 'probability', '15', '%', 'bottom', 'left', '90', '%', 'top', 'right', '.', 'All', 'flowers', 'beyond', 'top-right', 'line', '90', '%', 'chance', 'Iris-Virginica', 'according', 'model', '.', '138', '|', 'Chapter', '4', 'Training', 'Models', 'Figure', '4-24', '.', 'Linear', 'decision', 'boundary', 'Just', 'like', 'linear', 'models', 'Logistic', 'Regression', 'models', 'regularized', 'using', '—1', '—2', 'penalties', '.', 'Scitkit-Learn', 'actually', 'adds', '—2', 'penalty', 'default', '.', 'The', 'hyperparameter', 'controlling', 'regularization', 'strength', 'Scikit-Learn', 'LogisticRegression', 'model', 'alpha', 'otherlinear', 'models', 'inverse', 'C.', 'The', 'higher', 'value', 'C', 'less', 'model', 'regularized.Softmax', 'RegressionThe', 'Logistic', 'Regression', 'model', 'generalized', 'support', 'multiple', 'classes', 'directly', 'without', 'train', 'combine', 'multiple', 'binary', 'classifiers', 'discussed', 'Chapter', '3', '.', 'This', 'called', 'So', '“', 'max', 'Regression', 'Multinomial', 'Logistic', 'Regression', '.The', 'idea', 'quite', 'simple', 'given', 'instance', 'x', 'Softmax', 'Regression', 'modelfirst', 'computes', 'score', 'sk', 'x', 'class', 'k', 'estimates', 'probability', 'class', 'applying', '“', 'max', 'function', 'also', 'called', 'normalized', 'exponential', 'thescores', '.', 'The', 'equation', 'compute', 'sk', 'x', 'look', 'familiar', 'like', 'equa…', 'tion', 'Linear', 'Regression', 'prediction', 'see', 'Equation', '4-19', '.Equation', '4-19', '.', 'So', '“', 'max', 'score', 'class', 'k', 'sk=–kT', '’', 'Note', 'class', 'dedicated', 'parameter', 'vector', '–k', '.', 'All', 'vectors', 'aretypically', 'stored', 'rows', 'parameter', 'matrix', '”', '.Once', 'computed', 'score', 'every', 'class', 'instance', 'x', 'estimate', 'probability', 'pk', 'instance', 'belongs', 'class', 'k', 'running', 'scores', 'throughLogistic', 'Regression', '|', '139', 'softmax', 'function', 'Equation', '4-20', 'computes', 'exponential', 'every', 'score', 'normalizes', 'dividing', 'sum', 'exponentials', '.', 'Equation', '4-20', '.', 'So', '“', 'max', 'function', 'pk=„k=expsk', '“', 'j=1', 'Kexpsj‹K', 'number', 'classes', '.', '‹s', 'x', 'vector', 'containing', 'scores', 'class', 'instance', 'x.‹„', 'x', 'k', 'estimated', 'probability', 'instance', 'x', 'belongs', 'class', 'k', 'giventhe', 'scores', 'class', 'instance', '.', 'Just', 'like', 'Logistic', 'Regression', 'classifier', 'Softmax', 'Regression', 'classifier', 'predicts', 'class', 'highest', 'estimated', 'probability', 'simply', 'class', 'highest', 'score', 'shown', 'Equation', '4-21', '.Equation', '4-21', '.', 'So', '“', 'max', 'Regression', 'classi†er', 'prediction', 'y=argmax', 'k„k=argmax', 'ksk=argmax', 'k–kT', '’', '‹The', 'argmax', 'operator', 'returns', 'value', 'variable', 'maximizes', 'function', '.', 'In', 'equation', 'returns', 'value', 'k', 'maximizes', 'estimated', 'probability', '„', 'x', 'k.The', 'Softmax', 'Regression', 'classifier', 'predicts', 'one', 'class', 'time', 'i.e.', 'multiclass', 'multioutput', 'used', 'mutually', 'exclusive', 'classes', 'different', 'types', 'plants', '.', 'You', 'use', 'recognize', 'multiple', 'people', 'one', 'picture', '.', 'Now', 'know', 'model', 'estimates', 'probabilities', 'makes', 'predictions', 'let‡s', 'take', 'look', 'training', '.', 'The', 'objective', 'model', 'estimates', 'high', 'probability', 'target', 'class', 'consequently', 'low', 'probability', 'classes', '.', 'Minimizing', 'cost', 'function', 'shown', 'Equation', '4-22', 'called', 'cross', 'entropy', 'lead', 'objective', 'penalizes', 'model', 'estimates', 'low', 'probability', 'target', 'class', '.', 'Cross', 'entropy', 'frequently', 'used', 'measure', 'well', 'set', 'estimated', 'class', 'probabilities', 'match', 'target', 'classes', 'use', 'several', 'times', 'following', 'chapters', '.', '140', '|', 'Chapter', '4', 'Training', 'Models', 'Equation', '4-22', '.', 'Cross', 'entropy', 'cost', 'function', 'J', '”', '=', '”', '1m', '“', 'i=1', '“', 'k=1', 'Kykilogpki‹yki', 'equal', '1', 'target', 'class', 'ith', 'instance', 'k', 'otherwise', 'equal', '0.Notice', 'two', 'classes', 'K', '=', '2', 'cost', 'function', 'equivalent', 'Logistic', 'Regression‡s', 'cost', 'function', 'log', 'loss', 'see', 'Equation', '4-17', '.Cross', 'EntropyCross', 'entropy', 'originated', 'information', 'theory', '.', 'Suppose', 'want', 'efficiently', 'transmit', 'information', 'weather', 'every', 'day', '.', 'If', 'eight', 'options', 'sunny', 'rainy', 'etc', '.', 'could', 'encode', 'option', 'using', '3', 'bits', 'since', '2', '3', '=', '8', '.', 'However', 'think', 'sunny', 'almost', 'every', 'day', 'would', 'much', 'efficient', 'code', 'ƒsunny⁄', 'one', 'bit', '0', 'seven', 'options', '4', 'bits', 'starting', '1', '.', 'Cross', 'entropy', 'measures', 'average', 'number', 'bits', 'actually', 'send', 'per', 'option', '.', 'If', 'assumption', 'weather', 'perfect', 'cross', 'entropy', 'equal', 'entropy', 'weather', 'i.e.', 'intrinsic', 'unpredictability', '.', 'But', 'assump…', 'tions', 'wrong', 'e.g.', 'rains', 'often', 'cross', 'entropy', 'greater', 'amount', 'called', 'Kullback', '‘', 'Leibler', 'divergence', '.The', 'cross', 'entropy', 'two', 'probability', 'distributions', 'p', 'q', 'defined', 'asHp', 'q=', '”', '“', 'xpxlogqx', 'least', 'distributions', 'discrete', '.', 'The', 'gradient', 'vector', 'cost', 'function', 'regards', '–k', 'given', 'Equation', '4-23', 'Equation', '4-23', '.', 'Cross', 'entropy', 'gradient', 'vector', 'class', 'k', '–kJ', '”', '=1m', '“', 'i=1', 'mpki', '”', 'ykiiNow', 'compute', 'gradient', 'vector', 'every', 'class', 'use', 'Gradient', 'Descent', 'optimization', 'algorithm', 'find', 'parameter', 'matrix', '”', 'minimizes', 'cost', 'function.Let‡s', 'use', 'Softmax', 'Regression', 'classify', 'iris', 'flowers', 'three', 'classes', '.', 'Scikit-', 'Learn‡s', 'LogisticRegression', 'uses', 'one-versus-all', 'default', 'train', 'two', 'classes', 'set', 'multi_class', 'hyperparameter', \"''\", 'multinomial', \"''\", 'switch', 'Softmax', 'Regression', 'instead', '.', 'You', 'must', 'also', 'specify', 'solver', 'sup…', 'ports', 'Softmax', 'Regression', '``', 'lbfgs', \"''\", 'solver', 'see', 'Scikit-Learn‡s', 'documenta…', 'Logistic', 'Regression', '|', '141', 'tion', 'details', '.', 'It', 'also', 'applies', '—', '2', 'regularization', 'default', 'control', 'using', 'hyperparameter', 'C.X', '=', 'iris', '``', 'data', \"''\", '2', '3', '#', 'petal', 'length', 'petal', 'widthy', '=', 'iris', '``', 'target', \"''\", 'softmax_reg', '=', 'LogisticRegression', 'multi_class=', \"''\", 'multinomial', \"''\", 'solver=', \"''\", 'lbfgs', \"''\", 'C=10', 'softmax_reg.fit', 'X', 'So', 'next', 'time', 'find', 'iris', '5', 'cm', 'long', '2', 'cm', 'wide', 'petals', 'askyour', 'model', 'tell', 'type', 'iris', 'answer', 'Iris-Virginica', 'class', '2', '94.2', '%', 'probability', 'Iris-Versicolor', '5.8', '%', 'probability', '>', '>', '>', 'softmax_reg.predict', '5', '2', 'array', '2', '>', '>', '>', 'softmax_reg.predict_proba', '5', '2', 'array', '6.33134078e-07', '5.75276067e-02', '9.42471760e-01', 'Figure', '4-25', 'shows', 'resulting', 'decision', 'boundaries', 'represented', 'background', 'colors', '.', 'Notice', 'decision', 'boundaries', 'two', 'classes', 'linear', '.', 'The', 'figure', 'also', 'shows', 'probabilities', 'Iris-Versicolor', 'class', 'represented', 'curved', 'lines', 'e.g.', 'line', 'labeled', '0.450', 'represents', '45', '%', 'probability', 'bound…', 'ary', '.', 'Notice', 'model', 'predict', 'class', 'estimated', 'probability', '50', '%', '.', 'For', 'example', 'point', 'decision', 'boundaries', 'meet', 'classes', 'equal', 'estimated', 'probability', '33', '%', '.', 'Figure', '4-25', '.', 'So', '“', 'max', 'Regression', 'decision', 'boundaries', 'Exercises1.What', 'Linear', 'Regression', 'training', 'algorithm', 'use', 'training', 'set', 'millions', 'features', '?', '2.Suppose', 'features', 'training', 'set', 'different', 'scales', '.', 'What', 'algo…', 'rithms', 'might', 'suffer', '?', 'What', '?', '142', '|', 'Chapter', '4', 'Training', 'Models', '3.Can', 'Gradient', 'Descent', 'get', 'stuck', 'local', 'minimum', 'training', 'Logistic', 'Regression', 'model', '?', '4.Do', 'Gradient', 'Descent', 'algorithms', 'lead', 'model', 'provided', 'let', 'run', 'long', 'enough', '?', '5.Suppose', 'use', 'Batch', 'Gradient', 'Descent', 'plot', 'validation', 'error', 'every', 'epoch', '.', 'If', 'notice', 'validation', 'error', 'consistently', 'goes', 'likely', 'going', '?', 'How', 'fix', '?', '6.Is', 'good', 'idea', 'stop', 'Mini-batch', 'Gradient', 'Descent', 'immediately', 'vali…', 'dation', 'error', 'goes', '?', '7.Which', 'Gradient', 'Descent', 'algorithm', 'among', 'discussed', 'reach', 'vicinity', 'optimal', 'solution', 'fastest', '?', 'Which', 'actually', 'converge', '?', 'How', 'make', 'others', 'converge', 'well', '?', '8.Suppose', 'using', 'Polynomial', 'Regression', '.', 'You', 'plot', 'learning', 'curves', 'notice', 'large', 'gap', 'training', 'error', 'validation', 'error', '.', 'What', 'happening', '?', 'What', 'three', 'ways', 'solve', '?', '9.Suppose', 'using', 'Ridge', 'Regression', 'notice', 'training', 'error', 'validation', 'error', 'almost', 'equal', 'fairly', 'high', '.', 'Would', 'say', 'model', 'suffers', 'high', 'bias', 'high', 'variance', '?', 'Should', 'increase', 'regulari…zation', 'hyperparameter', '‰', 'reduce', '?', '10.Why', 'would', 'want', 'use', '‹Ridge', 'Regression', 'instead', 'Linear', 'Regression', '?', '‹Lasso', 'instead', 'Ridge', 'Regression', '?', '‹Elastic', 'Net', 'instead', 'Lasso', '?', '11.Suppose', 'want', 'classify', 'pictures', 'outdoor/indoor', 'daytime/nighttime', '.', 'Should', 'implement', 'two', 'Logistic', 'Regression', 'classifiers', 'one', 'Softmax', 'Regres…', 'sion', 'classifier', '?', '12.Implement', 'Batch', 'Gradient', 'Descent', 'early', 'stopping', 'Softmax', 'Regression', 'without', 'using', 'Scikit-Learn', '.Solutions', 'exercises', 'available', 'Appendix', 'A', '.Exercises', '|', '143', 'CHAPTER', '5Support', 'Vector', 'MachinesA', 'Support', 'Vector', 'Machine', 'SVM', 'powerful', 'versatile', 'Machine', 'Learning', 'model', 'capable', 'performing', 'linear', 'nonlinear', 'classification', 'regression', 'even', 'outlier', 'detection', '.', 'It', 'one', 'popular', 'models', 'Machine', 'Learning', 'any…', 'one', 'interested', 'Machine', 'Learning', 'toolbox', '.', 'SVMs', 'partic…', 'ularly', 'well', 'suited', 'classification', 'complex', 'small-', 'medium-sized', 'datasets', '.', 'This', 'chapter', 'explain', 'core', 'concepts', 'SVMs', 'use', 'work.Linear', 'SVM', 'Classi•cationThe', 'fundamental', 'idea', 'behind', 'SVMs', 'best', 'explained', 'pictures', '.', 'Figure', '5-1shows', 'part', 'iris', 'dataset', 'introduced', 'end', 'Chapter', '4', '.', 'The', 'twoclasses', 'clearly', 'separated', 'easily', 'straight', 'line', 'linearly', 'separable', '.The', 'left', 'plot', 'shows', 'decision', 'boundaries', 'three', 'possible', 'linear', 'classifiers', '.', 'Themodel', 'whose', 'decision', 'boundary', 'represented', 'dashed', 'line', 'bad', 'even', 'separate', 'classes', 'properly', '.', 'The', 'two', 'models', 'work', 'perfectly', 'training', 'set', 'decision', 'boundaries', 'come', 'close', 'instances', 'models', 'probably', 'perform', 'well', 'new', 'instances', '.', 'In', 'contrast', 'solid', 'line', 'plot', 'right', 'represents', 'decision', 'boundary', 'SVM', 'classi…', 'fier', 'line', 'separates', 'two', 'classes', 'also', 'stays', 'far', 'away', 'closest', 'training', 'instances', 'possible', '.', 'You', 'think', 'SVM', 'classifier', 'fitting', 'widest', 'possible', 'street', 'represented', 'parallel', 'dashed', 'lines', 'classes', '.', 'This', 'called', 'large', 'margin', 'classi†cation.145Figure', '5-1', '.', 'Large', 'margin', 'classi†cationNotice', 'adding', 'training', 'instances', 'ƒoff', 'street⁄', 'affect', 'decision', 'boundary', 'fully', 'determined', 'ƒsupported⁄', 'instances', 'located', 'edge', 'street', '.', 'These', 'instances', 'called', 'support', 'vectors', 'circled', 'inFigure', '5-1', '.SVMs', 'sensitive', 'feature', 'scales', 'see', 'Figure', '5-2', 'left', 'plot', 'vertical', 'scale', 'much', 'larger', 'horizontal', 'scale', 'widest', 'possible', 'street', 'close', 'horizontal', '.', 'After', 'feature', 'scaling', 'e.g.', 'using', 'Scikit-Learn‡s', 'StandardScaler', 'decision', 'boundary', 'looks', 'much', 'better', 'right', 'plot', '.', 'Figure', '5-2', '.', 'Sensitivity', 'feature', 'scales', 'Soft', 'Margin', 'Classi•cationIf', 'strictly', 'impose', 'instances', 'street', 'right', 'side', 'called', 'hard', 'margin', 'classi†cation', '.', 'There', 'two', 'main', 'issues', 'hard', 'margin', 'classifi…cation', '.', 'First', 'works', 'data', 'linearly', 'separable', 'second', 'quite', 'sensi…', 'tive', 'outliers', '.', 'Figure', '5-3', 'shows', 'iris', 'dataset', 'one', 'additional', 'outlier', 'left', 'impossible', 'find', 'hard', 'margin', 'right', 'decision', 'boundary', 'ends', 'different', 'one', 'saw', 'Figure', '5-1', 'without', 'outlier', 'probably', 'generalize', 'well.146', '|', 'Chapter', '5', 'Support', 'Vector', 'Machines', 'Figure', '5-3', '.', 'Hard', 'margin', 'sensitivity', 'outliers', 'To', 'avoid', 'issues', 'preferable', 'use', 'flexible', 'model', '.', 'The', 'objective', 'find', 'good', 'balance', 'keeping', 'street', 'large', 'possible', 'limiting', 'themargin', 'violations', 'i.e.', 'instances', 'end', 'middle', 'street', 'even', 'wrong', 'side', '.', 'This', 'called', '“', 'margin', 'classi†cation.In', 'Scikit-Learn‡s', 'SVM', 'classes', 'control', 'balance', 'using', 'C', 'hyperparame…', 'ter', 'smaller', 'C', 'value', 'leads', 'wider', 'street', 'margin', 'violations', '.', 'Figure', '5-4shows', 'decision', 'boundaries', 'margins', 'two', 'soft', 'margin', 'SVM', 'classifiers', 'anonlinearly', 'separable', 'dataset', '.', 'On', 'left', 'using', 'high', 'C', 'value', 'classifier', 'makesfewer', 'margin', 'violations', 'ends', 'smaller', 'margin', '.', 'On', 'right', 'using', 'low', 'C', 'value', 'margin', 'much', 'larger', 'many', 'instances', 'end', 'street', '.', 'However', 'seems', 'likely', 'second', 'classifier', 'generalize', 'better', 'fact', 'even', 'training', 'set', 'makes', 'fewer', 'prediction', 'errors', 'since', 'margin', 'violations', 'actually', 'correct', 'side', 'decision', 'boundary', '.', 'Figure', '5-4', '.', 'Fewer', 'margin', 'violations', 'versus', 'large', 'margin', 'If', 'SVM', 'model', 'overfitting', 'try', 'regularizing', 'reducing', 'C.The', 'following', 'Scikit-Learn', 'code', 'loads', 'iris', 'dataset', 'scales', 'features', 'trains', 'linear', 'SVM', 'model', 'using', 'LinearSVC', 'class', 'C', '=', '0.1', 'hinge', 'loss', 'Linear', 'SVM', 'Classi•cation', '|', '147', 'function', 'described', 'shortly', 'detect', 'Iris-Virginica', 'flowers', '.', 'The', 'resulting', 'model', 'represented', 'right', 'Figure', '5-4.import', 'numpy', 'npfrom', 'sklearn', 'import', 'datasetsfrom', 'sklearn.pipeline', 'import', 'Pipelinefrom', 'sklearn.preprocessing', 'import', 'StandardScalerfrom', 'sklearn.svm', 'import', 'LinearSVCiris', '=', 'datasets.load_iris', 'X', '=', 'iris', '``', 'data', \"''\", '2', '3', '#', 'petal', 'length', 'petal', 'widthy', '=', 'iris', '``', 'target', \"''\", '==', '2', '.astype', 'np.float64', '#', 'Iris-Virginicasvm_clf', '=', 'Pipeline', '``', 'scaler', \"''\", 'StandardScaler', '``', 'linear_svc', \"''\", 'LinearSVC', 'C=1', 'loss=', \"''\", 'hinge', \"''\", 'svm_clf.fit', 'X_scaled', 'Then', 'usual', 'use', 'model', 'make', 'predictions', '>', '>', '>', 'svm_clf.predict', '5.5', '1.7', 'array', '1', '.', 'Unlike', 'Logistic', 'Regression', 'classifiers', 'SVM', 'classifiers', 'out…', 'put', 'probabilities', 'class.Alternatively', 'could', 'use', 'SVC', 'class', 'using', 'SVC', 'kernel=', \"''\", 'linear', \"''\", 'C=1', 'itis', 'much', 'slower', 'especially', 'large', 'training', 'sets', 'recommended', '.', 'Another', 'option', 'use', 'SGDClassifier', 'class', 'SGDClassifier', 'loss=', \"''\", 'hinge', \"''\", 'alpha=1/', 'm*C', '.', 'This', 'applies', 'regular', 'Stochastic', 'Gradient', 'Descent', 'see', 'Chapter', '4', 'totrain', 'linear', 'SVM', 'classifier', '.', 'It', 'converge', 'fast', 'LinearSVC', 'class', 'itcan', 'useful', 'handle', 'huge', 'datasets', 'fit', 'memory', 'out-of-core', 'train…', 'ing', 'handle', 'online', 'classification', 'tasks', '.', 'The', 'LinearSVC', 'class', 'regularizes', 'bias', 'term', 'center', 'training', 'set', 'first', 'subtracting', 'mean', '.', 'This', 'automatic', 'scale', 'data', 'using', 'StandardScaler', '.', 'Moreover', 'make', 'sure', 'set', 'loss', 'hyperparameter', \"''\", 'hinge', \"''\", 'default', 'value', '.', 'Finally', 'better', 'performance', 'set', 'dualhyperparameter', 'False', 'unless', 'features', 'training', 'instances', 'discuss', 'duality', 'later', 'chapter', '.', '148', '|', 'Chapter', '5', 'Support', 'Vector', 'Machines', 'Nonlinear', 'SVM', 'Classi•cationAlthough', 'linear', 'SVM', 'classifiers', 'efficient', 'work', 'surprisingly', 'well', 'many', 'cases', 'many', 'datasets', 'even', 'close', 'linearly', 'separable', '.', 'One', 'approach', 'handling', 'nonlinear', 'datasets', 'add', 'features', 'polynomial', 'features', 'Chapter', '4', 'cases', 'result', 'linearly', 'separable', 'dataset', '.', 'Consider', 'left', 'plot', 'Figure', '5-5', 'represents', 'simple', 'dataset', 'one', 'feature', 'x1', '.', 'This', 'dataset', 'linearly', 'separable', 'see', '.', 'But', 'add', 'second', 'fea…', 'ture', 'x2', '=', 'x1', '2', 'resulting', '2D', 'dataset', 'perfectly', 'linearly', 'separable', '.', 'Figure', '5-5', '.', 'Adding', 'features', 'make', 'dataset', 'linearly', 'separable', 'To', 'implement', 'idea', 'using', 'Scikit-Learn', 'create', 'Pipeline', 'containing', 'PolynomialFeatures', 'transformer', 'discussed', 'ƒPolynomial', 'Regression⁄', 'page', '121', 'followed', 'StandardScaler', 'LinearSVC', '.', 'Let‡s', 'test', 'moons', 'dataset', 'see', 'Figure', '5-6', 'sklearn.datasets', 'import', 'make_moonsfrom', 'sklearn.pipeline', 'import', 'Pipelinefrom', 'sklearn.preprocessing', 'import', 'PolynomialFeaturespolynomial_svm_clf', '=', 'Pipeline', '``', 'poly_features', \"''\", 'PolynomialFeatures', 'degree=3', '``', 'scaler', \"''\", 'StandardScaler', '``', 'svm_clf', \"''\", 'LinearSVC', 'C=10', 'loss=', \"''\", 'hinge', \"''\", 'polynomial_svm_clf.fit', 'X', 'Nonlinear', 'SVM', 'Classi•cation', '|', '149', 'Figure', '5-6', '.', 'Linear', 'SVM', 'classi†er', 'using', 'polynomial', 'features', 'Polynomial', 'KernelAdding', 'polynomial', 'features', 'simple', 'implement', 'work', 'great', 'sorts', 'Machine', 'Learning', 'algorithms', 'SVMs', 'low', 'polynomial', 'degree', 'deal', 'complex', 'datasets', 'high', 'polynomial', 'degree', 'creates', 'huge', 'number', 'features', 'making', 'model', 'slow', '.', 'Fortunately', 'using', 'SVMs', 'apply', 'almost', 'miraculous', 'mathematical', 'technique', 'called', 'kernel', 'trick', 'explained', 'moment', '.', 'It', 'makes', 'possible', 'get', 'result', 'added', 'many', 'polynomial', 'features', 'even', 'high-', 'degree', 'polynomials', 'without', 'actually', 'add', '.', 'So', 'combinato…', 'rial', 'explosion', 'number', 'features', 'since', 'don‡t', 'actually', 'add', 'features', '.', 'This', 'trick', 'implemented', 'SVC', 'class', '.', 'Let‡s', 'test', 'moons', 'dataset', 'sklearn.svm', 'import', 'SVCpoly_kernel_svm_clf', '=', 'Pipeline', '``', 'scaler', \"''\", 'StandardScaler', '``', 'svm_clf', \"''\", 'SVC', 'kernel=', \"''\", 'poly', \"''\", 'degree=3', 'coef0=1', 'C=5', 'poly_kernel_svm_clf.fit', 'X', 'This', 'code', 'trains', 'SVM', 'classifier', 'using', '3rd-degree', 'polynomial', 'kernel', '.', 'It', 'repre…', 'sented', 'left', 'Figure', '5-7', '.', 'On', 'right', 'another', 'SVM', 'classifier', 'using', '10', 'th-degree', 'polynomial', 'kernel', '.', 'Obviously', 'model', 'overfitting', 'might', 'want', '150', '|', 'Chapter', '5', 'Support', 'Vector', 'Machines', 'reduce', 'polynomial', 'degree', '.', 'Conversely', 'underfitting', 'try', 'increasing', '.', 'The', 'hyperparameter', 'coef0', 'controls', 'much', 'model', 'influenced', 'high-', 'degree', 'polynomials', 'versus', 'low-degree', 'polynomials.Figure', '5-7', '.', 'SVM', 'classi†ers', 'polynomial', 'kernel', 'A', 'common', 'approach', 'find', 'right', 'hyperparameter', 'values', 'use', 'grid', 'search', 'see', 'Chapter', '2', '.', 'It', 'often', 'faster', 'first', 'coarse', 'grid', 'search', 'finer', 'grid', 'search', 'around', 'best', 'valuesfound', '.', 'Having', 'good', 'sense', 'hyperparameter', 'actually', 'also', 'help', 'search', 'right', 'part', 'hyperparame…', 'ter', 'space.Adding', 'Similarity', 'FeaturesAnother', 'technique', 'tackle', 'nonlinear', 'problems', 'add', 'features', 'computed', 'using', 'similarity', 'function', 'measures', 'much', 'instance', 'resembles', 'particular', 'landmark', '.', 'For', 'example', 'let‡s', 'take', 'one-dimensional', 'dataset', 'discussed', 'earlier', 'add', 'two', 'landmarks', 'x1', '=', '–2', 'x1', '=', '1', 'see', 'left', 'plot', 'Figure', '5-8', '.', 'Next', 'let‡s', 'define', 'similarity', 'function', 'Gaussian', 'Radial', 'Basis', 'Function', 'RBF', '’', '=', '0.3', 'see', 'Equation', '5-1', '.Equation', '5-1', '.', 'Gaussian', 'RBF', '‚', '’', '—', '=exp', '”', '’', '”', '—', '2It', 'bell-shaped', 'function', 'varying', '0', 'far', 'away', 'landmark', '1', 'landmark', '.', 'Now', 'ready', 'compute', 'new', 'features', '.', 'For', 'example', 'let‡s', 'look', 'instance', 'x1', '=', '–1', 'located', 'distance', '1', 'first', 'landmark', '2', 'second', 'landmark', '.', 'Therefore', 'new', 'features', 'x2', '=', 'exp', '–0.3', '‰', '12', 'Ÿ', '0.74and', 'x3', '=', 'exp', '–0.3', '‰', '22', 'Ÿ', '0.30', '.', 'The', 'plot', 'right', 'Figure', '5-8', 'shows', 'trans…formed', 'dataset', 'dropping', 'original', 'features', '.', 'As', 'see', 'linearly', 'separable.Nonlinear', 'SVM', 'Classi•cation', '|', '151', 'Figure', '5-8', '.', 'Similarity', 'features', 'using', 'Gaussian', 'RBF', 'You', 'may', 'wonder', 'select', 'landmarks', '.', 'The', 'simplest', 'approach', 'create', 'landmark', 'location', 'every', 'instance', 'dataset', '.', 'This', 'creates', 'many', 'dimensions', 'thus', 'increases', 'chances', 'transformed', 'training', 'set', 'linearly', 'separable', '.', 'The', 'downside', 'training', 'set', 'instances', 'n', 'features', 'gets', 'transformed', 'training', 'set', 'instances', 'features', 'assuming', 'drop', 'original', 'features', '.', 'If', 'training', 'set', 'large', 'end', 'equally', 'large', 'number', 'features', '.', 'Gaussian', 'RBF', 'KernelJust', 'like', 'polynomial', 'features', 'method', 'similarity', 'features', 'method', 'useful', 'Machine', 'Learning', 'algorithm', 'may', 'computationally', 'expensive', 'compute', 'additional', 'features', 'especially', 'large', 'training', 'sets', '.', 'However', 'kernel', 'trick', 'SVM', 'magic', 'makes', 'possible', 'obtain', 'similarresult', 'added', 'many', 'similarity', 'features', 'without', 'actually', 'add', '.', 'Let‡s', 'try', 'Gaussian', 'RBF', 'kernel', 'using', 'SVC', 'class', 'rbf_kernel_svm_clf', '=', 'Pipeline', '``', 'scaler', \"''\", 'StandardScaler', '``', 'svm_clf', \"''\", 'SVC', 'kernel=', \"''\", 'rbf', \"''\", 'gamma=5', 'C=0.001', 'rbf_kernel_svm_clf.fit', 'X', 'This', 'model', 'represented', 'bottom', 'left', 'Figure', '5-9', '.', 'The', 'plots', 'showmodels', 'trained', 'different', 'values', 'hyperparameters', 'gamma', '’', 'C.', 'Increasinggamma', 'makes', 'bell-shape', 'curve', 'narrower', 'see', 'left', 'plot', 'Figure', '5-8', 'aresult', 'instance‡s', 'range', 'influence', 'smaller', 'decision', 'boundary', 'ends', 'irregular', 'wiggling', 'around', 'individual', 'instances', '.', 'Conversely', 'small', 'gamma', 'value', 'makes', 'bell-shaped', 'curve', 'wider', 'instances', 'larger', 'range', 'influ…', 'ence', 'decision', 'boundary', 'ends', 'smoother', '.', 'So', '’', 'acts', 'like', 'regularization', 'hyperparameter', 'model', 'overfitting', 'reduce', 'under…fitting', 'increase', 'similar', 'C', 'hyperparameter', '.', '152', '|', 'Chapter', '5', 'Support', 'Vector', 'Machines', '1ƒA', 'Dual', 'Coordinate', 'Descent', 'Method', 'Large-scale', 'Linear', 'SVM', '⁄', 'Lin', 'et', 'al', '.', '2008', '.', 'Figure', '5-9', '.', 'SVM', 'classi†ers', 'using', 'RBF', 'kernel', 'Other', 'kernels', 'exist', 'used', 'much', 'rarely', '.', 'For', 'example', 'kernels', 'specialized', 'specific', 'data', 'structures', '.', 'String', 'kernels', 'sometimes', 'used', 'classi…fying', 'text', 'documents', 'DNA', 'sequences', 'e.g.', 'using', 'string', 'subsequence', 'kernel', 'orkernels', 'based', 'Levenshtein', 'distance', '.With', 'many', 'kernels', 'choose', 'decide', 'one', 'use', '?', 'As', 'rule', 'thumb', 'always', 'try', 'linear', 'kernel', 'first', 'remember', 'LinearSVC', 'much', 'faster', 'SVC', 'kernel=', \"''\", 'linear', \"''\", 'especially', 'training', 'set', 'large', 'plenty', 'features', '.', 'If', 'training', 'set', 'large', 'try', 'Gaussian', 'RBF', 'kernel', 'well', 'works', 'well', 'cases', '.', 'Then', 'spare', 'time', 'computing', 'power', 'also', 'experiment', 'kernels', 'using', 'cross-validation', 'grid', 'search', 'especially', 'kernels', 'specialized', 'trainingset‡s', 'data', 'structure', '.', 'Computational', 'ComplexityThe', 'LinearSVC', 'class', 'based', 'liblinear', 'library', 'implements', 'optimizedalgorithm', 'linear', 'SVMs.1', 'It', 'support', 'kernel', 'trick', 'scales', 'almost', 'Nonlinear', 'SVM', 'Classi•cation', '|', '153', '2ƒSequential', 'Minimal', 'Optimization', 'SMO', '⁄', 'J.', 'Platt', '1998', '.', 'linearly', 'number', 'training', 'instances', 'number', 'features', 'training', 'time', 'complexity', 'roughly', 'O', '‰', 'n', '.The', 'algorithm', 'takes', 'longer', 'require', 'high', 'precision', '.', 'This', 'controlled', 'tolerance', 'hyperparameter', 'called', 'tol', 'Scikit-Learn', '.', 'In', 'classification', 'tasks', 'default', 'tolerance', 'fine', '.', 'The', 'SVC', 'class', 'based', 'libsvm', 'library', 'implements', 'algorithm', 'sup…', 'ports', 'kernel', 'trick.2', 'The', 'training', 'time', 'complexity', 'usually', 'O', 'm2', '‰', 'n', 'O', 'm3', '‰', 'n', '.', 'Unfortunately', 'means', 'gets', 'dreadfully', 'slow', 'num…', 'ber', 'training', 'instances', 'gets', 'large', 'e.g.', 'hundreds', 'thousands', 'instances', '.', 'This', 'algorithm', 'perfect', 'complex', 'small', 'medium', 'training', 'sets', '.', 'However', 'scales', 'well', 'number', 'features', 'especially', 'sparse', 'features', 'i.e.', 'eachinstance', 'nonzero', 'features', '.', 'In', 'case', 'algorithm', 'scales', 'roughly', 'average', 'number', 'nonzero', 'features', 'per', 'instance', '.', 'Table', '5-1', 'compares', 'Scikit-Learn‡s', 'SVM', 'classification', 'classes', '.', 'Table', '5-1', '.', 'Comparison', 'Scikit-Learn', 'classes', 'SVM', 'classi†cationClassTime', 'complexity', 'Out-of-core', 'support', 'Scaling', 'required', 'Kernel', 'trick', 'LinearSVCO', '†', 'n', 'NoYesNoSGDClassifierO', '†', 'n', 'YesYesNoSVCO', 'm‡', '†', 'n', 'O', 'm…', '†', 'n', 'NoYesYesSVM', 'RegressionAs', 'mentioned', 'earlier', 'SVM', 'algorithm', 'quite', 'versatile', 'sup…', 'port', 'linear', 'nonlinear', 'classification', 'also', 'supports', 'linear', 'nonlinear', 'regression', '.', 'The', 'trick', 'reverse', 'objective', 'instead', 'trying', 'fit', 'largest', 'pos…', 'sible', 'street', 'two', 'classes', 'limiting', 'margin', 'violations', 'SVM', 'Regression', 'tries', 'fit', 'many', 'instances', 'possible', 'street', 'limiting', 'margin', 'violations', 'i.e.', 'instances', 'o›', 'street', '.', 'The', 'width', 'street', 'controlled', 'hyperparame…', 'ter', '.', 'Figure', '5-10', 'shows', 'two', 'linear', 'SVM', 'Regression', 'models', 'trained', 'randomlinear', 'data', 'one', 'large', 'margin', '=', '1.5', 'small', 'margin', '=0.5', '.154', '|', 'Chapter', '5', 'Support', 'Vector', 'Machines', 'Figure', '5-10', '.', 'SVM', 'Regression', 'Adding', 'training', 'instances', 'within', 'margin', 'affect', 'model‡s', 'predic…', 'tions', 'thus', 'model', 'said', '-insensitive', '.You', 'use', 'Scikit-Learn‡s', 'LinearSVR', 'class', 'perform', 'linear', 'SVM', 'Regression', '.', 'Thefollowing', 'code', 'produces', 'model', 'represented', 'left', 'Figure', '5-10', 'train…ing', 'data', 'scaled', 'centered', 'first', 'sklearn.svm', 'import', 'LinearSVRsvm_reg', '=', 'LinearSVR', 'epsilon=1.5', 'svm_reg.fit', 'X', 'To', 'tackle', 'nonlinear', 'regression', 'tasks', 'use', 'kernelized', 'SVM', 'model', '.', 'For', 'exam…', 'ple', 'Figure', '5-11', 'shows', 'SVM', 'Regression', 'random', 'quadratic', 'training', 'set', 'using', '2nd-degree', 'polynomial', 'kernel', '.', 'There', 'little', 'regularization', 'left', 'plot', 'i.e.', 'large', 'C', 'value', 'much', 'regularization', 'right', 'plot', 'i.e.', 'small', 'C', 'value', '.Figure', '5-11', '.', 'SVM', 'regression', 'using', '2', 'nd', '-degree', 'polynomial', 'kernel', 'SVM', 'Regression', '|', '155', 'The', 'following', 'code', 'produces', 'model', 'represented', 'left', 'Figure', '5-11', 'usingScikit-Learn‡s', 'SVR', 'class', 'supports', 'kernel', 'trick', '.', 'The', 'SVR', 'class', 'regres…sion', 'equivalent', 'SVC', 'class', 'LinearSVR', 'class', 'regression', 'equivalent', 'LinearSVC', 'class', '.', 'The', 'LinearSVR', 'class', 'scales', 'linearly', 'size', 'train…ing', 'set', 'like', 'LinearSVC', 'class', 'SVR', 'class', 'gets', 'much', 'slow', 'training', 'set', 'grows', 'large', 'like', 'SVC', 'class', '.from', 'sklearn.svm', 'import', 'SVRsvm_poly_reg', '=', 'SVR', 'kernel=', \"''\", 'poly', \"''\", 'degree=2', 'C=100', 'epsilon=0.1', 'svm_poly_reg.fit', 'X', 'SVMs', 'also', 'used', 'outlier', 'detection', 'see', 'Scikit-Learn‡s', 'doc…', 'umentation', 'details', '.', 'Under', 'HoodThis', 'section', 'explains', 'SVMs', 'make', 'predictions', 'training', 'algorithmswork', 'starting', 'linear', 'SVM', 'classifiers', '.', 'You', 'safely', 'skip', 'go', 'straight', 'exercises', 'end', 'chapter', 'getting', 'started', 'Machine', 'Learn…', 'ing', 'come', 'back', 'later', 'want', 'get', 'deeper', 'understanding', 'SVMs', '.', 'First', 'word', 'notations', 'Chapter', '4', 'used', 'convention', 'putting', 'model', 'parameters', 'one', 'vector', '–', 'including', 'bias', 'term', '–0', 'input', 'feature', 'weights', '–1', '–n', 'adding', 'bias', 'input', 'x0', '=', '1', 'instances', '.', 'In', 'chapter', 'use', 'different', 'convention', 'convenient', 'common', 'dealing', 'SVMs', 'bias', 'term', 'called', 'b', 'feature', 'weights', 'vector', 'called', 'w.', 'No', 'bias', 'feature', 'added', 'input', 'feature', 'vectors', '.', 'Decision', 'Function', 'PredictionsThe', 'linear', 'SVM', 'classifier', 'model', 'predicts', 'class', 'new', 'instance', 'x', 'simply', 'com…', 'puting', 'decision', 'function', 'wT', '’', 'x', '+', 'b', '=', 'w1', 'x1', '+', '+', 'wn', 'xn', '+', 'b', 'result', 'posi…tive', 'predicted', 'class', 'ƒ', 'positive', 'class', '1', 'else', 'negative', 'class', '0', 'see', 'Equation', '5-2', '.Equation', '5-2', '.', 'Linear', 'SVM', 'classi†er', 'prediction', 'y=0if', 'T', '’', '+b', '<', '0', '1if', 'T', '’', '+bŠ0', '156', '|', 'Chapter', '5', 'Support', 'Vector', 'Machines', '3More', 'generally', 'n', 'features', 'decision', 'function', 'n-dimensional', 'hyperplane', 'deci…sion', 'boundary', 'n', '–', '1', '-dimensional', 'hyperplane', '.', 'Figure', '5-12', 'shows', 'decision', 'function', 'corresponds', 'model', 'right', 'Figure', '5-4', 'two-dimensional', 'plane', 'since', 'dataset', 'two', 'features', 'petal', 'width', 'petal', 'length', '.', 'The', 'decision', 'boundary', 'set', 'points', 'decision', 'function', 'equal', '0', 'intersection', 'two', 'planes', 'straight', 'line', 'rep…', 'resented', 'thick', 'solid', 'line', '.', '3Figure', '5-12', '.', 'Decision', 'function', 'iris', 'dataset', 'The', 'dashed', 'lines', 'represent', 'points', 'decision', 'function', 'equal', '1', '–1', 'parallel', 'equal', 'distance', 'decision', 'boundary', 'forming', 'margin', 'around', '.', 'Training', 'linear', 'SVM', 'classifier', 'means', 'finding', 'value', 'w', 'b', 'make', 'margin', 'wide', 'possible', 'avoiding', 'margin', 'violations', 'hard', 'margin', 'limiting', 'soft', 'margin', '.Training', 'ObjectiveConsider', 'slope', 'decision', 'function', 'equal', 'norm', 'weight', 'vec…', 'tor', 'w', '.', 'If', 'divide', 'slope', '2', 'points', 'decision', 'function', 'equal', 'ﬁ1', 'going', 'twice', 'far', 'away', 'decision', 'boundary', '.', 'In', 'words', 'dividing', 'slope', '2', 'multiply', 'margin', '2', '.', 'Perhaps', 'easier', 'visual…', 'ize', '2D', 'Figure', '5-13', '.', 'The', 'smaller', 'weight', 'vector', 'w', 'larger', 'margin.Under', 'Hood', '|', '157', '4Zeta', '™', '8th', 'letter', 'Greek', 'alphabet.Figure', '5-13', '.', 'A', 'smaller', 'weight', 'vector', 'results', 'larger', 'margin', 'So', 'want', 'minimize', 'w', 'get', 'large', 'margin', '.', 'However', 'also', 'want', 'avoid', 'margin', 'violation', 'hard', 'margin', 'need', 'decision', 'function', 'greater', '1', 'positive', 'training', 'instances', 'lower', '–1', 'negative', 'training', 'instances', '.', 'If', 'define', '=', '–1', 'negative', 'instances', '=', '0', '=', '1', 'positive', 'instances', '=', '1', 'express', 'constraint', 'wT', '’', 'x', '+', 'b', 'Š', '1', 'allinstances.We', 'therefore', 'express', 'hard', 'margin', 'linear', 'SVM', 'classifier', 'objective', 'con…', 'strained', 'optimization', 'problem', 'Equation', '5-3', '.Equation', '5-3', '.', 'Hard', 'margin', 'linear', 'SVM', 'classi†er', 'objective', 'minimize', 'b12T', '’', 'subjectto', 'tiT', '’', 'i+bŠ1for', 'i=1,2', 'mWe', 'minimizing', '12wT', '’', 'w', 'equal', '12', 'w', '2', 'rather', 'minimizing', 'w', '.', 'This', 'give', 'result', 'since', 'values', 'w', 'b', 'minimize', 'value', 'also', 'minimize', 'half', 'square', '12', 'w', '2', 'nice', 'simple', 'derivative', 'w', 'w', 'differentiable', 'w', '=', '0', '.', 'Optimization', 'algo…', 'rithms', 'work', 'much', 'better', 'differentiable', 'functions', '.', 'To', 'get', 'soft', 'margin', 'objective', 'need', 'introduce', 'slack', 'variable', '™', 'Š', '0', 'eachinstance:4', '™', 'measures', 'much', 'th', 'instance', 'allowed', 'violate', 'margin', '.', 'We', 'two', 'conflicting', 'objectives', 'making', 'slack', 'variables', 'small', 'possible', 'reduce', 'margin', 'violations', 'making', '12wT', '’', 'w', 'small', 'possible', 'increase', 'themargin', '.', 'This', 'C', 'hyperparameter', 'comes', 'allows', 'us', 'define', 'trade…', '158', '|', 'Chapter', '5', 'Support', 'Vector', 'Machines', '5To', 'learn', 'Quadratic', 'Programming', 'start', 'reading', 'Stephen', 'Boyd', 'Lieven', 'Vanden…', 'berghe', 'Convex', 'Optimization', 'Cambridge', 'UK', 'Cambridge', 'University', 'Press', '2004', 'watch', 'Richard', 'Brown‡s', 'series', 'video', 'lectures.off', 'two', 'objectives', '.', 'This', 'gives', 'us', 'constrained', 'optimization', 'problem', 'Equation', '5-4', '.Equation', '5-4', '.', 'So', '“', 'margin', 'linear', 'SVM', 'classi†er', 'objective', 'minimize', 'b', '™12T', '’', '+C', '“', 'i=1', 'm™isubjectto', 'tiT', '’', 'i+bŠ1', '”', '™iand™iŠ0for', 'i=1,2', 'mQuadratic', 'ProgrammingThe', 'hard', 'margin', 'soft', 'margin', 'problems', 'convex', 'quadratic', 'optimization', 'problems', 'linear', 'constraints', '.', 'Such', 'problems', 'known', 'Quadratic', 'Program…', 'ming', 'QP', 'problems', '.', 'Many', 'off-the-shelf', 'solvers', 'available', 'solve', 'QP', 'problems', 'using', 'variety', 'techniques', 'outside', 'scope', 'book', '.', '5', 'The', 'generalproblem', 'formulation', 'given', 'Equation', '5-5', '.Equation', '5-5', '.', 'Quadratic', 'Programming', 'problem', 'Minimize12T', '’', '’', '+T', '’', 'subjectto', '’', 'Žwhereisan', 'np…dimensionalvector', 'np=numberofparameters', 'isan', 'np‰npmatrix', 'isan', 'np…dimensionalvector', 'isan', 'nc‰npmatrix', 'nc=numberofconstraints', 'isan', 'nc…dimensionalvector', '.', 'Note', 'expression', 'A', '’', 'p', 'Ž', 'b', 'actually', 'defines', 'nc', 'constraints', 'pT', '’', 'Ž', 'b', '=1', '2', 'nc', 'vector', 'containing', 'elements', 'th', 'row', 'A', 'b', 'ith', 'element', 'b.You', 'easily', 'verify', 'set', 'QP', 'parameters', 'following', 'way', 'get', 'hard', 'margin', 'linear', 'SVM', 'classifier', 'objective', '‹np', '=', 'n', '+', '1', 'n', 'number', 'features', '+1', 'bias', 'term', '.', 'Under', 'Hood', '|', '159', '6The', 'objective', 'function', 'convex', 'inequality', 'constraints', 'continuously', 'differentiable', 'convex', 'functions.‹nc', '=', 'number', 'training', 'instances', '.', '‹H', 'np', '‰', 'np', 'identity', 'matrix', 'except', 'zero', 'top-left', 'cell', 'ignore', 'bias', 'term', '.‹f', '=', '0', 'np-dimensional', 'vector', 'full', '0s.‹b', '=', '1', 'nc-dimensional', 'vector', 'full', '1s.‹a', '=', '–t', 'ı', 'ı', 'equal', 'x', 'extra', 'bias', 'feature', 'ı0', '=', '1.So', 'one', 'way', 'train', 'hard', 'margin', 'linear', 'SVM', 'classifier', 'use', 'off-the-shelf', 'QP', 'solver', 'passing', 'preceding', 'parameters', '.', 'The', 'resulting', 'vector', 'p', 'contain', 'bias', 'term', 'b', '=', 'p0', 'feature', 'weights', 'wi', '=', 'pi', '=', '1', '2', 'm.', 'Similarly', 'use', 'QP', 'solver', 'solve', 'soft', 'margin', 'problem', 'see', 'exercises', 'end', 'chapter', '.', 'However', 'use', 'kernel', 'trick', 'going', 'look', 'different', 'constrained', 'opti…', 'mization', 'problem', '.', 'The', 'Dual', 'ProblemGiven', 'constrained', 'optimization', 'problem', 'known', 'primal', 'problem', 'possi…ble', 'express', 'different', 'closely', 'related', 'problem', 'called', 'dual', 'problem', '.', 'The', 'sol…ution', 'dual', 'problem', 'typically', 'gives', 'lower', 'bound', 'solution', 'primalproblem', 'conditions', 'even', 'solutions', 'pri…', 'mal', 'problem', '.', 'Luckily', 'SVM', 'problem', 'happens', 'meet', 'conditions', '6', 'youcan', 'choose', 'solve', 'primal', 'problem', 'dual', 'problem', 'solution', '.', 'Equation', '5-6', 'shows', 'dual', 'form', 'linear', 'SVM', 'objective', 'areinterested', 'knowing', 'derive', 'dual', 'problem', 'primal', 'problem', 'see', 'Appendix', 'C', '.Equation', '5-6', '.', 'Dual', 'form', 'linear', 'SVM', 'objective', 'minimize‰12', '“', 'i=1', '“', 'j=1', 'm‰i‰jtitjiT', '’', 'j', '”', '“', 'i=1', 'm‰isubjectto', '‰iŠ0for', 'i=1,2', 'm160', '|', 'Chapter', '5', 'Support', 'Vector', 'Machines', 'Once', 'find', 'vector', '‰', 'minimizes', 'equation', 'using', 'QP', 'solver', 'compute', 'b', 'minimize', 'primal', 'problem', 'using', 'Equation', '5-7', '.Equation', '5-7', '.', 'From', 'dual', 'solution', 'primal', 'solution', '=', '“', 'i=1', 'm‰itiib=1ns', '“', 'i=1', '‰i', '>', '0', 'm1', '”', 'tiT', '’', 'iThe', 'dual', 'problem', 'faster', 'solve', 'primal', 'number', 'training', 'instances', 'smaller', 'number', 'features', '.', 'More', 'importantly', 'makes', 'ker…', 'nel', 'trick', 'possible', 'primal', '.', 'So', 'kernel', 'trick', 'anyway', '?', 'Kernelized', 'SVMSuppose', 'want', 'apply', '2', 'nd-degree', 'polynomial', 'transformation', 'two-', 'dimensional', 'training', 'set', 'moons', 'training', 'set', 'train', 'linear', 'SVMclassifier', 'transformed', 'training', 'set', '.', 'Equation', '5-8', 'shows', '2nd-degree', 'polyno…mial', 'mapping', 'function', '‚', 'want', 'apply', '.', 'Equation', '5-8', '.', 'Second-degree', 'polynomial', 'mapping', '‚=‚x1x2=x122x1x2x22Notice', 'transformed', 'vector', 'three-dimensional', 'instead', 'two-dimensional', '.', 'Now', 'let‡s', 'look', 'happens', 'couple', 'two-dimensional', 'vectors', 'b', 'weapply', '2', 'nd-degree', 'polynomial', 'mapping', 'compute', 'dot', 'product', 'transformed', 'vectors', 'See', 'Equation', '5-9', '.Under', 'Hood', '|', '161', 'Equation', '5-9', '.', 'Kernel', 'trick', '2', 'nd', '-degree', 'polynomial', 'mapping', '‚T', '’', '‚=a122a1a2a22T', '’', 'b122b1b2b22=a12b12+2', 'a1b1a2b2+a22b22=a1b1+a2b22=a1a2T', '’', 'b1b22=T', '’', '2How', '?', 'The', 'dot', 'product', 'transformed', 'vectors', 'equal', 'square', 'dot', 'product', 'original', 'vectors', '‚', 'T', '’', '‚', 'b', '=', 'aT', '’', 'b', '2.Now', 'key', 'insight', 'apply', 'transformation', '‚', 'training', 'instan…ces', 'dual', 'problem', 'see', 'Equation', '5-6', 'contain', 'dot', 'product', '‚', 'x', 'T', '’', '‚', 'x', 'j', '.', 'But', '‚', '2nd-degree', 'polynomial', 'transformation', 'defined', 'Equation', '5-8', 'replace', 'dot', 'product', 'transformed', 'vectors', 'simply', 'iT', '’', 'j2.So', 'don‡t', 'actually', 'need', 'transform', 'training', 'instances', 'replace', 'dot', 'product', 'square', 'Equation', '5-6', '.', 'The', 'result', 'strictly', 'youwent', 'trouble', 'actually', 'transforming', 'training', 'set', 'fitting', 'linear', 'SVM', 'algorithm', 'trick', 'makes', 'whole', 'process', 'much', 'computationally', 'efficient', '.', 'This', 'essence', 'kernel', 'trick', '.', 'The', 'function', 'K', 'b', '=', 'aT', '’', 'b', '2', 'called', '2nd-degree', 'polynomial', 'kernel', '.', 'In', 'Machine', 'Learning', 'kernel', 'function', 'capable', 'computing', 'dot', 'product', '‚', 'T', '’', '‚', 'b', 'based', 'original', 'vectors', 'b', 'without', 'compute', 'even', 'know', 'transformation', '‚', '.', 'Equation', '5-10', 'lists', 'commonlyused', 'kernels.Equation', '5-10', '.', 'Common', 'kernels', 'Linear', 'K', '=T', '’', 'Polynomial', 'K', '=', '’', 'T', '’', '+rdGaussianRBF', 'K', '=exp', '”', '’', '”', '2Sigmoid', 'K', '=tanh', '’', 'T', '’', '+r162', '|', 'Chapter', '5', 'Support', 'Vector', 'Machines', 'Mercer‡s', 'TheoremAccording', 'Mercer‹s', 'theorem', 'function', 'K', 'b', 'respects', 'mathematical', 'con…', 'ditions', 'called', 'Mercer‹s', 'conditions', 'K', 'must', 'continuous', 'symmetric', 'arguments', 'K', 'b', '=', 'K', 'b', 'etc', '.', 'exists', 'function', '‚', 'maps', 'b', 'another', 'space', 'possibly', 'much', 'higher', 'dimensions', 'K', 'b', '=', '‚', 'T', '’', '‚', 'b', '.', 'So', 'use', 'K', 'kernel', 'since', 'know', '‚', 'exists', 'even', 'don‡t', 'know', '‚', '.', 'In', 'case', 'Gaussian', 'RBF', 'kernel', 'shown', '‚', 'actuallymaps', 'training', 'instance', 'infinite-dimensional', 'space', 'it‡s', 'good', 'thing', 'don‡t', 'need', 'actually', 'perform', 'mapping', '!', 'Note', 'frequently', 'used', 'kernels', 'Sigmoid', 'kernel', 'don‡t', 'respect', 'Mercer‡s', 'conditions', 'yet', 'generally', 'work', 'well', 'practice', '.', 'There', 'still', 'one', 'loose', 'end', 'must', 'tie', '.', 'Equation', '5-7', 'shows', 'go', 'dualsolution', 'primal', 'solution', 'case', 'linear', 'SVM', 'classifier', 'apply', 'kernel', 'trick', 'end', 'equations', 'include', '‚', 'x', '.', 'In', 'fact', 'must', 'number', 'dimensions', '‚', 'x', 'may', 'huge', 'even', 'infinite', 'can‡t', 'compute', '.', 'But', 'make', 'predictions', 'without', 'knowing', '?', 'Well', 'good', 'news', 'plug', 'formula', 'Equation', '5-7', 'deci…', 'sion', 'function', 'new', 'instance', 'x', 'n', 'get', 'equation', 'dot', 'products', 'input', 'vectors', '.', 'This', 'makes', 'possible', 'use', 'kernel', 'trick', 'Equation', '5-11', '.Equation', '5-11', '.', 'Making', 'predictions', 'kernelized', 'SVM', 'h', 'b‚n=T', '’', '‚n+b=', '“', 'i=1', 'm‰iti‚iT', '’', '‚n+b=', '“', 'i=1', 'm‰iti‚iT', '’', '‚n+b=', '“', 'i=1', '‰i', '>', '0', 'm‰itiKi', 'n+bNote', 'since', '‰', 'ł', '0', 'support', 'vectors', 'making', 'predictions', 'involves', 'comput…', 'ing', 'dot', 'product', 'new', 'input', 'vector', 'x', 'n', 'support', 'vectors', 'allthe', 'training', 'instances', '.', 'Of', 'course', 'also', 'need', 'compute', 'bias', 'term', 'b', 'using', 'thesame', 'trick', 'Equation', '5-12', '.Under', 'Hood', '|', '163', 'Equation', '5-12', '.', 'Computing', 'bias', 'term', 'using', 'kernel', 'trick', 'b=1ns', '“', 'i=1', '‰i', '>', '0', 'm1', '”', 'tiT', '’', '‚i=1ns', '“', 'i=1', '‰i', '>', '0', 'm1', '”', 'ti', '“', 'j=1', 'm‰jtj‚jT', '’', '‚i=1ns', '“', 'i=1', '‰i', '>', '0', 'm1', '”', 'ti', '“', 'j=1', '‰j', '>', '0', 'm‰jtjKi', 'jIf', 'starting', 'get', 'headache', 'it‡s', 'perfectly', 'normal', 'it‡s', 'unfortunate', 'side', 'effects', 'kernel', 'trick.Online', 'SVMsBefore', 'concluding', 'chapter', 'let‡s', 'take', 'quick', 'look', 'online', 'SVM', 'classifiers', 'recall', 'online', 'learning', 'means', 'learning', 'incrementally', 'typically', 'new', 'instances', 'arrive', '.', 'For', 'linear', 'SVM', 'classifiers', 'one', 'method', 'use', 'Gradient', 'Descent', 'e.g.', 'using', 'SGDClassifier', 'minimize', 'cost', 'function', 'Equation', '5-13', 'derivedfrom', 'primal', 'problem', '.', 'Unfortunately', 'converges', 'much', 'slowly', 'methods', 'based', 'QP', '.', 'Equation', '5-13', '.', 'Linear', 'SVM', 'classi†er', 'cost', 'function', 'J', 'b=12T', '’', '+C', '“', 'i=1', 'mmax0,1', '”', 'tiT', '’', 'i+bThe', 'first', 'sum', 'cost', 'function', 'push', 'model', 'small', 'weight', 'vector', 'w', 'leading', 'larger', 'margin', '.', 'The', 'second', 'sum', 'computes', 'total', 'margin', 'viola…', 'tions', '.', 'An', 'instance‡s', 'margin', 'violation', 'equal', '0', 'located', 'street', 'correct', 'side', 'else', 'proportional', 'distance', 'correct', 'side', 'thestreet', '.', 'Minimizing', 'term', 'ensures', 'model', 'makes', 'margin', 'violations', 'small', 'possibleHinge', 'LossThe', 'function', 'max', '0', '1', '–', 'called', 'hinge', 'loss', 'function', 'represented', '.', 'It', 'equal', '0', 'Š', '1', '.', 'Its', 'derivative', 'slope', 'equal', '–1', '<', '1', '0', '>', '1', '.', 'It', 'differentiable', '=', '1', 'like', 'Lasso', 'Regression', 'see', 'ƒLasso', 'Regression⁄', 'onpage', '130', 'still', 'use', 'Gradient', 'Descent', 'using', 'subderivative', '=', '0', 'i.e.', 'value', '–1', '0', '.164', '|', 'Chapter', '5', 'Support', 'Vector', 'Machines', '7ƒIncremental', 'Decremental', 'Support', 'Vector', 'Machine', 'Learning', '⁄', 'G.', 'Cauwenberghs', 'T.', 'Poggio', '2001', '.', '8ƒFast', 'Kernel', 'Classifiers', 'Online', 'Active', 'Learning', 'ƒ', 'A.', 'Bordes', 'S.', 'Ertekin', 'J.', 'Weston', 'L.', 'Bottou', '2005', '.', 'It', 'also', 'possible', 'implement', 'online', 'kernelized', 'SVMs›for', 'example', 'using', 'ƒIncre…mental', 'Decremental', 'SVM', 'Learning⁄', '7', 'ƒFast', 'Kernel', 'Classifiers', 'Online', 'Active', 'Learning.⁄', '8', 'However', 'implemented', 'Matlab', 'C++', '.', 'For', 'large-', 'scale', 'nonlinear', 'problems', 'may', 'want', 'consider', 'using', 'neural', 'networks', 'instead', 'see', 'Part', 'II', '.Exercises1.What', 'fundamental', 'idea', 'behind', 'Support', 'Vector', 'Machines', '?', '2.What', 'support', 'vector', '?', '3.Why', 'important', 'scale', 'inputs', 'using', 'SVMs', '?', '4.Can', 'SVM', 'classifier', 'output', 'confidence', 'score', 'classifies', 'instance', '?', 'What', 'probability', '?', '5.Should', 'use', 'primal', 'dual', 'form', 'SVM', 'problem', 'train', 'modelon', 'training', 'set', 'millions', 'instances', 'hundreds', 'features', '?', '6.Say', 'trained', 'SVM', 'classifier', 'RBF', 'kernel', '.', 'It', 'seems', 'underfit', 'training', 'set', 'increase', 'decrease', '’', 'gamma', '?', 'What', 'C', '?', '7.How', 'set', 'QP', 'parameters', 'H', 'f', 'A', 'b', 'solve', 'soft', 'marginlinear', 'SVM', 'classifier', 'problem', 'using', 'off-the-shelf', 'QP', 'solver', '?', '8.Train', 'LinearSVC', 'linearly', 'separable', 'dataset', '.', 'Then', 'train', 'SVC', 'aSGDClassifier', 'dataset', '.', 'See', 'get', 'produce', 'roughly', 'model.9.Train', 'SVM', 'classifier', 'MNIST', 'dataset', '.', 'Since', 'SVM', 'classifiers', 'binary', 'classifiers', 'need', 'use', 'one-versus-all', 'classify', '10', 'digits', '.', 'You', 'may', 'Exercises', '|', '165', 'want', 'tune', 'hyperparameters', 'using', 'small', 'validation', 'sets', 'speed', 'pro…', 'cess', '.', 'What', 'accuracy', 'reach', '?', '10.Train', 'SVM', 'regressor', 'California', 'housing', 'dataset', '.', 'Solutions', 'exercises', 'available', 'Appendix', 'A', '.166', '|', 'Chapter', '5', 'Support', 'Vector', 'Machines', 'CHAPTER', '6Decision', 'TreesLike', 'SVMs', 'Decision', 'Trees', 'versatile', 'Machine', 'Learning', 'algorithms', 'per…', 'form', 'classification', 'regression', 'tasks', 'even', 'multioutput', 'tasks', '.', 'They', 'powerful', 'algorithms', 'capable', 'fitting', 'complex', 'datasets', '.', 'For', 'example', 'Chap…', 'ter', '2', 'trained', 'DecisionTreeRegressor', 'model', 'California', 'housing', 'dataset', 'fitting', 'perfectly', 'actually', 'overfitting', '.Decision', 'Trees', 'also', 'fundamental', 'components', 'Random', 'Forests', 'see', 'Chap…', 'ter', '7', 'among', 'powerful', 'Machine', 'Learning', 'algorithms', 'available', 'today', '.', 'In', 'chapter', 'start', 'discussing', 'train', 'visualize', 'make', 'predic…', 'tions', 'Decision', 'Trees', '.', 'Then', 'go', 'CART', 'training', 'algorithm', 'used', 'Scikit-Learn', 'discuss', 'regularize', 'trees', 'use', 'forregression', 'tasks', '.', 'Finally', 'discuss', 'limitations', 'Decision', 'Trees', '.', 'Training', 'Visualizing', 'Decision', 'TreeTo', 'understand', 'Decision', 'Trees', 'let‡s', 'build', 'one', 'take', 'look', 'makes', 'pre…', 'dictions', '.', 'The', 'following', 'code', 'trains', 'DecisionTreeClassifier', 'iris', 'dataset', 'see', 'Chapter', '4', 'sklearn.datasets', 'import', 'load_irisfrom', 'sklearn.tree', 'import', 'DecisionTreeClassifieriris', '=', 'load_iris', 'X', '=', 'iris.data', '2', '#', 'petal', 'length', 'widthy', '=', 'iris.targettree_clf', '=', 'DecisionTreeClassifier', 'max_depth=2', 'tree_clf.fit', 'X', '1671Graphviz', 'open', 'source', 'graph', 'visualization', 'software', 'package', 'available', 'http', '//www.graphviz.org/', '.You', 'visualize', 'trained', 'Decision', 'Tree', 'first', 'using', 'export_graphviz', 'method', 'output', 'graph', 'definition', 'file', 'called', 'iris_tree.dot', 'sklearn.tree', 'import', 'export_graphvizexport_graphviz', 'tree_clf', 'out_file=image_path', '``', 'iris_tree.dot', \"''\", 'feature_names=iris.feature_names', '2', 'class_names=iris.target_names', 'rounded=True', 'filled=True', 'Then', 'convert', '.dot', 'file', 'variety', 'formats', 'PDF', 'PNG', 'using', 'dot', 'command-line', 'tool', 'graphviz', 'package.1', 'This', 'command', 'line', 'converts', '.dot', 'file', '.png', 'image', 'file', '$', 'dot', '-Tpng', 'iris_tree.dot', '-o', 'iris_tree.pngYour', 'first', 'decision', 'tree', 'looks', 'like', 'Figure', '6-1.Figure', '6-1', '.', 'Iris', 'Decision', 'Tree', '168', '|', 'Chapter', '6', 'Decision', 'Trees', 'Making', 'PredictionsLet‡s', 'see', 'tree', 'represented', 'Figure', '6-1', 'makes', 'predictions', '.', 'Suppose', 'findan', 'iris', 'flower', 'want', 'classify', '.', 'You', 'start', 'root', 'node', 'depth', '0', 'top', 'node', 'asks', 'whether', 'flower‡s', 'petal', 'length', 'smaller', '2.45', 'cm', '.', 'If', 'move', 'root‡s', 'left', 'child', 'node', 'depth', '1', 'left', '.', 'In', 'case', 'leaf', 'node', 'i.e.', 'children', 'nodes', 'ask', 'questions', 'simply', 'look', 'predicted', 'class', 'node', 'Decision', 'Tree', 'predicts', 'flower', 'Iris-Setosa', 'class=setosa', '.Now', 'suppose', 'find', 'another', 'flower', 'time', 'petal', 'length', 'greater', '2.45', 'cm', '.', 'You', 'must', 'move', 'root‡s', 'right', 'child', 'node', 'depth', '1', 'right', 'leaf', 'node', 'asks', 'another', 'question', 'petal', 'width', 'smaller', '1.75', 'cm', '?', 'Ifit', 'flower', 'likely', 'Iris-Versicolor', 'depth', '2', 'left', '.', 'If', 'likely', 'Iris-Virginica', 'depth', '2', 'right', '.', 'It‡s', 'really', 'simple', '.', 'One', 'many', 'qualities', 'Decision', 'Trees', 'require', 'little', 'data', 'preparation', '.', 'In', 'particular', 'don‡t', 'require', 'feature', 'scaling', 'centering', '.', 'A', 'node‡s', 'samples', 'attribute', 'counts', 'many', 'training', 'instances', 'applies', '.', 'For', 'example', '100', 'training', 'instances', 'petal', 'length', 'greater', '2.45', 'cm', 'depth', '1', 'right', 'among', '54', 'petal', 'width', 'smaller', '1.75', 'cm', 'depth', '2', 'left', '.', 'A', 'node‡s', 'value', 'attribute', 'tells', 'many', 'training', 'instances', 'class', 'node', 'applies', 'example', 'bottom-right', 'node', 'applies', '0', 'Iris-Setosa', '1', 'Iris-', 'Versicolor', '45', 'Iris-Virginica', '.', 'Finally', 'node‡s', 'gini', 'attribute', 'measures', 'impur…', 'ity', 'node', 'ƒpure⁄', 'gini=0', 'training', 'instances', 'applies', 'belong', 'class', '.', 'For', 'example', 'since', 'depth-1', 'left', 'node', 'applies', 'Iris-Setosa', 'training', 'instances', 'pure', 'gini', 'score', '0', '.', 'Equation', '6-1', 'shows', 'training', 'algo…rithm', 'computes', 'gini', 'score', 'Gi', 'ith', 'node', '.', 'For', 'example', 'depth-2', 'left', 'node', 'gini', 'score', 'equal', '1', '–', '0/54', '2', '–', '49/54', '2', '–', '5/54', '2', 'Ÿ', '0.168', '.', 'Another', 'impurity', 'measure', 'discussed', 'shortly', '.', 'Equation', '6-1', '.', 'Gini', 'impurity', 'Gi=1', '”', '“', 'k=1', 'npi', 'k2‹pi', 'k', 'ratio', 'class', 'k', 'instances', 'among', 'training', 'instances', 'ith', 'node.Making', 'Predictions', '|', '169', 'Scikit-Learn', 'uses', 'CART', 'algorithm', 'produces', 'binary', 'trees', 'nonleaf', 'nodes', 'always', 'two', 'children', 'i.e.', 'questions', 'yes/no', 'answers', '.', 'However', 'algorithms', 'ID3', 'produce', 'Decision', 'Trees', 'nodes', 'two', 'chil…', 'dren.Figure', '6-2', 'shows', 'Decision', 'Tree‡s', 'decision', 'boundaries', '.', 'The', 'thick', 'vertical', 'line', 'rep…', 'resents', 'decision', 'boundary', 'root', 'node', 'depth', '0', 'petal', 'length', '=', '2.45', 'cm', '.', 'Since', 'left', 'area', 'pure', 'Iris-Setosa', 'split', '.', 'However', 'right', 'area', 'impure', 'depth-1', 'right', 'node', 'splits', 'petal', 'width', '=', '1.75', 'cm', 'represented', 'dashed', 'line', '.', 'Since', 'max_depth', 'set', '2', 'Decision', 'Tree', 'stops', 'right', '.', 'However', 'set', 'max_depth', '3', 'two', 'depth-2', 'nodeswould', 'add', 'another', 'decision', 'boundary', 'represented', 'dotted', 'lines', '.', 'Figure', '6-2', '.', 'Decision', 'Tree', 'decision', 'boundaries', 'Model', 'Interpretation', 'White', 'Box', 'Versus', 'Black', 'BoxAs', 'see', 'Decision', 'Trees', 'fairly', 'intuitive', 'decisions', 'easy', 'inter…', 'pret', '.', 'Such', 'models', 'often', 'called', 'white', 'box', 'models', '.', 'In', 'contrast', 'see', 'Ran…', 'dom', 'Forests', 'neural', 'networks', 'generally', 'considered', 'black', 'box', 'models', '.', 'Theymake', 'great', 'predictions', 'easily', 'check', 'calculations', 'performed', 'make', 'predictions', 'nevertheless', 'usually', 'hard', 'explain', 'simple', 'terms', 'predictions', 'made', '.', 'For', 'example', 'neural', 'network', 'says', 'particu…', 'lar', 'person', 'appears', 'picture', 'hard', 'know', 'actually', 'contributed', 'prediction', 'model', 'recognize', 'person‡s', 'eyes', '?', 'Her', 'mouth', '?', 'Her', 'nose', '?', 'Her', 'shoes', '?', 'Or', 'even', 'couch', 'sitting', '?', 'Conversely', 'Decision', 'Trees', 'provide', 'nice', 'simple', 'classification', 'rules', 'even', 'applied', 'manually', 'need', 'e.g.', 'flower', 'classification', '.', '170', '|', 'Chapter', '6', 'Decision', 'Trees', 'Estimating', 'Class', 'ProbabilitiesA', 'Decision', 'Tree', 'also', 'estimate', 'probability', 'instance', 'belongs', 'partic…', 'ular', 'class', 'k', 'first', 'traverses', 'tree', 'find', 'leaf', 'node', 'instance', 'returns', 'ratio', 'training', 'instances', 'class', 'k', 'node', '.', 'For', 'example', 'suppose', 'found', 'flower', 'whose', 'petals', '5', 'cm', 'long', '1.5', 'cm', 'wide', '.', 'The', 'corre…', 'sponding', 'leaf', 'node', 'depth-2', 'left', 'node', 'Decision', 'Tree', 'output', 'following', 'probabilities', '0', '%', 'Iris-Setosa', '0/54', '90.7', '%', 'Iris-Versicolor', '49/54', '9.3', '%', 'Iris-Virginica', '5/54', '.', 'And', 'course', 'ask', 'predict', 'class', 'output', 'Iris-Versicolor', 'class', '1', 'since', 'highest', 'probability', '.', 'Let‡s', 'check', '>', '>', '>', 'tree_clf.predict_proba', '5', '1.5', 'array', '0.', '0.90740741', '0.09259259', '>', '>', '>', 'tree_clf.predict', '5', '1.5', 'array', '1', 'Perfect', '!', 'Notice', 'estimated', 'probabilities', 'would', 'identical', 'anywhere', 'else', 'bottom-right', 'rectangle', 'Figure', '6-2›for', 'example', 'petals', '6', 'cm', 'long', '1.5', 'cm', 'wide', 'even', 'though', 'seems', 'obvious', 'would', 'likely', 'Iris-', 'Virginica', 'case', '.', 'The', 'CART', 'Training', 'AlgorithmScikit-Learn', 'uses', 'Classi†cation', 'And', 'Regression', 'Tree', 'CART', 'algorithm', 'train', 'Decision', 'Trees', 'also', 'called', 'ƒgrowing⁄', 'trees', '.', 'The', 'idea', 'really', 'quite', 'simple', 'algo…', 'rithm', 'first', 'splits', 'training', 'set', 'two', 'subsets', 'using', 'single', 'feature', 'k', 'thres…hold', 'tk', 'e.g.', 'ƒpetal', 'length', 'Ž', '2.45', 'cm⁄', '.', 'How', 'choose', 'k', 'tk', '?', 'It', 'searches', 'pair', 'k', 'tk', 'produces', 'purest', 'subsets', 'weighted', 'size', '.', 'The', 'cost', 'function', 'algorithm', 'tries', 'minimize', 'given', 'Equation', '6-2', '.Equation', '6-2', '.', 'CART', 'cost', 'function', 'classi†cationJk', 'tk=mleftmGleft+mrightmGrightwhereGleft/rightmeasurestheimpurityoftheleft/rightsubset', 'mleft/rightisthenumberofinstancesintheleft/rightsubset', '.', 'Once', 'successfully', 'split', 'training', 'set', 'two', 'splits', 'subsets', 'using', 'logic', 'sub-subsets', 'recursively', '.', 'It', 'stops', 'recursing', 'rea…', 'ches', 'maximum', 'depth', 'defined', 'max_depth', 'hyperparameter', 'find', 'split', 'reduce', 'impurity', '.', 'A', 'hyperparameters', 'described', 'Estimating', 'Class', 'Probabilities', '|', '171', '2P', 'set', 'problems', 'solved', 'polynomial', 'time', '.', 'NP', 'set', 'problems', 'whose', 'solutions', 'verified', 'polynomial', 'time', '.', 'An', 'NP-Hard', 'problem', 'problem', 'NP', 'problem', 'reduced', 'polynomial', 'time', '.', 'An', 'NP-Complete', 'problem', 'NP', 'NP-Hard', '.', 'A', 'major', 'open', 'mathematical', 'ques…', 'tion', 'whether', 'P', '=', 'NP', '.', 'If', 'P', 'ł', 'NP', 'seems', 'likely', 'polynomial', 'algorithm', 'ever', 'found', 'NP-Complete', 'problem', 'except', 'perhaps', 'quantum', 'computer', '.', '3log', '2', 'binary', 'logarithm', '.', 'It', 'equal', 'log', '2', '=', 'log', '/', 'log', '2', '.moment', 'control', 'additional', 'stopping', 'conditions', 'min_samples_split', 'min_samples_leaf', 'min_weight_fraction_leaf', 'max_leaf_nodes', '.As', 'see', 'CART', 'algorithm', 'greedy', 'algorithm', 'greed…ily', 'searches', 'optimum', 'split', 'top', 'level', 'repeats', 'process', 'level', '.', 'It', 'check', 'whether', 'split', 'lead', 'lowest', 'possible', 'impurity', 'several', 'levels', '.', 'A', 'greedy', 'algorithm', 'often', 'produces', 'reasonably', 'good', 'solution', 'notguaranteed', 'optimal', 'solution', '.', 'Unfortunately', 'finding', 'optimal', 'tree', 'known', 'NP-Complete', 'problem:2', 'itrequires', 'O', 'exp', 'time', 'making', 'problem', 'intractable', 'even', 'fairly', 'small', 'train…', 'ing', 'sets', '.', 'This', 'must', 'settle', 'ƒreasonably', 'good⁄', 'solution', '.', 'Computational', 'ComplexityMaking', 'predictions', 'requires', 'traversing', 'Decision', 'Tree', 'root', 'leaf', '.', 'Decision', 'Trees', 'generally', 'approximately', 'balanced', 'traversing', 'Decision', 'Tree', 'requires', 'going', 'roughly', 'O', 'log', '2', 'nodes.3', 'Since', 'node', 'requireschecking', 'value', 'one', 'feature', 'overall', 'prediction', 'complexity', 'O', 'log', '2', 'independent', 'number', 'features', '.', 'So', 'predictions', 'fast', 'even', 'deal…', 'ing', 'large', 'training', 'sets.However', 'training', 'algorithm', 'compares', 'features', 'less', 'max_features', 'set', 'samples', 'node', '.', 'This', 'results', 'training', 'complexity', 'O', 'n', '‰', 'log', '.For', 'small', 'training', 'sets', 'less', 'thousand', 'instances', 'Scikit-Learn', 'speed', 'training', 'presorting', 'data', 'set', 'presort=True', 'slows', 'training', 'con…siderably', 'larger', 'training', 'sets.Gini', 'Impurity', 'Entropy', '?', 'By', 'default', 'Gini', 'impurity', 'measure', 'used', 'select', 'entropy', 'impurity', 'measure', 'instead', 'setting', 'criterion', 'hyperparameter', \"''\", 'entropy', \"''\", '.', 'The', 'conceptof', 'entropy', 'originated', 'thermodynamics', 'measure', 'molecular', 'disorder', 'entropy', 'approaches', 'zero', 'molecules', 'still', 'well', 'ordered', '.', 'It', 'later', 'spread', 'wide', 'variety', 'domains', 'including', 'Shannon‡s', 'information', 'theory', 'measures172', '|', 'Chapter', '6', 'Decision', 'Trees', '4A', 'reduction', 'entropy', 'often', 'called', 'information', 'gain', '.5See', 'Sebastian', 'Raschka‡s', 'interesting', 'analysis', 'details', '.the', 'average', 'information', 'content', 'message', '4', 'entropy', 'zero', 'messages', 'identical', '.', 'In', 'Machine', 'Learning', 'frequently', 'used', 'impurity', 'measure', 'set‡s', 'entropy', 'zero', 'contains', 'instances', 'one', 'class', '.', 'Equation', '6-3', 'shows', 'thedefinition', 'entropy', 'th', 'node', '.', 'For', 'example', 'depth-2', 'left', 'node', 'Figure', '6-1', 'entropy', 'equal', '”', '4954log4954', '”', '554log554', 'Ÿ', '0.31.Equation', '6-3', '.', 'Entropy', 'Hi=', '”', '“', 'k=1', 'pi', 'kł0', 'npi', 'klogpi', 'kSo', 'use', 'Gini', 'impurity', 'entropy', '?', 'The', 'truth', 'time', 'make', 'big', 'difference', 'lead', 'similar', 'trees', '.', 'Gini', 'impurity', 'slightly', 'faster', 'compute', 'good', 'default', '.', 'However', 'differ', 'Gini', 'impurity', 'tends', 'isolate', 'frequent', 'class', 'branch', 'tree', 'entropy', 'tends', 'produce', 'slightly', 'balanced', 'trees', '.', '5Regularization', 'HyperparametersDecision', 'Trees', 'make', 'assumptions', 'training', 'data', 'opposed', 'lin…', 'ear', 'models', 'obviously', 'assume', 'data', 'linear', 'example', '.', 'If', 'left', 'unconstrained', 'tree', 'structure', 'adapt', 'training', 'data', 'fitting', 'closely', 'likely', 'overfitting', '.', 'Such', 'model', 'often', 'called', 'nonparametric', 'model', 'parameters', 'often', 'lot', 'number', 'parameters', 'determined', 'prior', 'training', 'model', 'structure', 'free', 'stick', 'closely', 'data', '.', 'In', 'contrast', 'parametric', 'model', 'linear', 'modelhas', 'predetermined', 'number', 'parameters', 'degree', 'freedom', 'limited', 'reducing', 'risk', 'overfitting', 'increasing', 'risk', 'underfitting', '.To', 'avoid', 'overfitting', 'training', 'data', 'need', 'restrict', 'Decision', 'Tree‡s', 'freedom', 'training', '.', 'As', 'know', 'called', 'regularization', '.', 'The', 'regularization', 'hyperparameters', 'depend', 'algorithm', 'used', 'generally', 'least', 'restrict', 'maximum', 'depth', 'Decision', 'Tree', '.', 'In', 'Scikit-Learn', 'controlled', 'max_depth', 'hyperparameter', 'default', 'value', 'None', 'means', 'unlimited', '.Reducing', 'max_depth', 'regularize', 'model', 'thus', 'reduce', 'risk', 'overfitting', '.', 'The', 'DecisionTreeClassifier', 'class', 'parameters', 'similarly', 'restrict', 'shape', 'Decision', 'Tree', 'min_samples_split', 'minimum', 'number', 'sam…', 'Regularization', 'Hyperparameters', '|', '173', 'ples', 'node', 'must', 'split', 'min_samples_leaf', 'minimum', 'num…', 'ber', 'samples', 'leaf', 'node', 'must', 'min_weight_fraction_leaf', 'asmin_samples_leaf', 'expressed', 'fraction', 'total', 'number', 'weighted', 'instances', 'max_leaf_nodes', 'maximum', 'number', 'leaf', 'nodes', 'max_features', 'maximum', 'number', 'features', 'evaluated', 'splitting', 'node', '.', 'Increas…', 'ing', 'min_*', 'hyperparameters', 'reducing', 'max_*', 'hyperparameters', 'regularize', 'model.Other', 'algorithms', 'work', 'first', 'training', 'Decision', 'Tree', 'without', 'restrictions', 'pruning', 'deleting', 'unnecessary', 'nodes', '.', 'A', 'node', 'whose', 'children', 'leaf', 'nodes', 'considered', 'unnecessary', 'purity', 'improvement', 'provides', 'statistically', 'signi†cant', '.', 'Stan…dard', 'statistical', 'tests', 'ﬁ2', 'test', 'used', 'estimate', 'probability', 'improvement', 'purely', 'result', 'chance', 'called', 'null', 'hypothesis', '.', 'If', 'probability', 'called', 'p-value', 'higher', 'given', 'threshold', 'typically', '5', '%', 'controlled', 'hyperparameter', 'node', 'considered', 'unnecessary', 'children', 'deleted', '.', 'The', 'pruning', 'continues', 'unnecessary', 'nodes', 'pruned', '.', 'Figure', '6-3', 'shows', 'two', 'Decision', 'Trees', 'trained', 'moons', 'dataset', 'introduced', 'Chapter', '5', '.', 'On', 'left', 'Decision', 'Tree', 'trained', 'default', 'hyperparameters', 'i.e.', 'restrictions', 'right', 'Decision', 'Tree', 'trained', 'min_samples_leaf=4', '.', 'It', 'quite', 'obvious', 'model', 'left', 'overfitting', 'model', 'right', 'probably', 'generalize', 'better', '.', 'Figure', '6-3', '.', 'Regularization', 'using', 'min_samples_leaf', '174', '|', 'Chapter', '6', 'Decision', 'Trees', 'RegressionDecision', 'Trees', 'also', 'capable', 'performing', 'regression', 'tasks', '.', 'Let‡s', 'build', 'regres…', 'sion', 'tree', 'using', 'Scikit-Learn‡s', 'DecisionTreeRegressor', 'class', 'training', 'noisyquadratic', 'dataset', 'max_depth=2', 'sklearn.tree', 'import', 'DecisionTreeRegressortree_reg', '=', 'DecisionTreeRegressor', 'max_depth=2', 'tree_reg.fit', 'X', 'The', 'resulting', 'tree', 'represented', 'Figure', '6-4.Figure', '6-4', '.', 'A', 'Decision', 'Tree', 'regression', 'This', 'tree', 'looks', 'similar', 'classification', 'tree', 'built', 'earlier', '.', 'The', 'main', 'differ…', 'ence', 'instead', 'predicting', 'class', 'node', 'predicts', 'value', '.', 'For', 'example', 'suppose', 'want', 'make', 'prediction', 'new', 'instance', 'x1', '=', '0.6', '.', 'You', 'traverse', 'tree', 'starting', 'root', 'eventually', 'reach', 'leaf', 'node', 'predicts', 'value=0.1106', '.', 'This', 'prediction', 'simply', 'average', 'target', 'value', '110', 'training', 'instances', 'associated', 'leaf', 'node', '.', 'This', 'prediction', 'results', 'Mean', 'Squared', 'Error', 'MSE', 'equal', '0.0151', '110', 'instances.This', 'model‡s', 'predictions', 'represented', 'left', 'Figure', '6-5', '.', 'If', 'setmax_depth=3', 'get', 'predictions', 'represented', 'right', '.', 'Notice', 'pre…', 'dicted', 'value', 'region', 'always', 'average', 'target', 'value', 'instances', 'region', '.', 'The', 'algorithm', 'splits', 'region', 'way', 'makes', 'training', 'instances', 'close', 'possible', 'predicted', 'value', '.', 'Regression', '|', '175', 'Figure', '6-5', '.', 'Predictions', 'two', 'Decision', 'Tree', 'regression', 'models', 'The', 'CART', 'algorithm', 'works', 'mostly', 'way', 'earlier', 'except', 'instead', 'try…', 'ing', 'split', 'training', 'set', 'way', 'minimizes', 'impurity', 'tries', 'split', 'training', 'set', 'way', 'minimizes', 'MSE', '.', 'Equation', '6-4', 'shows', 'cost', 'functionthat', 'algorithm', 'tries', 'minimize', '.', 'Equation', '6-4', '.', 'CART', 'cost', 'function', 'regression', 'Jk', 'tk=mleftmMSEleft+mrightmMSErightwhereMSEnode=', '“', 'inodeynode', '”', 'yi2ynode=1mnode', '“', 'inodeyiJust', 'like', 'classification', 'tasks', 'Decision', 'Trees', 'prone', 'overfitting', 'dealing', 'regression', 'tasks', '.', 'Without', 'regularization', 'i.e.', 'using', 'default', 'hyperpara…', 'meters', 'get', 'predictions', 'left', 'Figure', '6-6', '.', 'It', 'obviously', 'overfitting', 'training', 'set', 'badly', '.', 'Just', 'setting', 'min_samples_leaf=10', 'results', 'much', 'reasonable', 'model', 'represented', 'right', 'Figure', '6-6.Figure', '6-6', '.', 'Regularizing', 'Decision', 'Tree', 'regressor', '176', '|', 'Chapter', '6', 'Decision', 'Trees', '6It', 'randomly', 'selects', 'set', 'features', 'evaluate', 'node', '.', 'InstabilityHopefully', 'convinced', 'Decision', 'Trees', 'lot', 'going', 'simple', 'understand', 'interpret', 'easy', 'use', 'versatile', 'powerful', '.', 'However', 'limitations', '.', 'First', 'may', 'noticed', 'Decision', 'Trees', 'love', 'orthogonal', 'decision', 'boundaries', 'splits', 'perpendicular', 'axis', 'makes', 'sensitive', 'training', 'set', 'rotation', '.', 'For', 'example', 'Figure', '6-7', 'shows', 'asimple', 'linearly', 'separable', 'dataset', 'left', 'Decision', 'Tree', 'split', 'easily', 'right', 'dataset', 'rotated', '45', '‘', 'decision', 'boundary', 'looks', 'unneces…', 'sarily', 'convoluted', '.', 'Although', 'Decision', 'Trees', 'fit', 'training', 'set', 'perfectly', 'likely', 'model', 'right', 'generalize', 'well', '.', 'One', 'way', 'limit', 'prob…', 'lem', 'use', 'PCA', 'see', 'Chapter', '8', 'often', 'results', 'better', 'orientation', 'training', 'data', '.', 'Figure', '6-7', '.', 'Sensitivity', 'training', 'set', 'rotation', 'More', 'generally', 'main', 'issue', 'Decision', 'Trees', 'sensitive', 'small', 'variations', 'training', 'data', '.', 'For', 'example', 'remove', 'widest', 'Iris-', 'Versicolor', 'iris', 'training', 'set', 'one', 'petals', '4.8', 'cm', 'long', '1.8', 'cm', 'wide', 'train', 'new', 'Decision', 'Tree', 'may', 'get', 'model', 'represented', 'Figure', '6-8', '.', 'Asyou', 'see', 'looks', 'different', 'previous', 'Decision', 'Tree', 'Figure', '6-2', '.Actually', 'since', 'training', 'algorithm', 'used', 'Scikit-Learn', 'stochastic', '6', 'may', 'get', 'different', 'models', 'even', 'training', 'data', 'unless', 'set', 'random_state', 'hyperparameter', '.', 'Instability', '|', '177', 'Figure', '6-8', '.', 'Sensitivity', 'training', 'set', 'details', 'Random', 'Forests', 'limit', 'instability', 'averaging', 'predictions', 'many', 'trees', 'see', 'next', 'chapter', '.', 'Exercises1.What', 'approximate', 'depth', 'Decision', 'Tree', 'trained', 'without', 'restrictions', 'training', 'set', '1', 'million', 'instances', '?', '2.Is', 'node‡s', 'Gini', 'impurity', 'generally', 'lower', 'greater', 'parent‡s', '?', 'Is', 'gener…', 'ally', 'lower/greater', 'always', 'lower/greater', '?', '3.If', 'Decision', 'Tree', 'overfitting', 'training', 'set', 'good', 'idea', 'try', 'decreasing', 'max_depth', '?', '4.If', 'Decision', 'Tree', 'underfitting', 'training', 'set', 'good', 'idea', 'try', 'scaling', 'input', 'features', '?', '5.If', 'takes', 'one', 'hour', 'train', 'Decision', 'Tree', 'training', 'set', 'containing', '1', 'million', 'instances', 'roughly', 'much', 'time', 'take', 'train', 'another', 'Decision', 'Tree', 'training', 'set', 'containing', '10', 'million', 'instances', '?', '6.If', 'training', 'set', 'contains', '100,000', 'instances', 'setting', 'presort=True', 'speedup', 'training', '?', '7.Train', 'fine-tune', 'Decision', 'Tree', 'moons', 'dataset', '.', 'a.Generate', 'moons', 'dataset', 'using', 'make_moons', 'n_samples=10000', 'noise=0.4', '.b', '.', 'Split', 'training', 'set', 'test', 'set', 'using', 'train_test_split', '.178', '|', 'Chapter', '6', 'Decision', 'Trees', 'c.Use', 'grid', 'search', 'cross-validation', 'help', 'GridSearchCVclass', 'find', 'good', 'hyperparameter', 'values', 'DecisionTreeClassifier', '.', 'Hint', 'try', 'various', 'values', 'max_leaf_nodes.d.Train', 'full', 'training', 'set', 'using', 'hyperparameters', 'measure', 'model‡s', 'performance', 'test', 'set', '.', 'You', 'get', 'roughly', '85', '%', '87', '%', 'accuracy', '.', '8.Grow', 'forest.a.Continuing', 'previous', 'exercise', 'generate', '1,000', 'subsets', 'training', 'set', 'containing', '100', 'instances', 'selected', 'randomly', '.', 'Hint', 'use', 'Scikit-', 'Learn‡s', 'ShuffleSplit', 'class', 'this.b', '.', 'Train', 'one', 'Decision', 'Tree', 'subset', 'using', 'best', 'hyperparameter', 'values', 'found', '.', 'Evaluate', '1,000', 'Decision', 'Trees', 'test', 'set', '.', 'Since', 'trained', 'smaller', 'sets', 'Decision', 'Trees', 'likely', 'perform', 'worse', 'first', 'Decision', 'Tree', 'achieving', '80', '%', 'accuracy', '.', 'c.Now', 'comes', 'magic', '.', 'For', 'test', 'set', 'instance', 'generate', 'predictions', '1,000', 'Decision', 'Trees', 'keep', 'frequent', 'prediction', 'use', 'SciPy‡s', 'mode', 'function', '.', 'This', 'gives', 'majority-vote', 'predictions', 'test', 'set.d.Evaluate', 'predictions', 'test', 'set', 'obtain', 'slightly', 'higher', 'accuracy', 'first', 'model', '0.5', '1.5', '%', 'higher', '.', 'Congratulations', 'trained', 'Random', 'Forest', 'classifier', '!', 'Solutions', 'exercises', 'available', 'Appendix', 'A', '.Exercises', '|', '179', 'CHAPTER', '7Ensemble', 'Learning', 'Random', 'ForestsSuppose', 'ask', 'complex', 'question', 'thousands', 'random', 'people', 'aggregate', 'answers', '.', 'In', 'many', 'cases', 'find', 'aggregated', 'answer', 'better', 'expert‡s', 'answer', '.', 'This', 'called', 'wisdom', 'crowd', '.', 'Similarly', 'aggregate', 'predictions', 'group', 'predictors', 'classifiers', 'regressors', 'willoften', 'get', 'better', 'predictions', 'best', 'individual', 'predictor', '.', 'A', 'group', 'pre…', 'dictors', 'called', 'ensemble', 'thus', 'technique', 'called', 'Ensemble', 'Learning', 'anEnsemble', 'Learning', 'algorithm', 'called', 'Ensemble', 'method', '.For', 'example', 'train', 'group', 'Decision', 'Tree', 'classifiers', 'different', 'random', 'subset', 'training', 'set', '.', 'To', 'make', 'predictions', 'obtain', 'predic…', 'tions', 'individual', 'trees', 'predict', 'class', 'gets', 'votes', 'see', 'last', 'exercise', 'Chapter', '6', '.', 'Such', 'ensemble', 'Decision', 'Trees', 'called', 'Random', 'Forest', 'despite', 'simplicity', 'one', 'powerful', 'Machine', 'Learning', 'algo…', 'rithms', 'available', 'today', '.', 'Moreover', 'discussed', 'Chapter', '2', 'often', 'use', 'Ensemble', 'methods', 'nearthe', 'end', 'project', 'already', 'built', 'good', 'predictors', 'combine', 'even', 'better', 'predictor', '.', 'In', 'fact', 'winning', 'solutions', 'Machine', 'Learn…', 'ing', 'competitions', 'often', 'involve', 'several', 'Ensemble', 'methods', 'famously', 'Net…', 'flix', 'Prize', 'competition', '.In', 'chapter', 'discuss', 'popular', 'Ensemble', 'methods', 'including', 'bag…', 'ging', 'boosting', 'stacking', 'others', '.', 'We', 'also', 'explore', 'Random', 'Forests', '.', 'Voting', 'Classi•ersSuppose', 'trained', 'classifiers', 'one', 'achieving', '80', '%', 'accuracy', '.', 'You', 'may', 'Logistic', 'Regression', 'classifier', 'SVM', 'classifier', 'Random', 'Forest', 'classifier', 'K-Nearest', 'Neighbors', 'classifier', 'perhaps', 'see', 'Figure', '7-1', '.181Figure', '7-1', '.', 'Training', 'diverse', 'classi†ersA', 'simple', 'way', 'create', 'even', 'better', 'classifier', 'aggregate', 'predictions', 'classifier', 'predict', 'class', 'gets', 'votes', '.', 'This', 'majority-vote', 'classi…', 'fier', 'called', 'hard', 'voting', 'classifier', 'see', 'Figure', '7-2', '.Figure', '7-2', '.', 'Hard', 'voting', 'classi†er', 'predictions', 'Somewhat', 'surprisingly', 'voting', 'classifier', 'often', 'achieves', 'higher', 'accuracy', 'best', 'classifier', 'ensemble', '.', 'In', 'fact', 'even', 'classifier', 'weak', 'learner', 'mean…ing', 'slightly', 'better', 'random', 'guessing', 'ensemble', 'still', 'strong', 'learner', 'achieving', 'high', 'accuracy', 'provided', 'sufficient', 'number', 'weak', 'learners', 'sufficiently', 'diverse', '.', '182', '|', 'Chapter', '7', 'Ensemble', 'Learning', 'Random', 'Forests', 'How', 'possible', '?', 'The', 'following', 'analogy', 'help', 'shed', 'light', 'mystery', '.', 'Suppose', 'slightly', 'biased', 'coin', '51', '%', 'chance', 'coming', 'heads', '49', '%', 'chance', 'coming', 'tails', '.', 'If', 'toss', '1,000', 'times', 'generally', 'getmore', 'less', '510', 'heads', '490', 'tails', 'hence', 'majority', 'heads', '.', 'If', 'math', 'find', 'probability', 'obtaining', 'majority', 'heads', '1,000', 'tosses', 'close', '75', '%', '.', 'The', 'toss', 'coin', 'higher', 'probability', 'e.g.', '10,000', 'tosses', 'probability', 'climbs', '97', '%', '.', 'This', 'due', 'law', 'large', 'numbers', 'keep', 'tossing', 'coin', 'ratio', 'heads', 'gets', 'closer', 'closer', 'probability', 'heads', '51', '%', '.', 'Figure', '7-3', 'shows', '10', 'series', 'biased', 'coin', 'tosses', '.', 'You', 'see', 'number', 'tosses', 'increases', 'ratio', 'heads', 'approaches', '51', '%', '.', 'Eventu…', 'ally', '10', 'series', 'end', 'close', '51', '%', 'consistently', '50', '%', '.', 'Figure', '7-3', '.', '•e', 'law', 'large', 'numbers', 'Similarly', 'suppose', 'build', 'ensemble', 'containing', '1,000', 'classifiers', 'individ…', 'ually', 'correct', '51', '%', 'time', 'barely', 'better', 'random', 'guessing', '.', 'If', 'pre…dict', 'majority', 'voted', 'class', 'hope', '75', '%', 'accuracy', '!', 'However', 'true', 'classifiers', 'perfectly', 'independent', 'making', 'uncorrelated', 'errors', 'clearly', 'case', 'since', 'trained', 'data', '.', 'They', 'likely', 'make', 'types', 'errors', 'many', 'majority', 'votes', 'wrong', 'class', 'reducing', 'ensemble‡s', 'accuracy', '.', 'Ensemble', 'methods', 'work', 'best', 'predictors', 'independ…ent', 'one', 'another', 'possible', '.', 'One', 'way', 'get', 'diverse', 'classifiers', 'train', 'using', 'different', 'algorithms', '.', 'This', 'increases', 'chance', 'make', 'different', 'types', 'errors', 'improving', 'ensemble‡s', 'accuracy', '.', 'Voting', 'Classi•ers', '|', '183', 'The', 'following', 'code', 'creates', 'trains', 'voting', 'classifier', 'Scikit-Learn', 'composed', 'three', 'diverse', 'classifiers', 'training', 'set', 'moons', 'dataset', 'introduced', 'Chap…', 'ter', '5', 'sklearn.ensemble', 'import', 'RandomForestClassifierfrom', 'sklearn.ensemble', 'import', 'VotingClassifierfrom', 'sklearn.linear_model', 'import', 'LogisticRegressionfrom', 'sklearn.svm', 'import', 'SVClog_clf', '=', 'LogisticRegression', 'rnd_clf', '=', 'RandomForestClassifier', 'svm_clf', '=', 'SVC', 'voting_clf', '=', 'VotingClassifier', 'estimators=', '•lr•', 'log_clf', '•rf•', 'rnd_clf', '•svc•', 'svm_clf', 'voting=•hard•', 'voting_clf.fit', 'X_train', 'y_train', 'Let‡s', 'look', 'classifier‡s', 'accuracy', 'test', 'set', '>', '>', '>', 'sklearn.metrics', 'import', 'accuracy_score', '>', '>', '>', 'clf', 'log_clf', 'rnd_clf', 'svm_clf', 'voting_clf', '>', '>', '>', 'clf.fit', 'X_train', 'y_train', '>', '>', '>', 'y_pred', '=', 'clf.predict', 'X_test', '>', '>', '>', 'print', 'clf.__class__.__name__', 'accuracy_score', 'y_test', 'y_pred', 'LogisticRegression', '0.864RandomForestClassifier', '0.872SVC', '0.888VotingClassifier', '0.896There', '!', 'The', 'voting', 'classifier', 'slightly', 'outperforms', 'individual', 'classifi…', 'ers.If', 'classifiers', 'able', 'estimate', 'class', 'probabilities', 'i.e.', 'predict_proba', 'method', 'tell', 'Scikit-Learn', 'predict', 'class', 'thehighest', 'class', 'probability', 'averaged', 'individual', 'classifiers', '.', 'This', 'called', '“', 'voting', '.', 'It', 'often', 'achieves', 'higher', 'performance', 'hard', 'voting', 'gives', 'weight', 'highly', 'confident', 'votes', '.', 'All', 'need', 'replace', 'voting=', \"''\", 'hard', \"''\", 'withvoting=', \"''\", 'soft', \"''\", 'ensure', 'classifiers', 'estimate', 'class', 'probabilities', '.', 'This', 'case', 'SVC', 'class', 'default', 'need', 'set', 'probability', 'hyperpara…', 'meter', 'True', 'make', 'SVC', 'class', 'use', 'cross-validation', 'estimate', 'class', 'prob…', 'abilities', 'slowing', 'training', 'add', 'predict_proba', 'method', '.', 'If', 'youmodify', 'preceding', 'code', 'use', 'soft', 'voting', 'find', 'voting', 'classifier', 'achieves', '91', '%', 'accuracy', '!', '184', '|', 'Chapter', '7', 'Ensemble', 'Learning', 'Random', 'Forests', '1ƒBagging', 'Predictors', '⁄', 'L.', 'Breiman', '1996', '.', '2In', 'statistics', 'resampling', 'replacement', 'called', 'bootstrapping', '.3ƒPasting', 'small', 'votes', 'classification', 'large', 'databases', 'on-line', '⁄', 'L.', 'Breiman', '1999', '.', '4Bias', 'variance', 'introduced', 'Chapter', '4', '.Bagging', 'PastingOne', 'way', 'get', 'diverse', 'set', 'classifiers', 'use', 'different', 'training', 'algorithms', 'discussed', '.', 'Another', 'approach', 'use', 'training', 'algorithm', 'every', 'predictor', 'train', 'different', 'random', 'subsets', 'training', 'set', '.', 'When', 'sampling', 'performed', 'replacement', 'method', 'called', 'bagging', '1', 'short', 'forbootstrap', 'aggregating', '2', '.', 'When', 'sampling', 'performed', 'without', 'replacement', 'called', 'pasting', '.3In', 'words', 'bagging', 'pasting', 'allow', 'training', 'instances', 'sampled', 'sev…', 'eral', 'times', 'across', 'multiple', 'predictors', 'bagging', 'allows', 'training', 'instances', 'sampled', 'several', 'times', 'predictor', '.', 'This', 'sampling', 'training', 'process', 'represented', 'Figure', '7-4.Figure', '7-4', '.', 'Pasting/bagging', 'training', 'set', 'sampling', 'training', 'Once', 'predictors', 'trained', 'ensemble', 'make', 'prediction', 'newinstance', 'simply', 'aggregating', 'predictions', 'predictors', '.', 'The', 'aggregation', 'function', 'typically', 'statistical', 'mode', 'i.e.', 'frequent', 'prediction', 'like', 'hard', 'voting', 'classifier', 'classification', 'average', 'regression', '.', 'Each', 'individual', 'predictor', 'higher', 'bias', 'trained', 'original', 'training', 'set', 'butaggregation', 'reduces', 'bias', 'variance', '.', '4', 'Generally', 'net', 'result', 'Bagging', 'Pasting', '|', '185', '5max_samples', 'alternatively', 'set', 'float', '0.0', '1.0', 'case', 'max', 'number', 'instances', 'sample', 'equal', 'size', 'training', 'set', 'times', 'max_samples.ensemble', 'similar', 'bias', 'lower', 'variance', 'single', 'predictor', 'trained', 'theoriginal', 'training', 'set.As', 'see', 'Figure', '7-4', 'predictors', 'trained', 'parallel', 'via', 'different', 'CPU', 'cores', 'even', 'different', 'servers', '.', 'Similarly', 'predictions', 'made', 'parallel', '.', 'This', 'one', 'reasons', 'bagging', 'pasting', 'popular', 'methods', 'scale', 'well', '.', 'Bagging', 'Pasting', 'Scikit-LearnScikit-Learn', 'offers', 'simple', 'API', 'bagging', 'pasting', 'BaggingClassifier', 'class', 'BaggingRegressor', 'regression', '.', 'The', 'following', 'code', 'trains', 'anensemble', '500', 'Decision', 'Tree', 'classifiers', '5', 'trained', '100', 'training', 'instances', 'ran…domly', 'sampled', 'training', 'set', 'replacement', 'example', 'bagging', 'want', 'use', 'pasting', 'instead', 'set', 'bootstrap=False', '.', 'The', 'n_jobs', 'param…', 'eter', 'tells', 'Scikit-Learn', 'number', 'CPU', 'cores', 'use', 'training', 'predictions', '–1', 'tells', 'Scikit-Learn', 'use', 'available', 'cores', 'sklearn.ensemble', 'import', 'BaggingClassifierfrom', 'sklearn.tree', 'import', 'DecisionTreeClassifierbag_clf', '=', 'BaggingClassifier', 'DecisionTreeClassifier', 'n_estimators=500', 'max_samples=100', 'bootstrap=True', 'n_jobs=-1', 'bag_clf.fit', 'X_train', 'y_train', 'y_pred', '=', 'bag_clf.predict', 'X_test', 'The', 'BaggingClassifier', 'automatically', 'performs', 'soft', 'voting', 'instead', 'hard', 'voting', 'base', 'classifier', 'estimate', 'class', 'proba…', 'bilities', 'i.e.', 'predict_proba', 'method', 'casewith', 'Decision', 'Trees', 'classifiers', '.', 'Figure', '7-5', 'compares', 'decision', 'boundary', 'single', 'Decision', 'Tree', 'deci…', 'sion', 'boundary', 'bagging', 'ensemble', '500', 'trees', 'preceding', 'code', 'trained', 'moons', 'dataset', '.', 'As', 'see', 'ensemble‡s', 'predictions', 'likely', 'generalize', 'much', 'better', 'single', 'Decision', 'Tree‡s', 'predictions', 'ensemble', 'comparable', 'bias', 'smaller', 'variance', 'makes', 'roughly', 'number', 'errors', 'training', 'set', 'decision', 'boundary', 'less', 'irregular', '.', '186', '|', 'Chapter', '7', 'Ensemble', 'Learning', 'Random', 'Forests', '6As', 'grows', 'ratio', 'approaches', '1', '–', 'exp', '–1', 'Ÿ', '63.212', '%', '.', 'Figure', '7-5', '.', 'A', 'single', 'Decision', 'Tree', 'versus', 'bagging', 'ensemble', '500', 'trees', 'Bootstrapping', 'introduces', 'bit', 'diversity', 'subsets', 'predictor', 'trained', 'bagging', 'ends', 'slightly', 'higher', 'bias', 'pasting', 'also', 'means', 'predictors', 'end', 'less', 'correlated', 'ensemble‡s', 'variance', 'reduced', '.', 'Overall', 'bagging', 'often', 'results', 'better', 'models', 'explains', 'gen…', 'erally', 'preferred', '.', 'However', 'spare', 'time', 'CPU', 'power', 'use', 'cross-', 'validation', 'evaluate', 'bagging', 'pasting', 'select', 'one', 'works', 'best', '.', 'Out-of-Bag', 'EvaluationWith', 'bagging', 'instances', 'may', 'sampled', 'several', 'times', 'given', 'predictor', 'others', 'may', 'sampled', '.', 'By', 'default', 'BaggingClassifier', 'samples', 'mtraining', 'instances', 'replacement', 'bootstrap=True', 'size', 'thetraining', 'set', '.', 'This', 'means', '63', '%', 'training', 'instances', 'sampled', 'average', 'predictor', '.', '6', 'The', 'remaining', '37', '%', 'training', 'instances', 'sampled', 'called', 'out-of-bag', 'oob', 'instances', '.', 'Note', '37', '%', 'predictors.Since', 'predictor', 'never', 'sees', 'oob', 'instances', 'training', 'evaluated', 'instances', 'without', 'need', 'separate', 'validation', 'set', 'cross-validation', '.', 'You', 'evaluate', 'ensemble', 'averaging', 'oob', 'evaluations', 'predic…', 'tor', '.', 'In', 'Scikit-Learn', 'set', 'oob_score=True', 'creating', 'BaggingClassifier', 'torequest', 'automatic', 'oob', 'evaluation', 'training', '.', 'The', 'following', 'code', 'demonstrates', '.', 'The', 'resulting', 'evaluation', 'score', 'available', 'oob_score_', 'variable', '>', '>', '>', 'bag_clf', '=', 'BaggingClassifier', '>', '>', '>', 'DecisionTreeClassifier', 'n_estimators=500', '>', '>', '>', 'bootstrap=True', 'n_jobs=-1', 'oob_score=True', 'Bagging', 'Pasting', '|', '187', '7ƒEnsembles', 'Random', 'Patches', '⁄', 'G.', 'Louppe', 'P.', 'Geurts', '2012', '.', '8ƒThe', 'random', 'subspace', 'method', 'constructing', 'decision', 'forests', '⁄', 'Tin', 'Kam', 'Ho', '1998', '.', '>', '>', '>', 'bag_clf.fit', 'X_train', 'y_train', '>', '>', '>', 'bag_clf.oob_score_0.93066666666666664According', 'oob', 'evaluation', 'BaggingClassifier', 'likely', 'achieve', 'about93.1', '%', 'accuracy', 'test', 'set', '.', 'Let‡s', 'verify', '>', '>', '>', 'sklearn.metrics', 'import', 'accuracy_score', '>', '>', '>', 'y_pred', '=', 'bag_clf.predict', 'X_test', '>', '>', '>', 'accuracy_score', 'y_test', 'y_pred', '0.93600000000000005We', 'get', '93.6', '%', 'accuracy', 'test', 'set›close', 'enough', '!', 'The', 'oob', 'decision', 'function', 'training', 'instance', 'also', 'available', 'oob_decision_function_', 'variable', '.', 'In', 'case', 'since', 'base', 'estimator', 'predict_proba', 'method', 'decision', 'function', 'returns', 'class', 'probabilities', 'eachtraining', 'instance', '.', 'For', 'example', 'oob', 'evaluation', 'estimates', 'second', 'training', 'instance', '60.6', '%', 'probability', 'belonging', 'positive', 'class', '39.4', '%', 'ofbelonging', 'positive', 'class', '>', '>', '>', 'bag_clf.oob_decision_function_array', '0.', '1', '.', '0.60588235', '0.39411765', '1.', '0', '.', '...', '1.', '0', '.', '0.', '1', '.', '0.48958333', '0.51041667', 'Random', 'Patches', 'Random', 'SubspacesThe', 'BaggingClassifier', 'class', 'supports', 'sampling', 'features', 'well', '.', 'This', 'con…', 'trolled', 'two', 'hyperparameters', 'max_features', 'bootstrap_features', '.', 'They', 'workthe', 'way', 'max_samples', 'bootstrap', 'feature', 'sampling', 'instead', 'instance', 'sampling', '.', 'Thus', 'predictor', 'trained', 'random', 'subset', 'input', 'features', '.', 'This', 'particularly', 'useful', 'dealing', 'high-dimensional', 'inputs', 'images', '.', 'Sampling', 'training', 'instances', 'features', 'called', 'Random', 'Patches', 'method.7', 'Keeping', 'training', 'instances', 'i.e.', 'bootstrap=False', 'max_samples=1.0', 'sampling', 'features', 'i.e.', 'bootstrap_features=True', 'and/or', 'max_features', 'smaller', '1.0', 'called', 'Random', 'Subspaces', 'method.8188', '|', 'Chapter', '7', 'Ensemble', 'Learning', 'Random', 'Forests', '9ƒRandom', 'Decision', 'Forests', '⁄', 'T.', 'Ho', '1995', '.', '10The', 'BaggingClassifier', 'class', 'remains', 'useful', 'want', 'bag', 'something', 'Decision', 'Trees', '.', '11There', 'notable', 'exceptions', 'splitter', 'absent', 'forced', \"''\", 'random', \"''\", 'presort', 'absent', 'forced', 'False', 'max_samples', 'absent', 'forced', '1.0', 'base_estimator', 'absent', 'forced', 'DecisionTreeClassifier', 'provided', 'hyperparameters', '.', 'Sampling', 'features', 'results', 'even', 'predictor', 'diversity', 'trading', 'bit', 'bias', 'lower', 'variance.Random', 'ForestsAs', 'discussed', 'Random', 'Forest', '9', 'ensemble', 'Decision', 'Trees', 'generally', 'trained', 'via', 'bagging', 'method', 'sometimes', 'pasting', 'typically', 'max_samplesset', 'size', 'training', 'set', '.', 'Instead', 'building', 'BaggingClassifier', 'pass…ing', 'DecisionTreeClassifier', 'instead', 'use', 'RandomForestClassifierclass', 'convenient', 'optimized', 'Decision', 'Trees', '10', 'similarly', 'RandomForestRegressor', 'class', 'regression', 'tasks', '.', 'The', 'following', 'code', 'trains', 'aRandom', 'Forest', 'classifier', '500', 'trees', 'limited', 'maximum', '16', 'nodes', 'using', 'available', 'CPU', 'cores', 'sklearn.ensemble', 'import', 'RandomForestClassifierrnd_clf', '=', 'RandomForestClassifier', 'n_estimators=500', 'max_leaf_nodes=16', 'n_jobs=-1', 'rnd_clf.fit', 'X_train', 'y_train', 'y_pred_rf', '=', 'rnd_clf.predict', 'X_test', 'With', 'exceptions', 'RandomForestClassifier', 'hyperparameters', 'DecisionTreeClassifier', 'control', 'trees', 'grown', 'plus', 'hyperpara…', 'meters', 'BaggingClassifier', 'control', 'ensemble', '.', '11The', 'Random', 'Forest', 'algorithm', 'introduces', 'extra', 'randomness', 'growing', 'trees', 'instead', 'searching', 'best', 'feature', 'splitting', 'node', 'see', 'Chapter', '6', 'itsearches', 'best', 'feature', 'among', 'random', 'subset', 'features', '.', 'This', 'results', 'greater', 'tree', 'diversity', 'trades', 'higher', 'bias', 'lower', 'variance', 'generally', 'yielding', 'overall', 'better', 'model', '.', 'The', 'following', 'BaggingClassifier', 'isroughly', 'equivalent', 'previous', 'RandomForestClassifier', 'bag_clf', '=', 'BaggingClassifier', 'DecisionTreeClassifier', 'splitter=', \"''\", 'random', \"''\", 'max_leaf_nodes=16', 'n_estimators=500', 'max_samples=1.0', 'bootstrap=True', 'n_jobs=-1', 'Random', 'Forests', '|', '189', '12ƒExtremely', 'randomized', 'trees', '⁄', 'P.', 'Geurts', 'D.', 'Ernst', 'L.', 'Wehenkel', '2005', '.', 'Extra-TreesWhen', 'growing', 'tree', 'Random', 'Forest', 'node', 'random', 'subset', 'features', 'considered', 'splitting', 'discussed', 'earlier', '.', 'It', 'possible', 'make', 'trees', 'even', 'random', 'also', 'using', 'random', 'thresholds', 'feature', 'rather', 'searching', 'best', 'possible', 'thresholds', 'like', 'regular', 'Decision', 'Trees', '.', 'A', 'forest', 'extremely', 'random', 'trees', 'simply', 'called', 'Extremely', 'Randomized', 'Trees', 'ensemble12', 'Extra-Trees', 'short', '.', 'Once', 'trades', 'bias', 'alower', 'variance', '.', 'It', 'also', 'makes', 'Extra-Trees', 'much', 'faster', 'train', 'regular', 'Random', 'Forests', 'since', 'finding', 'best', 'possible', 'threshold', 'feature', 'every', 'node', 'one', 'time-consuming', 'tasks', 'growing', 'tree.You', 'create', 'Extra-Trees', 'classifier', 'using', 'Scikit-Learn‡s', 'ExtraTreesClassifierclass', '.', 'Its', 'API', 'identical', 'RandomForestClassifier', 'class', '.', 'Similarly', 'ExtraTreesRegressor', 'class', 'API', 'RandomForestRegressor', 'class.It', 'hard', 'tell', 'advance', 'whether', 'RandomForestClassifierwill', 'perform', 'better', 'worse', 'ExtraTreesClassifier', '.', 'Gen…erally', 'way', 'know', 'try', 'compare', 'using', 'cross-validation', 'tuning', 'hyperparameters', 'using', 'grid', 'search', '.Feature', 'ImportanceLastly', 'look', 'single', 'Decision', 'Tree', 'important', 'features', 'likely', 'appear', 'closer', 'root', 'tree', 'unimportant', 'features', 'often', 'appear', 'closer', 'leaves', '.', 'It', 'therefore', 'possible', 'get', 'estimate', 'feature‡s', 'impor…', 'tance', 'computing', 'average', 'depth', 'appears', 'across', 'trees', 'forest', '.', 'Scikit-Learn', 'computes', 'automatically', 'every', 'feature', 'training', '.', 'You', 'access', 'result', 'using', 'feature_importances_', 'variable', '.', 'For', 'example', 'follow…', 'ing', 'code', 'trains', 'RandomForestClassifier', 'iris', 'dataset', 'introduced', 'Chap…', 'ter', '4', 'outputs', 'feature‡s', 'importance', '.', 'It', 'seems', 'important', 'features', 'petal', 'length', '44', '%', 'width', '42', '%', 'sepal', 'length', 'width', 'rather', 'unimportant', 'comparison', '11', '%', '2', '%', 'respectively', '>', '>', '>', 'sklearn.datasets', 'import', 'load_iris', '>', '>', '>', 'iris', '=', 'load_iris', '>', '>', '>', 'rnd_clf', '=', 'RandomForestClassifier', 'n_estimators=500', 'n_jobs=-1', '>', '>', '>', 'rnd_clf.fit', 'iris', '``', 'data', \"''\", 'iris', '``', 'target', \"''\", '>', '>', '>', 'name', 'score', 'zip', 'iris', '``', 'feature_names', \"''\", 'rnd_clf.feature_importances_', '>', '>', '>', 'print', 'name', 'score', 'sepal', 'length', 'cm', '0.112492250999190', '|', 'Chapter', '7', 'Ensemble', 'Learning', 'Random', 'Forests', '13ƒA', 'Decision-Theoretic', 'Generalization', 'On-Line', 'Learning', 'Application', 'Boosting', '⁄', 'Yoav', 'Freund', 'Robert', 'E.', 'Schapire', '1997', '.', 'sepal', 'width', 'cm', '0.0231192882825petal', 'length', 'cm', '0.441030464364petal', 'width', 'cm', '0.423357996355Similarly', 'train', 'Random', 'Forest', 'classifier', 'MNIST', 'dataset', 'introduced', 'Chapter', '3', 'plot', 'pixel‡s', 'importance', 'get', 'image', 'represented', 'Figure', '7-6.Figure', '7-6', '.', 'MNIST', 'pixel', 'importance', 'according', 'Random', 'Forest', 'classi†er', 'Random', 'Forests', 'handy', 'get', 'quick', 'understanding', 'features', 'actually', 'matter', 'particular', 'need', 'perform', 'feature', 'selection', '.', 'BoostingBoosting', 'originally', 'called', 'hypothesis', 'boosting', 'refers', 'Ensemble', 'method', 'combine', 'several', 'weak', 'learners', 'strong', 'learner', '.', 'The', 'general', 'idea', 'boosting', 'methods', 'train', 'predictors', 'sequentially', 'trying', 'correct', 'prede…', 'cessor', '.', 'There', 'many', 'boosting', 'methods', 'available', 'far', 'popular', 'AdaBoost', '13', 'short', 'Adaptive', 'Boosting', 'Gradient', 'Boosting', '.', 'Let‡s', 'start', 'Ada…', 'Boost', '.', 'Boosting', '|', '191', '14This', 'illustrative', 'purposes', '.', 'SVMs', 'generally', 'good', 'base', 'predictors', 'AdaBoost', 'slow', 'tend', 'unstable', 'AdaBoost', '.', 'AdaBoostOne', 'way', 'new', 'predictor', 'correct', 'predecessor', 'pay', 'bit', 'attention', 'training', 'instances', 'predecessor', 'underfitted', '.', 'This', 'results', 'new', 'predic…', 'tors', 'focusing', 'hard', 'cases', '.', 'This', 'technique', 'used', 'Ada…', 'Boost', '.', 'For', 'example', 'build', 'AdaBoost', 'classifier', 'first', 'base', 'classifier', 'Decision', 'Tree', 'trained', 'used', 'make', 'predictions', 'training', 'set', '.', 'The', 'relative', 'weight', 'misclassified', 'training', 'instances', 'increased', '.', 'A', 'second', 'classifier', 'trainedusing', 'updated', 'weights', 'makes', 'predictions', 'training', 'set', 'weights', 'updated', 'see', 'Figure', '7-7', '.Figure', '7-7', '.', 'AdaBoost', 'sequential', 'training', 'instance', 'weight', 'updates', 'Figure', '7-8', 'shows', 'decision', 'boundaries', 'five', 'consecutive', 'predictors', 'themoons', 'dataset', 'example', 'predictor', 'highly', 'regularized', 'SVM', 'classifier', 'RBF', 'kernel14', '.', 'The', 'first', 'classifier', 'gets', 'many', 'instances', 'wrong', 'weights', 'get', 'boosted', '.', 'The', 'second', 'classifier', 'therefore', 'better', 'job', 'instances', 'andso', '.', 'The', 'plot', 'right', 'represents', 'sequence', 'predictors', 'except', 'learning', 'rate', 'halved', 'i.e.', 'misclassified', 'instance', 'weights', 'boosted', 'half', 'much', 'every', 'iteration', '.', 'As', 'see', 'sequential', 'learning', 'technique', 'similarities', 'Gradient', 'Descent', 'except', 'instead', 'tweaking', 'single', 'predictor‡s', '192', '|', 'Chapter', '7', 'Ensemble', 'Learning', 'Random', 'Forests', 'parameters', 'minimize', 'cost', 'function', 'AdaBoost', 'adds', 'predictors', 'ensemble', 'gradually', 'making', 'better', '.', 'Figure', '7-8', '.', 'Decision', 'boundaries', 'consecutive', 'predictors', 'Once', 'predictors', 'trained', 'ensemble', 'makes', 'predictions', 'much', 'like', 'bag…', 'ging', 'pasting', 'except', 'predictors', 'different', 'weights', 'depending', 'overall', 'accuracy', 'weighted', 'training', 'set', '.', 'There', 'one', 'important', 'drawback', 'sequential', 'learning', 'techni…', 'que', 'parallelized', 'partially', 'since', 'predic…tor', 'trained', 'previous', 'predictor', 'beentrained', 'evaluated', '.', 'As', 'result', 'scale', 'well', 'bag…', 'ging', 'pasting.Let‡s', 'take', 'closer', 'look', 'AdaBoost', 'algorithm', '.', 'Each', 'instance', 'weight', 'w', 'initially', 'set', '1m', '.', 'A', 'first', 'predictor', 'trained', 'weighted', 'error', 'rate', 'r1', 'computed', 'training', 'set', 'see', 'Equation', '7-1', '.Equation', '7-1', '.', 'Weighted', 'error', 'rate', 'j', 'th', 'predictor', 'rj=', '“', 'i=1', 'yjiłyimwi', '“', 'i=1', 'mwiwhereyjiisthe', 'jthpredictor‡spredictionforthe', 'ithinstance.Boosting', '|', '193', '15The', 'original', 'AdaBoost', 'algorithm', 'use', 'learning', 'rate', 'hyperparameter', '.', 'The', 'predictor‡s', 'weight', '‰j', 'computed', 'using', 'Equation', '7-2', '−', 'learn…ing', 'rate', 'hyperparameter', 'defaults', '1', '.', '15', 'The', 'accurate', 'predictor', 'higher', 'weight', '.', 'If', 'guessing', 'randomly', 'weight', 'close', 'zero', '.', 'However', 'often', 'wrong', 'i.e.', 'less', 'accurate', 'random', 'guessing', 'weight', 'negative', '.', 'Equation', '7-2', '.', 'Predictor', 'weight', '‰j=−log1', '”', 'rjrjNext', 'instance', 'weights', 'updated', 'using', 'Equation', '7-3', 'misclassified', 'instancesare', 'boosted.Equation', '7-3', '.', 'Weight', 'update', 'rule', 'fori=1,2', 'mwiwiifyji=yiwiexp‰jifyjiłyiThen', 'instance', 'weights', 'normalized', 'i.e.', 'divided', '“', 'i=1', 'mwi', '.Finally', 'new', 'predictor', 'trained', 'using', 'updated', 'weights', 'whole', 'process', 'repeated', 'new', 'predictor‡s', 'weight', 'computed', 'instance', 'weights', 'updated', 'another', 'predictor', 'trained', '.', 'The', 'algorithm', 'stops', 'desirednumber', 'predictors', 'reached', 'perfect', 'predictor', 'found', '.', 'To', 'make', 'predictions', 'AdaBoost', 'simply', 'computes', 'predictions', 'predictors', 'weighs', 'using', 'predictor', 'weights', '‰j', '.', 'The', 'predicted', 'class', 'one', 'receives', 'majority', 'weighted', 'votes', 'see', 'Equation', '7-4', '.Equation', '7-4', '.', 'AdaBoost', 'predictions', 'y=argmax', 'k', '“', 'j=1', 'yj=kN‰jwhereNisthenumberofpredictors', '.', '194', '|', 'Chapter', '7', 'Ensemble', 'Learning', 'Random', 'Forests', '16For', 'details', 'see', 'ƒMulti-Class', 'AdaBoost', '⁄', 'J.', 'Zhu', 'et', 'al', '.', '2006', '.', '17First', 'introduced', 'ƒArcing', 'Edge', '⁄', 'L.', 'Breiman', '1997', '.', 'Scikit-Learn', 'actually', 'uses', 'multiclass', 'version', 'AdaBoost', 'called', 'SAMME16', 'whichstands', 'Stagewise', 'Additive', 'Modeling', 'using', 'Multiclass', 'Exponential', 'loss', 'function', '.When', 'two', 'classes', 'SAMME', 'equivalent', 'AdaBoost', '.', 'Moreover', 'predictors', 'estimate', 'class', 'probabilities', 'i.e.', 'predict_proba', 'method', 'Scikit-Learn', 'use', 'variant', 'SAMME', 'called', 'SAMME.R', 'R', 'standsfor', 'ƒReal⁄', 'relies', 'class', 'probabilities', 'rather', 'predictions', 'generally', 'performs', 'better', '.', 'The', 'following', 'code', 'trains', 'AdaBoost', 'classifier', 'based', '200', 'Decision', 'Stumps', 'usingScikit-Learn‡s', 'AdaBoostClassifier', 'class', 'might', 'expect', 'also', 'AdaBoostRegressor', 'class', '.', 'A', 'Decision', 'Stump', 'Decision', 'Tree', 'max_depth=1›inother', 'words', 'tree', 'composed', 'single', 'decision', 'node', 'plus', 'two', 'leaf', 'nodes', '.', 'This', 'default', 'base', 'estimator', 'AdaBoostClassifier', 'class', 'sklearn.ensemble', 'import', 'AdaBoostClassifierada_clf', '=', 'AdaBoostClassifier', 'DecisionTreeClassifier', 'max_depth=1', 'n_estimators=200', 'algorithm=', \"''\", 'SAMME.R', \"''\", 'learning_rate=0.5', 'ada_clf.fit', 'X_train', 'y_train', 'If', 'AdaBoost', 'ensemble', 'overfitting', 'training', 'set', 'try', 'reducing', 'number', 'estimators', 'strongly', 'regulariz…', 'ing', 'base', 'estimator', '.', 'Gradient', 'BoostingAnother', 'popular', 'Boosting', 'algorithm', 'Gradient', 'Boosting', '.17', 'Just', 'like', 'AdaBoost', 'Gradient', 'Boosting', 'works', 'sequentially', 'adding', 'predictors', 'ensemble', 'one', 'correcting', 'predecessor', '.', 'However', 'instead', 'tweaking', 'instance', 'weights', 'every', 'iteration', 'like', 'AdaBoost', 'method', 'tries', 'fit', 'new', 'predictor', 'residual', 'errors', 'made', 'previous', 'predictor', '.', 'Let‡s', 'go', 'simple', 'regression', 'example', 'using', 'Decision', 'Trees', 'base', 'predic…', 'tors', 'course', 'Gradient', 'Boosting', 'also', 'works', 'great', 'regression', 'tasks', '.', 'This', 'called', 'Gradient', 'Tree', 'Boosting', 'Gradient', 'Boosted', 'Regression', 'Trees', 'GBRT', '.', 'First', 'let‡s', 'fit', 'DecisionTreeRegressor', 'training', 'set', 'example', 'noisy', 'quadratic', 'train…', 'ing', 'set', 'Boosting', '|', '195', 'sklearn.tree', 'import', 'DecisionTreeRegressortree_reg1', '=', 'DecisionTreeRegressor', 'max_depth=2', 'tree_reg1.fit', 'X', 'Now', 'train', 'second', 'DecisionTreeRegressor', 'residual', 'errors', 'made', 'firstpredictor', 'y2', '=', '-', 'tree_reg1.predict', 'X', 'tree_reg2', '=', 'DecisionTreeRegressor', 'max_depth=2', 'tree_reg2.fit', 'X', 'y2', 'Then', 'train', 'third', 'regressor', 'residual', 'errors', 'made', 'second', 'predictor', 'y3', '=', 'y2', '-', 'tree_reg2.predict', 'X', 'tree_reg3', '=', 'DecisionTreeRegressor', 'max_depth=2', 'tree_reg3.fit', 'X', 'y3', 'Now', 'ensemble', 'containing', 'three', 'trees', '.', 'It', 'make', 'predictions', 'new', 'instance', 'simply', 'adding', 'predictions', 'trees', 'y_pred', '=', 'sum', 'tree.predict', 'X_new', 'tree', 'tree_reg1', 'tree_reg2', 'tree_reg3', 'Figure', '7-9', 'represents', 'predictions', 'three', 'trees', 'left', 'column', 'ensemble‡s', 'predictions', 'right', 'column', '.', 'In', 'first', 'row', 'ensemble', 'one', 'tree', 'predictions', 'exactly', 'first', 'tree‡s', 'predictions', '.', 'In', 'second', 'row', 'new', 'tree', 'trained', 'residual', 'errors', 'first', 'tree', '.', 'On', 'right', 'see', 'ensemble‡s', 'predictions', 'equal', 'sum', 'predictions', 'first', 'two', 'trees', '.', 'Similarly', 'third', 'row', 'another', 'tree', 'trained', 'residual', 'errors', 'second', 'tree', '.', 'You', 'see', 'ensemble‡s', 'predictions', 'gradually', 'get', 'better', 'trees', 'added', 'ensemble.A', 'simpler', 'way', 'train', 'GBRT', 'ensembles', 'use', 'Scikit-Learn‡s', 'GradientBoostingRegressor', 'class', '.', 'Much', 'like', 'RandomForestRegressor', 'class', 'hyperparameters', 'control', 'growth', 'Decision', 'Trees', 'e.g.', 'max_depth', 'min_samples_leaf', 'well', 'hyperparameters', 'control', 'ensemble', 'training', 'number', 'trees', 'n_estimators', '.', 'The', 'following', 'code', 'creates', 'ensemble', 'previous', 'one', 'sklearn.ensemble', 'import', 'GradientBoostingRegressorgbrt', '=', 'GradientBoostingRegressor', 'max_depth=2', 'n_estimators=3', 'learning_rate=1.0', 'gbrt.fit', 'X', '196', '|', 'Chapter', '7', 'Ensemble', 'Learning', 'Random', 'Forests', 'Figure', '7-9', '.', 'Gradient', 'Boosting', 'The', 'learning_rate', 'hyperparameter', 'scales', 'contribution', 'tree', '.', 'If', 'set', 'low', 'value', '0.1', 'need', 'trees', 'ensemble', 'fit', 'train…ing', 'set', 'predictions', 'usually', 'generalize', 'better', '.', 'This', 'regularization', 'tech…', 'nique', 'called', 'shrinkage', '.', 'Figure', '7-10', 'shows', 'two', 'GBRT', 'ensembles', 'trained', 'low', 'learning', 'rate', 'one', 'left', 'enough', 'trees', 'fit', 'training', 'set', 'one', 'right', 'many', 'trees', 'overfits', 'training', 'set', '.', 'Boosting', '|', '197', 'Figure', '7-10', '.', 'GBRT', 'ensembles', 'enough', 'predictors', 'le', '“', 'many', 'right', 'In', 'order', 'find', 'optimal', 'number', 'trees', 'use', 'early', 'stopping', 'see', 'Chap…', 'ter', '4', '.', 'A', 'simple', 'way', 'implement', 'use', 'staged_predict', 'method', 'itreturns', 'iterator', 'predictions', 'made', 'ensemble', 'stage', 'train…', 'ing', 'one', 'tree', 'two', 'trees', 'etc.', '.', 'The', 'following', 'code', 'trains', 'GBRT', 'ensemble', '120', 'trees', 'measures', 'validation', 'error', 'stage', 'training', 'find', 'opti…', 'mal', 'number', 'trees', 'finally', 'trains', 'another', 'GBRT', 'ensemble', 'using', 'optimal', 'number', 'trees', 'import', 'numpy', 'npfrom', 'sklearn.model_selection', 'import', 'train_test_splitfrom', 'sklearn.metrics', 'import', 'mean_squared_errorX_train', 'X_val', 'y_train', 'y_val', '=', 'train_test_split', 'X', 'gbrt', '=', 'GradientBoostingRegressor', 'max_depth=2', 'n_estimators=120', 'gbrt.fit', 'X_train', 'y_train', 'errors', '=', 'mean_squared_error', 'y_val', 'y_pred', 'y_pred', 'gbrt.staged_predict', 'X_val', 'bst_n_estimators', '=', 'np.argmin', 'errors', 'gbrt_best', '=', 'GradientBoostingRegressor', 'max_depth=2', 'n_estimators=bst_n_estimators', 'gbrt_best.fit', 'X_train', 'y_train', 'The', 'validation', 'errors', 'represented', 'left', 'Figure', '7-11', 'best', 'model‡s', 'predictions', 'represented', 'right', '.', '198', '|', 'Chapter', '7', 'Ensemble', 'Learning', 'Random', 'Forests', 'Figure', '7-11', '.', 'Tuning', 'number', 'trees', 'using', 'early', 'stopping', 'It', 'also', 'possible', 'implement', 'early', 'stopping', 'actually', 'stopping', 'training', 'early', 'instead', 'training', 'large', 'number', 'trees', 'first', 'looking', 'back', 'find', 'optimal', 'number', '.', 'You', 'setting', 'warm_start=True', 'makes', 'Scikit-Learn', 'keep', 'existing', 'trees', 'fit', 'method', 'called', 'allowing', 'incremental', 'training', '.', 'The', 'following', 'code', 'stops', 'training', 'validation', 'error', 'improve', 'five', 'iterations', 'row', 'gbrt', '=', 'GradientBoostingRegressor', 'max_depth=2', 'warm_start=True', 'min_val_error', '=', 'float', '``', 'inf', \"''\", 'error_going_up', '=', '0for', 'n_estimators', 'range', '1', '120', 'gbrt.n_estimators', '=', 'n_estimators', 'gbrt.fit', 'X_train', 'y_train', 'y_pred', '=', 'gbrt.predict', 'X_val', 'val_error', '=', 'mean_squared_error', 'y_val', 'y_pred', 'val_error', '<', 'min_val_error', 'min_val_error', '=', 'val_error', 'error_going_up', '=', '0', 'else', 'error_going_up', '+=', '1', 'error_going_up', '==', '5', 'break', '#', 'early', 'stoppingThe', 'GradientBoostingRegressor', 'class', 'also', 'supports', 'subsample', 'hyperparameter', 'specifies', 'fraction', 'training', 'instances', 'used', 'training', 'tree', '.', 'For', 'example', 'subsample=0.25', 'tree', 'trained', '25', '%', 'training', 'instan…ces', 'selected', 'randomly', '.', 'As', 'probably', 'guess', 'trades', 'higher', 'bias', 'lower', 'variance', '.', 'It', 'also', 'speeds', 'training', 'considerably', '.', 'This', 'technique', 'called', 'Stochastic', 'Gradient', 'Boosting', '.Boosting', '|', '199', '18ƒStacked', 'Generalization', '⁄', 'D.', 'Wolpert', '1992', '.', '19Alternatively', 'possible', 'use', 'out-of-fold', 'predictions', '.', 'In', 'contexts', 'called', 'stacking', 'using', 'ahold-out', 'set', 'called', 'blending', '.', 'However', 'many', 'people', 'terms', 'synonymous', '.', 'It', 'possible', 'use', 'Gradient', 'Boosting', 'cost', 'functions', '.', 'This', 'controlled', 'loss', 'hyperparameter', 'see', 'Scikit-Learn‡s', 'documentation', 'details', '.', 'StackingThe', 'last', 'Ensemble', 'method', 'discuss', 'chapter', 'called', 'stacking', 'short', 'forstacked', 'generalization', '.18', 'It', 'based', 'simple', 'idea', 'instead', 'using', 'trivial', 'functions', 'hard', 'voting', 'aggregate', 'predictions', 'predictors', 'ensemble', 'don‡t', 'train', 'model', 'perform', 'aggregation', '?', 'Figure', '7-12', 'shows', 'anensemble', 'performing', 'regression', 'task', 'new', 'instance', '.', 'Each', 'bottom', 'threepredictors', 'predicts', 'different', 'value', '3.1', '2.7', '2.9', 'final', 'predictor', 'called', 'blender', 'meta', 'learner', 'takes', 'predictions', 'inputs', 'makes', 'final', 'prediction', '3.0', '.Figure', '7-12', '.', 'Aggregating', 'predictions', 'using', 'blending', 'predictor', 'To', 'train', 'blender', 'common', 'approach', 'use', 'hold-out', 'set.19', 'Let‡s', 'see', 'works', '.', 'First', 'training', 'set', 'split', 'two', 'subsets', '.', 'The', 'first', 'subset', 'used', 'train', 'thepredictors', 'first', 'layer', 'see', 'Figure', '7-13', '.200', '|', 'Chapter', '7', 'Ensemble', 'Learning', 'Random', 'Forests', 'Figure', '7-13', '.', 'Training', '†rst', 'layer', 'Next', 'first', 'layer', 'predictors', 'used', 'make', 'predictions', 'second', 'held-out', 'set', 'see', 'Figure', '7-14', '.', 'This', 'ensures', 'predictions', 'ƒclean', '⁄', 'since', 'predictors', 'never', 'saw', 'instances', 'training', '.', 'Now', 'instance', 'hold-out', 'set', 'three', 'predicted', 'values', '.', 'We', 'create', 'new', 'training', 'set', 'using', 'predic…', 'ted', 'values', 'input', 'features', 'makes', 'new', 'training', 'set', 'three-dimensional', 'keeping', 'target', 'values', '.', 'The', 'blender', 'trained', 'new', 'training', 'set', 'itlearns', 'predict', 'target', 'value', 'given', 'first', 'layer‡s', 'predictions', '.', 'Figure', '7-14', '.', 'Training', 'blender', 'Stacking', '|', '201', 'It', 'actually', 'possible', 'train', 'several', 'different', 'blenders', 'way', 'e.g.', 'one', 'using', 'Lin…', 'ear', 'Regression', 'another', 'using', 'Random', 'Forest', 'Regression', 'get', 'whole', 'layer', 'blenders', '.', 'The', 'trick', 'split', 'training', 'set', 'three', 'subsets', 'first', 'one', 'used', 'train', 'first', 'layer', 'second', 'one', 'used', 'create', 'training', 'set', 'used', 'train', 'second', 'layer', 'using', 'predictions', 'made', 'predictors', 'first', 'layer', 'third', 'one', 'used', 'create', 'training', 'set', 'train', 'third', 'layer', 'using', 'pre…', 'dictions', 'made', 'predictors', 'second', 'layer', '.', 'Once', 'done', 'make', 'prediction', 'new', 'instance', 'going', 'layer', 'sequentially', 'shown', 'Figure', '7-15.Figure', '7-15', '.', 'Predictions', 'multilayer', 'stacking', 'ensemble', 'Unfortunately', 'Scikit-Learn', 'support', 'stacking', 'directly', 'hard', 'roll', 'implementation', 'see', 'following', 'exercises', '.', 'Alternatively', 'use', 'open', 'source', 'implementation', 'brew', 'available', 'https', '//github.com/', 'viisar/brew', '.Exercises1.If', 'trained', 'five', 'different', 'models', 'exact', 'training', 'data', 'achieve', '95', '%', 'precision', 'chance', 'combine', 'models', 'get', 'better', 'results', '?', 'If', '?', 'If', '?', '2.What', 'difference', 'hard', 'soft', 'voting', 'classifiers', '?', '202', '|', 'Chapter', '7', 'Ensemble', 'Learning', 'Random', 'Forests', '3.Is', 'possible', 'speed', 'training', 'bagging', 'ensemble', 'distributing', 'across', 'multiple', 'servers', '?', 'What', 'pasting', 'ensembles', 'boosting', 'ensembles', 'random', 'forests', 'stacking', 'ensembles', '?', '4.What', 'benefit', 'out-of-bag', 'evaluation', '?', '5.What', 'makes', 'Extra-Trees', 'random', 'regular', 'Random', 'Forests', '?', 'How', 'extra', 'randomness', 'help', '?', 'Are', 'Extra-Trees', 'slower', 'faster', 'regular', 'Ran…', 'dom', 'Forests', '?', '6.If', 'AdaBoost', 'ensemble', 'underfits', 'training', 'data', 'hyperparameters', 'tweak', '?', '7.If', 'Gradient', 'Boosting', 'ensemble', 'overfits', 'training', 'set', 'increase', 'decrease', 'learning', 'rate', '?', '8.Load', 'MNIST', 'data', 'introduced', 'Chapter', '3', 'split', 'training', 'set', 'validation', 'set', 'test', 'set', 'e.g.', 'use', 'first', '40,000', 'instances', 'training', 'next', '10,000', 'validation', 'last', '10,000', 'testing', '.', 'Then', 'train', 'various', 'classifiers', 'Random', 'Forest', 'classifier', 'Extra-Trees', 'classifier', 'SVM', '.', 'Next', 'try', 'combine', 'ensemble', 'outperforms', 'validation', 'set', 'using', 'soft', 'hard', 'voting', 'classifier', '.', 'Once', 'found', 'one', 'try', 'test', 'set', '.', 'How', 'much', 'better', 'perform', 'compared', 'individ…', 'ual', 'classifiers', '?', '9.Run', 'individual', 'classifiers', 'previous', 'exercise', 'make', 'predictions', 'validation', 'set', 'create', 'new', 'training', 'set', 'resulting', 'predictions', 'training', 'instance', 'vector', 'containing', 'set', 'predictions', 'classifiers', 'image', 'target', 'image‡s', 'class', '.', 'Congratulations', 'trained', 'blender', 'together', 'classifiers', 'form', 'stacking', 'ensemble', '!', 'Now', 'let‡s', 'evaluate', 'ensemble', 'test', 'set', '.', 'For', 'image', 'test', 'set', 'make', 'predictions', 'classifiers', 'feed', 'predictions', 'theblender', 'get', 'ensemble‡s', 'predictions', '.', 'How', 'compare', 'voting', 'clas…', 'sifier', 'trained', 'earlier', '?', 'Solutions', 'exercises', 'available', 'Appendix', 'A', '.Exercises', '|', '203', 'CHAPTER', '8Dimensionality', 'ReductionMany', 'Machine', 'Learning', 'problems', 'involve', 'thousands', 'even', 'millions', 'features', 'training', 'instance', '.', 'Not', 'make', 'training', 'extremely', 'slow', 'also', 'make', 'much', 'harder', 'find', 'good', 'solution', 'see', '.', 'This', 'problem', 'often', 'referred', 'curse', 'dimensionality', '.Fortunately', 'real-world', 'problems', 'often', 'possible', 'reduce', 'number', 'fea…', 'tures', 'considerably', 'turning', 'intractable', 'problem', 'tractable', 'one', '.', 'For', 'example', 'consider', 'MNIST', 'images', 'introduced', 'Chapter', '3', 'pixels', 'image', 'bor…ders', 'almost', 'always', 'white', 'could', 'completely', 'drop', 'pixels', 'training', 'set', 'without', 'losing', 'much', 'information', '.', 'Figure', '7-6', 'confirms', 'pixels', 'utterly', 'unimportant', 'classification', 'task', '.', 'Moreover', 'two', 'neighboring', 'pixels', 'often', 'highly', 'correlated', 'merge', 'single', 'pixel', 'e.g.', 'taking', 'mean', 'two', 'pixel', 'intensities', 'lose', 'much', 'information', '.', 'Reducing', 'dimensionality', 'lose', 'information', 'like', 'compressing', 'image', 'JPEG', 'degrade', 'quality', 'even', 'though', 'speed', 'training', 'may', 'also', 'make', 'system', 'per…', 'form', 'slightly', 'worse', '.', 'It', 'also', 'makes', 'pipelines', 'bit', 'com…', 'plex', 'thus', 'harder', 'maintain', '.', 'So', 'first', 'try', 'train', 'system', 'original', 'data', 'considering', 'using', 'dimen…', 'sionality', 'reduction', 'training', 'slow', '.', 'In', 'cases', 'however', 'reducing', 'dimensionality', 'training', 'data', 'may', 'filter', 'noise', 'unnecessary', 'details', 'thus', 'result', 'higher', 'per…', 'formance', 'general', 'won‡t', 'speed', 'training', '.', 'Apart', 'speeding', 'training', 'dimensionality', 'reduction', 'also', 'extremely', 'useful', 'data', 'visualization', 'DataViz', '.', 'Reducing', 'number', 'dimensions', 'two', '2051Well', 'four', 'dimensions', 'count', 'time', 'string', 'theorist', '.', '2Watch', 'rotating', 'tesseract', 'projected', '3D', 'space', 'http', '//goo.gl/OM7ktJ', '.', 'Image', 'Wikipedia', 'user', 'Nerd…', 'Boy1392', 'Creative', 'Commons', 'BY-SA', '3.0', '.', 'Reproduced', 'https', '//en.wikipedia.org/wiki/Tesseract', '.3Fun', 'fact', 'anyone', 'know', 'probably', 'extremist', 'least', 'one', 'dimension', 'e.g.', 'much', 'sugar', 'put', 'coffee', 'consider', 'enough', 'dimensions', '.', 'three', 'makes', 'possible', 'plot', 'high-dimensional', 'training', 'set', 'graph', 'often', 'gain', 'important', 'insights', 'visually', 'detecting', 'patterns', 'clusters', '.', 'In', 'chapter', 'discuss', 'curse', 'dimensionality', 'get', 'sense', 'goes', 'high-dimensional', 'space', '.', 'Then', 'present', 'two', 'main', 'approaches', 'dimensionality', 'reduction', 'projection', 'Manifold', 'Learning', 'go', 'three', 'popular', 'dimensionality', 'reduction', 'techniques', 'PCA', 'Kernel', 'PCA', 'LLE.The', 'Curse', 'DimensionalityWe', 'used', 'living', 'three', 'dimensions', '1', 'intuition', 'fails', 'us', 'try', 'imagine', 'high-dimensional', 'space', '.', 'Even', 'basic', '4D', 'hypercube', 'incredibly', 'hard', 'picture', 'mind', 'see', 'Figure', '8-1', 'let', 'alone', '200-dimensional', 'ellipsoid', 'bent', '1,000-dimensional', 'space.Figure', '8-1', '.', 'Point', 'segment', 'square', 'cube', 'tesseract', '0D', '4D', 'hypercubes', '2It', 'turns', 'many', 'things', 'behave', 'differently', 'high-dimensional', 'space', '.', 'For', 'example', 'pick', 'random', 'point', 'unit', 'square', '1', '‰', '1', 'square', '0.4', '%', 'chance', 'located', 'less', '0.001', 'border', 'words', 'unlikely', 'random', 'point', 'ƒextreme⁄', 'along', 'dimension', '.', 'But', '10,000-dimensional', 'unit', 'hypercube', '1', '‰', '1', '‰', '‰', '1', 'cube', 'ten', 'thousand', '1s', 'thisprobability', 'greater', '99.999999', '%', '.', 'Most', 'points', 'high-dimensional', 'hypercube', 'close', 'border', '.', '3206', '|', 'Chapter', '8', 'Dimensionality', 'Reduction', 'Here', 'troublesome', 'difference', 'pick', 'two', 'points', 'randomly', 'unit', 'square', 'distance', 'two', 'points', 'average', 'roughly', '0.52', '.', 'If', 'pick', 'two', 'random', 'points', 'unit', '3D', 'cube', 'average', 'distance', 'roughly', '0.66', '.', 'But', 'two', 'points', 'picked', 'randomly', '1,000,000-dimensional', 'hypercube', '?', 'Well', 'average', 'distance', 'believe', '408.25', 'roughly', '1,000,000/6', '!', 'This', 'quite', 'counterintuitive', 'two', 'points', 'far', 'apart', 'lie', 'within', 'unit', 'hypercube', '?', 'This', 'fact', 'implies', 'high-', 'dimensional', 'datasets', 'risk', 'sparse', 'training', 'instances', 'likely', 'far', 'away', '.', 'Of', 'course', 'also', 'means', 'new', 'instance', 'likely', 'far', 'away', 'training', 'instance', 'making', 'predictions', 'much', 'less', 'relia…', 'ble', 'lower', 'dimensions', 'since', 'based', 'much', 'larger', 'extrapolations', '.', 'In', 'short', 'dimensions', 'training', 'set', 'greater', 'risk', 'overfitting', 'it.In', 'theory', 'one', 'solution', 'curse', 'dimensionality', 'could', 'increase', 'size', 'training', 'set', 'reach', 'sufficient', 'density', 'training', 'instances', '.', 'Unfortunately', 'practice', 'number', 'training', 'instances', 'required', 'reach', 'given', 'density', 'grows', 'exponentially', 'number', 'dimensions', '.', 'With', '100', 'features', 'much', 'less', 'MNIST', 'problem', 'would', 'need', 'training', 'instances', 'atoms', 'observable', 'universe', 'order', 'training', 'instances', 'within', '0.1', 'average', 'assuming', 'spread', 'uniformly', 'across', 'dimensions', '.', 'Main', 'Approaches', 'Dimensionality', 'ReductionBefore', 'dive', 'specific', 'dimensionality', 'reduction', 'algorithms', 'let‡s', 'take', 'look', 'two', 'main', 'approaches', 'reducing', 'dimensionality', 'projection', 'Manifold', 'Learning.ProjectionIn', 'real-world', 'problems', 'training', 'instances', 'spread', 'uniformly', 'acrossall', 'dimensions', '.', 'Many', 'features', 'almost', 'constant', 'others', 'highly', 'correlated', 'discussed', 'earlier', 'MNIST', '.', 'As', 'result', 'training', 'instances', 'actually', 'lie', 'within', 'close', 'much', 'lower-dimensional', 'subspace', 'high-dimensional', 'space', '.', 'Thissounds', 'abstract', 'let‡s', 'look', 'example', '.', 'In', 'Figure', '8-2', 'see', '3D', 'data…', 'set', 'represented', 'circles', '.', 'Main', 'Approaches', 'Dimensionality', 'Reduction', '|', '207', 'Figure', '8-2', '.', 'A', '3D', 'dataset', 'lying', 'close', '2D', 'subspace', 'Notice', 'training', 'instances', 'lie', 'close', 'plane', 'lower-dimensional', '2D', 'subspace', 'high-dimensional', '3D', 'space', '.', 'Now', 'project', 'every', 'training', 'instance', 'perpendicularly', 'onto', 'subspace', 'represented', 'short', 'lines', 'con…', 'necting', 'instances', 'plane', 'get', 'new', '2D', 'dataset', 'shown', 'Figure', '8-3.Ta-da', '!', 'We', 'reduced', 'dataset‡s', 'dimensionality', '3D', '2D', '.', 'Note', 'axes', 'correspond', 'new', 'features', 'z1', 'z2', 'coordinates', 'projections', 'plane', '.Figure', '8-3', '.', '•e', 'new', '2D', 'dataset', '“', 'er', 'projection', '208', '|', 'Chapter', '8', 'Dimensionality', 'Reduction', 'However', 'projection', 'always', 'best', 'approach', 'dimensionality', 'reduction', '.', 'In', 'many', 'cases', 'subspace', 'may', 'twist', 'turn', 'famous', 'Swiss', 'roll', 'toy', 'data…', 'set', 'represented', 'Figure', '8-4.Figure', '8-4', '.', 'Swiss', 'roll', 'dataset', 'Simply', 'projecting', 'onto', 'plane', 'e.g.', 'dropping', 'x3', 'would', 'squash', 'different', 'layers', 'Swiss', 'roll', 'together', 'shown', 'left', 'Figure', '8-5', '.', 'However', 'really', 'want', 'unroll', 'Swiss', 'roll', 'obtain', '2D', 'dataset', 'right', 'Figure', '8-5.Figure', '8-5', '.', 'Squashing', 'projecting', 'onto', 'plane', 'le', '“', 'versus', 'unrolling', 'Swiss', 'roll', 'right', 'Main', 'Approaches', 'Dimensionality', 'Reduction', '|', '209', 'Manifold', 'LearningThe', 'Swiss', 'roll', 'example', '2D', 'manifold', '.', 'Put', 'simply', '2D', 'manifold', '2D', 'shape', 'bent', 'twisted', 'higher-dimensional', 'space', '.', 'More', 'generally', 'd-dimensional', 'manifold', 'part', 'n-dimensional', 'space', '<', 'n', 'locally', 'resembles', 'd-dimensional', 'hyperplane', '.', 'In', 'case', 'Swiss', 'roll', '=', '2', 'n', '=', '3', 'locally', 'resembles', '2D', 'plane', 'rolled', 'third', 'dimension.Many', 'dimensionality', 'reduction', 'algorithms', 'work', 'modeling', 'manifold', 'whichthe', 'training', 'instances', 'lie', 'called', 'Manifold', 'Learning', '.', 'It', 'relies', 'manifold', 'assumption', 'also', 'called', 'manifold', 'hypothesis', 'holds', 'real-world', 'high-dimensional', 'datasets', 'lie', 'close', 'much', 'lower-dimensional', 'manifold', '.', 'This', 'assumption', 'often', 'empirically', 'observed', '.', 'Once', 'think', 'MNIST', 'dataset', 'handwritten', 'digit', 'images', 'similarities', '.', 'They', 'made', 'connected', 'lines', 'borders', 'white', 'moreor', 'less', 'centered', '.', 'If', 'randomly', 'generated', 'images', 'ridiculously', 'tiny', 'fraction', 'would', 'look', 'like', 'handwritten', 'digits', '.', 'In', 'words', 'degrees', 'freedom', 'available', 'try', 'create', 'digit', 'image', 'dramatically', 'lower', 'degrees', 'freedom', 'would', 'allowed', 'generate', 'image', 'wanted', '.', 'These', 'constraints', 'tend', 'squeeze', 'dataset', 'lower-', 'dimensional', 'manifold.The', 'manifold', 'assumption', 'often', 'accompanied', 'another', 'implicit', 'assumption', 'task', 'hand', 'e.g.', 'classification', 'regression', 'simpler', 'expressed', 'lower-dimensional', 'space', 'manifold', '.', 'For', 'example', 'top', 'row', 'Figure', '8-6the', 'Swiss', 'roll', 'split', 'two', 'classes', '3D', 'space', 'left', 'decision', 'boundary', 'would', 'fairly', 'complex', '2D', 'unrolled', 'manifold', 'space', 'right', 'decision', 'boundary', 'simple', 'straight', 'line', '.', 'However', 'assumption', 'always', 'hold', '.', 'For', 'example', 'bottom', 'row', 'Figure', '8-6', 'decision', 'boundary', 'located', 'x1', '=', '5', '.', 'This', 'decision', 'boundary', 'looks', 'simple', 'original', '3D', 'space', 'vertical', 'plane', 'looks', 'complex', 'unrolled', 'manifold', 'collection', 'four', 'independent', 'line', 'segments', '.', 'In', 'short', 'reduce', 'dimensionality', 'training', 'set', 'training', 'amodel', 'definitely', 'speed', 'training', 'may', 'always', 'lead', 'better', 'simpler', 'solution', 'depends', 'dataset', '.', 'Hopefully', 'good', 'sense', 'curse', 'dimensionality', 'dimensionality', 'reduction', 'algorithms', 'fight', 'especially', 'manifold', 'assumption', 'holds', '.', 'The', 'rest', 'chapter', 'go', 'popular', 'algorithms.210', '|', 'Chapter', '8', 'Dimensionality', 'Reduction', 'Figure', '8-6', '.', '•e', 'decision', 'boundary', 'may', 'always', 'simpler', 'lower', 'dimensions', 'PCAPrincipal', 'Component', 'Analysis', 'PCA', 'far', 'popular', 'dimensionality', 'reduc…', 'tion', 'algorithm', '.', 'First', 'identifies', 'hyperplane', 'lies', 'closest', 'data', 'projects', 'data', 'onto', '.', 'Preserving', 'VarianceBefore', 'project', 'training', 'set', 'onto', 'lower-dimensional', 'hyperplane', 'first', 'need', 'choose', 'right', 'hyperplane', '.', 'For', 'example', 'simple', '2D', 'dataset', 'repre…', 'sented', 'left', 'Figure', '8-7', 'along', 'three', 'different', 'axes', 'i.e.', 'one-dimensional', 'hyperplanes', '.', 'On', 'right', 'result', 'projection', 'dataset', 'onto', 'axes', '.', 'As', 'see', 'projection', 'onto', 'solid', 'line', 'preserves', 'maximum', 'variance', 'projection', 'onto', 'dotted', 'line', 'preserves', 'little', 'variance', 'projection', 'onto', 'dashed', 'line', 'preserves', 'intermediate', 'amount', 'variance', '.', 'PCA', '|', '211', '4ƒOn', 'Lines', 'Planes', 'Closest', 'Fit', 'Systems', 'Points', 'Space', '⁄', 'K.', 'Pearson', '1901', '.', 'Figure', '8-7', '.', 'Selecting', 'subspace', 'onto', 'project', 'It', 'seems', 'reasonable', 'select', 'axis', 'preserves', 'maximum', 'amount', 'var…', 'iance', 'likely', 'lose', 'less', 'information', 'projections', '.', 'Another', 'way', 'justify', 'choice', 'axis', 'minimizes', 'mean', 'squared', 'dis…', 'tance', 'original', 'dataset', 'projection', 'onto', 'axis', '.', 'This', 'rather', 'simple', 'idea', 'behind', 'PCA.4Principal', 'ComponentsPCA', 'identifies', 'axis', 'accounts', 'largest', 'amount', 'variance', 'train…', 'ing', 'set', '.', 'In', 'Figure', '8-7', 'solid', 'line', '.', 'It', 'also', 'finds', 'second', 'axis', 'orthogonal', 'first', 'one', 'accounts', 'largest', 'amount', 'remaining', 'variance', '.', 'In', '2D', 'example', 'choice', 'dotted', 'line', '.', 'If', 'higher-dimensional', 'data…', 'set', 'PCA', 'would', 'also', 'find', 'third', 'axis', 'orthogonal', 'previous', 'axes', 'fourth', 'fifth', 'on›as', 'many', 'axes', 'number', 'dimensions', 'dataset', '.', 'The', 'unit', 'vector', 'defines', 'th', 'axis', 'called', 'ith', 'principal', 'component', 'PC', '.', 'InFigure', '8-7', '1st', 'PC', 'c1', '2nd', 'PC', 'c2', '.', 'In', 'Figure', '8-2', 'first', 'two', 'PCs', 'arerepresented', 'orthogonal', 'arrows', 'plane', 'third', 'PC', 'would', 'orthogonal', 'plane', 'pointing', '.', '212', '|', 'Chapter', '8', 'Dimensionality', 'Reduction', 'The', 'direction', 'principal', 'components', 'stable', 'per…', 'turb', 'training', 'set', 'slightly', 'run', 'PCA', 'new', 'PCs', 'may', 'point', 'opposite', 'direction', 'original', 'PCs', '.', 'How…', 'ever', 'generally', 'still', 'lie', 'axes', '.', 'In', 'cases', 'pair', 'PCs', 'may', 'even', 'rotate', 'swap', 'plane', 'define', 'generally', 'remain', 'same.So', 'find', 'principal', 'components', 'training', 'set', '?', 'Luckily', 'standard', 'matrix', 'factorization', 'technique', 'called', 'Singular', 'Value', 'Decomposition', 'SVD', 'decompose', 'training', 'set', 'matrix', 'X', 'dot', 'product', 'three', 'matrices', 'U', '’', 'œ', '’', 'VT', 'VT', 'contains', 'principal', 'components', 'looking', 'shown', 'Equation', '8-1', '.Equation', '8-1', '.', 'Principal', 'components', 'matrix', 'T=', '12', 'The', 'following', 'Python', 'code', 'uses', 'NumPy‡s', 'svd', 'function', 'obtain', 'principalcomponents', 'training', 'set', 'extracts', 'first', 'two', 'PCs', 'X_centered', '=', 'X', '-', 'X.mean', 'axis=0', 'U', 'V', '=', 'np.linalg.svd', 'X_centered', 'c1', '=', 'V.T', '0', 'c2', '=', 'V.T', '1', 'PCA', 'assumes', 'dataset', 'centered', 'around', 'origin', '.', 'As', 'see', 'Scikit-Learn‡s', 'PCA', 'classes', 'take', 'care', 'centering', 'data', '.', 'However', 'implement', 'PCA', 'pre…', 'ceding', 'example', 'use', 'libraries', 'don‡t', 'forget', 'center', 'data', 'first', '.', 'Projecting', 'Down', 'DimensionsOnce', 'identified', 'principal', 'components', 'reduce', 'dimen…', 'sionality', 'dataset', 'dimensions', 'projecting', 'onto', 'hyperplane', 'defined', 'first', 'principal', 'components', '.', 'Selecting', 'hyperplane', 'ensures', 'projection', 'preserve', 'much', 'variance', 'possible', '.', 'For', 'example', 'Figure', '8-2', 'the3D', 'dataset', 'projected', '2D', 'plane', 'defined', 'first', 'two', 'principal', 'com…', 'ponents', 'preserving', 'large', 'part', 'dataset‡s', 'variance', '.', 'As', 'result', '2D', 'projec…', 'tion', 'looks', 'much', 'like', 'original', '3D', 'dataset', '.', 'To', 'project', 'training', 'set', 'onto', 'hyperplane', 'simply', 'compute', 'dot', 'product', 'training', 'set', 'matrix', 'X', 'matrix', 'Wd', 'defined', 'matrix', 'contain…', 'PCA', '|', '213', 'ing', 'first', 'principal', 'components', 'i.e.', 'matrix', 'composed', 'first', 'columnsof', 'VT', 'shown', 'Equation', '8-2', '.Equation', '8-2', '.', 'Projecting', 'training', 'set', 'dimensions', 'd…proj=', '’', 'dThe', 'following', 'Python', 'code', 'projects', 'training', 'set', 'onto', 'plane', 'defined', 'first', 'two', 'principal', 'components', 'W2', '=', 'V.T', ':2', 'X2D', '=', 'X_centered.dot', 'W2', 'There', '!', 'You', 'know', 'reduce', 'dimensionality', 'dataset', 'number', 'dimensions', 'preserving', 'much', 'variance', 'possible', '.', 'Using', 'Scikit-LearnScikit-Learn‡s', 'PCA', 'class', 'implements', 'PCA', 'using', 'SVD', 'decomposition', 'like', '.', 'The', 'following', 'code', 'applies', 'PCA', 'reduce', 'dimensionality', 'dataset', 'two', 'dimensions', 'note', 'automatically', 'takes', 'care', 'centering', 'data', 'sklearn.decomposition', 'import', 'PCApca', '=', 'PCA', 'n_components', '=', '2', 'X2D', '=', 'pca.fit_transform', 'X', 'After', 'fitting', 'PCA', 'transformer', 'dataset', 'access', 'principal', 'compo…', 'nents', 'using', 'components_', 'variable', 'note', 'contains', 'PCs', 'horizontal', 'vec…', 'tors', 'example', 'first', 'principal', 'component', 'equal', 'pca.components_.T', ',0', '.Explained', 'Variance', 'RatioAnother', 'useful', 'piece', 'information', 'explained', 'variance', 'ratio', 'prin…cipal', 'component', 'available', 'via', 'explained_variance_ratio_', 'variable', '.', 'It', 'indicates', 'proportion', 'dataset‡s', 'variance', 'lies', 'along', 'axis', 'principal', 'com…', 'ponent', '.', 'For', 'example', 'let‡s', 'look', 'explained', 'variance', 'ratios', 'first', 'two', 'compo…', 'nents', '3D', 'dataset', 'represented', 'Figure', '8-2', '>', '>', '>', 'print', 'pca.explained_variance_ratio_', 'array', '0.84248607', '0.14631839', 'This', 'tells', '84.2', '%', 'dataset‡s', 'variance', 'lies', 'along', 'first', 'axis', '14.6', '%', 'lies', 'along', 'second', 'axis', '.', 'This', 'leaves', 'less', '1.2', '%', 'third', 'axis', 'reason…', 'able', 'assume', 'probably', 'carries', 'little', 'information', '.', '214', '|', 'Chapter', '8', 'Dimensionality', 'Reduction', 'Choosing', 'Right', 'Number', 'DimensionsInstead', 'arbitrarily', 'choosing', 'number', 'dimensions', 'reduce', 'generally', 'preferable', 'choose', 'number', 'dimensions', 'add', 'sufficiently', 'large', 'portion', 'variance', 'e.g.', '95', '%', '.', 'Unless', 'course', 'reducing', 'dimen…', 'sionality', 'data', 'visualization›in', 'case', 'generally', 'want', 'reduce', 'dimensionality', '2', '3.The', 'following', 'code', 'computes', 'PCA', 'without', 'reducing', 'dimensionality', 'computes', 'minimum', 'number', 'dimensions', 'required', 'preserve', '95', '%', 'training', 'set‡s', 'variance', 'pca', '=', 'PCA', 'pca.fit', 'X', 'cumsum', '=', 'np.cumsum', 'pca.explained_variance_ratio_', '=', 'np.argmax', 'cumsum', '>', '=', '0.95', '+', '1You', 'could', 'set', 'n_components=d', 'run', 'PCA', '.', 'However', 'much', 'better', 'option', 'instead', 'specifying', 'number', 'principal', 'components', 'want', 'preserve', 'set', 'n_components', 'float', '0.0', '1.0', 'indicating', 'ratio', 'variance', 'wish', 'preserve', 'pca', '=', 'PCA', 'n_components=0.95', 'X_reduced', '=', 'pca.fit_transform', 'X', 'Yet', 'another', 'option', 'plot', 'explained', 'variance', 'function', 'number', 'dimensions', 'simply', 'plot', 'cumsum', 'see', 'Figure', '8-8', '.', 'There', 'usually', 'elbow', 'thecurve', 'explained', 'variance', 'stops', 'growing', 'fast', '.', 'You', 'think', 'intrinsic', 'dimensionality', 'dataset', '.', 'In', 'case', 'see', 'reducing', 'dimensionality', '100', 'dimensions', 'wouldn‡t', 'lose', 'much', 'explained', 'var…', 'iance.Figure', '8-8', '.', 'Explained', 'variance', 'function', 'number', 'dimensions', 'PCA', '|', '215', 'PCA', 'CompressionObviously', 'dimensionality', 'reduction', 'training', 'set', 'takes', 'much', 'less', 'space', '.', 'For', 'example', 'try', 'applying', 'PCA', 'MNIST', 'dataset', 'preserving', '95', '%', 'var…', 'iance', '.', 'You', 'find', 'instance', '150', 'features', 'instead', 'original', '784', 'features', '.', 'So', 'variance', 'preserved', 'dataset', 'less', '20', '%', 'original', 'size', '!', 'This', 'reasonable', 'compression', 'ratio', 'see', 'speed', 'classification', 'algorithm', 'SVM', 'classifier', 'tremendously', '.', 'It', 'also', 'possible', 'decompress', 'reduced', 'dataset', 'back', '784', 'dimensions', 'applying', 'inverse', 'transformation', 'PCA', 'projection', '.', 'Of', 'course', 'won‡t', 'give', 'back', 'original', 'data', 'since', 'projection', 'lost', 'bit', 'information', 'within', '5', '%', 'variance', 'dropped', 'likely', 'quite', 'close', 'original', 'data', '.', 'The', 'mean', 'squared', 'distance', 'original', 'data', 'reconstructed', 'data', 'compressed', 'decompressed', 'called', 'reconstruction', 'error', '.', 'For', 'example', 'following', 'code', 'compresses', 'MNIST', 'dataset', '154', 'dimensions', 'uses', 'inverse_transform', 'method', 'decompress', 'back', '784', 'dimensions', '.', 'Figure', '8-9', 'shows', 'digits', 'original', 'training', 'set', 'left', 'cor…responding', 'digits', 'compression', 'decompression', '.', 'You', 'see', 'slight', 'image', 'quality', 'loss', 'digits', 'still', 'mostly', 'intact', '.', 'pca', '=', 'PCA', 'n_components', '=', '154', 'X_mnist_reduced', '=', 'pca.fit_transform', 'X_mnist', 'X_mnist_recovered', '=', 'pca.inverse_transform', 'X_mnist_reduced', 'Figure', '8-9', '.', 'MNIST', 'compression', 'preserving', '95', '%', 'variance', '216', '|', 'Chapter', '8', 'Dimensionality', 'Reduction', '5Scikit-Learn', 'uses', 'algorithm', 'described', 'ƒIncremental', 'Learning', 'Robust', 'Visual', 'Tracking', '⁄', 'D.', 'Ross', 'et', 'al', '.', '2007', '.The', 'equation', 'inverse', 'transformation', 'shown', 'Equation', '8-3', '.Equation', '8-3', '.', 'PCA', 'inverse', 'transformation', 'back', 'original', 'number', 'dimensions', 'recovered=d…proj', '’', 'dTIncremental', 'PCAOne', 'problem', 'preceding', 'implementation', 'PCA', 'requires', 'whole', 'training', 'set', 'fit', 'memory', 'order', 'SVD', 'algorithm', 'run', '.', 'Fortunately', 'Incremental', 'PCA', 'IPCA', 'algorithms', 'developed', 'split', 'training', 'set', 'mini-batches', 'feed', 'IPCA', 'algorithm', 'one', 'mini-batch', 'time', '.', 'This', 'useful', 'large', 'training', 'sets', 'also', 'apply', 'PCA', 'online', 'i.e.', 'fly', 'new', 'instances', 'arrive', '.The', 'following', 'code', 'splits', 'MNIST', 'dataset', '100', 'mini-batches', 'using', 'NumPy‡s', 'array_split', 'function', 'feeds', 'Scikit-Learn‡s', 'IncrementalPCA', 'class5', 'reduce', 'dimensionality', 'MNIST', 'dataset', '154', 'dimensions', 'like', '.', 'Note', 'must', 'call', 'partial_fit', 'method', 'mini-batch', 'rather', 'fit', 'method', 'whole', 'training', 'set', 'sklearn.decomposition', 'import', 'IncrementalPCAn_batches', '=', '100inc_pca', '=', 'IncrementalPCA', 'n_components=154', 'X_batch', 'np.array_split', 'X_mnist', 'n_batches', 'inc_pca.partial_fit', 'X_batch', 'X_mnist_reduced', '=', 'inc_pca.transform', 'X_mnist', 'Alternatively', 'use', 'NumPy‡s', 'memmap', 'class', 'allows', 'manipulate', 'large', 'array', 'stored', 'binary', 'file', 'disk', 'entirely', 'memory', 'class', 'loads', 'data', 'needs', 'memory', 'needs', '.', 'Since', 'IncrementalPCAclass', 'uses', 'small', 'part', 'array', 'given', 'time', 'memory', 'usage', 'remains', 'control', '.', 'This', 'makes', 'possible', 'call', 'usual', 'fit', 'method', 'seein', 'following', 'code', 'X_mm', '=', 'np.memmap', 'filename', 'dtype=', \"''\", 'float32', \"''\", 'mode=', \"''\", 'readonly', \"''\", 'shape=', 'n', 'batch_size', '=', '//', 'n_batchesinc_pca', '=', 'IncrementalPCA', 'n_components=154', 'batch_size=batch_size', 'inc_pca.fit', 'X_mm', 'PCA', '|', '217', '6ƒKernel', 'Principal', 'Component', 'Analysis', '⁄', 'B.', 'Schšlkopf', 'A.', 'Smola', 'K.', 'M™ller', '1999', '.', 'Randomized', 'PCAScikit-Learn', 'offers', 'yet', 'another', 'option', 'perform', 'PCA', 'called', 'Randomized', 'PCA', '.', 'Thisis', 'stochastic', 'algorithm', 'quickly', 'finds', 'approximation', 'first', 'principalcomponents', '.', 'Its', 'computational', 'complexity', 'O', '‰', 'd2', '+', 'O', 'd3', 'instead', 'O', '‰', 'n2', '+', 'O', 'n3', 'dramatically', 'faster', 'previous', 'algorithms', 'much', 'smaller', 'n.rnd_pca', '=', 'PCA', 'n_components=154', 'svd_solver=', \"''\", 'randomized', \"''\", 'X_reduced', '=', 'rnd_pca.fit_transform', 'X_mnist', 'Kernel', 'PCAIn', 'Chapter', '5', 'discussed', 'kernel', 'trick', 'mathematical', 'technique', 'implicitly', 'maps', 'instances', 'high-dimensional', 'space', 'called', 'feature', 'space', 'enablingnonlinear', 'classification', 'regression', 'Support', 'Vector', 'Machines', '.', 'Recall', 'linear', 'decision', 'boundary', 'high-dimensional', 'feature', 'space', 'corresponds', 'complex', 'nonlinear', 'decision', 'boundary', 'original', 'space', '.It', 'turns', 'trick', 'applied', 'PCA', 'making', 'possible', 'perform', 'complex', 'nonlinear', 'projections', 'dimensionality', 'reduction', '.', 'This', 'called', 'Kernel', 'PCA', 'kPCA', '.6', 'It', 'often', 'good', 'preserving', 'clusters', 'instances', 'projection', 'sometimes', 'even', 'unrolling', 'datasets', 'lie', 'close', 'twisted', 'manifold', '.', 'For', 'example', 'following', 'code', 'uses', 'Scikit-Learn‡s', 'KernelPCA', 'class', 'perform', 'kPCAwith', 'RBF', 'kernel', 'see', 'Chapter', '5', 'details', 'RBF', 'kernel', 'theother', 'kernels', 'sklearn.decomposition', 'import', 'KernelPCArbf_pca', '=', 'KernelPCA', 'n_components', '=', '2', 'kernel=', \"''\", 'rbf', \"''\", 'gamma=0.04', 'X_reduced', '=', 'rbf_pca.fit_transform', 'X', 'Figure', '8-10', 'shows', 'Swiss', 'roll', 'reduced', 'two', 'dimensions', 'using', 'linear', 'kernel', 'equivalent', 'simply', 'using', 'PCA', 'class', 'RBF', 'kernel', 'sigmoid', 'kernel', 'Logistic', '.218', '|', 'Chapter', '8', 'Dimensionality', 'Reduction', 'Figure', '8-10', '.', 'Swiss', 'roll', 'reduced', '2D', 'using', 'kPCA', 'various', 'kernels', 'Selecting', 'Kernel', 'Tuning', 'HyperparametersAs', 'kPCA', 'unsupervised', 'learning', 'algorithm', 'obvious', 'performance', 'measure', 'help', 'select', 'best', 'kernel', 'hyperparameter', 'values', '.', 'However', 'dimensionality', 'reduction', 'often', 'preparation', 'step', 'supervised', 'learning', 'task', 'e.g.', 'classification', 'simply', 'use', 'grid', 'search', 'select', 'kernel', 'hyper…', 'parameters', 'lead', 'best', 'performance', 'task', '.', 'For', 'example', 'following', 'code', 'creates', 'two-step', 'pipeline', 'first', 'reducing', 'dimensionality', 'two', 'dimensions', 'using', 'kPCA', 'applying', 'Logistic', 'Regression', 'classification', '.', 'Then', 'uses', 'GridSearchCV', 'find', 'best', 'kernel', 'gamma', 'value', 'kPCA', 'order', 'get', 'bestclassification', 'accuracy', 'end', 'pipeline', 'sklearn.model_selection', 'import', 'GridSearchCVfrom', 'sklearn.linear_model', 'import', 'LogisticRegressionfrom', 'sklearn.pipeline', 'import', 'Pipelineclf', '=', 'Pipeline', '``', 'kpca', \"''\", 'KernelPCA', 'n_components=2', '``', 'log_reg', \"''\", 'LogisticRegression', 'param_grid', '=', '{', '``', 'kpca__gamma', \"''\", 'np.linspace', '0.03', '0.05', '10', '``', 'kpca__kernel', \"''\", '``', 'rbf', \"''\", '``', 'sigmoid', \"''\", '}', 'grid_search', '=', 'GridSearchCV', 'clf', 'param_grid', 'cv=3', 'grid_search.fit', 'X', 'The', 'best', 'kernel', 'hyperparameters', 'available', 'best_params_variable', '>', '>', '>', 'print', 'grid_search.best_params_', '{', '•kpca__gamma•', '0.043333333333333335', '•kpca__kernel•', '•rbf•', '}', 'Kernel', 'PCA', '|', '219', 'Another', 'approach', 'time', 'entirely', 'unsupervised', 'select', 'kernel', 'hyper…', 'parameters', 'yield', 'lowest', 'reconstruction', 'error', '.', 'However', 'reconstruction', 'easy', 'linear', 'PCA', '.', 'Here‡s', '.', 'Figure', '8-11', 'shows', 'original', 'Swiss', 'roll', '3Ddataset', 'top', 'left', 'resulting', '2D', 'dataset', 'kPCA', 'applied', 'using', 'RBF', 'kernel', 'top', 'right', '.', 'Thanks', 'kernel', 'trick', 'mathematically', 'equivalent', 'mapping', 'training', 'set', 'infinite-dimensional', 'feature', 'space', 'bottom', 'right', 'using', 'feature', 'map', 'ž', 'projecting', 'transformed', 'training', 'set', '2Dusing', 'linear', 'PCA', '.', 'Notice', 'could', 'invert', 'linear', 'PCA', 'step', 'given', 'instance', 'reduced', 'space', 'reconstructed', 'point', 'would', 'lie', 'feature', 'space', 'original', 'space', 'e.g.', 'like', 'one', 'represented', 'x', 'diagram', '.', 'Since', 'feature', 'space', 'infinite-dimensional', 'compute', 'reconstructed', 'point', 'therefore', 'compute', 'true', 'reconstruction', 'error', '.', 'Fortunately', 'pos…', 'sible', 'find', 'point', 'original', 'space', 'would', 'map', 'close', 'reconstructed', 'point', '.', 'This', 'called', 'reconstruction', 'pre-image', '.', 'Once', 'pre-image', 'measure', 'squared', 'distance', 'original', 'instance', '.', 'You', 'select', 'ker…', 'nel', 'hyperparameters', 'minimize', 'reconstruction', 'pre-image', 'error', '.', 'Figure', '8-11', '.', 'Kernel', 'PCA', 'reconstruction', 'pre-image', 'error', '220', '|', 'Chapter', '8', 'Dimensionality', 'Reduction', '7Scikit-Learn', 'uses', 'algorithm', 'based', 'K', 'Jason', 'Weston', 'Bernhard', 'Scholkopf', 'ƒLearning', 'Find', 'Pre-images⁄', 'Tubingen', 'Germany', 'Max', 'Planck', 'Institute', 'Biological', 'Cybernetics', '2004', '.8ƒNonlinear', 'Dimensionality', 'Reduction', 'Locally', 'Linear', 'Embedding', '⁄', 'S.', 'Roweis', 'L.', 'Saul', '2000', '.', 'You', 'may', 'wondering', 'perform', 'reconstruction', '.', 'One', 'solution', 'train', 'supervised', 'regression', 'model', 'projected', 'instances', 'training', 'set', 'original', 'instances', 'targets', '.', 'Scikit-Learn', 'automatically', 'setfit_inverse_transform=True', 'shown', 'following', 'code:7rbf_pca', '=', 'KernelPCA', 'n_components', '=', '2', 'kernel=', \"''\", 'rbf', \"''\", 'gamma=0.0433', 'fit_inverse_transform=True', 'X_reduced', '=', 'rbf_pca.fit_transform', 'X', 'X_preimage', '=', 'rbf_pca.inverse_transform', 'X_reduced', 'By', 'default', 'fit_inverse_transform=False', 'KernelPCA', 'noinverse_transform', 'method', '.', 'This', 'method', 'gets', 'created', 'set', 'fit_inverse_transform=True.You', 'compute', 'reconstruction', 'pre-image', 'error', '>', '>', '>', 'sklearn.metrics', 'import', 'mean_squared_error', '>', '>', '>', 'mean_squared_error', 'X', 'X_preimage', '32.786308795766132Now', 'use', 'grid', 'search', 'cross-validation', 'find', 'kernel', 'hyperpara…', 'meters', 'minimize', 'pre-image', 'reconstruction', 'error', '.', 'LLELocally', 'Linear', 'Embedding', 'LLE', '8', 'another', 'powerful', 'nonlinear', 'dimensionality', 'reduction', 'NLDR', 'technique', '.', 'It', 'Manifold', 'Learning', 'technique', 'rely', 'projections', 'like', 'previous', 'algorithms', '.', 'In', 'nutshell', 'LLE', 'works', 'first', 'measur…', 'ing', 'training', 'instance', 'linearly', 'relates', 'closest', 'neighbors', 'c.n', '.', 'looking', 'low-dimensional', 'representation', 'training', 'set', 'local', 'relationships', 'best', 'preserved', 'details', 'shortly', '.', 'This', 'makes', 'particularly', 'good', 'unrolling', 'twisted', 'manifolds', 'especially', 'much', 'noise', '.', 'For', 'example', 'following', 'code', 'uses', 'Scikit-Learn‡s', 'LocallyLinearEmbedding', 'class', 'unroll', 'Swiss', 'roll', '.', 'The', 'resulting', '2D', 'dataset', 'shown', 'Figure', '8-12', '.', 'As', 'cansee', 'Swiss', 'roll', 'completely', 'unrolled', 'distances', 'instances', 'locally', 'well', 'preserved', '.', 'However', 'distances', 'preserved', 'larger', 'scale', 'left', 'part', 'unrolled', 'Swiss', 'roll', 'squeezed', 'right', 'part', 'stretched', '.', 'Neverthe…', 'less', 'LLE', 'pretty', 'good', 'job', 'modeling', 'manifold', '.', 'LLE', '|', '221', 'sklearn.manifold', 'import', 'LocallyLinearEmbeddinglle', '=', 'LocallyLinearEmbedding', 'n_components=2', 'n_neighbors=10', 'X_reduced', '=', 'lle.fit_transform', 'X', 'Figure', '8-12', '.', 'Unrolled', 'Swiss', 'roll', 'using', 'LLE', 'Here‡s', 'LLE', 'works', 'first', 'training', 'instance', 'x', 'algorithm', 'identifies', 'k', 'closest', 'neighbors', 'preceding', 'code', 'k', '=', '10', 'tries', 'reconstruct', 'x', 'alinear', 'function', 'neighbors', '.', 'More', 'specifically', 'finds', 'weights', 'wi', 'j', 'squared', 'distance', 'x', '“', 'j=1', 'mwi', 'jj', 'small', 'possible', 'assumingwi', 'j', '=', '0', 'x', 'j', 'one', 'k', 'closest', 'neighbors', 'x', '.', 'Thus', 'first', 'step', 'LLE', 'constrained', 'optimization', 'problem', 'described', 'Equation', '8-4', 'W', 'theweight', 'matrix', 'containing', 'weights', 'wi', 'j', '.', 'The', 'second', 'constraint', 'simply', 'normalizes', 'weights', 'training', 'instance', 'x', '.222', '|', 'Chapter', '8', 'Dimensionality', 'Reduction', 'Equation', '8-4', '.', 'LLE', 'step', '1', 'linearly', 'modeling', 'local', 'relationships', '=argmin', '“', 'i=1', '”', '“', 'j=1', 'mwi', 'jj2subjectto', 'wi', 'j=0if', 'jisnotoneofthe', 'kc.n.of', '“', 'j=1', 'mwi', 'j=1for', 'i=1,2', 'mAfter', 'step', 'weight', 'matrix', 'containing', 'weights', 'wi', 'j', 'encodes', 'locallinear', 'relationships', 'training', 'instances', '.', 'Now', 'second', 'step', 'map', 'training', 'instances', 'd-dimensional', 'space', '<', 'n', 'preserving', 'local', 'relationships', 'much', 'possible', '.', 'If', 'z', 'image', 'x', 'd-dimensionalspace', 'want', 'squared', 'distance', 'z', '“', 'j=1', 'mwi', 'jj', 'smallas', 'possible', '.', 'This', 'idea', 'leads', 'unconstrained', 'optimization', 'problem', 'described', 'Equation', '8-5', '.', 'It', 'looks', 'similar', 'first', 'step', 'instead', 'keeping', 'instan…', 'ces', 'fixed', 'finding', 'optimal', 'weights', 'reverse', 'keeping', 'weights', 'fixed', 'finding', 'optimal', 'position', 'instances‡', 'images', 'low-', 'dimensional', 'space', '.', 'Note', 'Z', 'matrix', 'containing', 'z', '.Equation', '8-5', '.', 'LLE', 'step', '2', 'reducing', 'dimensionality', 'preserving', 'relationships', '=argmin', '“', 'i=1', '”', '“', 'j=1', 'mwi', 'jj2Scikit-Learn‡s', 'LLE', 'implementation', 'following', 'computational', 'complexity', 'O', 'log', 'n', 'log', 'k', 'finding', 'k', 'nearest', 'neighbors', 'O', 'mnk', '3', 'optimizing', 'theweights', 'O', 'dm', '2', 'constructing', 'low-dimensional', 'representations', '.', 'Unfortu…', 'nately', 'm2', 'last', 'term', 'makes', 'algorithm', 'scale', 'poorly', 'large', 'datasets', '.', 'Other', 'Dimensionality', 'Reduction', 'TechniquesThere', 'many', 'dimensionality', 'reduction', 'techniques', 'several', 'available', 'Scikit-Learn', '.', 'Here', 'popular', '‹Multidimensional', 'Scaling', 'MDS', 'reduces', 'dimensionality', 'trying', 'preserve', 'distances', 'instances', 'see', 'Figure', '8-13', '.Other', 'Dimensionality', 'Reduction', 'Techniques', '|', '223', '9The', 'geodesic', 'distance', 'two', 'nodes', 'graph', 'number', 'nodes', 'shortest', 'path', 'nodes.‹Isomap', 'creates', 'graph', 'connecting', 'instance', 'nearest', 'neighbors', 'reduces', 'dimensionality', 'trying', 'preserve', 'geodesic', 'distances', '9', 'betweenthe', 'instances.‹t-Distributed', 'Stochastic', 'Neighbor', 'Embedding', 't-SNE', 'reduces', 'dimensionalitywhile', 'trying', 'keep', 'similar', 'instances', 'close', 'dissimilar', 'instances', 'apart', '.', 'It', 'mostly', 'used', 'visualization', 'particular', 'visualize', 'clusters', 'instances', 'high-dimensional', 'space', 'e.g.', 'visualize', 'MNIST', 'images', '2D', '.‹Linear', 'Discriminant', 'Analysis', 'LDA', 'actually', 'classification', 'algorithm', 'dur…', 'ing', 'training', 'learns', 'discriminative', 'axes', 'classes', 'axes', 'used', 'define', 'hyperplane', 'onto', 'project', 'data', '.', 'The', 'benefit', 'projection', 'keep', 'classes', 'far', 'apart', 'possible', 'LDA', 'good', 'technique', 'reduce', 'dimensionality', 'running', 'another', 'classification', 'algorithm', 'SVM', 'classifier', '.', 'Figure', '8-13', '.', 'Reducing', 'Swiss', 'roll', '2D', 'using', 'various', 'techniques', 'Exercises1.What', 'main', 'motivations', 'reducing', 'dataset‡s', 'dimensionality', '?', 'What', 'main', 'drawbacks', '?', '2.What', 'curse', 'dimensionality', '?', '3.Once', 'dataset‡s', 'dimensionality', 'reduced', 'possible', 'reverse', 'operation', '?', 'If', '?', 'If', '?', '4.Can', 'PCA', 'used', 'reduce', 'dimensionality', 'highly', 'nonlinear', 'dataset', '?', '5.Suppose', 'perform', 'PCA', '1,000-dimensional', 'dataset', 'setting', 'explained', 'variance', 'ratio', '95', '%', '.', 'How', 'many', 'dimensions', 'resulting', 'dataset', '?', '224', '|', 'Chapter', '8', 'Dimensionality', 'Reduction', '6.In', 'cases', 'would', 'use', 'vanilla', 'PCA', 'Incremental', 'PCA', 'Randomized', 'PCA', 'Kernel', 'PCA', '?', '7.How', 'evaluate', 'performance', 'dimensionality', 'reduction', 'algorithm', 'dataset', '?', '8.Does', 'make', 'sense', 'chain', 'two', 'different', 'dimensionality', 'reduction', 'algo…', 'rithms', '?', '9.Load', 'MNIST', 'dataset', 'introduced', 'Chapter', '3', 'split', 'training', 'set', 'test', 'set', 'take', 'first', '60,000', 'instances', 'training', 'remaining10,000', 'testing', '.', 'Train', 'Random', 'Forest', 'classifier', 'dataset', 'time', 'long', 'takes', 'evaluate', 'resulting', 'model', 'test', 'set', '.', 'Next', 'use', 'PCA', 'reduce', 'dataset‡s', 'dimensionality', 'explained', 'variance', 'ratio', '95', '%', '.', 'Train', 'new', 'Random', 'Forest', 'classifier', 'reduced', 'dataset', 'see', 'long', 'takes', '.', 'Was', 'training', 'much', 'faster', '?', 'Next', 'evaluate', 'classifier', 'test', 'set', 'compare', 'previous', 'classifier', '?', '10.Use', 't-SNE', 'reduce', 'MNIST', 'dataset', 'two', 'dimensions', 'plot', 'result', 'using', 'Matplotlib', '.', 'You', 'use', 'scatterplot', 'using', '10', 'different', 'colors', 'rep…', 'resent', 'image‡s', 'target', 'class', '.', 'Alternatively', 'write', 'colored', 'digits', 'location', 'instance', 'even', 'plot', 'scaled-down', 'versions', 'digit', 'images', 'plot', 'digits', 'visualization', 'cluttered', 'either', 'draw', 'random', 'sample', 'plot', 'instance', 'instance', 'already', 'plotted', 'close', 'distance', '.', 'You', 'get', 'nice', 'visualization', 'well-separated', 'clusters', 'digits', '.', 'Try', 'using', 'dimensionality', 'reduction', 'algorithms', 'PCA', 'LLE', 'MDS', 'compare', 'resulting', 'visualizations', '.', 'Solutions', 'exercises', 'available', 'Appendix', 'A', '.Exercises', '|', '225', 'PART', 'IINeural', 'Networks', 'Deep', 'LearningCHAPTER', '9Up', 'Running', 'TensorFlowTensorFlow', 'powerful', 'open', 'source', 'software', 'library', 'numerical', 'computation', 'particularly', 'well', 'suited', 'fine-tuned', 'large-scale', 'Machine', 'Learning', '.', 'Its', 'basic', 'principle', 'simple', 'first', 'define', 'Python', 'graph', 'computations', 'perform', 'example', 'one', 'Figure', '9-1', 'TensorFlow', 'takes', 'graph', 'runs', 'efficiently', 'using', 'optimized', 'C++', 'code', '.', 'Figure', '9-1', '.', 'A', 'simple', 'computation', 'graph', 'Most', 'importantly', 'possible', 'break', 'graph', 'several', 'chunks', 'run', 'parallel', 'across', 'multiple', 'CPUs', 'GPUs', 'shown', 'Figure', '9-2', '.', 'TensorFlow', 'also', 'supports', 'distributed', 'computing', 'train', 'colossal', 'neural', 'networks', 'humongous', 'training', 'sets', 'reasonable', 'amount', 'time', 'splitting', 'computa…', 'tions', 'across', 'hundreds', 'servers', 'see', 'Chapter', '12', '.', 'TensorFlow', 'train', 'network', 'millions', 'parameters', 'training', 'set', 'composed', 'billions', 'instances', 'millions', 'features', '.', 'This', 'come', 'surprise', 'since', 'TensorFlow', '2291TensorFlow', 'limited', 'neural', 'networks', 'even', 'Machine', 'Learning', 'could', 'run', 'quantum', 'physics', 'sim…', 'ulations', 'wanted', '.', 'developed', 'Google', 'Brain', 'team', 'powers', 'many', 'Google‡s', 'large-scale', 'serv…', 'ices', 'Google', 'Cloud', 'Speech', 'Google', 'Photos', 'Google', 'Search.Figure', '9-2', '.', 'Parallel', 'computation', 'multiple', 'CPUs/GPUs/servers', 'When', 'TensorFlow', 'open-sourced', 'November', '2015', 'already', 'many', 'popular', 'open', 'source', 'libraries', 'Deep', 'Learning', 'Table', '9-1', 'lists', 'fairmost', 'TensorFlow‡s', 'features', 'already', 'existed', 'one', 'library', 'another', '.', 'Nevertheless', 'TensorFlow‡s', 'clean', 'design', 'scalability', 'flexibility', '1', 'great', 'documentation', 'mention', 'Google‡s', 'name', 'quickly', 'boosted', 'top', 'list', '.', 'In', 'short', 'TensorFlow', 'designed', 'flexible', 'scalable', 'production-ready', 'existing', 'frameworks', 'arguably', 'hit', 'two', 'three', '.', 'Here', 'TensorFlow‡s', 'high…', 'lights', '‹It', 'runs', 'Windows', 'Linux', 'macOS', 'also', 'mobile', 'devices', 'including', 'iOS', 'Android.230', '|', 'Chapter', '9', 'Up', 'Running', 'TensorFlow', '2Not', 'confused', 'TFLearn', 'library', 'independent', 'project', '.', '‹It', 'provides', 'simple', 'Python', 'API', 'called', 'TF.Learn', '2', 'tensorflow.contrib.learn', 'compatible', 'Scikit-Learn', '.', 'As', 'see', 'use', 'train', 'various', 'types', 'neural', 'networks', 'lines', 'code', '.', 'It', 'previ…', 'ously', 'independent', 'project', 'called', 'Scikit', 'Flow', 'sk‡ow', '.‹It', 'also', 'provides', 'another', 'simple', 'API', 'called', 'TF-slim', 'tensorflow.contrib.slim', 'simplify', 'building', 'training', 'evaluating', 'neural', 'networks', '.', '‹Several', 'high-level', 'APIs', 'built', 'independently', 'top', 'Tensor…', 'Flow', 'Keras', 'Pretty', 'Tensor', '.‹Its', 'main', 'Python', 'API', 'offers', 'much', 'flexibility', 'cost', 'higher', 'complex…', 'ity', 'create', 'sorts', 'computations', 'including', 'neural', 'network', 'architecture', 'think', 'of.‹It', 'includes', 'highly', 'efficient', 'C++', 'implementations', 'many', 'ML', 'operations', 'partic…', 'ularly', 'needed', 'build', 'neural', 'networks', '.', 'There', 'also', 'C++', 'API', 'defineyour', 'high-performance', 'operations', '.', '‹It', 'provides', 'several', 'advanced', 'optimization', 'nodes', 'search', 'parameters', 'minimize', 'cost', 'function', '.', 'These', 'easy', 'use', 'since', 'TensorFlow', 'automati…', 'cally', 'takes', 'care', 'computing', 'gradients', 'functions', 'define', '.', 'This', 'called', 'automatic', 'di›erentiating', 'autodi›', '.‹It', 'also', 'comes', 'great', 'visualization', 'tool', 'called', 'TensorBoard', 'allows', 'browse', 'computation', 'graph', 'view', 'learning', 'curves', '.', '‹Google', 'also', 'launched', 'cloud', 'service', 'run', 'TensorFlow', 'graphs', '.‹Last', 'least', 'dedicated', 'team', 'passionate', 'helpful', 'developers', 'growing', 'community', 'contributing', 'improving', '.', 'It', 'one', 'popular', 'open', 'source', 'projects', 'GitHub', 'great', 'projects', 'built', 'top', 'examples', 'check', 'resources', 'page', 'https', '//www.tensor‡ow.org/', 'https', '//github.com/jtoy/awesome-tensor‡ow', '.', 'To', 'ask', 'technical', 'questions', 'use', 'http', '//stackover‡ow.com/', 'tag', 'ques…tion', '``', 'tensorflow', \"''\", '.', 'You', 'file', 'bugs', 'feature', 'requests', 'GitHub', '.', 'For', 'general', 'discussions', 'join', 'Google', 'group.In', 'chapter', 'go', 'basics', 'TensorFlow', 'installation', 'cre…', 'ating', 'running', 'saving', 'visualizing', 'simple', 'computational', 'graphs', '.', 'Mastering', 'basics', 'important', 'build', 'first', 'neural', 'network', 'next', 'chapter', '.', 'Up', 'Running', 'TensorFlow', '|', '231', 'Table', '9-1', '.', 'Open', 'source', 'Deep', 'Learning', 'libraries', 'exhaustive', 'list', 'LibraryAPIPlatformsStarted', 'YearCa—ePython', 'C++', 'Matlab', 'Linux', 'macOS', 'Windows', 'Y.', 'Jia', 'UC', 'Berkeley', 'BVLC', '2013Deeplearning4jJava', 'Scala', 'Clojure', 'Linux', 'macOS', 'Windows', 'Android', 'A.', 'Gibson', 'J.Patterson', '2014H2OPython', 'R', 'Linux', 'macOS', 'Windows', 'H2O.ai2014MXNetPython', 'C++', 'others', 'Linux', 'macOS', 'Windows', 'iOS', 'Android', 'DMLC2015TensorFlowPython', 'C++', 'Linux', 'macOS', 'Windows', 'iOS', 'Android', 'Google2015TheanoPythonLinux', 'macOS', 'iOS', 'University', 'Montreal', '2010TorchC++', 'Lua', 'Linux', 'macOS', 'iOS', 'Android', 'R.', 'Collobert', 'K.', 'Kavukcuoglu', 'C.', 'Farabet2002InstallationLet‡s', 'get', 'started', '!', 'Assuming', 'installed', 'Jupyter', 'Scikit-Learn', 'following', 'installation', 'instructions', 'Chapter', '2', 'simply', 'use', 'pip', 'install', 'TensorFlow', '.', 'If', 'created', 'isolated', 'environment', 'using', 'virtualenv', 'first', 'need', 'activate', '$', 'cd', '$', 'ML_PATH', '#', 'Your', 'ML', 'working', 'directory', 'e.g.', '$', 'HOME/ml', '$', 'source', 'env/bin/activateNext', 'install', 'TensorFlow', '$', 'pip3', 'install', '--', 'upgrade', 'tensorflowFor', 'GPU', 'support', 'need', 'install', 'tensorflow-gpu', 'instead', 'oftensorflow', '.', 'See', 'Chapter', '12', 'details.To', 'test', 'installation', 'type', 'following', 'command', '.', 'It', 'output', 'version', 'TensorFlow', 'installed', '.', '$', 'python3', '-c', '•import', 'tensorflow', 'print', 'tensorflow.__version__', '•1.0.0Creating', 'Your', 'First', 'Graph', 'Running', 'It', 'SessionThe', 'following', 'code', 'creates', 'graph', 'represented', 'Figure', '9-1', 'import', 'tensorflow', 'tfx', '=', 'tf.Variable', '3', 'name=', \"''\", 'x', \"''\", '=', 'tf.Variable', '4', 'name=', \"''\", \"''\", 'f', '=', 'x*x*y', '+', '+', '2232', '|', 'Chapter', '9', 'Up', 'Running', 'TensorFlow', '3In', 'distributed', 'TensorFlow', 'variable', 'values', 'stored', 'servers', 'instead', 'session', 'see', 'Chapter', '12', '.That‡s', '!', 'The', 'important', 'thing', 'understand', 'code', 'actually', 'perform', 'computation', 'even', 'though', 'looks', 'like', 'especially', 'last', 'line', '.', 'It', 'creates', 'computation', 'graph', '.', 'In', 'fact', 'even', 'variables', 'ini…', 'tialized', 'yet', '.', 'To', 'evaluate', 'graph', 'need', 'open', 'TensorFlow', 'session', 'use', 'itto', 'initialize', 'variables', 'evaluate', 'f.', 'A', 'TensorFlow', 'session', 'takes', 'care', 'placing', 'operations', 'onto', 'devices', 'CPUs', 'GPUs', 'running', 'holds', 'variable', 'values.3', 'The', 'following', 'code', 'creates', 'session', 'initializes', 'variables', 'evaluates', 'f', 'closes', 'session', 'frees', 'resources', '>', '>', '>', 'sess', '=', 'tf.Session', '>', '>', '>', 'sess.run', 'x.initializer', '>', '>', '>', 'sess.run', 'y.initializer', '>', '>', '>', 'result', '=', 'sess.run', 'f', '>', '>', '>', 'print', 'result', '42', '>', '>', '>', 'sess.close', 'Having', 'repeat', 'sess.run', 'time', 'bit', 'cumbersome', 'fortunately', 'better', 'way', 'tf.Session', 'sess', 'x.initializer.run', 'y.initializer.run', 'result', '=', 'f.eval', 'Inside', 'block', 'session', 'set', 'default', 'session', '.', 'Calling', 'x.initializer.run', 'equivalent', 'calling', 'tf.get_default_session', '.run', 'x.initializer', 'similarly', 'f.eval', 'equivalent', 'calling', 'tf.get_default_session', '.run', 'f', '.', 'This', 'makes', 'code', 'easier', 'read', '.', 'Moreover', 'session', 'automatically', 'closed', 'end', 'block', '.', 'Instead', 'manually', 'running', 'initializer', 'every', 'single', 'variable', 'use', 'global_variables_initializer', 'function', '.', 'Note', 'actually', 'perform', 'initialization', 'immediately', 'rather', 'creates', 'node', 'graph', 'initialize', 'variables', 'run', 'init', '=', 'tf.global_variables_initializer', '#', 'prepare', 'init', 'nodewith', 'tf.Session', 'sess', 'init.run', '#', 'actually', 'initialize', 'variables', 'result', '=', 'f.eval', 'Inside', 'Jupyter', 'within', 'Python', 'shell', 'may', 'prefer', 'create', 'InteractiveSession', '.', 'The', 'difference', 'regular', 'Session', 'InteractiveSession', 'created', 'automatically', 'sets', 'default', 'session', 'don‡t', 'need', 'Creating', 'Your', 'First', 'Graph', 'Running', 'It', 'Session', '|', '233', 'block', 'need', 'close', 'session', 'manually', 'done', '>', '>', '>', 'sess', '=', 'tf.InteractiveSession', '>', '>', '>', 'init.run', '>', '>', '>', 'result', '=', 'f.eval', '>', '>', '>', 'print', 'result', '42', '>', '>', '>', 'sess.close', 'A', 'TensorFlow', 'program', 'typically', 'split', 'two', 'parts', 'first', 'part', 'builds', 'compu…', 'tation', 'graph', 'called', 'construction', 'phase', 'second', 'part', 'runs', 'isthe', 'execution', 'phase', '.', 'The', 'construction', 'phase', 'typically', 'builds', 'computation', 'graph', 'representing', 'ML', 'model', 'computations', 'required', 'train', '.', 'The', 'execution', 'phase', 'generally', 'runs', 'loop', 'evaluates', 'training', 'step', 'repeatedly', 'example', 'one', 'step', 'per', 'mini-batch', 'gradually', 'improving', 'model', 'parameters', '.', 'We', 'go', 'example', 'shortly', '.', 'Managing', 'GraphsAny', 'node', 'create', 'automatically', 'added', 'default', 'graph', '>', '>', '>', 'x1', '=', 'tf.Variable', '1', '>', '>', '>', 'x1.graph', 'tf.get_default_graph', 'TrueIn', 'cases', 'fine', 'sometimes', 'may', 'want', 'manage', 'multiple', 'independ…', 'ent', 'graphs', '.', 'You', 'creating', 'new', 'Graph', 'temporarily', 'making', 'default', 'graph', 'inside', 'block', 'like', '>', '>', '>', 'graph', '=', 'tf.Graph', '>', '>', '>', 'graph.as_default', '...', 'x2', '=', 'tf.Variable', '2', '...', '>', '>', '>', 'x2.graph', 'graphTrue', '>', '>', '>', 'x2.graph', 'tf.get_default_graph', 'FalseIn', 'Jupyter', 'Python', 'shell', 'common', 'run', 'commands', 'experimenting', '.', 'As', 'result', 'may', 'end', 'default', 'graph', 'containing', 'many', 'duplicate', 'nodes', '.', 'One', 'solution', 'restart', 'Jupyter', 'kernel', 'Python', 'shell', 'convenient', 'solution', 'reset', 'default', 'graph', 'running', 'tf.reset_default_graph', '.234', '|', 'Chapter', '9', 'Up', 'Running', 'TensorFlow', 'Lifecycle', 'Node', 'ValueWhen', 'evaluate', 'node', 'TensorFlow', 'automatically', 'determines', 'set', 'nodes', 'depends', 'evaluates', 'nodes', 'first', '.', 'For', 'example', 'consider', 'follow…ing', 'code', 'w', '=', 'tf.constant', '3', 'x', '=', 'w', '+', '2y', '=', 'x', '+', '5z', '=', 'x', '*', '3with', 'tf.Session', 'sess', 'print', 'y.eval', '#', '10', 'print', 'z.eval', '#', '15First', 'code', 'defines', 'simple', 'graph', '.', 'Then', 'starts', 'session', 'runs', 'graph', 'evaluate', 'TensorFlow', 'automatically', 'detects', 'depends', 'w', 'whichdepends', 'x', 'first', 'evaluates', 'w', 'x', 'returns', 'value', '.', 'Finally', 'code', 'runs', 'graph', 'evaluate', 'z', '.', 'Once', 'TensorFlow', 'detects', 'must', 'first', 'evaluate', 'w', 'x', '.', 'It', 'important', 'note', 'reuse', 'result', 'theprevious', 'evaluation', 'w', 'x', '.', 'In', 'short', 'preceding', 'code', 'evaluates', 'w', 'x', 'twice.All', 'node', 'values', 'dropped', 'graph', 'runs', 'except', 'variable', 'values', 'maintained', 'session', 'across', 'graph', 'runs', 'queues', 'readers', 'also', 'maintain', 'state', 'see', 'Chapter', '12', '.', 'A', 'variable', 'starts', 'life', 'initializer', 'run', 'ends', 'session', 'closed.If', 'want', 'evaluate', 'z', 'efficiently', 'without', 'evaluating', 'w', 'x', 'twice', 'theprevious', 'code', 'must', 'ask', 'TensorFlow', 'evaluate', 'z', 'one', 'graph', 'run', 'shown', 'following', 'code', 'tf.Session', 'sess', 'y_val', 'z_val', '=', 'sess.run', 'z', 'print', 'y_val', '#', '10', 'print', 'z_val', '#', '15In', 'single-process', 'TensorFlow', 'multiple', 'sessions', 'share', 'state', 'even', 'reuse', 'graph', 'session', 'would', 'copy', 'every', 'variable', '.', 'In', 'distributed', 'TensorFlow', 'see', 'Chap…', 'ter', '12', 'variable', 'state', 'stored', 'servers', 'sessions', 'multiple', 'sessions', 'share', 'variables', '.', 'Linear', 'Regression', 'TensorFlowTensorFlow', 'operations', 'also', 'called', 'ops', 'short', 'take', 'number', 'inputs', 'produce', 'number', 'outputs', '.', 'For', 'example', 'addition', 'multiplication', 'ops', 'take', 'two', 'inputs', 'produce', 'one', 'output', '.', 'Constants', 'variables', 'take', 'input', 'Lifecycle', 'Node', 'Value', '|', '235', '4Note', 'housing.target', '1D', 'array', 'need', 'reshape', 'column', 'vector', 'compute', 'theta.Recall', 'NumPy‡s', 'reshape', 'function', 'accepts', '–1', 'meaning', 'ƒunspecified⁄', 'one', 'dimensions', 'dimension', 'computed', 'based', 'array‡s', 'length', 'remaining', 'dimensions', '.', 'called', 'source', 'ops', '.', 'The', 'inputs', 'outputs', 'multidimensional', 'arrays', 'called', 'tensors', 'hence', 'name', 'ƒtensor', 'flow⁄', '.', 'Just', 'like', 'NumPy', 'arrays', 'tensors', 'type', 'shape', '.', 'In', 'fact', 'Python', 'API', 'tensors', 'simply', 'represented', 'NumPy', 'ndarrays', '.', 'They', 'typically', 'contain', 'floats', 'also', 'use', 'carry', 'strings', 'arbitrary', 'byte', 'arrays', '.', 'In', 'examples', 'far', 'tensors', 'contained', 'single', 'scalar', 'value', 'course', 'perform', 'computations', 'arrays', 'shape', '.', 'For', 'example', 'following', 'code', 'manipulates', '2D', 'arrays', 'perform', 'Linear', 'Regression', 'California', 'housing', 'data…', 'set', 'introduced', 'Chapter', '2', '.', 'It', 'starts', 'fetching', 'dataset', 'adds', 'extra', 'bias', 'input', 'feature', 'x0', '=', '1', 'training', 'instances', 'using', 'NumPy', 'runs', 'immediately', 'creates', 'two', 'TensorFlow', 'constant', 'nodes', 'X', 'hold', 'thisdata', 'targets', '4', 'uses', 'matrix', 'operations', 'provided', 'Tensor…', 'Flow', 'define', 'theta', '.', 'These', 'matrix', 'functions›', 'transpose', 'matmul', 'andmatrix_inverse', '›are', 'self-explanatory', 'usual', 'perform', 'com…', 'putations', 'immediately', 'instead', 'create', 'nodes', 'graph', 'perform', 'graph', 'run', '.', 'You', 'may', 'recognize', 'definition', 'theta', 'corresponds', 'Normal', 'Equation', '–', '=', 'XT', '’', 'X', '–1', '’', 'XT', '’', 'see', 'Chapter', '4', '.', 'Finally', 'code', 'creates', 'session', 'uses', 'evaluate', 'theta.import', 'numpy', 'npfrom', 'sklearn.datasets', 'import', 'fetch_california_housinghousing', '=', 'fetch_california_housing', 'n', '=', 'housing.data.shapehousing_data_plus_bias', '=', 'np.c_', 'np.ones', '1', 'housing.data', 'X', '=', 'tf.constant', 'housing_data_plus_bias', 'dtype=tf.float32', 'name=', \"''\", 'X', \"''\", '=', 'tf.constant', 'housing.target.reshape', '-1', '1', 'dtype=tf.float32', 'name=', \"''\", \"''\", 'XT', '=', 'tf.transpose', 'X', 'theta', '=', 'tf.matmul', 'tf.matmul', 'tf.matrix_inverse', 'tf.matmul', 'XT', 'X', 'XT', 'tf.Session', 'sess', 'theta_value', '=', 'theta.eval', 'The', 'main', 'benefit', 'code', 'versus', 'computing', 'Normal', 'Equation', 'directly', 'using', 'NumPy', 'TensorFlow', 'automatically', 'run', 'GPU', 'card', 'one', 'provided', 'installed', 'TensorFlow', 'GPU', 'support', 'course', 'see', 'Chapter', '12', 'details', '.236', '|', 'Chapter', '9', 'Up', 'Running', 'TensorFlow', 'Implementing', 'Gradient', 'DescentLet‡s', 'try', 'using', 'Batch', 'Gradient', 'Descent', 'introduced', 'Chapter', '4', 'instead', 'Nor…', 'mal', 'Equation', '.', 'First', 'manually', 'computing', 'gradients', 'use', 'TensorFlow‡s', 'autodiff', 'feature', 'let', 'TensorFlow', 'compute', 'gradients', 'automati…', 'cally', 'finally', 'use', 'couple', 'TensorFlow‡s', 'out-of-the-box', 'optimizers', '.', 'When', 'using', 'Gradient', 'Descent', 'remember', 'important', 'first', 'normalize', 'input', 'feature', 'vectors', 'else', 'training', 'may', 'much', 'slower', '.', 'You', 'using', 'TensorFlow', 'NumPy', 'Scikit-Learn‡s', 'StandardScaler', 'solution', 'prefer', '.', 'The', 'fol…', 'lowing', 'code', 'assumes', 'normalization', 'already', 'done.Manually', 'Computing', 'GradientsThe', 'following', 'code', 'fairly', 'self-explanatory', 'except', 'new', 'elements', '‹The', 'random_uniform', 'function', 'creates', 'node', 'graph', 'generate', 'tensor', 'containing', 'random', 'values', 'given', 'shape', 'value', 'range', 'much', 'like', 'NumPy‡s', 'rand', 'function.‹The', 'assign', 'function', 'creates', 'node', 'assign', 'new', 'value', 'variable', '.', 'In', 'case', 'implements', 'Batch', 'Gradient', 'Descent', 'step', '–', 'next', 'step', '=', '–', '–−–MSE', '–', '.‹The', 'main', 'loop', 'executes', 'training', 'step', 'n_epochs', 'times', 'every', '100', 'iterations', 'prints', 'current', 'Mean', 'Squared', 'Error', 'mse', '.', 'You', 'see', 'MSE', 'go', 'every', 'iteration', '.', 'n_epochs', '=', '1000learning_rate', '=', '0.01X', '=', 'tf.constant', 'scaled_housing_data_plus_bias', 'dtype=tf.float32', 'name=', \"''\", 'X', \"''\", '=', 'tf.constant', 'housing.target.reshape', '-1', '1', 'dtype=tf.float32', 'name=', \"''\", \"''\", 'theta', '=', 'tf.Variable', 'tf.random_uniform', 'n', '+', '1', '1', '-1.0', '1.0', 'name=', \"''\", 'theta', \"''\", 'y_pred', '=', 'tf.matmul', 'X', 'theta', 'name=', \"''\", 'predictions', \"''\", 'error', '=', 'y_pred', '-', 'ymse', '=', 'tf.reduce_mean', 'tf.square', 'error', 'name=', \"''\", 'mse', \"''\", 'gradients', '=', '2/m', '*', 'tf.matmul', 'tf.transpose', 'X', 'error', 'training_op', '=', 'tf.assign', 'theta', 'theta', '-', 'learning_rate', '*', 'gradients', 'init', '=', 'tf.global_variables_initializer', 'tf.Session', 'sess', 'sess.run', 'init', 'epoch', 'range', 'n_epochs', 'Implementing', 'Gradient', 'Descent', '|', '237', 'epoch', '%', '100', '==', '0', 'print', '``', 'Epoch', \"''\", 'epoch', '``', 'MSE', '=', \"''\", 'mse.eval', 'sess.run', 'training_op', 'best_theta', '=', 'theta.eval', 'Using', 'autodi†The', 'preceding', 'code', 'works', 'fine', 'requires', 'mathematically', 'deriving', 'gradients', 'cost', 'function', 'MSE', '.', 'In', 'case', 'Linear', 'Regression', 'reasonably', 'easy', 'deep', 'neural', 'networks', 'would', 'get', 'quite', 'headache', 'would', 'tedious', 'error-prone', '.', 'You', 'could', 'use', 'symbolic', 'di›erentiation', 'auto…', 'matically', 'find', 'equations', 'partial', 'derivatives', 'resulting', 'code', 'would', 'necessarily', 'efficient', '.', 'To', 'understand', 'consider', 'function', 'f', 'x', '=', 'exp', 'exp', 'exp', 'x', '.', 'If', 'know', 'calcu…lus', 'figure', 'derivative', 'f', 'x', '=', 'exp', 'x', '‰', 'exp', 'exp', 'x', '‰', 'exp', 'exp', 'exp', 'x', '.If', 'code', 'f', 'x', 'f', 'x', 'separately', 'exactly', 'appear', 'code', 'efficient', 'could', '.', 'A', 'efficient', 'solution', 'would', 'write', 'function', 'first', 'computes', 'exp', 'x', 'exp', 'exp', 'x', 'exp', 'exp', 'exp', 'x', 'returns', 'three.This', 'gives', 'f', 'x', 'directly', 'third', 'term', 'need', 'derivative', 'multiply', 'three', 'terms', 'done', '.', 'With', 'na€ve', 'approach', 'would', 'call', 'exp', 'function', 'nine', 'times', 'compute', 'f', 'x', 'f', 'x', '.', 'With', 'approach', 'need', 'call', 'three', 'times', '.', 'It', 'gets', 'worse', 'function', 'defined', 'arbitrary', 'code', '.', 'Can', 'find', 'equation', 'code', 'compute', 'partial', 'derivatives', 'following', 'function', '?', 'Hint', 'don‡t', 'even', 'try', '.', 'def', 'my_func', 'b', 'z', '=', '0', 'range', '100', 'z', '=', '*', 'np.cos', 'z', '+', '+', 'z', '*', 'np.sin', 'b', '-', 'return', 'zFortunately', 'TensorFlow‡s', 'autodiff', 'feature', 'comes', 'rescue', 'automatically', 'efficiently', 'compute', 'gradients', '.', 'Simply', 'replace', 'gradients', '=', '...', 'line', 'Gradient', 'Descent', 'code', 'previous', 'section', 'following', 'line', 'code', 'continue', 'work', 'fine', 'gradients', '=', 'tf.gradients', 'mse', 'theta', '0', 'The', 'gradients', 'function', 'takes', 'op', 'case', 'mse', 'list', 'variables', 'thiscase', 'theta', 'creates', 'list', 'ops', 'one', 'per', 'variable', 'compute', 'gradi…', 'ents', 'op', 'regards', 'variable', '.', 'So', 'gradients', 'node', 'compute', 'gradient', 'vector', 'MSE', 'regards', 'theta.238', '|', 'Chapter', '9', 'Up', 'Running', 'TensorFlow', 'There', 'four', 'main', 'approaches', 'computing', 'gradients', 'automatically', '.', 'They', 'sum…', 'marized', 'Table', '9-2', '.', 'TensorFlow', 'uses', 'reverse-mode', 'autodi›', 'perfect', 'effi…cient', 'accurate', 'many', 'inputs', 'outputs', 'often', 'case', 'neural', 'networks', '.', 'It', 'computes', 'partial', 'derivatives', 'outputs', 'regards', 'inputs', 'noutputs', '+', '1', 'graph', 'traversals', '.', 'Table', '9-2', '.', 'Main', 'solutions', 'compute', 'gradients', 'automatically', 'TechniqueNb', 'graph', 'traversals', 'compute', 'gradients', 'AccuracySupportsarbitrary', 'code', 'CommentNumerical', 'di—erentiationninputs', '+', '1', 'LowYesTrivial', 'implement', 'Symbolic', 'di—erentiationN/AHighNoBuilds', 'di—erent', 'graph', 'Forward-mode', 'autodi—ninputsHighYesUses', 'dual', 'numbers', 'Reverse-mode', 'autodi—noutputs', '+', '1', 'HighYesImplemented', 'TensorFlow', 'If', 'interested', 'magic', 'works', 'check', 'Appendix', 'D', '.Using', 'OptimizerSo', 'TensorFlow', 'computes', 'gradients', '.', 'But', 'gets', 'even', 'easier', 'also', 'provides', 'number', 'optimizers', 'box', 'including', 'Gradient', 'Descent', 'optimizer', '.', 'You', 'simply', 'replace', 'preceding', 'gradients', '=', '...', 'training_op', '=', '...', 'lineswith', 'following', 'code', 'everything', 'work', 'fine', 'optimizer', '=', 'tf.train.GradientDescentOptimizer', 'learning_rate=learning_rate', 'training_op', '=', 'optimizer.minimize', 'mse', 'If', 'want', 'use', 'different', 'type', 'optimizer', 'need', 'change', 'one', 'line', '.', 'For', 'example', 'use', 'momentum', 'optimizer', 'often', 'converges', 'much', 'faster', 'Gradient', 'Descent', 'see', 'Chapter', '11', 'defining', 'optimizer', 'like', 'optimizer', '=', 'tf.train.MomentumOptimizer', 'learning_rate=learning_rate', 'momentum=0.9', 'Feeding', 'Data', 'Training', 'AlgorithmLet‡s', 'try', 'modify', 'previous', 'code', 'implement', 'Mini-batch', 'Gradient', 'Descent', '.', 'For', 'need', 'way', 'replace', 'X', 'every', 'iteration', 'next', 'mini-batch', '.', 'The', 'simplest', 'way', 'use', 'placeholder', 'nodes', '.', 'These', 'nodes', 'special', 'don‡t', 'actually', 'perform', 'computation', 'output', 'data', 'tell', 'output', 'runtime', '.', 'They', 'typically', 'used', 'pass', 'training', 'data', 'TensorFlow', 'training', '.', 'If', 'don‡t', 'specify', 'value', 'runtime', 'placeholder', 'get', 'exception.To', 'create', 'placeholder', 'node', 'must', 'call', 'placeholder', 'function', 'specify', 'output', 'tensor‡s', 'data', 'type', '.', 'Optionally', 'also', 'specify', 'shape', 'want', 'Feeding', 'Data', 'Training', 'Algorithm', '|', '239', 'enforce', '.', 'If', 'specify', 'None', 'dimension', 'means', 'ƒany', 'size.⁄', 'For', 'example', 'following', 'code', 'creates', 'placeholder', 'node', 'A', 'also', 'node', 'B', '=', 'A', '+', '5', '.', 'When', 'weevaluate', 'B', 'pass', 'feed_dict', 'eval', 'method', 'specifies', 'value', 'A.Note', 'A', 'must', 'rank', '2', 'i.e.', 'must', 'two-dimensional', 'must', 'three', 'columns', 'else', 'exception', 'raised', 'number', 'rows', '.', '>', '>', '>', 'A', '=', 'tf.placeholder', 'tf.float32', 'shape=', 'None', '3', '>', '>', '>', 'B', '=', 'A', '+', '5', '>', '>', '>', 'tf.Session', 'sess', '...', 'B_val_1', '=', 'B.eval', 'feed_dict=', '{', 'A', '1', '2', '3', '}', '...', 'B_val_2', '=', 'B.eval', 'feed_dict=', '{', 'A', '4', '5', '6', '7', '8', '9', '}', '...', '>', '>', '>', 'print', 'B_val_1', '6', '.', '7', '.', '8', '.', '>', '>', '>', 'print', 'B_val_2', '9', '.', '10', '.', '11', '.', '12', '.', '13', '.', '14', '.', 'You', 'actually', 'feed', 'output', 'operations', 'place…', 'holders', '.', 'In', 'case', 'TensorFlow', 'try', 'evaluate', 'operations', 'uses', 'values', 'feed', '.', 'To', 'implement', 'Mini-batch', 'Gradient', 'Descent', 'need', 'tweak', 'existing', 'code', 'slightly', '.', 'First', 'change', 'definition', 'X', 'construction', 'phase', 'make', 'placeholder', 'nodes', 'X', '=', 'tf.placeholder', 'tf.float32', 'shape=', 'None', 'n', '+', '1', 'name=', \"''\", 'X', \"''\", '=', 'tf.placeholder', 'tf.float32', 'shape=', 'None', '1', 'name=', \"''\", \"''\", 'Then', 'define', 'batch', 'size', 'compute', 'total', 'number', 'batches', 'batch_size', '=', '100n_batches', '=', 'int', 'np.ceil', '/', 'batch_size', 'Finally', 'execution', 'phase', 'fetch', 'mini-batches', 'one', 'one', 'provide', 'value', 'X', 'via', 'feed_dict', 'parameter', 'evaluating', 'node', 'depends', 'either', 'them.def', 'fetch_batch', 'epoch', 'batch_index', 'batch_size', '...', '#', 'load', 'data', 'disk', 'return', 'X_batch', 'y_batchwith', 'tf.Session', 'sess', 'sess.run', 'init', 'epoch', 'range', 'n_epochs', 'batch_index', 'range', 'n_batches', 'X_batch', 'y_batch', '=', 'fetch_batch', 'epoch', 'batch_index', 'batch_size', 'sess.run', 'training_op', 'feed_dict=', '{', 'X', 'X_batch', 'y_batch', '}', '240', '|', 'Chapter', '9', 'Up', 'Running', 'TensorFlow', 'best_theta', '=', 'theta.eval', 'We', 'don‡t', 'need', 'pass', 'value', 'X', 'evaluating', 'thetasince', 'depend', 'either', 'them.Saving', 'Restoring', 'ModelsOnce', 'trained', 'model', 'save', 'parameters', 'disk', 'come', 'back', 'whenever', 'want', 'use', 'another', 'program', 'compare', 'models', '.', 'Moreover', 'probably', 'want', 'save', 'checkpoints', 'regular', 'inter…', 'vals', 'training', 'computer', 'crashes', 'training', 'con…', 'tinue', 'last', 'checkpoint', 'rather', 'start', 'scratch', '.', 'TensorFlow', 'makes', 'saving', 'restoring', 'model', 'easy', '.', 'Just', 'create', 'Saver', 'node', 'end', 'construction', 'phase', 'variable', 'nodes', 'created', 'execution', 'phase', 'call', 'save', 'method', 'whenever', 'want', 'save', 'model', 'passing', 'session', 'path', 'checkpoint', 'file', '...', 'theta', '=', 'tf.Variable', 'tf.random_uniform', 'n', '+', '1', '1', '-1.0', '1.0', 'name=', \"''\", 'theta', \"''\", '...', 'init', '=', 'tf.global_variables_initializer', 'saver', '=', 'tf.train.Saver', 'tf.Session', 'sess', 'sess.run', 'init', 'epoch', 'range', 'n_epochs', 'epoch', '%', '100', '==', '0', '#', 'checkpoint', 'every', '100', 'epochs', 'save_path', '=', 'saver.save', 'sess', '``', '/tmp/my_model.ckpt', \"''\", 'sess.run', 'training_op', 'best_theta', '=', 'theta.eval', 'save_path', '=', 'saver.save', 'sess', '``', '/tmp/my_model_final.ckpt', \"''\", 'Restoring', 'model', 'easy', 'create', 'Saver', 'end', 'construction', 'phase', 'like', 'beginning', 'execution', 'phase', 'instead', 'ini…', 'tializing', 'variables', 'using', 'init', 'node', 'call', 'restore', 'method', 'theSaver', 'object', 'tf.Session', 'sess', 'saver.restore', 'sess', '``', '/tmp/my_model_final.ckpt', \"''\", '...', 'Saving', 'Restoring', 'Models', '|', '241', 'By', 'default', 'Saver', 'saves', 'restores', 'variables', 'name', 'need', 'control', 'specify', 'variables', 'save', 'restore', 'names', 'use', '.', 'For', 'example', 'following', 'Saver', 'save', 'restore', 'thetavariable', 'name', 'weights', 'saver', '=', 'tf.train.Saver', '{', '``', 'weights', \"''\", 'theta', '}', 'Visualizing', 'Graph', 'Training', 'Curves', 'UsingTensorBoardSo', 'computation', 'graph', 'trains', 'Linear', 'Regression', 'model', 'using', 'Mini-batch', 'Gradient', 'Descent', 'saving', 'checkpoints', 'regular', 'intervals', '.', 'Sounds', 'sophisticated', 'doesn‡t', '?', 'However', 'still', 'relying', 'print', 'func…tion', 'visualize', 'progress', 'training', '.', 'There', 'better', 'way', 'enter', 'TensorBoard', '.', 'If', 'feed', 'training', 'stats', 'display', 'nice', 'interactive', 'visualizations', 'stats', 'web', 'browser', 'e.g.', 'learning', 'curves', '.', 'You', 'also', 'provide', 'graph‡s', 'definition', 'give', 'great', 'interface', 'browse', '.', 'This', 'use…', 'ful', 'identify', 'errors', 'graph', 'find', 'bottlenecks', '.', 'The', 'first', 'step', 'tweak', 'program', 'bit', 'writes', 'graph', 'definition', 'training', 'stats›for', 'example', 'training', 'error', 'MSE', '›to', 'log', 'directory', 'TensorBoard', 'read', '.', 'You', 'need', 'use', 'different', 'log', 'directory', 'every', 'time', 'run', 'program', 'else', 'TensorBoard', 'merge', 'stats', 'different', 'runs', 'mess', 'visualizations', '.', 'The', 'simplest', 'solution', 'include', 'time…', 'stamp', 'log', 'directory', 'name', '.', 'Add', 'following', 'code', 'beginning', 'pro…', 'gram', 'datetime', 'import', 'datetimenow', '=', 'datetime.utcnow', '.strftime', '``', '%', 'Y', '%', '%', '%', 'H', '%', 'M', '%', 'S', \"''\", 'root_logdir', '=', '``', 'tf_logs', \"''\", 'logdir', '=', '``', '{', '}', '/run-', '{', '}', '/', \"''\", '.format', 'root_logdir', 'Next', 'add', 'following', 'code', 'end', 'construction', 'phase', 'mse_summary', '=', 'tf.summary.scalar', '•MSE•', 'mse', 'file_writer', '=', 'tf.summary.FileWriter', 'logdir', 'tf.get_default_graph', 'The', 'first', 'line', 'creates', 'node', 'graph', 'evaluate', 'MSE', 'value', 'write', 'TensorBoard-compatible', 'binary', 'log', 'string', 'called', 'summary', '.', 'The', 'second', 'line', 'cre…ates', 'FileWriter', 'use', 'write', 'summaries', 'logfiles', 'log', 'directory', '.', 'The', 'first', 'parameter', 'indicates', 'path', 'log', 'directory', 'case', 'something', 'like', 'tf_logs/run-20160906091959/', 'relative', 'current', 'directory', '.', 'The', 'second', 'optional', 'parameter', 'graph', 'want', 'visualize', '.', 'Upon', 'creation', 'FileWriter', 'creates', 'log', 'directory', 'already', 'exist', 'parent', 'directories', 'needed', 'writes', 'graph', 'definition', 'binary', 'logfile', 'called', 'events', '†le.242', '|', 'Chapter', '9', 'Up', 'Running', 'TensorFlow', 'Next', 'need', 'update', 'execution', 'phase', 'evaluate', 'mse_summary', 'node', 'regu…', 'larly', 'training', 'e.g.', 'every', '10', 'mini-batches', '.', 'This', 'output', 'summary', 'write', 'events', 'file', 'using', 'file_writer', '.', 'Here', 'updated', 'code', '...', 'batch_index', 'range', 'n_batches', 'X_batch', 'y_batch', '=', 'fetch_batch', 'epoch', 'batch_index', 'batch_size', 'batch_index', '%', '10', '==', '0', 'summary_str', '=', 'mse_summary.eval', 'feed_dict=', '{', 'X', 'X_batch', 'y_batch', '}', 'step', '=', 'epoch', '*', 'n_batches', '+', 'batch_index', 'file_writer.add_summary', 'summary_str', 'step', 'sess.run', 'training_op', 'feed_dict=', '{', 'X', 'X_batch', 'y_batch', '}', '...', 'Avoid', 'logging', 'training', 'stats', 'every', 'single', 'training', 'step', 'would', 'significantly', 'slow', 'training', '.', 'Finally', 'want', 'close', 'FileWriter', 'end', 'program', 'file_writer.close', 'Now', 'run', 'program', 'create', 'log', 'directory', 'write', 'events', 'file', 'directory', 'containing', 'graph', 'definition', 'MSE', 'values', '.', 'Open', 'shell', 'go', 'working', 'directory', 'type', 'ls', '-l', 'tf_logs/run*', 'list', 'contents', 'log', 'directory', '$', 'cd', '$', 'ML_PATH', '#', 'Your', 'ML', 'working', 'directory', 'e.g.', '$', 'HOME/ml', '$', 'ls', '-l', 'tf_logs/run*total', '40-rw-r', '--', 'r', '--', '1', 'ageron', 'staff', '18620', 'Sep', '6', '11:10', 'events.out.tfevents.1472553182.mymacIf', 'run', 'program', 'second', 'time', 'see', 'second', 'directory', 'tf_logs/', 'directory', '$', 'ls', '-l', 'tf_logs/total', '0drwxr-xr-x', '3', 'ageron', 'staff', '102', 'Sep', '6', '10:07', 'run-20160906091959drwxr-xr-x', '3', 'ageron', 'staff', '102', 'Sep', '6', '10:22', 'run-20160906092202Great', '!', 'Now', 'it‡s', 'time', 'fire', 'TensorBoard', 'server', '.', 'You', 'need', 'activate', 'vir…', 'tualenv', 'environment', 'created', 'one', 'start', 'server', 'running', 'tensorboard', 'command', 'pointing', 'root', 'log', 'directory', '.', 'This', 'starts', 'TensorBoard', 'web', 'server', 'listening', 'port', '6006', 'ƒgoog⁄', 'written', 'upside', '$', 'source', 'env/bin/activate', '$', 'tensorboard', '--', 'logdir', 'tf_logs/Starting', 'TensorBoard', 'port', '6006', 'You', 'navigate', 'http', '//0.0.0.0:6006', 'Visualizing', 'Graph', 'Training', 'Curves', 'Using', 'TensorBoard', '|', '243', 'Next', 'open', 'browser', 'go', 'http', '//0.0.0.0:6006/', 'http', '//localhost:6006/', '.', 'Wel…', 'come', 'TensorBoard', '!', 'In', 'Events', 'tab', 'see', 'MSE', 'right', '.', 'If', 'click', 'see', 'plot', 'MSE', 'training', 'runs', 'Figure', '9-3', '.', 'You', 'check', 'uncheck', 'runs', 'want', 'see', 'zoom', 'hover', 'curve', 'get', 'details', 'on.Figure', '9-3', '.', 'Visualizing', 'training', 'stats', 'using', 'TensorBoard', 'Now', 'click', 'Graphs', 'tab', '.', 'You', 'see', 'graph', 'shown', 'Figure', '9-4.To', 'reduce', 'clutter', 'nodes', 'many', 'edges', 'i.e.', 'connections', 'nodes', 'separated', 'auxiliary', 'area', 'right', 'move', 'node', 'back', 'forth', 'main', 'graph', 'auxiliary', 'area', 'right-clicking', '.', 'Some', 'parts', 'graph', 'also', 'collapsed', 'default', '.', 'For', 'example', 'try', 'hovering', 'gradients', 'node', 'click', 'icon', 'expand', 'subgraph', '.', 'Next', 'sub…', 'graph', 'try', 'expanding', 'mse_grad', 'subgraph', '.', '244', '|', 'Chapter', '9', 'Up', 'Running', 'TensorFlow', 'Figure', '9-4', '.', 'Visualizing', 'graph', 'using', 'TensorBoard', 'If', 'want', 'take', 'peek', 'graph', 'directly', 'within', 'Jupyter', 'use', 'show_graph', 'function', 'available', 'notebook', 'chapter', '.', 'It', 'originally', 'written', 'A.', 'Mordvintsev', 'great', 'deepdream', 'tutorial', 'notebook', '.', 'Another', 'option', 'install', 'E.', 'Jang‡s', 'TensorFlow', 'debugger', 'tool', 'includes', 'Jupyter', 'extension', 'forgraph', 'visualization', '.', 'Name', 'ScopesWhen', 'dealing', 'complex', 'models', 'neural', 'networks', 'graph', 'easily', 'become', 'cluttered', 'thousands', 'nodes', '.', 'To', 'avoid', 'create', 'name', 'scopes', 'group', 'related', 'nodes', '.', 'For', 'example', 'let‡s', 'modify', 'previous', 'code', 'define', 'error', 'mse', 'ops', 'within', 'name', 'scope', 'called', '``', 'loss', \"''\", 'tf.name_scope', '``', 'loss', \"''\", 'scope', 'error', '=', 'y_pred', '-', 'mse', '=', 'tf.reduce_mean', 'tf.square', 'error', 'name=', \"''\", 'mse', \"''\", 'The', 'name', 'op', 'defined', 'within', 'scope', 'prefixed', '``', 'loss/', \"''\", '>', '>', '>', 'print', 'error.op.name', 'loss/sub', '>', '>', '>', 'print', 'mse.op.name', 'loss/mseIn', 'TensorBoard', 'mse', 'error', 'nodes', 'appear', 'inside', 'loss', 'namespace', 'appears', 'collapsed', 'default', 'Figure', '9-5', '.Name', 'Scopes', '|', '245', 'Figure', '9-5', '.', 'A', 'collapsed', 'namescope', 'TensorBoard', 'ModularitySuppose', 'want', 'create', 'graph', 'adds', 'output', 'two', 'recti†ed', 'linear', 'units', 'ReLU', '.', 'A', 'ReLU', 'computes', 'linear', 'function', 'inputs', 'outputs', 'result', 'positive', '0', 'otherwise', 'shown', 'Equation', '9-1', '.Equation', '9-1', '.', 'Recti†ed', 'linear', 'unit', 'h', 'b=max', '’', '+b,0', 'The', 'following', 'code', 'job', 'it‡s', 'quite', 'repetitive', 'n_features', '=', '3X', '=', 'tf.placeholder', 'tf.float32', 'shape=', 'None', 'n_features', 'name=', \"''\", 'X', \"''\", 'w1', '=', 'tf.Variable', 'tf.random_normal', 'n_features', '1', 'name=', \"''\", 'weights1', \"''\", 'w2', '=', 'tf.Variable', 'tf.random_normal', 'n_features', '1', 'name=', \"''\", 'weights2', \"''\", 'b1', '=', 'tf.Variable', '0.0', 'name=', \"''\", 'bias1', \"''\", 'b2', '=', 'tf.Variable', '0.0', 'name=', \"''\", 'bias2', \"''\", 'z1', '=', 'tf.add', 'tf.matmul', 'X', 'w1', 'b1', 'name=', \"''\", 'z1', \"''\", 'z2', '=', 'tf.add', 'tf.matmul', 'X', 'w2', 'b2', 'name=', \"''\", 'z2', \"''\", 'relu1', '=', 'tf.maximum', 'z1', '0.', 'name=', \"''\", 'relu1', \"''\", 'relu2', '=', 'tf.maximum', 'z1', '0.', 'name=', \"''\", 'relu2', \"''\", 'output', '=', 'tf.add', 'relu1', 'relu2', 'name=', \"''\", 'output', \"''\", 'Such', 'repetitive', 'code', 'hard', 'maintain', 'error-prone', 'fact', 'code', 'contains', 'cut-and-paste', 'error', 'spot', '?', '.', 'It', 'would', 'become', 'even', 'worse', 'wanted', '246', '|', 'Chapter', '9', 'Up', 'Running', 'TensorFlow', 'add', 'ReLUs', '.', 'Fortunately', 'TensorFlow', 'lets', 'stay', 'DRY', 'Don‡t', 'Repeat', 'Yourself', 'simply', 'create', 'function', 'build', 'ReLU', '.', 'The', 'following', 'code', 'creates', 'five', 'ReLUs', 'outputs', 'sum', 'note', 'add_n', 'creates', 'operation', 'com…', 'pute', 'sum', 'list', 'tensors', 'def', 'relu', 'X', 'w_shape', '=', 'int', 'X.get_shape', '1', '1', 'w', '=', 'tf.Variable', 'tf.random_normal', 'w_shape', 'name=', \"''\", 'weights', \"''\", 'b', '=', 'tf.Variable', '0.0', 'name=', \"''\", 'bias', \"''\", 'z', '=', 'tf.add', 'tf.matmul', 'X', 'w', 'b', 'name=', \"''\", 'z', \"''\", 'return', 'tf.maximum', 'z', '0.', 'name=', \"''\", 'relu', \"''\", 'n_features', '=', '3X', '=', 'tf.placeholder', 'tf.float32', 'shape=', 'None', 'n_features', 'name=', \"''\", 'X', \"''\", 'relus', '=', 'relu', 'X', 'range', '5', 'output', '=', 'tf.add_n', 'relus', 'name=', \"''\", 'output', \"''\", 'Note', 'create', 'node', 'TensorFlow', 'checks', 'whether', 'name', 'already', 'exists', 'appends', 'underscore', 'followed', 'index', 'make', 'name', 'unique', '.', 'So', 'first', 'ReLU', 'contains', 'nodes', 'named', \"''\", 'weights', \"''\", '``', 'bias', \"''\", '``', 'z', \"''\", '``', 'relu', \"''\", 'plus', 'many', 'nodes', 'default', 'name', \"''\", 'MatMul', \"''\", 'second', 'ReLU', 'contains', 'nodes', 'named', \"''\", 'weights_1', \"''\", '``', 'bias_1', \"''\", 'third', 'ReLU', 'contains', 'nodes', 'named', '``', 'weights_2', \"''\", '``', 'bias_2', \"''\", '.', 'TensorBoard', 'identifies', 'series', 'collapses', 'together', 'reduce', 'clutter', 'see', 'Figure', '9-6', '.Figure', '9-6', '.', 'Collapsed', 'node', 'series', 'Modularity', '|', '247', 'Using', 'name', 'scopes', 'make', 'graph', 'much', 'clearer', '.', 'Simply', 'move', 'con…', 'tent', 'relu', 'function', 'inside', 'name', 'scope', '.', 'Figure', '9-7', 'shows', 'resultinggraph', '.', 'Notice', 'TensorFlow', 'also', 'gives', 'name', 'scopes', 'unique', 'names', 'append…', 'ing', '_1', '_2', 'on.def', 'relu', 'X', 'tf.name_scope', '``', 'relu', \"''\", '...', 'Figure', '9-7', '.', 'A', 'clearer', 'graph', 'using', 'name-scoped', 'units', 'Sharing', 'VariablesIf', 'want', 'share', 'variable', 'various', 'components', 'graph', 'one', 'sim…', 'ple', 'option', 'create', 'first', 'pass', 'parameter', 'functions', 'need', '.', 'For', 'example', 'suppose', 'want', 'control', 'ReLU', 'threshold', 'currently', 'hardcoded', '0', 'using', 'shared', 'threshold', 'variable', 'ReLUs', '.', 'You', 'could', 'create', 'vari…', 'able', 'first', 'pass', 'relu', 'function', 'def', 'relu', 'X', 'threshold', 'tf.name_scope', '``', 'relu', \"''\", '...', 'return', 'tf.maximum', 'z', 'threshold', 'name=', \"''\", 'max', \"''\", 'threshold', '=', 'tf.Variable', '0.0', 'name=', \"''\", 'threshold', \"''\", 'X', '=', 'tf.placeholder', 'tf.float32', 'shape=', 'None', 'n_features', 'name=', \"''\", 'X', \"''\", 'relus', '=', 'relu', 'X', 'threshold', 'range', '5', 'output', '=', 'tf.add_n', 'relus', 'name=', \"''\", 'output', \"''\", 'This', 'works', 'fine', 'control', 'threshold', 'ReLUs', 'using', 'thresholdvariable', '.', 'However', 'many', 'shared', 'parameters', 'one', 'painful', 'pass', 'around', 'parameters', 'time', '.', 'Many', 'people', 'create', 'Python', 'dictionary', 'containing', 'variables', 'model', 'pass', 'around', 'every', 'function', '.', 'Others', 'create', 'class', 'module', 'e.g.', 'ReLU', 'class', 'using', 'classvariables', 'handle', 'shared', 'parameter', '.', 'Yet', 'another', 'option', 'set', 'shared', 'vari…', 'able', 'attribute', 'relu', 'function', 'upon', 'first', 'call', 'like', 'so:248', '|', 'Chapter', '9', 'Up', 'Running', 'TensorFlow', '5Creating', 'ReLU', 'class', 'arguably', 'cleanest', 'option', 'rather', 'heavyweight', '.', 'def', 'relu', 'X', 'tf.name_scope', '``', 'relu', \"''\", 'hasattr', 'relu', '``', 'threshold', \"''\", 'relu.threshold', '=', 'tf.Variable', '0.0', 'name=', \"''\", 'threshold', \"''\", '...', 'return', 'tf.maximum', 'z', 'relu.threshold', 'name=', \"''\", 'max', \"''\", 'TensorFlow', 'offers', 'another', 'option', 'may', 'lead', 'slightly', 'cleaner', 'mod…', 'ular', 'code', 'previous', 'solutions.5', 'This', 'solution', 'bit', 'tricky', 'understand', 'first', 'since', 'used', 'lot', 'TensorFlow', 'worth', 'going', 'bit', 'detail', '.', 'The', 'idea', 'use', 'get_variable', 'function', 'create', 'shared', 'variable', 'exist', 'yet', 'reuse', 'already', 'exists', '.', 'The', 'desired', 'behavior', 'creating', 'reusing', 'controlled', 'attribute', 'current', 'variable_scope', '.', 'For', 'example', 'follow…', 'ing', 'code', 'create', 'variable', 'named', \"''\", 'relu/threshold', \"''\", 'scalar', 'since', 'shape=', 'using', '0.0', 'initial', 'value', 'tf.variable_scope', '``', 'relu', \"''\", 'threshold', '=', 'tf.get_variable', '``', 'threshold', \"''\", 'shape=', 'initializer=tf.constant_initializer', '0.0', 'Note', 'variable', 'already', 'created', 'earlier', 'call', 'get_variable', 'code', 'raise', 'exception', '.', 'This', 'behavior', 'prevents', 'reusing', 'variables', 'mistake', '.', 'If', 'want', 'reuse', 'variable', 'need', 'explicitly', 'say', 'setting', 'variable', 'scope‡s', 'reuse', 'attribute', 'True', 'case', 'don‡t', 'specify', 'shape', 'initializer', 'tf.variable_scope', '``', 'relu', \"''\", 'reuse=True', 'threshold', '=', 'tf.get_variable', '``', 'threshold', \"''\", 'This', 'code', 'fetch', 'existing', '``', 'relu/threshold', \"''\", 'variable', 'raise', 'exception', 'exist', 'created', 'using', 'get_variable', '.', 'Alternatively', 'set', 'reuse', 'attribute', 'True', 'inside', 'block', 'calling', 'scope‡s', 'reuse_variables', 'method', 'tf.variable_scope', '``', 'relu', \"''\", 'scope', 'scope.reuse_variables', 'threshold', '=', 'tf.get_variable', '``', 'threshold', \"''\", 'Once', 'reuse', 'set', 'True', 'set', 'back', 'False', 'within', 'block', '.', 'Moreover', 'define', 'variable', 'scopes', 'inside', 'one', 'automatically', 'inherit', 'reuse=True', '.', 'Lastly', 'variables', 'created', 'get_variable', 'reused', 'way', '.', 'Sharing', 'Variables', '|', '249', 'Now', 'pieces', 'need', 'make', 'relu', 'function', 'access', 'threshold', 'variable', 'without', 'pass', 'parameter', 'def', 'relu', 'X', 'tf.variable_scope', '``', 'relu', \"''\", 'reuse=True', 'threshold', '=', 'tf.get_variable', '``', 'threshold', \"''\", '#', 'reuse', 'existing', 'variable', '...', 'return', 'tf.maximum', 'z', 'threshold', 'name=', \"''\", 'max', \"''\", 'X', '=', 'tf.placeholder', 'tf.float32', 'shape=', 'None', 'n_features', 'name=', \"''\", 'X', \"''\", 'tf.variable_scope', '``', 'relu', \"''\", '#', 'create', 'variable', 'threshold', '=', 'tf.get_variable', '``', 'threshold', \"''\", 'shape=', 'initializer=tf.constant_initializer', '0.0', 'relus', '=', 'relu', 'X', 'relu_index', 'range', '5', 'output', '=', 'tf.add_n', 'relus', 'name=', \"''\", 'output', \"''\", 'This', 'code', 'first', 'defines', 'relu', 'function', 'creates', 'relu/threshold', 'variable', 'scalar', 'later', 'initialized', '0.0', 'builds', 'five', 'ReLUs', 'calling', 'relu', 'function', '.', 'The', 'relu', 'function', 'reuses', 'relu/threshold', 'variable', 'cre…ates', 'ReLU', 'nodes', '.', 'Variables', 'created', 'using', 'get_variable', 'always', 'named', 'using', 'name', 'variable_scope', 'prefix', 'e.g.', '``', 'relu/threshold', \"''\", 'nodes', 'including', 'variables', 'created', 'tf.Variable', 'variable', 'scope', 'acts', 'like', 'new', 'name', 'scope', '.', 'Inparticular', 'name', 'scope', 'identical', 'name', 'already', 'cre…', 'ated', 'suffix', 'added', 'make', 'name', 'unique', '.', 'For', 'example', 'nodes', 'created', 'preceding', 'code', 'except', 'threshold', 'vari…', 'able', 'name', 'prefixed', '``', 'relu_1/', \"''\", \"''\", 'relu_5/', \"''\", 'shownin', 'Figure', '9-8.Figure', '9-8', '.', 'Five', 'ReLUs', 'sharing', 'threshold', 'variable', 'It', 'somewhat', 'unfortunate', 'threshold', 'variable', 'must', 'defined', 'outside', 'relu', 'function', 'rest', 'ReLU', 'code', 'resides', '.', 'To', 'fix', 'following', 'code', 'creates', 'threshold', 'variable', 'within', 'relu', 'function', 'upon', 'first', 'call', 'reuses', 'subsequent', 'calls', '.', 'Now', 'relu', 'function', 'worry', 'name', 'scopes', 'variable', 'sharing', 'calls', 'get_variable', 'create', '250', '|', 'Chapter', '9', 'Up', 'Running', 'TensorFlow', 'reuse', 'threshold', 'variable', 'need', 'know', 'case', '.', 'The', 'restof', 'code', 'calls', 'relu', 'five', 'times', 'making', 'sure', 'set', 'reuse=False', 'first', 'call', 'reuse=True', 'calls.def', 'relu', 'X', 'threshold', '=', 'tf.get_variable', '``', 'threshold', \"''\", 'shape=', 'initializer=tf.constant_initializer', '0.0', '...', 'return', 'tf.maximum', 'z', 'threshold', 'name=', \"''\", 'max', \"''\", 'X', '=', 'tf.placeholder', 'tf.float32', 'shape=', 'None', 'n_features', 'name=', \"''\", 'X', \"''\", 'relus', '=', 'relu_index', 'range', '5', 'tf.variable_scope', '``', 'relu', \"''\", 'reuse=', 'relu_index', '>', '=', '1', 'scope', 'relus.append', 'relu', 'X', 'output', '=', 'tf.add_n', 'relus', 'name=', \"''\", 'output', \"''\", 'The', 'resulting', 'graph', 'slightly', 'different', 'since', 'shared', 'variable', 'lives', 'within', 'first', 'ReLU', 'see', 'Figure', '9-9', '.Figure', '9-9', '.', 'Five', 'ReLUs', 'sharing', 'threshold', 'variable', 'This', 'concludes', 'introduction', 'TensorFlow', '.', 'We', 'discuss', 'advanced', 'top…', 'ics', 'go', 'following', 'chapters', 'particular', 'many', 'operations', 'related', 'deep', 'neural', 'networks', 'convolutional', 'neural', 'networks', 'recurrent', 'neural', 'networks', 'well', 'scale', 'TensorFlow', 'using', 'multithreading', 'queues', 'multiple', 'GPUs', 'multiple', 'servers', '.', 'Exercises1.What', 'main', 'benefits', 'creating', 'computation', 'graph', 'rather', 'directly', 'executing', 'computations', '?', 'What', 'main', 'drawbacks', '?', '2.Is', 'statement', 'a_val', '=', 'a.eval', 'session=sess', 'equivalent', 'a_val', '=sess.run', '?', '3.Is', 'statement', 'a_val', 'b_val', '=', 'a.eval', 'session=sess', 'b.eval', 'session=sess', 'equivalent', 'a_val', 'b_val', '=', 'sess.run', 'b', '?', '4.Can', 'run', 'two', 'graphs', 'session', '?', 'Exercises', '|', '251', '5.If', 'create', 'graph', 'g', 'containing', 'variable', 'w', 'start', 'two', 'threads', 'open', 'asession', 'thread', 'using', 'graph', 'g', 'session', 'copy', 'variable', 'w', 'shared', '?', '6.When', 'variable', 'initialized', '?', 'When', 'destroyed', '?', '7.What', 'difference', 'placeholder', 'variable', '?', '8.What', 'happens', 'run', 'graph', 'evaluate', 'operation', 'depends', 'placeholder', 'don‡t', 'feed', 'value', '?', 'What', 'happens', 'operation', 'depend', 'placeholder', '?', '9.When', 'run', 'graph', 'feed', 'output', 'value', 'operation', 'value', 'placeholders', '?', '10.How', 'set', 'variable', 'value', 'want', 'execution', 'phase', '?', '11.How', 'many', 'times', 'reverse-mode', 'autodiff', 'need', 'traverse', 'graph', 'order', 'compute', 'gradients', 'cost', 'function', 'regards', '10', 'variables', '?', 'What', 'forward-mode', 'autodiff', '?', 'And', 'symbolic', 'differentiation', '?', '12.Implement', 'Logistic', 'Regression', 'Mini-batch', 'Gradient', 'Descent', 'using', 'Tensor…', 'Flow', '.', 'Train', 'evaluate', 'moons', 'dataset', 'introduced', 'Chapter', '5', '.', 'Try', 'adding', 'bells', 'whistles', '‹Define', 'graph', 'within', 'logistic_regression', 'function', 'reused', 'easily', '.', '‹Save', 'checkpoints', 'using', 'Saver', 'regular', 'intervals', 'training', 'save', 'final', 'model', 'end', 'training', '.', '‹Restore', 'last', 'checkpoint', 'upon', 'startup', 'training', 'interrupted', '.', '‹Define', 'graph', 'using', 'nice', 'scopes', 'graph', 'looks', 'good', 'TensorBoard', '.', '‹Add', 'summaries', 'visualize', 'learning', 'curves', 'TensorBoard', '.', '‹Try', 'tweaking', 'hyperparameters', 'learning', 'rate', 'mini-', 'batch', 'size', 'look', 'shape', 'learning', 'curve', '.', 'Solutions', 'exercises', 'available', 'Appendix', 'A', '.252', '|', 'Chapter', '9', 'Up', 'Running', 'TensorFlow', '1You', 'get', 'best', 'worlds', 'open', 'biological', 'inspirations', 'without', 'afraid', 'create', 'biologically', 'unrealistic', 'models', 'long', 'work', 'well.CHAPTER', '10Introduction', 'Arti•cial', 'Neural', 'NetworksBirds', 'inspired', 'us', 'fly', 'burdock', 'plants', 'inspired', 'velcro', 'nature', 'inspired', 'many', 'inventions', '.', 'It', 'seems', 'logical', 'look', 'brain‡s', 'architecture', 'inspiration', 'build', 'intelligent', 'machine', '.', 'This', 'key', 'idea', 'inspired', 'arti†cial', 'neural', 'networks', 'ANNs', '.', 'However', 'although', 'planes', 'inspired', 'birds', 'don‡t', 'flap', 'wings', '.', 'Similarly', 'ANNs', 'gradually', 'become', 'quite', 'dif…', 'ferent', 'biological', 'cousins', '.', 'Some', 'researchers', 'even', 'argue', 'drop', 'biological', 'analogy', 'altogether', 'e.g.', 'saying', 'ƒunits⁄', 'rather', 'ƒneurons⁄', 'lest', 'restrict', 'creativity', 'biologically', 'plausible', 'systems', '.', '1ANNs', 'core', 'Deep', 'Learning', '.', 'They', 'versatile', 'powerful', 'scala…', 'ble', 'making', 'ideal', 'tackle', 'large', 'highly', 'complex', 'Machine', 'Learning', 'tasks', 'classifying', 'billions', 'images', 'e.g.', 'Google', 'Images', 'powering', 'speech', 'recogni…', 'tion', 'services', 'e.g.', 'Apple‡s', 'Siri', 'recommending', 'best', 'videos', 'watch', 'hundreds', 'millions', 'users', 'every', 'day', 'e.g.', 'YouTube', 'learning', 'beat', 'world', 'champion', 'game', 'Go', 'examining', 'millions', 'past', 'games', 'playing', 'DeepMind‡s', 'AlphaGo', '.', 'In', 'chapter', 'introduce', 'artificial', 'neural', 'networks', 'starting', 'quick', 'tour', 'first', 'ANN', 'architectures', '.', 'Then', 'present', 'Multi-Layer', 'Perceptrons', 'MLPs', 'implement', 'one', 'using', 'TensorFlow', 'tackle', 'MNIST', 'digit', 'classification', 'problem', 'introduced', 'Chapter', '3', '.2532ƒA', 'Logical', 'Calculus', 'Ideas', 'Immanent', 'Nervous', 'Activity', '⁄', 'W.', 'McCulloch', 'W.', 'Pitts', '1943', '.', 'From', 'Biological', 'Arti•cial', 'NeuronsSurprisingly', 'ANNs', 'around', 'quite', 'first', 'introduced', 'back', '1943', 'neurophysiologist', 'Warren', 'McCulloch', 'mathematician', 'Walter', 'Pitts', '.', 'In', 'landmark', 'paper', ',2', 'ƒA', 'Logical', 'Calculus', 'Ideas', 'Immanent', 'Nervous', 'Activity', '⁄', 'McCulloch', 'Pitts', 'presented', 'simplified', 'computational', 'model', 'biological', 'neurons', 'might', 'work', 'together', 'animal', 'brains', 'perform', 'complex', 'computations', 'using', 'propositional', 'logic', '.', 'This', 'first', 'artificial', 'neural', 'networkarchitecture', '.', 'Since', 'many', 'architectures', 'invented', 'see', '.', 'The', 'early', 'successes', 'ANNs', '1960s', 'led', 'widespread', 'belief', 'would', 'soon', 'conversing', 'truly', 'intelligent', 'machines', '.', 'When', 'became', 'clear', 'promise', 'would', 'go', 'unfulfilled', 'least', 'quite', 'funding', 'flew', 'elsewhere', 'ANNs', 'entered', 'long', 'dark', 'era', '.', 'In', 'early', '1980s', 'revival', 'interest', 'ANNs', 'new', 'network', 'architectures', 'invented', 'better', 'training', 'techniques', 'developed', '.', 'But', '1990s', 'powerful', 'alternative', 'Machine', 'Learning', 'techniques', 'Support', 'Vector', 'Machines', 'see', 'Chapter', '5', 'favored', 'researchers', 'seemed', 'offer', 'better', 'results', 'stronger', 'theoretical', 'foundations', '.', 'Finally', 'witnessing', 'yet', 'another', 'wave', 'interest', 'ANNs', '.', 'Will', 'wave', 'die', 'like', 'previous', 'ones', '?', 'There', 'good', 'reasons', 'believe', 'one', 'differ…', 'ent', 'much', 'profound', 'impact', 'lives', '‹There', 'huge', 'quantity', 'data', 'available', 'train', 'neural', 'networks', 'ANNs', 'frequently', 'outperform', 'ML', 'techniques', 'large', 'complex', 'problems.‹The', 'tremendous', 'increase', 'computing', 'power', 'since', '1990s', 'makes', 'pos…', 'sible', 'train', 'large', 'neural', 'networks', 'reasonable', 'amount', 'time', '.', 'This', 'part', 'due', 'Moore‡s', 'Law', 'also', 'thanks', 'gaming', 'industry', 'pro…', 'duced', 'powerful', 'GPU', 'cards', 'millions.‹The', 'training', 'algorithms', 'improved', '.', 'To', 'fair', 'slightly', 'dif…', 'ferent', 'ones', 'used', '1990s', 'relatively', 'small', 'tweaks', 'huge', 'positive', 'impact', '.', '‹Some', 'theoretical', 'limitations', 'ANNs', 'turned', 'benign', 'practice', '.', 'For', 'example', 'many', 'people', 'thought', 'ANN', 'training', 'algorithms', 'doomed', 'likely', 'get', 'stuck', 'local', 'optima', 'turns', 'rather', 'rare', 'practice', 'case', 'usually', 'fairly', 'close', 'global', 'optimum', '.', '‹ANNs', 'seem', 'entered', 'virtuous', 'circle', 'funding', 'progress', '.', 'Amazing', 'products', 'based', 'ANNs', 'regularly', 'make', 'headline', 'news', 'pulls', '254', '|', 'Chapter', '10', 'Introduction', 'Arti•cial', 'Neural', 'Networks3Image', 'Bruce', 'Blaus', 'Creative', 'Commons', '3.0', '.', 'Reproduced', 'https', '//en.wikipedia.org/wiki/Neuron', '.and', 'attention', 'funding', 'toward', 'resulting', 'pro…', 'gress', 'even', 'amazing', 'products.Biological', 'NeuronsBefore', 'discuss', 'artificial', 'neurons', 'let‡s', 'take', 'quick', 'look', 'biological', 'neuron', 'rep…', 'resented', 'Figure', '10-1', '.', 'It', 'unusual-looking', 'cell', 'mostly', 'found', 'animal', 'cerebral', 'cortexes', 'e.g.', 'brain', 'composed', 'cell', 'body', 'containing', 'nucleus', 'cell‡s', 'complex', 'components', 'many', 'branching', 'extensions', 'called', 'dendrites', 'plus', 'one', 'long', 'extension', 'called', 'axon', '.', 'The', 'axon‡s', 'length', 'may', 'times', 'longer', 'cell', 'body', 'tens', 'thousands', 'times', 'longer', '.', 'Near', 'extremity', 'axon', 'splits', 'many', 'branches', 'called', 'telodendria', 'tip', 'branches', 'minuscule', 'structures', 'called', 'synaptic', 'terminals', 'simply', 'synap…', 'ses', 'connected', 'dendrites', 'directly', 'cell', 'body', 'neu…rons', '.', 'Biological', 'neurons', 'receive', 'short', 'electrical', 'impulses', 'called', 'signals', 'otherneurons', 'via', 'synapses', '.', 'When', 'neuron', 'receives', 'sufficient', 'number', 'signals', 'neurons', 'within', 'milliseconds', 'fires', 'signals.Figure', '10-1', '.', 'Biological', 'neuron', '3Thus', 'individual', 'biological', 'neurons', 'seem', 'behave', 'rather', 'simple', 'way', 'organized', 'vast', 'network', 'billions', 'neurons', 'neuron', 'typically', 'connec…ted', 'thousands', 'neurons', '.', 'Highly', 'complex', 'computations', 'performed', 'vast', 'network', 'fairly', 'simple', 'neurons', 'much', 'like', 'complex', 'anthill', 'emerge', 'combined', 'efforts', 'simple', 'ants', '.', 'The', 'architecture', 'biological', 'neural', 'net…', 'From', 'Biological', 'Arti•cial', 'Neurons', '|', '255', '4In', 'context', 'Machine', 'Learning', 'phrase', 'ƒneural', 'networks⁄', 'generally', 'refers', 'ANNs', 'BNNs', '.', '5Drawing', 'cortical', 'lamination', 'S.', 'Ramon', 'Cajal', 'public', 'domain', '.', 'Reproduced', 'https', '//en.wikipe', 'dia.org/wiki/Cerebral_cortex', '.works', 'BNN', '4', 'still', 'subject', 'active', 'research', 'parts', 'brain', 'mapped', 'seems', 'neurons', 'often', 'organized', 'consecutive', 'layers', 'shown', 'Figure', '10-2.Figure', '10-2', '.', 'Multiple', 'layers', 'biological', 'neural', 'network', 'human', 'cortex', '5Logical', 'Computations', 'NeuronsWarren', 'McCulloch', 'Walter', 'Pitts', 'proposed', 'simple', 'model', 'biological', 'neuron', 'later', 'became', 'known', 'arti†cial', 'neuron', 'one', 'binary', 'on/off', 'inputs', 'one', 'binary', 'output', '.', 'The', 'artificial', 'neuron', 'simply', 'activates', 'out…', 'put', 'certain', 'number', 'inputs', 'active', '.', 'McCulloch', 'Pitts', 'showed', 'even', 'simplified', 'model', 'possible', 'build', 'network', 'artificial', 'neurons', 'computes', 'logical', 'proposition', 'want', '.', 'For', 'example', 'let‡s', 'build', 'ANNs', 'perform', 'various', 'logical', 'computations', 'see', 'Figure', '10-3', 'assuming', 'neuron', 'activated', 'least', 'two', 'inputs', 'active', '.', 'Figure', '10-3', '.', 'ANNs', 'performing', 'simple', 'logical', 'computations', '256', '|', 'Chapter', '10', 'Introduction', 'Arti•cial', 'Neural', 'Networks‹The', 'first', 'network', 'left', 'simply', 'identity', 'function', 'neuron', 'A', 'activa…', 'ted', 'neuron', 'C', 'gets', 'activated', 'well', 'since', 'receives', 'two', 'input', 'signals', 'neuron', 'A', 'neuron', 'A', 'neuron', 'C', 'well.‹The', 'second', 'network', 'performs', 'logical', 'AND', 'neuron', 'C', 'activated', 'neurons', 'A', 'B', 'activated', 'single', 'input', 'signal', 'enough', 'acti…', 'vate', 'neuron', 'C', '.', '‹The', 'third', 'network', 'performs', 'logical', 'OR', 'neuron', 'C', 'gets', 'activated', 'either', 'neu…', 'ron', 'A', 'neuron', 'B', 'activated', '.', '‹Finally', 'suppose', 'input', 'connection', 'inhibit', 'neuron‡s', 'activity', 'case', 'biological', 'neurons', 'fourth', 'network', 'computes', 'slightly', 'complex', 'logical', 'proposition', 'neuron', 'C', 'activated', 'neuron', 'A', 'active', 'neuron', 'B', '.', 'If', 'neuron', 'A', 'active', 'time', 'get', 'alogical', 'NOT', 'neuron', 'C', 'active', 'neuron', 'B', 'vice', 'versa', '.', 'You', 'easily', 'imagine', 'networks', 'combined', 'compute', 'complex', 'logical', 'expressions', 'see', 'exercises', 'end', 'chapter', '.', 'The', 'PerceptronThe', 'Perceptron', 'one', 'simplest', 'ANN', 'architectures', 'invented', '1957', 'Frank', 'Rosenblatt', '.', 'It', 'based', 'slightly', 'different', 'artificial', 'neuron', 'see', 'Figure', '10-4', 'called', 'linear', 'threshold', 'unit', 'LTU', 'inputs', 'output', 'numbers', 'instead', 'binary', 'on/off', 'values', 'input', 'connection', 'associated', 'weight', '.', 'The', 'LTU', 'computes', 'weighted', 'sum', 'inputs', 'z', '=', 'w1', 'x1', '+', 'w2', 'x2', '+', '+', 'wn', 'xn', '=', 'wT', '’', 'x', 'thenapplies', 'step', 'function', 'sum', 'outputs', 'result', 'hw', 'x', '=', 'step', 'z', '=', 'step', 'wT', '’', 'x', '.Figure', '10-4', '.', 'Linear', 'threshold', 'unit', 'The', 'common', 'step', 'function', 'used', 'Perceptrons', 'Heaviside', 'step', 'function', 'see', 'Equation', '10-1', '.', 'Sometimes', 'sign', 'function', 'used', 'instead.From', 'Biological', 'Arti•cial', 'Neurons', '|', '257', '6The', 'name', 'Perceptron', 'sometimes', 'used', 'mean', 'tiny', 'network', 'single', 'LTU', '.', 'Equation', '10-1', '.', 'Common', 'step', 'functions', 'used', 'Perceptrons', 'heavisidez=0if', 'z', '<', '0', '1if', 'zŠ0', 'sgnz=', '”', '1if', 'z', '<', '0', '0if', 'z=0', '+1if', 'z', '>', '0', 'A', 'single', 'LTU', 'used', 'simple', 'linear', 'binary', 'classification', '.', 'It', 'computes', 'linear', 'combination', 'inputs', 'result', 'exceeds', 'threshold', 'outputs', 'positive', 'class', 'else', 'outputs', 'negative', 'class', 'like', 'Logistic', 'Regression', 'classifier', 'linear', 'SVM', '.', 'For', 'example', 'could', 'use', 'single', 'LTU', 'classify', 'iris', 'flowers', 'based', 'petal', 'length', 'width', 'also', 'adding', 'extra', 'bias', 'feature', 'x0', '=', '1', 'like', 'inprevious', 'chapters', '.', 'Training', 'LTU', 'means', 'finding', 'right', 'values', 'w0', 'w1', 'w2', 'training', 'algorithm', 'discussed', 'shortly', '.A', 'Perceptron', 'simply', 'composed', 'single', 'layer', 'LTUs', '6', 'neuron', 'con…nected', 'inputs', '.', 'These', 'connections', 'often', 'represented', 'using', 'special', 'pass…', 'neurons', 'called', 'input', 'neurons', 'output', 'whatever', 'input', 'fed', '.', 'Moreover', 'extra', 'bias', 'feature', 'generally', 'added', 'x0', '=', '1', '.', 'This', 'bias', 'feature', 'typi…', 'cally', 'represented', 'using', 'special', 'type', 'neuron', 'called', 'bias', 'neuron', 'out…puts', '1', 'time.A', 'Perceptron', 'two', 'inputs', 'three', 'outputs', 'represented', 'Figure', '10-5', '.', 'ThisPerceptron', 'classify', 'instances', 'simultaneously', 'three', 'different', 'binary', 'classes', 'makes', 'multioutput', 'classifier', '.', 'Figure', '10-5', '.', 'Perceptron', 'diagram', 'So', 'Perceptron', 'trained', '?', 'The', 'Perceptron', 'training', 'algorithm', 'proposed', 'Frank', 'Rosenblatt', 'largely', 'inspired', 'Hebb‹s', 'rule', '.', 'In', 'book', '•e', 'Organization', 'Behavior', 'published', '1949', 'Donald', 'Hebb', 'suggested', 'biological', 'neuron', '258', '|', 'Chapter', '10', 'Introduction', 'Arti•cial', 'Neural', 'Networks7Note', 'solution', 'generally', 'unique', 'general', 'data', 'linearly', 'separable', 'infinity', 'hyperplanes', 'separate', '.', 'often', 'triggers', 'another', 'neuron', 'connection', 'two', 'neurons', 'growsstronger', '.', 'This', 'idea', 'later', 'summarized', 'Siegrid', 'Lšwel', 'catchy', 'phrase', 'ƒCells', 'fire', 'together', 'wire', 'together.⁄', 'This', 'rule', 'later', 'became', 'known', 'Hebb‡s', 'rule', 'Hebbian', 'learning', 'connection', 'weight', 'two', 'neurons', 'increased', 'whenever', 'output', '.', 'Perceptrons', 'trained', 'using', 'var…', 'iant', 'rule', 'takes', 'account', 'error', 'made', 'network', 'reinforce', 'connections', 'lead', 'wrong', 'output', '.', 'More', 'specifically', 'Perceptron', 'fed', 'one', 'training', 'instance', 'time', 'instance', 'makes', 'predictions', '.', 'For', 'every', 'output', 'neuron', 'produced', 'wrong', 'prediction', 'reinforces', 'connec…', 'tion', 'weights', 'inputs', 'would', 'contributed', 'correct', 'prediction', '.', 'The', 'rule', 'shown', 'Equation', '10-2', '.Equation', '10-2', '.', 'Perceptron', 'learning', 'rule', 'weight', 'update', 'wi', 'jnextstep', '=wi', 'j+−yj', '”', 'yjxi‹wi', 'j', 'connection', 'weight', 'th', 'input', 'neuron', 'j', 'th', 'output', 'neu…ron.‹xi', 'ith', 'input', 'value', 'current', 'training', 'instance', '.', '‹yj', 'output', 'jth', 'output', 'neuron', 'current', 'training', 'instance', '.', '‹yj', 'target', 'output', 'jth', 'output', 'neuron', 'current', 'training', 'instance', '.', '‹−', 'learning', 'rate', '.', 'The', 'decision', 'boundary', 'output', 'neuron', 'linear', 'Perceptrons', 'incapable', 'learning', 'complex', 'patterns', 'like', 'Logistic', 'Regression', 'classifiers', '.', 'However', 'training', 'instances', 'linearly', 'separable', 'Rosenblatt', 'demonstrated', 'algorithm', 'would', 'converge', 'solution', '.', '7', 'This', 'called', 'Perceptron', 'convergence', 'theorem', '.Scikit-Learn', 'provides', 'Perceptron', 'class', 'implements', 'single', 'LTU', 'network', '.', 'It', 'used', 'pretty', 'much', 'would', 'expect›for', 'example', 'iris', 'dataset', 'intro…', 'duced', 'Chapter', '4', 'import', 'numpy', 'npfrom', 'sklearn.datasets', 'import', 'load_irisfrom', 'sklearn.linear_model', 'import', 'Perceptroniris', '=', 'load_iris', 'X', '=', 'iris.data', '2', '3', '#', 'petal', 'length', 'petal', 'widthy', '=', 'iris.target', '==', '0', '.astype', 'np.int', '#', 'Iris', 'Setosa', '?', 'From', 'Biological', 'Arti•cial', 'Neurons', '|', '259', 'per_clf', '=', 'Perceptron', 'random_state=42', 'per_clf.fit', 'X', 'y_pred', '=', 'per_clf.predict', '2', '0.5', 'You', 'may', 'recognized', 'Perceptron', 'learning', 'algorithm', 'strongly', 'resembles', 'Stochastic', 'Gradient', 'Descent', '.', 'In', 'fact', 'Scikit-Learn‡s', 'Perceptron', 'class', 'equivalent', 'using', 'SGDClassifier', 'following', 'hyperparameters', 'loss=', \"''\", 'perceptron', \"''\", 'learning_rate=', \"''\", 'constant', \"''\", 'eta0=1', 'learning', 'rate', 'penalty=None', 'regu…', 'larization', '.', 'Note', 'contrary', 'Logistic', 'Regression', 'classifiers', 'Perceptrons', 'output', 'class', 'probability', 'rather', 'make', 'predictions', 'based', 'hard', 'threshold', '.', 'This', 'one', 'good', 'reasons', 'prefer', 'Logistic', 'Regression', 'Perceptrons', '.', 'In', '1969', 'monograph', 'titled', 'Perceptrons', 'Marvin', 'Minsky', 'Seymour', 'Papert', 'highlighted', 'number', 'serious', 'weaknesses', 'Perceptrons', 'particular', 'fact', 'incapable', 'solving', 'trivial', 'problems', 'e.g.', 'Exclusive', 'OR', 'XOR', 'classification', 'problem', 'see', 'left', 'side', 'Figure', '10-6', '.', 'Of', 'course', 'true', 'linear', 'classification', 'model', 'well', 'Logistic', 'Regression', 'classifiers', 'researchers', 'expected', 'much', 'Perceptrons', 'disappointment', 'great', 'result', 'many', 'researchers', 'dropped', 'connectionism', 'altogether', 'i.e.', 'thestudy', 'neural', 'networks', 'favor', 'higher-level', 'problems', 'logic', 'problem', 'solving', 'search.However', 'turns', 'limitations', 'Perceptrons', 'eliminated', 'stacking', 'multiple', 'Perceptrons', '.', 'The', 'resulting', 'ANN', 'called', 'Multi-Layer', 'Perceptron', 'MLP', '.', 'In', 'particular', 'MLP', 'solve', 'XOR', 'problem', 'verify', 'com…', 'puting', 'output', 'MLP', 'represented', 'right', 'Figure', '10-6', 'com…bination', 'inputs', 'inputs', '0', '0', '1', '1', 'network', 'outputs', '0', 'inputs', '0', '1', '1', '0', 'outputs', '1.Figure', '10-6', '.', 'XOR', 'classi†cation', 'problem', 'MLP', 'solves', '260', '|', 'Chapter', '10', 'Introduction', 'Arti•cial', 'Neural', 'Networks8ƒLearning', 'Internal', 'Representations', 'Error', 'Propagation', '⁄', 'D.', 'Rumelhart', 'G.', 'Hinton', 'R.', 'Williams', '1986', '.', '9This', 'algorithm', 'actually', 'invented', 'several', 'times', 'various', 'researchers', 'different', 'fields', 'starting', 'P.', 'Werbos', '1974', '.', 'Multi-Layer', 'Perceptron', 'BackpropagationAn', 'MLP', 'composed', 'one', 'passthrough', 'input', 'layer', 'one', 'layers', 'LTUs', 'called', 'hidden', 'layers', 'one', 'final', 'layer', 'LTUs', 'called', 'output', 'layer', 'seeFigure', '10-7', '.', 'Every', 'layer', 'except', 'output', 'layer', 'includes', 'bias', 'neuron', 'fully', 'connected', 'next', 'layer', '.', 'When', 'ANN', 'two', 'hidden', 'layers', 'called', 'deep', 'neural', 'network', 'DNN', '.Figure', '10-7', '.', 'Multi-Layer', 'Perceptron', 'For', 'many', 'years', 'researchers', 'struggled', 'find', 'way', 'train', 'MLPs', 'without', 'success', '.', 'But', '1986', 'D.', 'E.', 'Rumelhart', 'et', 'al', '.', 'published', 'groundbreaking', 'article8', 'introducing', 'backpropagation', 'training', 'algorithm.9', 'Today', 'would', 'describe', 'Gradient', 'Descent', 'using', 'reverse-mode', 'autodiff', 'Gradient', 'Descent', 'introduced', 'Chapter', '4', 'autodiff', 'discussed', 'Chapter', '9', '.For', 'training', 'instance', 'algorithm', 'feeds', 'network', 'computes', 'output', 'every', 'neuron', 'consecutive', 'layer', 'forward', 'pass', 'like', 'making', 'predictions', '.', 'Then', 'measures', 'network‡s', 'output', 'error', 'i.e.', 'dif…', 'ference', 'desired', 'output', 'actual', 'output', 'network', 'itcomputes', 'much', 'neuron', 'last', 'hidden', 'layer', 'contributed', 'output', 'neuron‡s', 'error', '.', 'It', 'proceeds', 'measure', 'much', 'error', 'contributions', 'came', 'neuron', 'previous', 'hidden', 'layer›and', 'algorithm', 'reaches', 'input', 'layer', '.', 'This', 'reverse', 'pass', 'efficiently', 'measures', 'error', 'gradient', 'across', 'connection', 'weights', 'network', 'propagating', 'error', 'gradient', 'backward', 'network', 'hence', 'name', 'algorithm', '.', 'If', 'check', 'theFrom', 'Biological', 'Arti•cial', 'Neurons', '|', '261', 'reverse-mode', 'autodiff', 'algorithm', 'Appendix', 'D', 'find', 'forward', 'reverse', 'passes', 'backpropagation', 'simply', 'perform', 'reverse-mode', 'autodiff', '.', 'The', 'last', 'step', 'backpropagation', 'algorithm', 'Gradient', 'Descent', 'step', 'connec…', 'tion', 'weights', 'network', 'using', 'error', 'gradients', 'measured', 'earlier', '.', 'Let‡s', 'make', 'even', 'shorter', 'training', 'instance', 'backpropagation', 'algo…', 'rithm', 'first', 'makes', 'prediction', 'forward', 'pass', 'measures', 'error', 'goes', 'layer', 'reverse', 'measure', 'error', 'contribution', 'connection', 'reverse', 'pass', 'finally', 'slightly', 'tweaks', 'connection', 'weights', 'reduce', 'error', 'Gradient', 'Descent', 'step', '.', 'In', 'order', 'algorithm', 'work', 'properly', 'authors', 'made', 'key', 'change', 'MLP‡s', 'architecture', 'replaced', 'step', 'function', 'logistic', 'function', '„', 'z', '=1', '/', '1', '+', 'exp', '–z', '.', 'This', 'essential', 'step', 'function', 'contains', 'flat', 'seg…', 'ments', 'gradient', 'work', 'Gradient', 'Descent', 'move', 'flat', 'surface', 'logistic', 'function', 'well-defined', 'nonzero', 'derivative', 'every…', 'allowing', 'Gradient', 'Descent', 'make', 'progress', 'every', 'step', '.', 'The', 'backpro…', 'pagation', 'algorithm', 'may', 'used', 'activation', 'functions', 'instead', 'logisticfunction', '.', 'Two', 'popular', 'activation', 'functions', '•e', 'hyperbolic', 'tangent', 'function', 'tanh', 'z', '=', '2„', '2z', '‘', '1', 'Just', 'like', 'logistic', 'function', 'S-shaped', 'continuous', 'differentiable', 'output', 'value', 'ranges', '–1', '1', 'instead', '0', '1', 'case', 'logistic', 'func…tion', 'tends', 'make', 'layer‡s', 'output', 'less', 'normalized', 'i.e.', 'cen…', 'tered', 'around', '0', 'beginning', 'training', '.', 'This', 'often', 'helps', 'speed', 'convergence', '.', '•e', 'ReLU', 'function', 'introduced', 'Chapter', '9', 'ReLU', 'z', '=', 'max', '0', 'z', '.', 'It', 'continuous', 'unfortunately', 'differentiable', 'z', '=', '0', 'slope', 'changes', 'abruptly', 'make', 'Gradient', 'Descent', 'bounce', 'around', '.', 'However', 'practice', 'works', 'well', 'advantage', 'fast', 'compute', '.', 'Most', 'importantly', 'fact', 'maximum', 'out…', 'put', 'value', 'also', 'helps', 'reduce', 'issues', 'Gradient', 'Descent', 'come', 'back', 'Chapter', '11', '.These', 'popular', 'activation', 'functions', 'derivatives', 'represented', 'Figure', '10-8.262', '|', 'Chapter', '10', 'Introduction', 'Arti•cial', 'Neural', 'NetworksFigure', '10-8', '.', 'Activation', 'functions', 'derivatives', 'An', 'MLP', 'often', 'used', 'classification', 'output', 'corresponding', 'different', 'binary', 'class', 'e.g.', 'spam/ham', 'urgent/not-urgent', '.', 'When', 'classes', 'exclusive', 'e.g.', 'classes', '0', '9', 'digit', 'image', 'classification', 'output', 'layer', 'typically', 'modified', 'replacing', 'individual', 'activation', 'functions', 'shared', '“', '…max', 'function', 'see', 'Figure', '10-9', '.', 'The', 'softmax', 'function', 'introduced', 'Chapter', '3', '.The', 'output', 'neuron', 'corresponds', 'estimated', 'probability', 'corre…', 'sponding', 'class', '.', 'Note', 'signal', 'flows', 'one', 'direction', 'inputs', 'outputs', 'architecture', 'example', 'feedforward', 'neural', 'network', 'FNN', '.Figure', '10-9', '.', 'A', 'modern', 'MLP', 'including', 'ReLU', '“', 'max', 'classi†cationFrom', 'Biological', 'Arti•cial', 'Neurons', '|', '263', 'Biological', 'neurons', 'seem', 'implement', 'roughly', 'sigmoid', 'S-', 'shaped', 'activation', 'function', 'researchers', 'stuck', 'sigmoid', 'func…', 'tions', 'long', 'time', '.', 'But', 'turns', 'ReLU', 'activation', 'function', 'generally', 'works', 'better', 'ANNs', '.', 'This', 'one', 'cases', 'biological', 'analogy', 'misleading', '.', 'Training', 'MLP', 'TensorFlow‡s', 'High-Level', 'APIThe', 'simplest', 'way', 'train', 'MLP', 'TensorFlow', 'use', 'high-level', 'API', 'TF.Learn', 'quite', 'similar', 'Scikit-Learn‡s', 'API', '.', 'The', 'DNNClassifier', 'classmakes', 'trivial', 'train', 'deep', 'neural', 'network', 'number', 'hidden', 'layers', 'softmax', 'output', 'layer', 'output', 'estimated', 'class', 'probabilities', '.', 'For', 'example', 'fol…', 'lowing', 'code', 'trains', 'DNN', 'classification', 'two', 'hidden', 'layers', 'one', '300', 'neurons', '100', 'neurons', 'softmax', 'output', 'layer', '10', 'neurons', 'import', 'tensorflow', 'tffeature_columns', '=', 'tf.contrib.learn.infer_real_valued_columns_from_input', 'X_train', 'dnn_clf', '=', 'tf.contrib.learn.DNNClassifier', 'hidden_units=', '300', '100', 'n_classes=10', 'feature_columns=feature_columns', 'dnn_clf.fit', 'x=X_train', 'y=y_train', 'batch_size=50', 'steps=40000', 'If', 'run', 'code', 'MNIST', 'dataset', 'scaling', 'e.g.', 'using', 'Scikit-', 'Learn‡s', 'StandardScaler', 'may', 'actually', 'get', 'model', 'achieves', '98.1', '%', 'accuracy', 'test', 'set', '!', 'That‡s', 'better', 'best', 'model', 'trained', 'Chapter', '3', '>', '>', '>', 'sklearn.metrics', 'import', 'accuracy_score', '>', '>', '>', 'y_pred', '=', 'list', 'dnn_clf.predict', 'X_test', '>', '>', '>', 'accuracy_score', 'y_test', 'y_pred', '0.98180000000000001The', 'TF.Learn', 'library', 'also', 'provides', 'convenience', 'functions', 'evaluate', 'models', '>', '>', '>', 'dnn_clf.evaluate', 'X_test', 'y_test', '{', '•accuracy•', '0.98180002', '•global_step•', '40000', '•loss•', '0.073678359', '}', 'Under', 'hood', 'DNNClassifier', 'class', 'creates', 'neuron', 'layers', 'based', 'ReLU', 'activation', 'function', 'change', 'setting', 'activation_fn', 'hyper…', 'parameter', '.', 'The', 'output', 'layer', 'relies', 'softmax', 'function', 'cost', 'function', 'cross', 'entropy', 'introduced', 'Chapter', '4', '.The', 'TF.Learn', 'API', 'still', 'quite', 'new', 'names', 'func…', 'tions', 'used', 'examples', 'may', 'evolve', 'bit', 'time', 'read', 'book', '.', 'However', 'general', 'ideas', 'change', '.', '264', '|', 'Chapter', '10', 'Introduction', 'Arti•cial', 'Neural', 'NetworksTraining', 'DNN', 'Using', 'Plain', 'TensorFlowIf', 'want', 'control', 'architecture', 'network', 'may', 'prefer', 'use', 'TensorFlow‡s', 'lower-level', 'Python', 'API', 'introduced', 'Chapter', '9', '.', 'In', 'section', 'wewill', 'build', 'model', 'using', 'API', 'implement', 'Mini-', 'batch', 'Gradient', 'Descent', 'train', 'MNIST', 'dataset', '.', 'The', 'first', 'step', 'con…', 'struction', 'phase', 'building', 'TensorFlow', 'graph', '.', 'The', 'second', 'step', 'execution', 'phase', 'actually', 'run', 'graph', 'train', 'model', '.', 'Construction', 'PhaseLet‡s', 'start', '.', 'First', 'need', 'import', 'tensorflow', 'library', '.', 'Then', 'must', 'specify', 'number', 'inputs', 'outputs', 'set', 'number', 'hidden', 'neurons', 'layer', 'import', 'tensorflow', 'tfn_inputs', '=', '28*28', '#', 'MNISTn_hidden1', '=', '300n_hidden2', '=', '100n_outputs', '=', '10Next', 'like', 'Chapter', '9', 'use', 'placeholder', 'nodes', 'represent', 'training', 'data', 'targets', '.', 'The', 'shape', 'X', 'partially', 'defined', '.', 'We', 'know', '2D', 'tensor', 'i.e.', 'matrix', 'instances', 'along', 'first', 'dimension', 'features', 'along', 'second', 'dimension', 'know', 'number', 'features', 'going', '28', 'x', '28', 'one', 'feature', 'per', 'pixel', 'don‡t', 'know', 'yet', 'many', 'instances', 'train…', 'ing', 'batch', 'contain', '.', 'So', 'shape', 'X', 'None', 'n_inputs', '.', 'Similarly', 'know', '1D', 'tensor', 'one', 'entry', 'per', 'instance', 'don‡t', 'know', 'size', 'training', 'batch', 'point', 'shape', 'None', '.X', '=', 'tf.placeholder', 'tf.float32', 'shape=', 'None', 'n_inputs', 'name=', \"''\", 'X', \"''\", '=', 'tf.placeholder', 'tf.int64', 'shape=', 'None', 'name=', \"''\", \"''\", 'Now', 'let‡s', 'create', 'actual', 'neural', 'network', '.', 'The', 'placeholder', 'X', 'act', 'input', 'layer', 'execution', 'phase', 'replaced', 'one', 'training', 'batch', 'time', 'note', 'instances', 'training', 'batch', 'processed', 'simultaneously', 'neural', 'network', '.', 'Now', 'need', 'create', 'two', 'hidden', 'layers', 'output', 'layer', '.', 'The', 'two', 'hidden', 'layers', 'almost', 'identical', 'differ', 'inputs', 'connected', 'number', 'neurons', 'contain', '.', 'The', 'output', 'layer', 'also', 'similar', 'uses', 'softmax', 'activation', 'function', 'instead', 'ReLU', 'activation', 'function', '.', 'So', 'let‡s', 'create', 'neuron_layer', 'function', 'use', 'create', 'one', 'layer', 'time', '.', 'It', 'need', 'parameters', 'specify', 'inputs', 'number', 'neurons', 'activation', 'function', 'name', 'layer', 'def', 'neuron_layer', 'X', 'n_neurons', 'name', 'activation=None', 'tf.name_scope', 'name', 'n_inputs', '=', 'int', 'X.get_shape', '1', 'Training', 'DNN', 'Using', 'Plain', 'TensorFlow', '|', '265', '10Using', 'truncated', 'normal', 'distribution', 'rather', 'regular', 'normal', 'distribution', 'ensures', 'won‡t', 'large', 'weights', 'could', 'slow', 'training', '.', '11For', 'example', 'set', 'weights', '0', 'neurons', 'output', '0', 'error', 'gradient', 'neurons', 'given', 'hidden', 'layer', '.', 'The', 'Gradient', 'Descent', 'step', 'update', 'weights', 'exactly', 'way', 'layer', 'remain', 'equal', '.', 'In', 'words', 'despite', 'hundreds', 'neurons', 'per', 'layer', 'model', 'act', 'one', 'neuron', 'per', 'layer', '.', 'It', 'going', 'fly', '.', 'stddev', '=', '2', '/', 'np.sqrt', 'n_inputs', 'init', '=', 'tf.truncated_normal', 'n_inputs', 'n_neurons', 'stddev=stddev', 'W', '=', 'tf.Variable', 'init', 'name=', \"''\", 'weights', \"''\", 'b', '=', 'tf.Variable', 'tf.zeros', 'n_neurons', 'name=', \"''\", 'biases', \"''\", 'z', '=', 'tf.matmul', 'X', 'W', '+', 'b', 'activation==', \"''\", 'relu', \"''\", 'return', 'tf.nn.relu', 'z', 'else', 'return', 'zLet‡s', 'go', 'code', 'line', 'line', '1.First', 'create', 'name', 'scope', 'using', 'name', 'layer', 'contain', 'computation', 'nodes', 'neuron', 'layer', '.', 'This', 'optional', 'graph', 'look', 'much', 'nicer', 'TensorBoard', 'nodes', 'well', 'organized', '.', '2.Next', 'get', 'number', 'inputs', 'looking', 'input', 'matrix‡s', 'shape', 'getting', 'size', 'second', 'dimension', 'first', 'dimension', 'instances', '.3.The', 'next', 'three', 'lines', 'create', 'W', 'variable', 'hold', 'weights', 'matrix', '.', 'It', '2D', 'tensor', 'containing', 'connection', 'weights', 'input', 'neuron', 'hence', 'shape', 'n_inputs', 'n_neurons', '.', 'It', 'initialized', 'randomly', 'using', 'truncated', '10', 'normal', 'Gaussian', 'distribution', 'standard', 'deviation', '2/', 'ninputs', '.', 'Using', 'specific', 'standard', 'deviation', 'helps', 'algorithm', 'converge', 'much', 'faster', 'discuss', 'Chapter', '11', 'one', 'thosesmall', 'tweaks', 'neural', 'networks', 'tremendous', 'impact', 'effi…', 'ciency', '.', 'It', 'important', 'initialize', 'connection', 'weights', 'randomly', 'hidden', 'layers', 'avoid', 'symmetries', 'Gradient', 'Descent', 'algorithm', 'would', 'unable', 'break.114.The', 'next', 'line', 'creates', 'b', 'variable', 'biases', 'initialized', '0', 'symmetry', 'issue', 'case', 'one', 'bias', 'parameter', 'per', 'neuron.5.Then', 'create', 'subgraph', 'compute', 'z', '=', 'X', '’', 'W', '+', 'b', '.', 'This', 'vectorized', 'implemen…', 'tation', 'efficiently', 'compute', 'weighted', 'sums', 'inputs', 'plus', 'bias', 'term', 'every', 'neuron', 'layer', 'instances', 'batch', 'one', 'shot.6.Finally', 'activation', 'parameter', 'set', '``', 'relu', \"''\", 'code', 'returns', 'relu', 'z', 'i.e.', 'max', '0', 'z', 'else', 'returns', 'z.266', '|', 'Chapter', '10', 'Introduction', 'Arti•cial', 'Neural', 'NetworksOkay', 'nice', 'function', 'create', 'neuron', 'layer', '.', 'Let‡s', 'use', 'create', 'deep', 'neural', 'network', '!', 'The', 'first', 'hidden', 'layer', 'takes', 'X', 'input', '.', 'The', 'second', 'takes', 'output', 'first', 'hidden', 'layer', 'input', '.', 'And', 'finally', 'output', 'layer', 'takes', 'output', 'second', 'hidden', 'layer', 'input', '.', 'tf.name_scope', '``', 'dnn', \"''\", 'hidden1', '=', 'neuron_layer', 'X', 'n_hidden1', '``', 'hidden1', \"''\", 'activation=', \"''\", 'relu', \"''\", 'hidden2', '=', 'neuron_layer', 'hidden1', 'n_hidden2', '``', 'hidden2', \"''\", 'activation=', \"''\", 'relu', \"''\", 'logits', '=', 'neuron_layer', 'hidden2', 'n_outputs', '``', 'outputs', \"''\", 'Notice', 'used', 'name', 'scope', 'clarity', '.', 'Also', 'note', 'logits', 'theoutput', 'neural', 'network', 'going', 'softmax', 'activation', 'function', 'optimization', 'reasons', 'handle', 'softmax', 'computation', 'later', '.', 'As', 'might', 'expect', 'TensorFlow', 'comes', 'many', 'handy', 'functions', 'create', 'standard', 'neural', 'network', 'layers', 'there‡s', 'often', 'need', 'define', 'neuron_layer', 'function', 'like', '.', 'For', 'example', 'TensorFlow‡s', 'fully_connected', 'function', 'creates', 'fully', 'connected', 'layer', 'inputs', 'connected', 'neurons', 'layer', '.', 'It', 'takes', 'care', 'creating', 'weights', 'biases', 'variables', 'proper', 'initialization', 'strategy', 'uses', 'ReLU', 'activation', 'function', 'default', 'change', 'using', 'activation_fn', 'argument', '.', 'As', 'see', 'Chapter', '11', 'also', 'supports', 'regularization', 'normalization', 'parameters', '.', 'Let‡s', 'tweak', 'preceding', 'code', 'use', 'fully_connected', 'function', 'instead', 'neuron_layer', 'function', '.', 'Simply', 'import', 'function', 'replace', 'dnn', 'constructionsection', 'following', 'code', 'tensorflow.contrib.layers', 'import', 'fully_connectedwith', 'tf.name_scope', '``', 'dnn', \"''\", 'hidden1', '=', 'fully_connected', 'X', 'n_hidden1', 'scope=', \"''\", 'hidden1', \"''\", 'hidden2', '=', 'fully_connected', 'hidden1', 'n_hidden2', 'scope=', \"''\", 'hidden2', \"''\", 'logits', '=', 'fully_connected', 'hidden2', 'n_outputs', 'scope=', \"''\", 'outputs', \"''\", 'activation_fn=None', 'The', 'tensorflow.contrib', 'package', 'contains', 'many', 'useful', 'functions', 'place', 'experimental', 'code', 'yet', 'graduated', 'part', 'main', 'TensorFlow', 'API', '.', 'So', 'fully_connected', 'function', 'contrib', 'code', 'may', 'change', 'move', 'future.Now', 'neural', 'network', 'model', 'ready', 'go', 'need', 'define', 'costfunction', 'use', 'train', '.', 'Just', 'Softmax', 'Regression', 'Chap…', 'ter', '4', 'use', 'cross', 'entropy', '.', 'As', 'discussed', 'earlier', 'cross', 'entropy', 'penalize', 'models', 'estimate', 'low', 'probability', 'target', 'class', '.', 'TensorFlow', 'provides', 'several', 'functions', 'compute', 'cross', 'entropy', '.', 'We', 'use', 'sparse_softmax_cross_entropy_with_logits', 'computes', 'cross', 'entropy', 'based', 'Training', 'DNN', 'Using', 'Plain', 'TensorFlow', '|', '267', 'ƒlogits⁄', 'i.e.', 'output', 'network', 'going', 'softmax', 'activation', 'function', 'expects', 'labels', 'form', 'integers', 'ranging', '0', 'number', 'classes', 'minus', '1', 'case', '0', '9', '.', 'This', 'give', 'us', '1D', 'tensor', 'containing', 'cross', 'entropy', 'instance', '.', 'We', 'use', 'TensorFlow‡s', 'reduce_mean', 'function', 'compute', 'mean', 'cross', 'entropy', 'instances', '.', 'tf.name_scope', '``', 'loss', \"''\", 'xentropy', '=', 'tf.nn.sparse_softmax_cross_entropy_with_logits', 'labels=y', 'logits=logits', 'loss', '=', 'tf.reduce_mean', 'xentropy', 'name=', \"''\", 'loss', \"''\", 'The', 'sparse_softmax_cross_entropy_with_logits', 'function', 'isequivalent', 'applying', 'softmax', 'activation', 'function', 'computing', 'cross', 'entropy', 'efficient', 'prop…', 'erly', 'takes', 'care', 'corner', 'cases', 'like', 'logits', 'equal', '0', '.', 'This', 'apply', 'softmax', 'activation', 'function', 'earlier', '.', 'There', 'also', 'another', 'function', 'called', 'softmax_cross_entropy_with_logits', 'takes', 'labels', 'form', 'one-hot', 'vectors', 'instead', 'ints', '0', 'number', 'classes', 'minus', '1', '.', 'We', 'neural', 'network', 'model', 'cost', 'function', 'need', 'define', 'GradientDescentOptimizer', 'tweak', 'model', 'parameters', 'mini…', 'mize', 'cost', 'function', '.', 'Nothing', 'new', 'it‡s', 'like', 'Chapter', '9', 'learning_rate', '=', '0.01with', 'tf.name_scope', '``', 'train', \"''\", 'optimizer', '=', 'tf.train.GradientDescentOptimizer', 'learning_rate', 'training_op', '=', 'optimizer.minimize', 'loss', 'The', 'last', 'important', 'step', 'construction', 'phase', 'specify', 'evaluate', 'model', '.', 'We', 'simply', 'use', 'accuracy', 'performance', 'measure', '.', 'First', 'instance', 'determine', 'neural', 'network‡s', 'prediction', 'correct', 'checking', 'whether', 'highest', 'logit', 'corresponds', 'target', 'class', '.', 'For', 'use', 'in_top_k', 'function', '.', 'This', 'returns', '1D', 'tensor', 'full', 'boolean', 'values', 'need', 'tocast', 'booleans', 'floats', 'compute', 'average', '.', 'This', 'give', 'us', 'net…', 'work‡s', 'overall', 'accuracy', '.', 'tf.name_scope', '``', 'eval', \"''\", 'correct', '=', 'tf.nn.in_top_k', 'logits', '1', 'accuracy', '=', 'tf.reduce_mean', 'tf.cast', 'correct', 'tf.float32', 'And', 'usual', 'need', 'create', 'node', 'initialize', 'variables', 'also', 'cre…', 'ate', 'Saver', 'save', 'trained', 'model', 'parameters', 'disk', 'init', '=', 'tf.global_variables_initializer', 'saver', '=', 'tf.train.Saver', '268', '|', 'Chapter', '10', 'Introduction', 'Arti•cial', 'Neural', 'NetworksPhew', '!', 'This', 'concludes', 'construction', 'phase', '.', 'This', 'fewer', '40', 'lines', 'code', 'pretty', 'intense', 'created', 'placeholders', 'inputs', 'targets', 'created', 'function', 'build', 'neuron', 'layer', 'used', 'create', 'DNN', 'defined', 'cost', 'function', 'created', 'optimizer', 'finally', 'defined', 'performance', 'measure', '.', 'Now', 'execution', 'phase', '.', 'Execution', 'PhaseThis', 'part', 'much', 'shorter', 'simpler', '.', 'First', 'let‡s', 'load', 'MNIST', '.', 'We', 'could', 'use', 'Scikit-', 'Learn', 'previous', 'chapters', 'TensorFlow', 'offers', 'helper', 'fetches', 'data', 'scales', '0', '1', 'shuffles', 'provides', 'simple', 'function', 'load', 'one', 'mini-batches', 'time', '.', 'So', 'let‡s', 'use', 'instead', 'tensorflow.examples.tutorials.mnist', 'import', 'input_datamnist', '=', 'input_data.read_data_sets', '``', '/tmp/data/', \"''\", 'Now', 'define', 'number', 'epochs', 'want', 'run', 'well', 'size', 'mini-batches', 'n_epochs', '=', '400batch_size', '=', '50And', 'train', 'model', 'tf.Session', 'sess', 'init.run', 'epoch', 'range', 'n_epochs', 'iteration', 'range', 'mnist.train.num_examples', '//', 'batch_size', 'X_batch', 'y_batch', '=', 'mnist.train.next_batch', 'batch_size', 'sess.run', 'training_op', 'feed_dict=', '{', 'X', 'X_batch', 'y_batch', '}', 'acc_train', '=', 'accuracy.eval', 'feed_dict=', '{', 'X', 'X_batch', 'y_batch', '}', 'acc_test', '=', 'accuracy.eval', 'feed_dict=', '{', 'X', 'mnist.test.images', 'mnist.test.labels', '}', 'print', 'epoch', '``', 'Train', 'accuracy', \"''\", 'acc_train', '``', 'Test', 'accuracy', \"''\", 'acc_test', 'save_path', '=', 'saver.save', 'sess', '``', './my_model_final.ckpt', \"''\", 'This', 'code', 'opens', 'TensorFlow', 'session', 'runs', 'init', 'node', 'initializes', 'variables', '.', 'Then', 'runs', 'main', 'training', 'loop', 'epoch', 'code', 'iterates', 'number', 'mini-batches', 'corresponds', 'training', 'set', 'size', '.', 'Each', 'mini-batch', 'fetched', 'via', 'next_batch', 'method', 'code', 'simply', 'runs', 'training', 'operation', 'feeding', 'current', 'mini-batch', 'input', 'data', 'targets', '.', 'Next', 'end', 'epoch', 'code', 'evaluates', 'model', 'last', 'mini-batch', 'full', 'training', 'set', 'prints', 'result', '.', 'Finally', 'model', 'parameters', 'saved', 'disk', '.', 'Training', 'DNN', 'Using', 'Plain', 'TensorFlow', '|', '269', 'Using', 'Neural', 'NetworkNow', 'neural', 'network', 'trained', 'use', 'make', 'predictions', '.', 'To', 'reuse', 'construction', 'phase', 'change', 'execution', 'phase', 'like', 'tf.Session', 'sess', 'saver.restore', 'sess', '``', './my_model_final.ckpt', \"''\", 'X_new_scaled', '=', '...', '#', 'new', 'images', 'scaled', '0', '1', 'Z', '=', 'logits.eval', 'feed_dict=', '{', 'X', 'X_new_scaled', '}', 'y_pred', '=', 'np.argmax', 'Z', 'axis=1', 'First', 'code', 'loads', 'model', 'parameters', 'disk', '.', 'Then', 'loads', 'new', 'imagesthat', 'want', 'classify', '.', 'Remember', 'apply', 'feature', 'scaling', 'train…', 'ing', 'data', 'case', 'scale', '0', '1', '.', 'Then', 'code', 'evaluates', 'logits', 'node.If', 'wanted', 'know', 'estimated', 'class', 'probabilities', 'would', 'need', 'apply', 'softmax', 'function', 'logits', 'want', 'predict', 'class', 'simply', 'pick', 'class', 'highest', 'logit', 'value', 'using', 'argmax', 'functiondoes', 'trick', '.Fine-Tuning', 'Neural', 'Network', 'HyperparametersThe', 'flexibility', 'neural', 'networks', 'also', 'one', 'main', 'drawbacks', 'many', 'hyperparameters', 'tweak', '.', 'Not', 'use', 'imaginable', 'network', 'topology', 'neurons', 'interconnected', 'even', 'simple', 'MLP', 'change', 'number', 'layers', 'number', 'neurons', 'per', 'layer', 'type', 'activation', 'function', 'use', 'layer', 'weight', 'initialization', 'logic', 'much', '.', 'How', 'know', 'combination', 'hyperparameters', 'best', 'task', '?', 'Of', 'course', 'use', 'grid', 'search', 'cross-validation', 'find', 'right', 'hyperpara…', 'meters', 'like', 'previous', 'chapters', 'since', 'many', 'hyperparameters', 'tune', 'since', 'training', 'neural', 'network', 'large', 'dataset', 'takes', 'lot', 'time', 'able', 'explore', 'tiny', 'part', 'hyperparameter', 'space', 'reasonable', 'amount', 'time', '.', 'It', 'much', 'better', 'use', 'randomized', 'search', 'discussed', 'Chap…', 'ter', '2', '.', 'Another', 'option', 'use', 'tool', 'Oscar', 'implements', 'complex', 'algorithms', 'help', 'find', 'good', 'set', 'hyperparameters', 'quickly', '.', 'It', 'helps', 'idea', 'values', 'reasonable', 'hyperparameter', 'restrict', 'search', 'space', '.', 'Let‡s', 'start', 'number', 'hidden', 'layers', '.', 'Number', 'Hidden', 'LayersFor', 'many', 'problems', 'begin', 'single', 'hidden', 'layer', 'get', 'reasonable', 'results', '.', 'It', 'actually', 'shown', 'MLP', 'one', 'hidden', 'layer', 'model', 'even', 'complex', 'functions', 'provided', 'enough', 'neurons', '.', 'For', 'long', 'time', 'facts', 'convinced', 'researchers', 'need', 'investigate', '270', '|', 'Chapter', '10', 'Introduction', 'Arti•cial', 'Neural', 'Networksdeeper', 'neural', 'networks', '.', 'But', 'overlooked', 'fact', 'deep', 'networks', 'much', 'higher', 'parameter', 'e⁄ciency', 'shallow', 'ones', 'model', 'complex', 'functions', 'using', 'exponentially', 'fewer', 'neurons', 'shallow', 'nets', 'making', 'much', 'faster', 'train.To', 'understand', 'suppose', 'asked', 'draw', 'forest', 'using', 'drawing', 'soft…', 'ware', 'forbidden', 'use', 'copy/paste', '.', 'You', 'would', 'draw', 'tree', 'individually', 'branch', 'per', 'branch', 'leaf', 'per', 'leaf', '.', 'If', 'could', 'instead', 'draw', 'one', 'leaf', 'copy/paste', 'draw', 'branch', 'copy/paste', 'branch', 'create', 'tree', 'finally', 'copy/paste', 'tree', 'make', 'forest', 'would', 'finished', 'time', '.', 'Real-world', 'data', 'often', 'structured', 'hierarchical', 'way', 'DNNs', 'automatically', 'take', 'advantage', 'fact', 'lower', 'hidden', 'layers', 'model', 'low-level', 'structures', 'e.g.', 'line', 'segments', 'various', 'shapes', 'orientations', 'intermediate', 'hidden', 'layers', 'combine', 'low-level', 'structures', 'model', 'intermediate-level', 'structures', 'e.g.', 'squares', 'cir…', 'cles', 'highest', 'hidden', 'layers', 'output', 'layer', 'combine', 'intermediate', 'structures', 'model', 'high-level', 'structures', 'e.g.', 'faces', '.Not', 'hierarchical', 'architecture', 'help', 'DNNs', 'converge', 'faster', 'good', 'sol…', 'ution', 'also', 'improves', 'ability', 'generalize', 'new', 'datasets', '.', 'For', 'example', 'already', 'trained', 'model', 'recognize', 'faces', 'pictures', 'want', 'train', 'new', 'neural', 'network', 'recognize', 'hairstyles', 'kickstart', 'training', 'byreusing', 'lower', 'layers', 'first', 'network', '.', 'Instead', 'randomly', 'initializing', 'weights', 'biases', 'first', 'layers', 'new', 'neural', 'network', 'initialize', 'value', 'weights', 'biases', 'lower', 'layers', 'first', 'network', '.', 'This', 'way', 'network', 'learn', 'scratch', 'low-level', 'structures', 'occur', 'pictures', 'learn', 'higher-level', 'structures', 'e.g.', 'hairstyles', '.In', 'summary', 'many', 'problems', 'start', 'one', 'two', 'hidden', 'layers', 'work', 'fine', 'e.g.', 'easily', 'reach', '97', '%', 'accuracy', 'MNISTdataset', 'using', 'one', 'hidden', 'layer', 'hundred', 'neurons', '98', '%', 'accu…', 'racy', 'using', 'two', 'hidden', 'layers', 'total', 'amount', 'neurons', 'roughly', 'amount', 'training', 'time', '.', 'For', 'complex', 'problems', 'gradually', 'ramp', 'number', 'hidden', 'layers', 'start', 'overfitting', 'training', 'set', '.', 'Very', 'com…', 'plex', 'tasks', 'large', 'image', 'classification', 'speech', 'recognition', 'typically', 'require', 'networks', 'dozens', 'layers', 'even', 'hundreds', 'fully', 'connected', 'ones', 'see', 'Chapter', '13', 'need', 'huge', 'amount', 'training', 'data', '.', 'However', 'rarely', 'train', 'networks', 'scratch', 'much', 'common', 'reuse', 'parts', 'pretrained', 'state-of-the-art', 'network', 'performs', 'similar', 'task', '.', 'Training', 'lot', 'faster', 'require', 'much', 'less', 'data', 'discuss', 'Chap…', 'ter', '11', '.Fine-Tuning', 'Neural', 'Network', 'Hyperparameters', '|', '271', '12By', 'Vincent', 'Vanhoucke', 'Deep', 'Learning', 'class', 'Udacity.com', '.', '13A', 'extra', 'ANN', 'architectures', 'presented', 'Appendix', 'E', '.Number', 'Neurons', 'per', 'Hidden', 'LayerObviously', 'number', 'neurons', 'input', 'output', 'layers', 'determined', 'type', 'input', 'output', 'task', 'requires', '.', 'For', 'example', 'MNIST', 'task', 'requires', '28', 'x', '28', '=', '784', 'input', 'neurons', '10', 'output', 'neurons', '.', 'As', 'hidden', 'layers', 'common', 'practice', 'size', 'form', 'funnel', 'fewer', 'fewer', 'neurons', 'layer›', 'rationale', 'many', 'low-level', 'features', 'coalesce', 'far', 'fewer', 'high-level', 'features', '.', 'For', 'example', 'typical', 'neural', 'network', 'MNIST', 'may', 'two', 'hidden', 'lay…', 'ers', 'first', '300', 'neurons', 'second', '100', '.', 'However', 'practice', 'common', 'may', 'simply', 'use', 'size', 'hidden', 'layers›for', 'example', 'hidden', 'layers', '150', 'neurons', 'that‡s', 'one', 'hyperparameter', 'tune', 'instead', 'one', 'per', 'layer', '.', 'Just', 'like', 'number', 'layers', 'try', 'increasing', 'number', 'neurons', 'gradually', 'network', 'starts', 'overfitting', '.', 'In', 'general', 'get', 'bang', 'buck', 'increasing', 'number', 'layers', 'number', 'neurons', 'per', 'layer', '.', 'Unfortunately', 'see', 'finding', 'perfect', 'amount', 'neu…', 'rons', 'still', 'somewhat', 'black', 'art', '.', 'A', 'simpler', 'approach', 'pick', 'model', 'layers', 'neurons', 'actually', 'need', 'use', 'early', 'stopping', 'prevent', 'overfitting', 'regu…', 'larization', 'techniques', 'especially', 'dropout', 'see', 'Chapter', '11', '.', 'This', 'beendubbed', 'ƒstretch', 'pants⁄', 'approach', '12', 'instead', 'wasting', 'time', 'looking', 'pants', 'perfectly', 'match', 'size', 'use', 'large', 'stretch', 'pants', 'shrink', 'right', 'size', '.', 'Activation', 'FunctionsIn', 'cases', 'use', 'ReLU', 'activation', 'function', 'hidden', 'layers', 'one', 'variants', 'see', 'Chapter', '11', '.', 'It', 'bit', 'faster', 'compute', 'activation', 'functions', 'Gradient', 'Descent', 'get', 'stuck', 'much', 'plateaus', 'thanks', 'fact', 'saturate', 'large', 'input', 'values', 'opposed', 'logistic', 'function', 'hyperbolic', 'tangent', 'function', 'saturate', '1', '.', 'For', 'output', 'layer', 'softmax', 'activation', 'function', 'generally', 'good', 'choice', 'classification', 'tasks', 'classes', 'mutually', 'exclusive', '.', 'For', 'regression', 'tasks', 'simply', 'use', 'activation', 'function', '.', 'This', 'concludes', 'introduction', 'artificial', 'neural', 'networks', '.', 'In', 'following', 'chap…', 'ters', 'discuss', 'techniques', 'train', 'deep', 'nets', 'distribute', 'training', 'across', 'multiple', 'servers', 'GPUs', '.', 'Then', 'explore', 'popular', 'neural', 'network', 'architectures', 'convolutional', 'neural', 'networks', 'recurrent', 'neural', 'networks', 'autoen…', 'coders.13272', '|', 'Chapter', '10', 'Introduction', 'Arti•cial', 'Neural', 'NetworksExercises1.Draw', 'ANN', 'using', 'original', 'artificial', 'neurons', 'like', 'ones', 'Figure', '10-3', 'computes', 'A', 'B', 'represents', 'XOR', 'operation', '.', 'Hint', 'A', 'B', '=', 'A', '°', 'B', '°', 'A', 'B', '.2.Why', 'generally', 'preferable', 'use', 'Logistic', 'Regression', 'classifier', 'rather', 'classical', 'Perceptron', 'i.e.', 'single', 'layer', 'linear', 'threshold', 'units', 'trained', 'using', 'Perceptron', 'training', 'algorithm', '?', 'How', 'tweak', 'Perceptron', 'make', 'equivalent', 'Logistic', 'Regression', 'classifier', '?', '3.Why', 'logistic', 'activation', 'function', 'key', 'ingredient', 'training', 'first', 'MLPs', '?', '4.Name', 'three', 'popular', 'activation', 'functions', '.', 'Can', 'draw', '?', '5.Suppose', 'MLP', 'composed', 'one', 'input', 'layer', '10', 'passthrough', 'neurons', 'followed', 'one', 'hidden', 'layer', '50', 'artificial', 'neurons', 'finally', 'one', 'output', 'layer', '3', 'artificial', 'neurons', '.', 'All', 'artificial', 'neurons', 'use', 'ReLU', 'activa…', 'tion', 'function.‹What', 'shape', 'input', 'matrix', 'X', '?', '‹What', 'shape', 'hidden', 'layer‡s', 'weight', 'vector', 'Wh', 'shape', 'bias', 'vector', 'bh', '?', '‹What', 'shape', 'output', 'layer‡s', 'weight', 'vector', 'Wo', 'bias', 'vector', 'bo', '?', '‹What', 'shape', 'network‡s', 'output', 'matrix', 'Y', '?', '‹Write', 'equation', 'computes', 'network‡s', 'output', 'matrix', 'Y', 'functionof', 'X', 'Wh', 'bh', 'Wo', 'bo.6.How', 'many', 'neurons', 'need', 'output', 'layer', 'want', 'classify', 'email', 'spam', 'ham', '?', 'What', 'activation', 'function', 'use', 'output', 'layer', '?', 'If', 'instead', 'want', 'tackle', 'MNIST', 'many', 'neurons', 'need', 'out…', 'put', 'layer', 'using', 'activation', 'function', '?', 'Answer', 'questions', 'getting', 'network', 'predict', 'housing', 'prices', 'Chapter', '2', '.7.What', 'backpropagation', 'work', '?', 'What', 'difference', 'backpropagation', 'reverse-mode', 'autodiff', '?', '8.Can', 'list', 'hyperparameters', 'tweak', 'MLP', '?', 'If', 'MLP', 'over…', 'fits', 'training', 'data', 'could', 'tweak', 'hyperparameters', 'try', 'solve', 'problem', '?', '9.Train', 'deep', 'MLP', 'MNIST', 'dataset', 'see', 'get', '98', '%', 'preci…', 'sion', '.', 'Just', 'like', 'last', 'exercise', 'Chapter', '9', 'try', 'adding', 'bells', 'whistles', 'Exercises', '|', '273', 'i.e.', 'save', 'checkpoints', 'restore', 'last', 'checkpoint', 'case', 'interruption', 'add', 'summaries', 'plot', 'learning', 'curves', 'using', 'TensorBoard', '.', 'Solutions', 'exercises', 'available', 'Appendix', 'A', '.274', '|', 'Chapter', '10', 'Introduction', 'Arti•cial', 'Neural', 'NetworksCHAPTER', '11Training', 'Deep', 'Neural', 'NetsIn', 'Chapter', '10', 'introduced', 'artificial', 'neural', 'networks', 'trained', 'first', 'deep', 'neural', 'network', '.', 'But', 'shallow', 'DNN', 'two', 'hidden', 'layers', '.', 'What', 'need', 'tackle', 'complex', 'problem', 'detecting', 'hundreds', 'types', 'objects', 'high-resolution', 'images', '?', 'You', 'may', 'need', 'train', 'much', 'deeper', 'DNN', 'per…', 'haps', 'say', '10', 'layers', 'containing', 'hundreds', 'neurons', 'connected', 'hun…', 'dreds', 'thousands', 'connections', '.', 'This', 'would', 'walk', 'park', '‹First', 'would', 'faced', 'tricky', 'vanishing', 'gradients', 'problem', 'therelated', 'exploding', 'gradients', 'problem', 'affects', 'deep', 'neural', 'networks', 'makes', 'lower', 'layers', 'hard', 'train', '.', '‹Second', 'large', 'network', 'training', 'would', 'extremely', 'slow', '.', '‹Third', 'model', 'millions', 'parameters', 'would', 'severely', 'risk', 'overfitting', 'thetraining', 'set.In', 'chapter', 'go', 'problems', 'turn', 'present', 'techni…', 'ques', 'solve', '.', 'We', 'start', 'explaining', 'vanishing', 'gradients', 'problem', 'exploring', 'popular', 'solutions', 'problem', '.', 'Next', 'look', 'various', 'optimizers', 'speed', 'training', 'large', 'models', 'tremendously', 'compared', 'plain', 'Gradient', 'Descent', '.', 'Finally', 'go', 'popular', 'regularization', 'techniques', 'large', 'neural', 'networks.With', 'tools', 'able', 'train', 'deep', 'nets', 'welcome', 'Deep', 'Learning', '!', 'Vanishing/Exploding', 'Gradients', 'ProblemsAs', 'discussed', 'Chapter', '10', 'backpropagation', 'algorithm', 'works', 'going', 'output', 'layer', 'input', 'layer', 'propagating', 'error', 'gradient', 'way', '.', 'Once', 'algorithm', 'computed', 'gradient', 'cost', 'function', 'regards', '2751ƒUnderstanding', 'Difficulty', 'Training', 'Deep', 'Feedforward', 'Neural', 'Networks', '⁄', 'X.', 'Glorot', 'Y', 'Bengio', '2010', '.', 'parameter', 'network', 'uses', 'gradients', 'update', 'parameter', 'Gradient', 'Descent', 'step', '.', 'Unfortunately', 'gradients', 'often', 'get', 'smaller', 'smaller', 'algorithm', 'progresses', 'lower', 'layers', '.', 'As', 'result', 'Gradient', 'Descent', 'update', 'leaves', 'lower', 'layer', 'connection', 'weights', 'virtually', 'unchanged', 'training', 'never', 'converges', 'good', 'solution', '.', 'This', 'called', 'vanishing', 'gradients', 'problem', '.', 'In', 'cases', 'oppositecan', 'happen', 'gradients', 'grow', 'bigger', 'bigger', 'many', 'layers', 'get', 'insanely', 'large', 'weight', 'updates', 'algorithm', 'diverges', '.', 'This', 'exploding', 'gradients', 'prob…', 'lem', 'mostly', 'encountered', 'recurrent', 'neural', 'networks', 'see', 'Chapter', '14', '.More', 'generally', 'deep', 'neural', 'networks', 'suffer', 'unstable', 'gradients', 'different', 'layers', 'may', 'learn', 'widely', 'different', 'speeds', '.', 'Although', 'unfortunate', 'behavior', 'empirically', 'observed', 'quite', 'one', 'reasons', 'deep', 'neural', 'networks', 'mostly', 'abandoned', 'long', 'time', 'around', '2010', 'significant', 'progress', 'made', 'understand…', 'ing', '.', 'A', 'paper', 'titled', 'ƒUnderstanding', 'Difficulty', 'Training', 'Deep', 'Feedforward', 'Neural', 'Networks⁄', 'Xavier', 'Glorot', 'Yoshua', 'Bengio', '1', 'found', 'suspects', 'includ…ing', 'combination', 'popular', 'logistic', 'sigmoid', 'activation', 'function', 'weight', 'initialization', 'technique', 'popular', 'time', 'namely', 'random', 'ini…', 'tialization', 'using', 'normal', 'distribution', 'mean', '0', 'standard', 'deviation', '1', '.', 'In', 'short', 'showed', 'activation', 'function', 'initialization', 'scheme', 'variance', 'outputs', 'layer', 'much', 'greater', 'variance', 'inputs', '.', 'Going', 'forward', 'network', 'variance', 'keeps', 'increasing', 'layer', 'activation', 'function', 'saturates', 'top', 'layers', '.', 'This', 'actually', 'made', 'worse', 'fact', 'logistic', 'function', 'mean', '0.5', '0', 'hyperbolic', 'tangent', 'function', 'mean', '0', 'behaves', 'slightly', 'better', 'logistic', 'function', 'deep', 'networks', '.Looking', 'logistic', 'activation', 'function', 'see', 'Figure', '11-1', 'see', 'inputs', 'become', 'large', 'negative', 'positive', 'function', 'saturates', '0', '1', 'derivative', 'extremely', 'close', '0', '.', 'Thus', 'backpropagation', 'kicks', 'virtually', 'gradient', 'propagate', 'back', 'network', 'little', 'gradient', 'exists', 'keeps', 'getting', 'diluted', 'backpropagation', 'progresses', 'top', 'layers', 'really', 'nothing', 'left', 'lower', 'layers', '.', '276', '|', 'Chapter', '11', 'Training', 'Deep', 'Neural', 'Nets', '2Here‡s', 'analogy', 'set', 'microphone', 'amplifier‡s', 'knob', 'close', 'zero', 'people', 'won‡t', 'hear', 'voice', 'set', 'close', 'max', 'voice', 'saturated', 'people', 'won‡t', 'understand', 'say…', 'ing', '.', 'Now', 'imagine', 'chain', 'amplifiers', 'need', 'set', 'properly', 'order', 'voice', 'come', 'loud', 'clear', 'end', 'chain', '.', 'Your', 'voice', 'come', 'amplifier', 'amplitude', 'came', 'in.Figure', '11-1', '.', 'Logistic', 'activation', 'function', 'saturation', 'Xavier', 'He', 'InitializationIn', 'paper', 'Glorot', 'Bengio', 'propose', 'way', 'significantly', 'alleviate', 'prob…', 'lem', '.', 'We', 'need', 'signal', 'flow', 'properly', 'directions', 'forward', 'direction', 'making', 'predictions', 'reverse', 'direction', 'backpropagating', 'gradi…', 'ents', '.', 'We', 'don‡t', 'want', 'signal', 'die', 'want', 'explode', 'saturate', '.', 'For', 'signal', 'flow', 'properly', 'authors', 'argue', 'need', 'variance', 'outputs', 'layer', 'equal', 'variance', 'inputs', '2', 'also', 'need', 'thegradients', 'equal', 'variance', 'flowing', 'layer', 'reverse', 'direction', 'please', 'check', 'paper', 'interested', 'mathematical', 'details', '.', 'It', 'actually', 'possible', 'guarantee', 'unless', 'layer', 'equal', 'number', 'input', 'output', 'connections', 'proposed', 'good', 'compromise', 'proven', 'work', 'well', 'practice', 'connection', 'weights', 'must', 'initialized', 'randomly', 'described', 'Equation', '11-1', 'ninputs', 'noutputs', 'number', 'input', 'output', 'connections', 'layer', 'whose', 'weights', 'initialized', 'alsocalled', 'fan-in', 'fan-out', '.', 'This', 'initialization', 'strategy', 'often', 'called', 'Xavier', 'initializa…', 'tion', 'author‡s', 'first', 'name', 'sometimes', 'Glorot', 'initialization', '.Vanishing/Exploding', 'Gradients', 'Problems', '|', '277', '3This', 'simplified', 'strategy', 'actually', 'already', 'proposed', 'much', 'earlier›for', 'example', '1998', 'book', 'Neural', 'Networks', 'Tricks', 'Trade', 'Genevieve', 'Orr', 'Klaus-Robert', 'M™ller', 'Springer', '.', '4Such', 'ƒDelving', 'Deep', 'Rectifiers', 'Surpassing', 'Human-Level', 'Performance', 'ImageNet', 'Classification', '⁄', 'K.', 'He', 'et', 'al', '.', '2015', '.', 'Equation', '11-1', '.', 'Xavier', 'initialization', 'using', 'logistic', 'activation', 'function', 'Normaldistributionwithmean0andstandarddeviation', '„=2ninputs+noutputsOrauniformdistributionbetween…rand+r', 'r=6ninputs+noutputsWhen', 'number', 'input', 'connections', 'roughly', 'equal', 'number', 'output', 'connections', 'get', 'simpler', 'equations', 'e.g.', '„=1/', 'ninputs', 'r=3/', 'ninputs', '.', 'We', 'used', 'simplified', 'strategy', 'Chapter', '10', '.3Using', 'Xavier', 'initialization', 'strategy', 'speed', 'training', 'considerably', 'one', 'tricks', 'led', 'current', 'success', 'Deep', 'Learning', '.', 'Some', 'recent', 'papers', '4have', 'provided', 'similar', 'strategies', 'different', 'activation', 'functions', 'shown', 'Table', '11-1', '.', 'The', 'initialization', 'strategy', 'ReLU', 'activation', 'function', 'var…', 'iants', 'including', 'ELU', 'activation', 'described', 'shortly', 'sometimes', 'called', 'He', 'initiali…', 'zation', 'last', 'name', 'author', '.', 'Table', '11-1', '.', 'Initialization', 'parameters', 'type', 'activation', 'function', 'Activation', 'function', 'Uniform', 'distribution', '—r', 'r', 'Normal', 'distribution', 'Logisticr=6ninputs+noutputs•=2ninputs+noutputsHyperbolic', 'tangent', 'r=4', '6ninputs+noutputs•=4', '2ninputs+noutputsReLU', 'variants', 'r=26ninputs+noutputs•=22ninputs+noutputsBy', 'default', 'fully_connected', 'function', 'introduced', 'Chapter', '10', 'uses', 'Xavier', 'initialization', 'uniform', 'distribution', '.', 'You', 'change', 'He', 'initialization', 'using', 'variance_scaling_initializer', 'function', 'like', 'he_init', '=', 'tf.contrib.layers.variance_scaling_initializer', 'hidden1', '=', 'fully_connected', 'X', 'n_hidden1', 'weights_initializer=he_init', 'scope=', \"''\", 'h1', \"''\", '278', '|', 'Chapter', '11', 'Training', 'Deep', 'Neural', 'Nets', '5ƒEmpirical', 'Evaluation', 'Rectified', 'Activations', 'Convolution', 'Network', '⁄', 'B.', 'Xu', 'et', 'al', '.', '2015', '.', 'He', 'initialization', 'considers', 'fan-in', 'average', 'fan-in', 'fan-out', 'like', 'Xavier', 'initialization', '.', 'This', 'also', 'default', 'variance_scaling_initializer', 'function', 'butyou', 'change', 'setting', 'argument', 'mode=', \"''\", 'FAN_AVG', \"''\", '.Nonsaturating', 'Activation', 'FunctionsOne', 'insights', '2010', 'paper', 'Glorot', 'Bengio', 'vanishing/', 'exploding', 'gradients', 'problems', 'part', 'due', 'poor', 'choice', 'activation', 'func…', 'tion', '.', 'Until', 'people', 'assumed', 'Mother', 'Nature', 'chosen', 'use', 'roughly', 'sigmoid', 'activation', 'functions', 'biological', 'neurons', 'must', 'excellent', 'choice', '.', 'But', 'turns', 'activation', 'functions', 'behave', 'much', 'better', 'deep', 'neural', 'networks', 'particular', 'ReLU', 'activation', 'function', 'mostly', 'saturate', 'positive', 'values', 'also', 'quite', 'fast', 'compute', '.', 'Unfortunately', 'ReLU', 'activation', 'function', 'perfect', '.', 'It', 'suffers', 'problem', 'known', 'dying', 'ReLUs', 'training', 'neurons', 'effectively', 'die', 'meaningthey', 'stop', 'outputting', 'anything', '0', '.', 'In', 'cases', 'may', 'find', 'half', 'network‡s', 'neurons', 'dead', 'especially', 'used', 'large', 'learning', 'rate', '.', 'During', 'training', 'neuron‡s', 'weights', 'get', 'updated', 'weighted', 'sum', 'neuron‡s', 'inputs', 'negative', 'start', 'outputting', '0', '.', 'When', 'happen', 'neuron', 'unlikely', 'come', 'back', 'life', 'since', 'gradient', 'ReLU', 'function', '0', 'input', 'negative', '.', 'To', 'solve', 'problem', 'may', 'want', 'use', 'variant', 'ReLU', 'function', 'leaky', 'ReLU', '.', 'This', 'function', 'defined', 'LeakyReLU', '‰', 'z', '=', 'max', '‰z', 'z', 'seeFigure', '11-2', '.', 'The', 'hyperparameter', '‰', 'defines', 'much', 'function', 'ƒleaks⁄', 'slope', 'function', 'z', '<', '0', 'typically', 'set', '0.01', '.', 'This', 'small', 'slope', 'ensuresthat', 'leaky', 'ReLUs', 'never', 'die', 'go', 'long', 'coma', 'chance', 'eventually', 'wake', '.', 'A', 'recent', 'paper', '5', 'compared', 'several', 'variants', 'ReLU', 'activation', 'function', 'one', 'conclusions', 'leaky', 'variants', 'always', 'outperformed', 'strict', 'ReLU', 'activation', 'function', '.', 'In', 'fact', 'setting', '‰', '=', '0.2', 'huge', 'leak', 'seemed', 'result', 'better', 'performance', '‰', '=', '0.01', 'small', 'leak', '.', 'They', 'also', 'evaluated', 'randomized', 'leaky', 'ReLU', 'RReLU', '‰', 'picked', 'randomly', 'given', 'range', 'training', 'fixed', 'average', 'value', 'testing', '.', 'It', 'also', 'performed', 'fairly', 'well', 'seemed', 'act', 'regularizer', 'reducing', 'risk', 'overfitting', 'training', 'set', '.Finally', 'also', 'evaluated', 'parametric', 'leaky', 'ReLU', 'PReLU', '‰', 'authorized', 'learned', 'training', 'instead', 'hyperparameter', 'becomes', 'parameter', 'modified', 'backpropagation', 'like', 'parameter', '.', 'This', 'Vanishing/Exploding', 'Gradients', 'Problems', '|', '279', '6ƒFast', 'Accurate', 'Deep', 'Network', 'Learning', 'Exponential', 'Linear', 'Units', 'ELUs', '⁄', 'D.', 'Clevert', 'T.', 'Unterthiner', 'S.', 'Hochreiter', '2015', '.', 'reported', 'strongly', 'outperform', 'ReLU', 'large', 'image', 'datasets', 'smaller', 'datasets', 'runs', 'risk', 'overfitting', 'training', 'set', '.', 'Figure', '11-2', '.', 'Leaky', 'ReLU', 'Last', 'least', '2015', 'paper', 'Djork-Arn•', 'Clevert', 'et', 'al.6', 'proposed', 'new', 'activa…tion', 'function', 'called', 'exponential', 'linear', 'unit', 'ELU', 'outperformed', 'ReLU', 'variants', 'experiments', 'training', 'time', 'reduced', 'neural', 'network', 'per…', 'formed', 'better', 'test', 'set', '.', 'It', 'represented', 'Figure', '11-3', 'Equation', '11-2', 'shows', 'definition.Equation', '11-2', '.', 'ELU', 'activation', 'function', 'ELU‰z=‰expz', '”', '1', 'ifz', '<', '0', 'zifz', 'Š0', 'Figure', '11-3', '.', 'ELU', 'activation', 'function', '280', '|', 'Chapter', '11', 'Training', 'Deep', 'Neural', 'Nets', 'It', 'looks', 'lot', 'like', 'ReLU', 'function', 'major', 'differences', '‹First', 'takes', 'negative', 'values', 'z', '<', '0', 'allows', 'unit', 'average', 'output', 'closer', '0', '.', 'This', 'helps', 'alleviate', 'vanishing', 'gradients', 'problem', 'discussed', 'earlier', '.', 'The', 'hyperparameter', '‰', 'defines', 'value', 'ELU', 'func…', 'tion', 'approaches', 'z', 'large', 'negative', 'number', '.', 'It', 'usually', 'set', '1', 'tweak', 'like', 'hyperparameter', 'want', '.', '‹Second', 'nonzero', 'gradient', 'z', '<', '0', 'avoids', 'dying', 'units', 'issue', '.', '‹Third', 'function', 'smooth', 'everywhere', 'including', 'around', 'z', '=', '0', 'helpsspeed', 'Gradient', 'Descent', 'since', 'bounce', 'much', 'left', 'right', 'z', '=', '0.The', 'main', 'drawback', 'ELU', 'activation', 'function', 'slower', 'compute', 'ReLU', 'variants', 'due', 'use', 'exponential', 'function', 'dur…', 'ing', 'training', 'compensated', 'faster', 'convergence', 'rate', '.', 'However', 'test', 'time', 'ELU', 'network', 'slower', 'ReLU', 'network', '.', 'So', 'activation', 'function', 'use', 'hidden', 'layers', 'deep', 'neural', 'networks', '?', 'Although', 'mileage', 'vary', 'general', 'ELU', '>', 'leaky', 'ReLU', 'variants', '>', 'ReLU', '>', 'tanh', '>', 'logis…', 'tic', '.', 'If', 'care', 'lot', 'runtime', 'performance', 'may', 'pre…', 'fer', 'leaky', 'ReLUs', 'ELUs', '.', 'If', 'don‡t', 'want', 'tweak', 'yet', 'another', 'hyperparameter', 'may', 'use', 'default', '‰', 'values', 'suggestedearlier', '0.01', 'leaky', 'ReLU', '1', 'ELU', '.', 'If', 'spare', 'time', 'computing', 'power', 'use', 'cross-validation', 'evalu…', 'ate', 'activation', 'functions', 'particular', 'RReLU', 'network', 'overfitting', 'PReLU', 'huge', 'training', 'set', '.', 'TensorFlow', 'offers', 'elu', 'function', 'use', 'build', 'neural', 'network', '.', 'Simply', 'set', 'activation_fn', 'argument', 'calling', 'fully_connected', 'func…tion', 'like', 'hidden1', '=', 'fully_connected', 'X', 'n_hidden1', 'activation_fn=tf.nn.elu', 'TensorFlow', 'predefined', 'function', 'leaky', 'ReLUs', 'easy', 'enough', 'define', 'def', 'leaky_relu', 'z', 'name=None', 'return', 'tf.maximum', '0.01', '*', 'z', 'z', 'name=name', 'hidden1', '=', 'fully_connected', 'X', 'n_hidden1', 'activation_fn=leaky_relu', 'Vanishing/Exploding', 'Gradients', 'Problems', '|', '281', '7ƒBatch', 'Normalization', 'Accelerating', 'Deep', 'Network', 'Training', 'Reducing', 'Internal', 'Covariate', 'Shift', '⁄', 'S.', 'Ioffe', 'C.', 'Szegedy', '2015', '.Batch', 'NormalizationAlthough', 'using', 'He', 'initialization', 'along', 'ELU', 'variant', 'ReLU', 'signifi…', 'cantly', 'reduce', 'vanishing/exploding', 'gradients', 'problems', 'beginning', 'train…', 'ing', 'doesn‡t', 'guarantee', 'won‡t', 'come', 'back', 'training', '.', 'In', '2015', 'paper', ',7', 'Sergey', 'Ioffe', 'Christian', 'Szegedy', 'proposed', 'technique', 'called', 'Batch', 'Normalization', 'BN', 'address', 'vanishing/exploding', 'gradients', 'problems', 'generally', 'problem', 'distribution', 'layer‡s', 'inputs', 'changes', 'training', 'parameters', 'previous', 'layers', 'change', 'call', 'Internal', 'Covariate', 'Shi', '“', 'problem', '.The', 'technique', 'consists', 'adding', 'operation', 'model', 'activation', 'function', 'layer', 'simply', 'zero-centering', 'normalizing', 'inputs', 'scaling', 'shifting', 'result', 'using', 'two', 'new', 'parameters', 'per', 'layer', 'one', 'scaling', 'shifting', '.', 'In', 'words', 'operation', 'lets', 'model', 'learn', 'optimal', 'scale', 'mean', 'inputs', 'layer', '.', 'In', 'order', 'zero-center', 'normalize', 'inputs', 'algorithm', 'needs', 'estimate', 'inputs‡', 'mean', 'standard', 'deviation', '.', 'It', 'evaluating', 'mean', 'standard', 'deviation', 'inputs', 'current', 'mini-batch', 'hence', 'name', 'ƒBatch', 'Normal…', 'ization⁄', '.', 'The', 'whole', 'operation', 'summarized', 'Equation', '11-3', '.Equation', '11-3', '.', 'Batch', 'Normalization', 'algorithm', '1.', 'ﬂB=1mB', '“', 'i=1', 'mBi2', '.', '„B2=1mB', '“', 'i=1', 'mBi', '”', 'ﬂB23', '.', 'i=i', '”', 'ﬂB„B2+4', '.', 'i=', '’', 'i+Ł‹ﬂB', 'empirical', 'mean', 'evaluated', 'whole', 'mini-batch', 'B.‹„B', 'empirical', 'standard', 'deviation', 'also', 'evaluated', 'whole', 'mini-batch', '.', '‹mB', 'number', 'instances', 'mini-batch', '.', '282', '|', 'Chapter', '11', 'Training', 'Deep', 'Neural', 'Nets', '‹', 'zero-centered', 'normalized', 'input', '.', '‹', '’', 'scaling', 'parameter', 'layer', '.', '‹Ł', 'shifting', 'parameter', 'offset', 'layer', '.', '‹', 'tiny', 'number', 'avoid', 'division', 'zero', 'typically', '10', '–3', '.', 'This', 'called', 'asmoothing', 'term', '.‹z', 'output', 'BN', 'operation', 'scaled', 'shifted', 'version', 'inputs', '.', 'At', 'test', 'time', 'mini-batch', 'compute', 'empirical', 'mean', 'standard', 'deviation', 'instead', 'simply', 'use', 'whole', 'training', 'set‡s', 'mean', 'standard', 'devi…', 'ation', '.', 'These', 'typically', 'efficiently', 'computed', 'training', 'using', 'moving', 'aver…', 'age', '.', 'So', 'total', 'four', 'parameters', 'learned', 'batch-normalized', 'layer', '’', 'scale', 'Ł', 'offset', 'ﬂ', 'mean', '„', 'standard', 'deviation', '.', 'The', 'authors', 'demonstrated', 'technique', 'considerably', 'improved', 'deep', 'neural', 'networks', 'experimented', '.', 'The', 'vanishing', 'gradients', 'problem', 'strongly', 'reduced', 'point', 'could', 'use', 'saturating', 'activation', 'functions', 'tanh', 'even', 'logistic', 'activation', 'function', '.', 'The', 'networks', 'also', 'much', 'less', 'sensitive', 'weight', 'initialization', '.', 'They', 'able', 'use', 'much', 'larger', 'learning', 'rates', 'significantly', 'speeding', 'learning', 'process', '.', 'Specifically', 'note', 'ƒApplied', 'state-of-the-art', 'image', 'classification', 'model', 'Batch', 'Normalization', 'ach…', 'ieves', 'accuracy', '14', 'times', 'fewer', 'training', 'steps', 'beats', 'original', 'model', 'significant', 'margin', '.', 'µ', 'Using', 'ensemble', 'batch-normalized', 'net…', 'works', 'improve', 'upon', 'best', 'published', 'result', 'ImageNet', 'classification', 'reach…', 'ing', '4.9', '%', 'top-5', 'validation', 'error', '4.8', '%', 'test', 'error', 'exceeding', 'accuracy', 'human', 'raters.⁄', 'Finally', 'like', 'gift', 'keeps', 'giving', 'Batch', 'Normalization', 'also', 'acts', 'like', 'regularizer', 'reducing', 'need', 'regularization', 'techniques', 'dropout', 'described', 'later', 'chapter', '.', 'Batch', 'Normalization', 'however', 'add', 'complexity', 'model', 'although', 'removes', 'need', 'normalizing', 'input', 'data', 'since', 'first', 'hidden', 'layer', 'take', 'care', 'provided', 'batch-normalized', '.', 'Moreover', 'runtime', 'penalty', 'neural', 'network', 'makes', 'slower', 'predictions', 'due', 'extra', 'computations', 'required', 'layer', '.', 'So', 'need', 'predictions', 'lightning-fast', 'may', 'want', 'check', 'well', 'plain', 'ELU', '+', 'He', 'initialization', 'perform', 'playing', 'Batch', 'Normaliza…', 'tion.You', 'may', 'find', 'training', 'rather', 'slow', 'first', 'Gradient', 'Descent', 'searching', 'optimal', 'scales', 'offsets', 'layer', 'accelerates', 'found', 'reasonably', 'good', 'values', '.', 'Vanishing/Exploding', 'Gradients', 'Problems', '|', '283', 'Implementing', 'Batch', 'Normalization', 'TensorFlowTensorFlow', 'provides', 'batch_normalization', 'function', 'simply', 'centers', 'normalizes', 'inputs', 'must', 'compute', 'mean', 'standard', 'deviation', 'your…', 'self', 'based', 'mini-batch', 'data', 'training', 'full', 'dataset', 'test…', 'ing', 'discussed', 'pass', 'parameters', 'function', 'must', 'also', 'handle', 'creation', 'scaling', 'offset', 'parameters', 'pass', 'function', '.', 'It', 'doable', 'convenient', 'approach', '.', 'Instead', 'use', 'batch_norm', 'function', 'handles', '.', 'You', 'either', 'call', 'directly', 'tell', 'fully_connected', 'function', 'use', 'followingcode', 'import', 'tensorflow', 'tffrom', 'tensorflow.contrib.layers', 'import', 'batch_normn_inputs', '=', '28', '*', '28n_hidden1', '=', '300n_hidden2', '=', '100n_outputs', '=', '10X', '=', 'tf.placeholder', 'tf.float32', 'shape=', 'None', 'n_inputs', 'name=', \"''\", 'X', \"''\", 'is_training', '=', 'tf.placeholder', 'tf.bool', 'shape=', 'name=•is_training•', 'bn_params', '=', '{', '•is_training•', 'is_training', '•decay•', '0.99', '•updates_collections•', 'None', '}', 'hidden1', '=', 'fully_connected', 'X', 'n_hidden1', 'scope=', \"''\", 'hidden1', \"''\", 'normalizer_fn=batch_norm', 'normalizer_params=bn_params', 'hidden2', '=', 'fully_connected', 'hidden1', 'n_hidden2', 'scope=', \"''\", 'hidden2', \"''\", 'normalizer_fn=batch_norm', 'normalizer_params=bn_params', 'logits', '=', 'fully_connected', 'hidden2', 'n_outputs', 'activation_fn=None', 'scope=', \"''\", 'outputs', \"''\", 'normalizer_fn=batch_norm', 'normalizer_params=bn_params', 'Let‡s', 'walk', 'code', '.', 'The', 'first', 'lines', 'fairly', 'self-explanatory', 'define', 'is_training', 'placeholder', 'either', 'True', 'False', '.', 'This', 'used', 'totell', 'batch_norm', 'function', 'whether', 'use', 'current', 'mini-batch‡s', 'mean', 'standard', 'deviation', 'training', 'running', 'averages', 'keeps', 'track', 'testing', '.Next', 'define', 'bn_params', 'dictionary', 'defines', 'parameters', 'passed', 'batch_norm', 'function', 'including', 'is_training', 'course', '.', 'The', 'algo…rithm', 'uses', 'exponential', 'decay', 'compute', 'running', 'averages', 'requires', 'decay', 'parameters', '.', 'Given', 'new', 'value', 'v', 'running', 'average', 'v', 'updated', 'equation', 'vv‰decay+', 'v‰1', '”', 'decay', '.', 'A', 'good', 'decay', 'value', 'typically', 'close', '1›for', 'example', '0.9', '0.99', '0.999', 'want', '9s', 'larger', 'datasets', '284', '|', 'Chapter', '11', 'Training', 'Deep', 'Neural', 'Nets', 'smaller', 'mini-batches', '.', 'Finally', 'updates_collections', 'set', 'None', 'youwant', 'batch_norm', 'function', 'update', 'running', 'averages', 'right', 'per…', 'forms', 'batch', 'normalization', 'training', 'i.e.', 'is_training=True', '.', 'If', 'youdon‡t', 'set', 'parameter', 'default', 'TensorFlow', 'add', 'operations', 'update', 'running', 'averages', 'collection', 'operations', 'must', 'run', '.', 'Lastly', 'create', 'layers', 'calling', 'fully_connected', 'function', 'like', 'wedid', 'Chapter', '10', 'time', 'tell', 'use', 'batch_norm', 'function', 'theparameters', 'nb_params', 'normalize', 'inputs', 'right', 'calling', 'activation', 'function.Note', 'default', 'batch_norm', 'centers', 'normalizes', 'shifts', 'inputs', 'scale', 'i.e.', '’', 'fixed', '1', '.', 'This', 'makes', 'sense', 'layers', 'activa…', 'tion', 'function', 'ReLU', 'activation', 'function', 'since', 'next', 'layer‡s', 'weights', 'take', 'care', 'scaling', 'activation', 'function', 'add', \"''\", 'scale', \"''\", 'True', 'bn_params.You', 'may', 'noticed', 'defining', 'preceding', 'three', 'layers', 'fairly', 'repetitive', 'since', 'several', 'parameters', 'identical', '.', 'To', 'avoid', 'repeating', 'parameters', 'create', 'argument', 'scope', 'using', 'arg_scope', 'function', 'first', 'parameter', 'list', 'functions', 'parameters', 'passed', 'tothese', 'functions', 'automatically', '.', 'The', 'last', 'three', 'lines', 'preceding', 'code', 'modi…fied', 'like', '...', 'tf.contrib.framework.arg_scope', 'fully_connected', 'normalizer_fn=batch_norm', 'normalizer_params=bn_params', 'hidden1', '=', 'fully_connected', 'X', 'n_hidden1', 'scope=', \"''\", 'hidden1', \"''\", 'hidden2', '=', 'fully_connected', 'hidden1', 'n_hidden2', 'scope=', \"''\", 'hidden2', \"''\", 'logits', '=', 'fully_connected', 'hidden2', 'n_outputs', 'scope=', \"''\", 'outputs', \"''\", 'activation_fn=None', 'It', 'may', 'look', 'much', 'better', 'small', 'example', '10', 'lay…', 'ers', 'want', 'set', 'activation', 'function', 'initializers', 'normalizers', 'regu…', 'larizers', 'make', 'code', 'much', 'readable', '.', 'The', 'rest', 'construction', 'phase', 'Chapter', '10', 'define', 'cost', 'func…tion', 'create', 'optimizer', 'tell', 'minimize', 'cost', 'function', 'define', 'evaluation', 'operations', 'create', 'Saver', 'on.The', 'execution', 'phase', 'also', 'pretty', 'much', 'one', 'exception', '.', 'Whenever', 'run', 'operation', 'depends', 'batch_norm', 'layer', 'need', 'set', 'is_training', 'placeholder', 'True', 'False', 'Vanishing/Exploding', 'Gradients', 'Problems', '|', '285', '8ƒOn', 'difficulty', 'training', 'recurrent', 'neural', 'networks', '⁄', 'R.', 'Pascanu', 'et', 'al', '.', '2013', '.', 'tf.Session', 'sess', 'sess.run', 'init', 'epoch', 'range', 'n_epochs', '...', 'X_batch', 'y_batch', 'zip', 'X_batches', 'y_batches', 'sess.run', 'training_op', 'feed_dict=', '{', 'is_training', 'True', 'X', 'X_batch', 'y_batch', '}', 'accuracy_score', '=', 'accuracy.eval', 'feed_dict=', '{', 'is_training', 'False', 'X', 'X_test_scaled', 'y_test', '}', 'print', 'accuracy_score', 'That‡s', '!', 'In', 'tiny', 'example', 'two', 'layers', 'it‡s', 'unlikely', 'Batch', 'Normaliza…', 'tion', 'positive', 'impact', 'deeper', 'networks', 'make', 'tremen…', 'dous', 'difference.Gradient', 'ClippingA', 'popular', 'technique', 'lessen', 'exploding', 'gradients', 'problem', 'simply', 'clip', 'gradients', 'backpropagation', 'never', 'exceed', 'threshold', 'mostly', 'useful', 'recurrent', 'neural', 'networks', 'see', 'Chapter', '14', '.', 'This', 'called', 'Gradient', 'Clipping', '.8', 'In', 'general', 'people', 'prefer', 'Batch', 'Normalization', 'it‡s', 'still', 'useful', 'know', 'Gradient', 'Clipping', 'implement', '.', 'In', 'TensorFlow', 'optimizer‡s', 'minimize', 'function', 'takes', 'care', 'computing', 'gradients', 'applying', 'must', 'instead', 'call', 'optimizer‡s', 'compute_gradients', 'method', 'first', 'create', 'operation', 'clip', 'gradients', 'using', 'clip_by_value', 'function', 'finally', 'create', 'operation', 'apply', 'clipped', 'gradi…', 'ents', 'using', 'optimizer‡s', 'apply_gradients', 'method', 'threshold', '=', '1.0optimizer', '=', 'tf.train.GradientDescentOptimizer', 'learning_rate', 'grads_and_vars', '=', 'optimizer.compute_gradients', 'loss', 'capped_gvs', '=', 'tf.clip_by_value', 'grad', '-threshold', 'threshold', 'var', 'grad', 'var', 'grads_and_vars', 'training_op', '=', 'optimizer.apply_gradients', 'capped_gvs', 'You', 'would', 'run', 'training_op', 'every', 'training', 'step', 'usual', '.', 'It', 'compute', 'gradients', 'clip', '–1.0', '1.0', 'apply', '.', 'The', 'threshold', 'hyperparameter', 'tune', '.', 'Reusing', 'Pretrained', 'LayersIt', 'generally', 'good', 'idea', 'train', 'large', 'DNN', 'scratch', 'instead', 'always', 'try', 'find', 'existing', 'neural', 'network', 'accomplishes', 'similar', 'task', '286', '|', 'Chapter', '11', 'Training', 'Deep', 'Neural', 'Nets', 'one', 'trying', 'tackle', 'reuse', 'lower', 'layers', 'network', 'called', 'transfer', 'learning', '.', 'It', 'speed', 'training', 'considerably', 'also', 'require', 'much', 'less', 'training', 'data', '.', 'For', 'example', 'suppose', 'access', 'DNN', 'trained', 'classify', 'pic…', 'tures', '100', 'different', 'categories', 'including', 'animals', 'plants', 'vehicles', 'everyday', 'objects', '.', 'You', 'want', 'train', 'DNN', 'classify', 'specific', 'types', 'vehicles', '.', 'These', 'tasks', 'similar', 'try', 'reuse', 'parts', 'first', 'network', 'see', 'Figure', '11-4', '.Figure', '11-4', '.', 'Reusing', 'pretrained', 'layers', 'If', 'input', 'pictures', 'new', 'task', 'don‡t', 'size', 'ones', 'used', 'original', 'task', 'add', 'prepro…', 'cessing', 'step', 'resize', 'size', 'expected', 'originalmodel', '.', 'More', 'generally', 'transfer', 'learning', 'work', 'well', 'inputs', 'similar', 'low-level', 'features', '.', 'Reusing', 'TensorFlow', 'ModelIf', 'original', 'model', 'trained', 'using', 'TensorFlow', 'simply', 'restore', 'train', 'new', 'task', '...', '#', 'construct', 'original', 'modelwith', 'tf.Session', 'sess', 'saver.restore', 'sess', '``', './my_original_model.ckpt', \"''\", '...', '#', 'Train', 'new', 'taskReusing', 'Pretrained', 'Layers', '|', '287', 'However', 'general', 'want', 'reuse', 'part', 'original', 'model', 'discuss', 'moment', '.', 'A', 'simple', 'solution', 'configure', 'Saver', 'restore', 'asubset', 'variables', 'original', 'model', '.', 'For', 'example', 'following', 'code', 'restores', 'hidden', 'layers', '1', '2', '3', '...', '#', 'build', 'new', 'model', 'definition', 'hidden', 'layers', '1-3init', '=', 'tf.global_variables_initializer', 'reuse_vars', '=', 'tf.get_collection', 'tf.GraphKeys.TRAINABLE_VARIABLES', 'scope=', \"''\", 'hidden', '123', \"''\", 'reuse_vars_dict', '=', 'dict', 'var.name', 'var.name', 'var', 'reuse_vars', 'original_saver', '=', 'tf.Saver', 'reuse_vars_dict', '#', 'saver', 'restore', 'original', 'modelnew_saver', '=', 'tf.Saver', '#', 'saver', 'save', 'new', 'modelwith', 'tf.Session', 'sess', 'sess.run', 'init', 'original_saver.restore', '``', './my_original_model.ckpt', \"''\", '#', 'restore', 'layers', '1', '3', '...', '#', 'train', 'new', 'model', 'new_saver.save', '``', './my_new_model.ckpt', \"''\", '#', 'save', 'whole', 'modelFirst', 'build', 'new', 'model', 'making', 'sure', 'copy', 'original', 'model‡s', 'hidden', 'layers', '1', '3', '.', 'We', 'also', 'create', 'node', 'initialize', 'variables', '.', 'Then', 'get', 'list', 'vari…', 'ables', 'created', \"''\", 'trainable=True', \"''\", 'default', 'keep', 'ones', 'whose', 'scope', 'matches', 'regular', 'expression', \"''\", 'hidden', '123', \"''\", 'i.e.', 'get', 'trainable', 'variables', 'hidden', 'layers', '1', '3', '.', 'Next', 'create', 'dictionary', 'mapping', 'name', 'variable', 'original', 'model', 'name', 'new', 'model', 'generally', 'want', 'keep', 'exact', 'names', '.', 'Then', 'create', 'Saverthat', 'restore', 'variables', 'create', 'another', 'Saver', 'save', 'entire', 'new', 'model', 'layers', '1', '3', '.', 'We', 'start', 'session', 'initialize', 'variables', 'model', 'restore', 'variable', 'values', 'original', 'model‡s', 'layers', '1', '3', '.', 'Finally', 'train', 'model', 'new', 'task', 'save', '.', 'The', 'similar', 'tasks', 'layers', 'want', 'reuse', 'starting', 'lower', 'layers', '.', 'For', 'similar', 'tasks', 'try', 'keeping', 'hidden', 'layers', 'replace', 'output', 'layer', '.', 'Reusing', 'Models', 'Other', 'FrameworksIf', 'model', 'trained', 'using', 'another', 'framework', 'need', 'load', 'weights', 'manually', 'e.g.', 'using', 'Theano', 'code', 'trained', 'Theano', 'assign', 'appropriate', 'variables', '.', 'This', 'quite', 'tedious', '.', 'For', 'example', 'following', 'code', 'shows', 'would', 'copy', 'weight', 'biases', 'first', 'hidden', 'layer', 'model', 'trained', 'using', 'another', 'framework:288', '|', 'Chapter', '11', 'Training', 'Deep', 'Neural', 'Nets', 'original_w', '=', '...', '#', 'Load', 'weights', 'frameworkoriginal_b', '=', '...', '#', 'Load', 'biases', 'frameworkX', '=', 'tf.placeholder', 'tf.float32', 'shape=', 'None', 'n_inputs', 'name=', \"''\", 'X', \"''\", 'hidden1', '=', 'fully_connected', 'X', 'n_hidden1', 'scope=', \"''\", 'hidden1', \"''\", '...', '#', '#', 'Build', 'rest', 'model', '#', 'Get', 'handle', 'variables', 'created', 'fully_connected', 'tf.variable_scope', '``', \"''\", 'default_name=', \"''\", \"''\", 'reuse=True', '#', 'root', 'scope', 'hidden1_weights', '=', 'tf.get_variable', '``', 'hidden1/weights', \"''\", 'hidden1_biases', '=', 'tf.get_variable', '``', 'hidden1/biases', \"''\", '#', 'Create', 'nodes', 'assign', 'arbitrary', 'values', 'weights', 'biasesoriginal_weights', '=', 'tf.placeholder', 'tf.float32', 'shape=', 'n_inputs', 'n_hidden1', 'original_biases', '=', 'tf.placeholder', 'tf.float32', 'shape=', 'n_hidden1', 'assign_hidden1_weights', '=', 'tf.assign', 'hidden1_weights', 'original_weights', 'assign_hidden1_biases', '=', 'tf.assign', 'hidden1_biases', 'original_biases', 'init', '=', 'tf.global_variables_initializer', 'tf.Session', 'sess', 'sess.run', 'init', 'sess.run', 'assign_hidden1_weights', 'feed_dict=', '{', 'original_weights', 'original_w', '}', 'sess.run', 'assign_hidden1_biases', 'feed_dict=', '{', 'original_biases', 'original_b', '}', '...', '#', 'Train', 'model', 'new', 'taskFreezing', 'Lower', 'LayersIt', 'likely', 'lower', 'layers', 'first', 'DNN', 'learned', 'detect', 'low-level', 'fea…', 'tures', 'pictures', 'useful', 'across', 'image', 'classification', 'tasks', 'reuse', 'layers', '.', 'It', 'generally', 'good', 'idea', 'ƒfreeze⁄', 'weights', 'training', 'new', 'DNN', 'lower-layer', 'weights', 'fixed', 'higher-', 'layer', 'weights', 'easier', 'train', 'won‡t', 'learn', 'moving', 'target', '.', 'To', 'freeze', 'lower', 'layers', 'training', 'simplest', 'solution', 'give', 'opti…', 'mizer', 'list', 'variables', 'train', 'excluding', 'variables', 'lower', 'layers', 'train_vars', '=', 'tf.get_collection', 'tf.GraphKeys.TRAINABLE_VARIABLES', 'scope=', \"''\", 'hidden', '34', '|outputs', \"''\", 'training_op', '=', 'optimizer.minimize', 'loss', 'var_list=train_vars', 'The', 'first', 'line', 'gets', 'list', 'trainable', 'variables', 'hidden', 'layers', '3', '4', 'output', 'layer', '.', 'This', 'leaves', 'variables', 'hidden', 'layers', '1', '2', '.', 'Next', 'pro…', 'vide', 'restricted', 'list', 'trainable', 'variables', 'optimizer‡s', 'minimize', 'function.Ta-da', '!', 'Layers', '1', '2', 'frozen', 'budge', 'training', 'often', 'called', 'frozen', 'layers', '.Reusing', 'Pretrained', 'Layers', '|', '289', 'Caching', 'Frozen', 'LayersSince', 'frozen', 'layers', 'won‡t', 'change', 'possible', 'cache', 'output', 'topmost', 'frozen', 'layer', 'training', 'instance', '.', 'Since', 'training', 'goes', 'whole', 'dataset', 'many', 'times', 'give', 'huge', 'speed', 'boost', 'need', 'go', 'frozen', 'layers', 'per', 'training', 'instance', 'instead', 'per', 'epoch', '.', 'For', 'example', 'could', 'first', 'run', 'whole', 'training', 'set', 'lower', 'layers', 'assuming', 'enough', 'RAM', 'hidden2_outputs', '=', 'sess.run', 'hidden2', 'feed_dict=', '{', 'X', 'X_train', '}', 'Then', 'training', 'instead', 'building', 'batches', 'training', 'instances', 'would', 'build', 'batches', 'outputs', 'hidden', 'layer', '2', 'feed', 'training', 'operation', 'import', 'numpy', 'npn_epochs', '=', '100n_batches', '=', '500for', 'epoch', 'range', 'n_epochs', 'shuffled_idx', '=', 'rnd.permutation', 'len', 'hidden2_outputs', 'hidden2_batches', '=', 'np.array_split', 'hidden2_outputs', 'shuffled_idx', 'n_batches', 'y_batches', '=', 'np.array_split', 'y_train', 'shuffled_idx', 'n_batches', 'hidden2_batch', 'y_batch', 'zip', 'hidden2_batches', 'y_batches', 'sess.run', 'training_op', 'feed_dict=', '{', 'hidden2', 'hidden2_batch', 'y_batch', '}', 'The', 'last', 'line', 'runs', 'training', 'operation', 'defined', 'earlier', 'freezes', 'layers', '1', '2', 'feeds', 'batch', 'outputs', 'second', 'hidden', 'layer', 'well', 'targets', 'batch', '.', 'Since', 'give', 'TensorFlow', 'output', 'hidden', 'layer', '2', 'try', 'evaluate', 'node', 'depends', '.', 'Tweaking', 'Dropping', 'Replacing', 'Upper', 'LayersThe', 'output', 'layer', 'original', 'model', 'usually', 'replaced', 'since', 'likely', 'useful', 'new', 'task', 'may', 'even', 'right', 'number', 'outputs', 'new', 'task.Similarly', 'upper', 'hidden', 'layers', 'original', 'model', 'less', 'likely', 'useful', 'lower', 'layers', 'since', 'high-level', 'features', 'useful', 'new', 'task', 'may', 'differ', 'significantly', 'ones', 'useful', 'original', 'task', '.', 'You', 'want', 'find', 'right', 'number', 'layers', 'reuse', '.', 'Try', 'freezing', 'copied', 'layers', 'first', 'train', 'model', 'see', 'performs', '.', 'Then', 'try', 'unfreezing', 'one', 'two', 'top', 'hidden', 'layers', 'let', 'backpropagation', 'tweak', 'see', 'performance', 'improves', '.', 'The', 'training', 'data', 'layers', 'unfreeze', '.', 'If', 'still', 'get', 'good', 'performance', 'little', 'training', 'data', 'try', 'drop…', 'ping', 'top', 'hidden', 'layer', 'freeze', 'remaining', 'hidden', 'layers', '.', 'You', '290', '|', 'Chapter', '11', 'Training', 'Deep', 'Neural', 'Nets', 'iterate', 'find', 'right', 'number', 'layers', 'reuse', '.', 'If', 'plenty', 'train…', 'ing', 'data', 'may', 'try', 'replacing', 'top', 'hidden', 'layers', 'instead', 'dropping', 'even', 'add', 'hidden', 'layers', '.', 'Model', 'ZoosWhere', 'find', 'neural', 'network', 'trained', 'task', 'similar', 'one', 'want', 'tackle', '?', 'The', 'first', 'place', 'look', 'obviously', 'catalog', 'models', '.', 'This', 'one', 'good', 'reason', 'save', 'models', 'organize', 'retrieve', 'later', 'easily', '.', 'Another', 'option', 'search', 'model', 'zoo', '.', 'Many', 'people', 'train', 'Machine', 'Learn…', 'ing', 'models', 'various', 'tasks', 'kindly', 'release', 'pretrained', 'models', 'public.TensorFlow', 'model', 'zoo', 'available', 'https', '//github.com/tensor‡ow/models.In', 'particular', 'contains', 'state-of-the-art', 'image', 'classification', 'nets', 'VGG', 'Inception', 'ResNet', 'see', 'Chapter', '13', 'check', 'models/slim', 'direc…tory', 'including', 'code', 'pretrained', 'models', 'tools', 'download', 'popular', 'image', 'datasets', '.', 'Another', 'popular', 'model', 'zoo', 'Caffe‡s', 'Model', 'Zoo', '.', 'It', 'also', 'contains', 'many', 'computer', 'vision', 'models', 'e.g.', 'LeNet', 'AlexNet', 'ZFNet', 'GoogLeNet', 'VGGNet', 'inception', 'trained', 'various', 'datasets', 'e.g.', 'ImageNet', 'Places', 'Database', 'CIFAR10', 'etc.', '.', 'Saumitro', 'Das…', 'gupta', 'wrote', 'converter', 'available', 'https', '//github.com/ethereon/ca›e-tensor‡ow.Unsupervised', 'PretrainingSuppose', 'want', 'tackle', 'complex', 'task', 'don‡t', 'much', 'labeled', 'training', 'data', 'unfortunately', 'find', 'model', 'trained', 'similar', 'task', '.', 'Don‡t', 'lose', 'hope', '!', 'First', 'course', 'try', 'gather', 'labeled', 'training', 'data', 'hard', 'expensive', 'may', 'still', 'able', 'perform', 'unsuper…', 'vised', 'pretraining', 'see', 'Figure', '11-5', '.', 'That', 'plenty', 'unlabeled', 'training', 'data', 'try', 'train', 'layers', 'one', 'one', 'starting', 'lowest', 'layer', 'going', 'using', 'unsupervised', 'feature', 'detector', 'algorithm', 'Restricted', 'Boltz…', 'mann', 'Machines', 'RBMs', 'see', 'Appendix', 'E', 'autoencoders', 'see', 'Chapter', '15', '.', 'Eachlayer', 'trained', 'output', 'previously', 'trained', 'layers', 'layers', 'except', 'one', 'trained', 'frozen', '.', 'Once', 'layers', 'trained', 'way', 'fine-', 'tune', 'network', 'using', 'supervised', 'learning', 'i.e.', 'backpropagation', '.', 'This', 'rather', 'long', 'tedious', 'process', 'often', 'works', 'well', 'fact', 'technique', 'Geoffrey', 'Hinton', 'team', 'used', '2006', 'led', 'revival', 'neural', 'networks', 'success', 'Deep', 'Learning', '.', 'Until', '2010', 'unsuper…', 'vised', 'pretraining', 'typically', 'using', 'RBMs', 'norm', 'deep', 'nets', 'onlyafter', 'vanishing', 'gradients', 'problem', 'alleviated', 'became', 'much', 'com…', 'mon', 'train', 'DNNs', 'purely', 'using', 'backpropagation', '.', 'However', 'unsupervised', 'pretrain…', 'ing', 'today', 'typically', 'using', 'autoencoders', 'rather', 'RBMs', 'still', 'good', 'option', 'Reusing', 'Pretrained', 'Layers', '|', '291', '9Another', 'option', 'come', 'supervised', 'task', 'easily', 'gather', 'lot', 'labeled', 'training', 'data', 'use', 'transfer', 'learning', 'explained', 'earlier', '.', 'For', 'example', 'want', 'train', 'model', 'identify', 'friends', 'pictures', 'could', 'download', 'millions', 'faces', 'internet', 'train', 'classifier', 'detect', 'whether', 'two', 'faces', 'identical', 'use', 'classifier', 'compare', 'new', 'picture', 'picture', 'friends.you', 'complex', 'task', 'solve', 'similar', 'model', 'reuse', 'little', 'labeled', 'training', 'data', 'plenty', 'unlabeled', 'training', 'data', '.', '9Figure', '11-5', '.', 'Unsupervised', 'pretraining', 'Pretraining', 'Auxiliary', 'TaskOne', 'last', 'option', 'train', 'first', 'neural', 'network', 'auxiliary', 'task', 'easily', 'obtain', 'generate', 'labeled', 'training', 'data', 'reuse', 'lower', 'layers', 'network', 'actual', 'task', '.', 'The', 'first', 'neural', 'network‡s', 'lower', 'layers', 'learn', 'feature', 'detectors', 'likely', 'reusable', 'second', 'neural', 'network', '.', 'For', 'example', 'want', 'build', 'system', 'recognize', 'faces', 'may', 'pictures', 'individual›clearly', 'enough', 'train', 'good', 'classifier', '.', 'Gather…', 'ing', 'hundreds', 'pictures', 'person', 'would', 'practical', '.', 'However', 'could', 'gather', 'lot', 'pictures', 'random', 'people', 'internet', 'train', 'first', 'neural', 'net…', 'work', 'detect', 'whether', 'two', 'different', 'pictures', 'feature', 'person', '.', 'Such', '292', '|', 'Chapter', '11', 'Training', 'Deep', 'Neural', 'Nets', '10At', 'least', 'research', 'moving', 'fast', 'especially', 'field', 'optimization', '.', 'Be', 'sure', 'take', 'look', 'latest', 'greatest', 'optimizers', 'every', 'time', 'new', 'version', 'TensorFlow', 'released', '.', 'network', 'would', 'learn', 'good', 'feature', 'detectors', 'faces', 'reusing', 'lower', 'layers', 'would', 'allow', 'train', 'good', 'face', 'classifier', 'using', 'little', 'training', 'data', '.', 'It', 'often', 'rather', 'cheap', 'gather', 'unlabeled', 'training', 'examples', 'quite', 'expensive', 'label', '.', 'In', 'situation', 'common', 'technique', 'label', 'training', 'exam…', 'ples', 'ƒgood', '⁄', 'generate', 'many', 'new', 'training', 'instances', 'corrupting', 'good', 'ones', 'label', 'corrupted', 'instances', 'ƒbad.⁄', 'Then', 'train', 'first', 'neural', 'network', 'classify', 'instances', 'good', 'bad', '.', 'For', 'example', 'could', 'download', 'mil…', 'lions', 'sentences', 'label', 'ƒgood', '⁄', 'randomly', 'change', 'word', 'sen…', 'tence', 'label', 'resulting', 'sentences', 'ƒbad.⁄', 'If', 'neural', 'network', 'tell', 'ƒThe', 'dog', 'sleeps⁄', 'good', 'sentence', 'ƒThe', 'dog', 'they⁄', 'bad', 'probably', 'knows', 'quite', 'lot', 'language', '.', 'Reusing', 'lower', 'layers', 'likely', 'help', 'many', 'language', 'processing', 'tasks.Another', 'approach', 'train', 'first', 'network', 'output', 'score', 'training', 'instance', 'use', 'cost', 'function', 'ensures', 'good', 'instance‡s', 'score', 'greater', 'bad', 'instance‡s', 'score', 'least', 'margin', '.', 'This', 'called', 'max', 'margin', 'learn…', 'ing', '.Faster', 'OptimizersTraining', 'large', 'deep', 'neural', 'network', 'painfully', 'slow', '.', 'So', 'far', 'seen', 'four', 'ways', 'speed', 'training', 'reach', 'better', 'solution', 'applying', 'good', 'initiali…', 'zation', 'strategy', 'connection', 'weights', 'using', 'good', 'activation', 'function', 'using', 'Batch', 'Normalization', 'reusing', 'parts', 'pretrained', 'network', '.', 'Another', 'huge', 'speed', 'boost', 'comes', 'using', 'faster', 'optimizer', 'regular', 'Gradient', 'Descent', 'opti…', 'mizer', '.', 'In', 'section', 'present', 'popular', 'ones', 'Momentum', 'optimiza…', 'tion', 'Nesterov', 'Accelerated', 'Gradient', 'AdaGrad', 'RMSProp', 'finally', 'Adam', 'optimization', '.', 'Spoiler', 'alert', 'conclusion', 'section', 'almost', 'always', 'use', 'Adam', 'optimization', '10', 'don‡t', 'care', 'works', 'simply', 'replace', 'GradientDescentOptimizer', 'AdamOptimizer', 'skip', 'next', 'section', '!', 'With', 'small', 'change', 'training', 'typically', 'several', 'times', 'faster', '.', 'However', 'Adam', 'optimization', 'three', 'hyperparameters', 'tune', 'plus', 'learning', 'rate', 'default', 'values', 'usually', 'work', 'fine', 'ever', 'need', 'tweak', 'may', 'helpful', 'know', '.', 'Adam', 'optimization', 'combines', 'several', 'ideas', 'optimization', 'algorithms', 'useful', 'look', 'algorithms', 'first', '.', 'Faster', 'Optimizers', '|', '293', '11ƒSome', 'methods', 'speeding', 'convergence', 'iteration', 'methods', '⁄', 'B.', 'Polyak', '1964', '.', 'Momentum', 'optimizationImagine', 'bowling', 'ball', 'rolling', 'gentle', 'slope', 'smooth', 'surface', 'start', 'slowly', 'quickly', 'pick', 'momentum', 'eventually', 'reaches', 'terminal', 'velocity', 'friction', 'air', 'resistance', '.', 'This', 'simple', 'idea', 'behind', 'Momentum', 'optimization', 'proposed', 'Boris', 'Polyak', '1964', '.11', 'In', 'contrast', 'regular', 'Gradient', 'Descent', 'simply', 'take', 'small', 'regular', 'steps', 'slope', 'take', 'much', 'time', 'reach', 'bottom', '.', 'Recall', 'Gradient', 'Descent', 'simply', 'updates', 'weights', '–', 'directly', 'subtracting', 'thegradient', 'cost', 'function', 'J', '–', 'regards', 'weights', '–J', '–', 'multiplied', 'learning', 'rate', '−', '.', 'The', 'equation', '–', '¾', '–', '–', '−–J', '–', '.', 'It', 'care', 'earlier', 'gradients', '.', 'If', 'local', 'gradient', 'tiny', 'goes', 'slowly', '.', 'Momentum', 'optimization', 'cares', 'great', 'deal', 'previous', 'gradients', 'iteration', 'adds', 'local', 'gradient', 'momentum', 'vector', 'multiplied', 'learning', 'rate', '−', 'updates', 'weights', 'simply', 'subtracting', 'momentum', 'vector', 'see', 'Equation', '11-4', '.', 'In', 'words', 'gradient', 'used', 'acceleration', 'speed', '.', 'To', 'simulate', 'sort', 'friction', 'mechanism', 'prevent', 'momentum', 'growing', 'large', 'algorithm', 'introduces', 'new', 'hyperparameter', 'Ł', 'simply', 'called', 'momentum', 'must', 'set', '0', 'high', 'friction', '1', 'friction', '.', 'A', 'typical', 'momentum', 'value', '0.9', '.', 'Equation', '11-4', '.', 'Momentum', 'algorithm', '1', '.', 'Ł+−–J–2', '.', '––', '”', 'You', 'easily', 'verify', 'gradient', 'remains', 'constant', 'terminal', 'velocity', 'i.e.', 'maximum', 'size', 'weight', 'updates', 'equal', 'gradient', 'multiplied', 'learning', 'rate', '−', 'multiplied', '11', '”', 'Ł', '.', 'For', 'example', 'Ł', '=', '0.9', 'terminal', 'velocityis', 'equal', '10', 'times', 'gradient', 'times', 'learning', 'rate', 'Momentum', 'optimization', 'ends', 'going', '10', 'times', 'faster', 'Gradient', 'Descent', '!', 'This', 'allows', 'Momentum', 'opti…', 'mization', 'escape', 'plateaus', 'much', 'faster', 'Gradient', 'Descent', '.', 'In', 'particular', 'saw', 'Chapter', '4', 'inputs', 'different', 'scales', 'cost', 'function', 'look', 'like', 'elongated', 'bowl', 'see', 'Figure', '4-7', '.', 'Gradient', 'Descent', 'goes', 'steep', 'slope', 'quite', 'fast', 'takes', 'long', 'time', 'go', 'valley', '.', 'In', 'con…', 'trast', 'Momentum', 'optimization', 'roll', 'bottom', 'valley', 'faster', 'faster', 'reaches', 'bottom', 'optimum', '.', 'In', 'deep', 'neural', 'networks', 'don‡t', 'use', 'Batch', 'Normalization', 'upper', 'layers', 'often', 'end', 'inputs', '294', '|', 'Chapter', '11', 'Training', 'Deep', 'Neural', 'Nets', '12ƒA', 'Method', 'Unconstrained', 'Convex', 'Minimization', 'Problem', 'Rate', 'Convergence', 'O', '1/k', '2', '⁄', 'Yurii', 'Nesterov', '1983', '.', 'different', 'scales', 'using', 'Momentum', 'optimization', 'helps', 'lot', '.', 'It', 'also', 'help', 'roll', 'past', 'local', 'optima.Due', 'momentum', 'optimizer', 'may', 'overshoot', 'bit', 'come', 'back', 'overshoot', 'oscillate', 'like', 'many', 'times', 'stabilizing', 'minimum', '.', 'This', 'one', 'reasons', 'good', 'bit', 'friction', 'system', 'gets', 'rid', 'oscillations', 'thus', 'speeds', 'convergence', '.', 'Implementing', 'Momentum', 'optimization', 'TensorFlow', 'no-brainer', 'replace', 'GradientDescentOptimizer', 'MomentumOptimizer', 'lie', 'back', 'andprofit', '!', 'optimizer', '=', 'tf.train.MomentumOptimizer', 'learning_rate=learning_rate', 'momentum=0.9', 'The', 'one', 'drawback', 'Momentum', 'optimization', 'adds', 'yet', 'another', 'hyperpara…', 'meter', 'tune', '.', 'However', 'momentum', 'value', '0.9', 'usually', 'works', 'well', 'practice', 'almost', 'always', 'goes', 'faster', 'Gradient', 'Descent', '.', 'Nesterov', 'Accelerated', 'GradientOne', 'small', 'variant', 'Momentum', 'optimization', 'proposed', 'Yurii', 'Nesterov', '1983', ',12is', 'almost', 'always', 'faster', 'vanilla', 'Momentum', 'optimization', '.', 'The', 'idea', 'Nesterov', 'Momentum', 'optimization', 'Nesterov', 'Accelerated', 'Gradient', 'NAG', 'measure', 'gradient', 'cost', 'function', 'local', 'position', 'slightly', 'ahead', 'direc…', 'tion', 'momentum', 'see', 'Equation', '11-5', '.', 'The', 'difference', 'vanillaMomentum', 'optimization', 'gradient', 'measured', '–', '+', 'Łm', 'rather', '–.Equation', '11-5', '.', 'Nesterov', 'Accelerated', 'Gradient', 'algorithm', '1', '.', 'Ł+−–J–+Ł2', '.', '––', '”', 'This', 'small', 'tweak', 'works', 'general', 'momentum', 'vector', 'pointing', 'right', 'direction', 'i.e.', 'toward', 'optimum', 'slightly', 'accurate', 'use', 'gradient', 'measured', 'bit', 'farther', 'direction', 'rather', 'using', 'gradi…', 'ent', 'original', 'position', 'see', 'Figure', '11-6', '1', 'represents', 'gradient', 'cost', 'function', 'measured', 'starting', 'point', '–', '2', 'represents', 'gradient', 'point', 'located', '–', '+', 'Łm', '.', 'As', 'see', 'Nesterov', 'update', 'ends', 'Faster', 'Optimizers', '|', '295', 'slightly', 'closer', 'optimum', '.', 'After', 'small', 'improvements', 'add', 'NAG', 'ends', 'significantly', 'faster', 'regular', 'Momentum', 'optimization', '.', 'More…', 'note', 'momentum', 'pushes', 'weights', 'across', 'valley', '1', 'continues', 'push', 'across', 'valley', '2', 'pushes', 'back', 'toward', 'bottom', 'val…ley', '.', 'This', 'helps', 'reduce', 'oscillations', 'thus', 'converges', 'faster', '.', 'Figure', '11-6', '.', 'Regular', 'versus', 'Nesterov', 'Momentum', 'optimization', 'NAG', 'almost', 'always', 'speed', 'training', 'compared', 'regular', 'Momentum', 'optimi…', 'zation', '.', 'To', 'use', 'simply', 'set', 'use_nesterov=True', 'creating', 'MomentumOptimizer', 'optimizer', '=', 'tf.train.MomentumOptimizer', 'learning_rate=learning_rate', 'momentum=0.9', 'use_nesterov=True', 'AdaGradConsider', 'elongated', 'bowl', 'problem', 'Gradient', 'Descent', 'starts', 'quickly', 'going', 'steepest', 'slope', 'slowly', 'goes', 'bottom', 'valley', '.', 'It', 'would', 'nice', 'algorithm', 'could', 'detect', 'early', 'correct', 'direction', 'point', 'bit', 'toward', 'global', 'optimum', '.', '296', '|', 'Chapter', '11', 'Training', 'Deep', 'Neural', 'Nets', '13ƒAdaptive', 'Subgradient', 'Methods', 'Online', 'Learning', 'Stochastic', 'Optimization', '⁄', 'J.', 'Duchi', 'et', 'al', '.', '2011', '.', 'The', 'AdaGrad', 'algorithm13', 'achieves', 'scaling', 'gradient', 'vector', 'along', 'steepest', 'dimensions', 'see', 'Equation', '11-6', 'Equation', '11-6', '.', 'AdaGrad', 'algorithm', '1', '.', '+–J–', '–J–2', '.', '––', '”', '−–J–+The', 'first', 'step', 'accumulates', 'square', 'gradients', 'vector', 'symbolrepresents', 'element-wise', 'multiplication', '.', 'This', 'vectorized', 'form', 'equivalent', 'computing', 'si', '¾', 'si', '+', 'ﬂ', '/', 'ﬂ', '–i', 'J', '–', '2', 'element', 'si', 'vector', 'words', 'si', 'accumulates', 'squares', 'partial', 'derivative', 'cost', 'function', 'regards', 'parameter', '–i', '.', 'If', 'cost', 'function', 'steep', 'along', 'ith', 'dimension', 'siwill', 'get', 'larger', 'larger', 'iteration', '.', 'The', 'second', 'step', 'almost', 'identical', 'Gradient', 'Descent', 'one', 'big', 'difference', 'gradient', 'vector', 'scaled', 'factor', '+', 'symbol', 'represents', 'element-wise', 'division', 'smoothing', 'term', 'avoid', 'division', 'zero', 'typically', 'set', '10–10', '.', 'This', 'vectorized', 'form', 'equivalent', 'computing', '–i–i', '”', '−ﬂ/ﬂ', '–iJ–/si+', 'parameters', '–i', 'simultaneously', '.', 'In', 'short', 'algorithm', 'decays', 'learning', 'rate', 'faster', 'steep', 'dimen…', 'sions', 'dimensions', 'gentler', 'slopes', '.', 'This', 'called', 'adaptive', 'learning', 'rate', '.', 'It', 'helps', 'point', 'resulting', 'updates', 'directly', 'toward', 'global', 'optimum', 'see', 'Figure', '11-7', '.', 'One', 'additional', 'benefit', 'requires', 'much', 'less', 'tuning', 'learn…', 'ing', 'rate', 'hyperparameter', '−.Figure', '11-7', '.', 'AdaGrad', 'versus', 'Gradient', 'Descent', 'Faster', 'Optimizers', '|', '297', '14This', 'algorithm', 'created', 'Tijmen', 'Tieleman', 'Geoffrey', 'Hinton', '2012', 'presented', 'Geoffrey', 'Hinton', 'Coursera', 'class', 'neural', 'networks', 'slides', 'http', '//goo.gl/RsQeis', 'video', 'https', '//goo.gl/XUbIyJ', '.Amusingly', 'since', 'authors', 'written', 'paper', 'describe', 'researchers', 'often', 'cite', 'ƒslide', '29', 'lecture', '6⁄', 'papers', '.', '15ƒAdam', 'A', 'Method', 'Stochastic', 'Optimization', '⁄', 'D.', 'Kingma', 'J.', 'Ba', '2015', '.', 'AdaGrad', 'often', 'performs', 'well', 'simple', 'quadratic', 'problems', 'unfortunately', 'often', 'stops', 'early', 'training', 'neural', 'networks', '.', 'The', 'learning', 'rate', 'gets', 'scaled', 'much', 'algorithm', 'ends', 'stopping', 'entirely', 'reaching', 'global', 'optimum', '.', 'So', 'even', 'though', 'TensorFlow', 'AdagradOptimizer', 'shouldnot', 'use', 'train', 'deep', 'neural', 'networks', 'may', 'efficient', 'simpler', 'tasks', 'Linear', 'Regression', 'though', '.RMSPropAlthough', 'AdaGrad', 'slows', 'bit', 'fast', 'ends', 'never', 'converging', 'global', 'optimum', 'RMSProp', 'algorithm14', 'fixes', 'accumulating', 'gradi…', 'ents', 'recent', 'iterations', 'opposed', 'gradients', 'since', 'begin…', 'ning', 'training', '.', 'It', 'using', 'exponential', 'decay', 'first', 'step', 'see', 'Equation', '11-7', '.Equation', '11-7', '.', 'RMSProp', 'algorithm', '1', '.', 'Ł+1', '”', 'Ł–J–', '–J–2', '.', '––', '”', '−–J–+The', 'decay', 'rate', 'Ł', 'typically', 'set', '0.9', '.', 'Yes', 'new', 'hyperparameter', 'default', 'value', 'often', 'works', 'well', 'may', 'need', 'tune', '.', 'As', 'might', 'expect', 'TensorFlow', 'RMSPropOptimizer', 'class', 'optimizer', '=', 'tf.train.RMSPropOptimizer', 'learning_rate=learning_rate', 'momentum=0.9', 'decay=0.9', 'epsilon=1e-10', 'Except', 'simple', 'problems', 'optimizer', 'almost', 'always', 'performs', 'much', 'better', 'AdaGrad', '.', 'It', 'also', 'generally', 'performs', 'better', 'Momentum', 'optimization', 'Nesterov', 'Accelerated', 'Gradients', '.', 'In', 'fact', 'preferred', 'optimization', 'algorithm', 'many', 'researchers', 'Adam', 'optimization', 'came', 'around', '.', 'Adam', 'OptimizationAdam', ',15', 'stands', 'adaptive', 'moment', 'estimation', 'combines', 'ideas', 'Momen…', 'tum', 'optimization', 'RMSProp', 'like', 'Momentum', 'optimization', 'keeps', 'track', 'exponentially', 'decaying', 'average', 'past', 'gradients', 'like', 'RMSProp', 'keeps', '298', '|', 'Chapter', '11', 'Training', 'Deep', 'Neural', 'Nets', '16These', 'estimations', 'mean', 'uncentered', 'variance', 'gradients', '.', 'The', 'mean', 'often', 'called', '†rst', 'moment', 'variance', 'often', 'called', 'second', 'moment', 'hence', 'name', 'algorithm.track', 'exponentially', 'decaying', 'average', 'past', 'squared', 'gradients', 'see', 'Equation', '11-8', '.16Equation', '11-8', '.', 'Adam', 'algorithm', '1', '.', 'Ł1+1', '”', 'Ł1–J–2', '.', 'Ł2+1', '”', 'Ł2–J–', '–J–3', '.', '1', '”', 'Ł1T4', '.', '1', '”', 'Ł2T5', '.', '––', '”', '−', '+‹T', 'represents', 'iteration', 'number', 'starting', '1', '.', 'If', 'look', 'steps', '1', '2', '5', 'notice', 'Adam‡s', 'close', 'similarity', 'Momentum', 'optimization', 'RMSProp', '.', 'The', 'difference', 'step', '1', 'computes', 'exponentially', 'decaying', 'average', 'rather', 'exponentially', 'decaying', 'sum', 'actually', 'equivalent', 'except', 'constant', 'factor', 'decaying', 'average', '1', '–', 'Ł1', 'times', 'decaying', 'sum', '.', 'Steps', '3', '4', 'somewhat', 'technical', 'detail', 'since', 'initialized', '0', 'biased', 'toward', '0', 'beginning', 'training', 'two', 'steps', 'help', 'boost', 'beginning', 'training', '.', 'The', 'momentum', 'decay', 'hyperparameter', 'Ł1', 'typically', 'initialized', '0.9', 'scal…ing', 'decay', 'hyperparameter', 'Ł2', 'often', 'initialized', '0.999', '.', 'As', 'earlier', 'smoothingterm', 'usually', 'initialized', 'tiny', 'number', '10', '–8', '.', 'These', 'default', 'values', 'TensorFlow‡s', 'AdamOptimizer', 'class', 'simply', 'use', 'optimizer', '=', 'tf.train.AdamOptimizer', 'learning_rate=learning_rate', 'In', 'fact', 'since', 'Adam', 'adaptive', 'learning', 'rate', 'algorithm', 'like', 'AdaGrad', 'RMSProp', 'requires', 'less', 'tuning', 'learning', 'rate', 'hyperparameter', '−', '.', 'You', 'often', 'use', 'default', 'value', '−', '=', '0.001', 'making', 'Adam', 'even', 'easier', 'use', 'Gradient', 'Descent', '.', 'Faster', 'Optimizers', '|', '299', '17ƒPrimal-Dual', 'Subgradient', 'Methods', 'Convex', 'Problems', '⁄', 'Yurii', 'Nesterov', '2005', '.', '18ƒAd', 'Click', 'Prediction', 'View', 'Trenches', '⁄', 'H.', 'McMahan', 'et', 'al', '.', '2013', '.', 'All', 'optimization', 'techniques', 'discussed', 'far', 'rely', '†rst-order', 'partial', 'derivatives', 'Jacobians', '.', 'The', 'optimization', 'litera…', 'ture', 'contains', 'amazing', 'algorithms', 'based', 'second-order', 'partial', 'derivatives', 'Hessians', '.', 'Unfortunately', 'algorithms', 'hard', 'apply', 'deep', 'neural', 'networks', 'n2', 'Hessi…', 'ans', 'per', 'output', 'n', 'number', 'parameters', 'opposed', 'n', 'Jacobians', 'per', 'output', '.', 'Since', 'DNNs', 'typically', 'tens', 'thousands', 'parameters', 'second-order', 'optimization', 'algo…', 'rithms', 'often', 'don‡t', 'even', 'fit', 'memory', 'even', 'computing', 'Hessians', 'slow', '.', 'Training', 'Sparse', 'ModelsAll', 'optimization', 'algorithms', 'presented', 'produce', 'dense', 'models', 'meaning', 'parameters', 'nonzero', '.', 'If', 'need', 'blazingly', 'fast', 'model', 'runtime', 'need', 'take', 'less', 'memory', 'may', 'prefer', 'end', 'sparse', 'model', 'instead.One', 'trivial', 'way', 'achieve', 'train', 'model', 'usual', 'get', 'rid', 'tiny', 'weights', 'set', '0', '.', 'Another', 'option', 'apply', 'strong', '—', '1', 'regularization', 'training', 'pushes', 'optimizer', 'zero', 'many', 'weights', 'discussed', 'Chapter', '4', 'Lasso', 'Regression', '.However', 'cases', 'techniques', 'may', 'remain', 'insufficient', '.', 'One', 'last', 'option', 'apply', 'Dual', 'Averaging', 'often', 'called', 'Follow', '•e', 'Regularized', 'Leader', 'FTRL', 'techni…que', 'proposed', 'Yurii', 'Nesterov', '.17', 'When', 'used', '—1', 'regularization', 'technique', 'often', 'leads', 'sparse', 'models', '.', 'TensorFlow', 'implements', 'variant', 'FTRL', 'called', 'FTRL-Proximal', '18', 'FTRLOptimizer', 'class.Learning', 'Rate', 'SchedulingFinding', 'good', 'learning', 'rate', 'tricky', '.', 'If', 'set', 'way', 'high', 'training', 'may', 'actually', 'diverge', 'discussed', 'Chapter', '4', '.', 'If', 'set', 'low', 'training', 'eventually', 'converge', 'optimum', 'take', 'long', 'time', '.', 'If', 'set', 'slightly', 'high', 'make', 'progress', 'quickly', 'first', 'end', 'dancing', 'around', 'optimum', 'never', 'settling', 'unless', 'use', 'adaptive', 'learning', 'rate', 'optimization', 'algorithm', 'AdaGrad', 'RMSProp', 'Adam', 'even', 'may', 'take', 'time', 'settle', '.', 'If', 'limited', 'computing', 'budget', 'may', 'inter…', '300', '|', 'Chapter', '11', 'Training', 'Deep', 'Neural', 'Nets', 'rupt', 'training', 'converged', 'properly', 'yielding', 'suboptimal', 'solution', 'see', 'Figure', '11-8', '.Figure', '11-8', '.', 'Learning', 'curves', 'various', 'learning', 'rates', '−', 'You', 'may', 'able', 'find', 'fairly', 'good', 'learning', 'rate', 'training', 'network', 'several', 'times', 'epochs', 'using', 'various', 'learning', 'rates', 'comparing', 'learn…', 'ing', 'curves', '.', 'The', 'ideal', 'learning', 'rate', 'learn', 'quickly', 'converge', 'good', 'solution', '.', 'However', 'better', 'constant', 'learning', 'rate', 'start', 'high', 'learning', 'rate', 'reduce', 'stops', 'making', 'fast', 'progress', 'reach', 'good', 'solution', 'faster', 'optimal', 'constant', 'learning', 'rate', '.', 'There', 'many', 'dif…', 'ferent', 'strategies', 'reduce', 'learning', 'rate', 'training', '.', 'These', 'strategies', 'called', 'learning', 'schedules', 'briefly', 'introduced', 'concept', 'Chapter', '4', 'com…mon', 'Predetermined', 'piecewise', 'constant', 'learning', 'rate', 'For', 'example', 'set', 'learning', 'rate', '−0', '=', '0.1', 'first', '−1', '=', '0.001', '50epochs', '.', 'Although', 'solution', 'work', 'well', 'often', 'requires', 'fiddling', 'around', 'figure', 'right', 'learning', 'rates', 'use', '.', 'Performance', 'scheduling', 'Measure', 'validation', 'error', 'every', 'N', 'steps', 'like', 'early', 'stopping', 'andreduce', 'learning', 'rate', 'factor', 'Œ', 'error', 'stops', 'dropping.Exponential', 'scheduling', 'Set', 'learning', 'rate', 'function', 'iteration', 'number', '−', '=', '−0', '10', '‘', 't/r', '.', 'Thisworks', 'great', 'requires', 'tuning', '−0', 'r.', 'The', 'learning', 'rate', 'drop', 'fac…', 'tor', '10', 'every', 'r', 'steps.Power', 'scheduling', 'Set', 'learning', 'rate', '−', '=', '−0', '1', '+', 't/r', '‘', 'c', '.', 'The', 'hyperparameter', 'c', 'typically', 'setto', '1', '.', 'This', 'similar', 'exponential', 'scheduling', 'learning', 'rate', 'drops', 'much', 'slowly', '.', 'Faster', 'Optimizers', '|', '301', '19ƒAn', 'Empirical', 'Study', 'Learning', 'Rates', 'Deep', 'Neural', 'Networks', 'Speech', 'Recognition', '⁄', 'A', '.', 'Senior', 'et', 'al', '.', '2013', '.A', '2013', 'paper', '19', 'Andrew', 'Senior', 'et', 'al', '.', 'compared', 'performance', 'popular', 'learning', 'schedules', 'training', 'deep', 'neural', 'networks', 'speech', 'rec…ognition', 'using', 'Momentum', 'optimization', '.', 'The', 'authors', 'concluded', 'setting', 'performance', 'scheduling', 'exponential', 'scheduling', 'performed', 'well', 'favored', 'exponential', 'scheduling', 'simpler', 'implement', 'easy', 'tune', 'converged', 'slightly', 'faster', 'optimal', 'solution', '.', 'Implementing', 'learning', 'schedule', 'TensorFlow', 'fairly', 'straightforward', 'initial_learning_rate', '=', '0.1decay_steps', '=', '10000decay_rate', '=', '1/10global_step', '=', 'tf.Variable', '0', 'trainable=False', 'learning_rate', '=', 'tf.train.exponential_decay', 'initial_learning_rate', 'global_step', 'decay_steps', 'decay_rate', 'optimizer', '=', 'tf.train.MomentumOptimizer', 'learning_rate', 'momentum=0.9', 'training_op', '=', 'optimizer.minimize', 'loss', 'global_step=global_step', 'After', 'setting', 'hyperparameter', 'values', 'create', 'nontrainable', 'variable', 'global_step', 'initialized', '0', 'keep', 'track', 'current', 'training', 'iteration', 'number', '.', 'Then', 'define', 'exponentially', 'decaying', 'learning', 'rate', '−0', '=', '0.1', 'r', '=', '10,000', 'using', 'TensorFlow‡s', 'exponential_decay', 'function', '.', 'Next', 'create', 'optimizer', 'example', 'MomentumOptimizer', 'using', 'decaying', 'learning', 'rate', '.', 'Finally', 'cre…', 'ate', 'training', 'operation', 'calling', 'optimizer‡s', 'minimize', 'method', 'since', 'wepass', 'global_step', 'variable', 'kindly', 'take', 'care', 'incrementing', '.', 'That‡s', '!', 'Since', 'AdaGrad', 'RMSProp', 'Adam', 'optimization', 'automatically', 'reduce', 'learning', 'rate', 'training', 'necessary', 'add', 'extra', 'learning', 'schedule', '.', 'For', 'optimization', 'algorithms', 'using', 'exponential', 'decay', 'performance', 'scheduling', 'considerably', 'speed', 'convergence', '.', 'Avoiding', 'Over•tting', 'Through', 'RegularizationWith', 'four', 'parameters', 'I', 'fit', 'elephant', 'five', 'I', 'make', 'wiggle', 'trunk.›John', 'von', 'Neumann', 'cited', 'Enrico', 'Fermi', 'Nature', '427', 'Deep', 'neural', 'networks', 'typically', 'tens', 'thousands', 'parameters', 'sometimes', 'even', 'millions', '.', 'With', 'many', 'parameters', 'network', 'incredible', 'amount', 'freedom', 'fit', 'huge', 'variety', 'complex', 'datasets', '.', 'But', 'great', 'flexibility', 'also', 'means', 'prone', 'overfitting', 'training', 'set', '.', '302', '|', 'Chapter', '11', 'Training', 'Deep', 'Neural', 'Nets', 'With', 'millions', 'parameters', 'fit', 'whole', 'zoo', '.', 'In', 'section', 'present', 'popular', 'regularization', 'techniques', 'neural', 'networks', 'implement', 'TensorFlow', 'early', 'stopping', '—', '1', '—2', 'regularization', 'dropout', 'max-norm', 'regularization', 'data', 'augmentation', '.', 'Early', 'StoppingTo', 'avoid', 'overfitting', 'training', 'set', 'great', 'solution', 'early', 'stopping', 'introduced', 'Chapter', '4', 'interrupt', 'training', 'performance', 'validation', 'set', 'starts', 'dropping.One', 'way', 'implement', 'TensorFlow', 'evaluate', 'model', 'validation', 'set', 'regular', 'intervals', 'e.g.', 'every', '50', 'steps', 'save', 'ƒwinner⁄', 'snapshot', 'outper…', 'forms', 'previous', 'ƒwinner⁄', 'snapshots', '.', 'Count', 'number', 'steps', 'since', 'last', 'ƒwin…', 'ner⁄', 'snapshot', 'saved', 'interrupt', 'training', 'number', 'reaches', 'limit', 'e.g.', '2,000', 'steps', '.', 'Then', 'restore', 'last', 'ƒwinner⁄', 'snapshot', '.', 'Although', 'early', 'stopping', 'works', 'well', 'practice', 'usually', 'get', 'much', 'higher', 'performance', 'network', 'combining', 'regularization', 'techni…', 'ques.–1', '–2', 'RegularizationJust', 'like', 'Chapter', '4', 'simple', 'linear', 'models', 'use', '—', '1', '—', '2', 'regulari…', 'zation', 'constrain', 'neural', 'network‡s', 'connection', 'weights', 'typically', 'bia…', 'ses', '.One', 'way', 'using', 'TensorFlow', 'simply', 'add', 'appropriate', 'regularization', 'terms', 'cost', 'function', '.', 'For', 'example', 'assuming', 'one', 'hidden', 'layer', 'weights', 'weights1', 'one', 'output', 'layer', 'weights', 'weights2', 'canapply', '—', '1', 'regularization', 'like', '...', '#', 'construct', 'neural', 'networkbase_loss', '=', 'tf.reduce_mean', 'xentropy', 'name=', \"''\", 'avg_xentropy', \"''\", 'reg_losses', '=', 'tf.reduce_sum', 'tf.abs', 'weights1', '+', 'tf.reduce_sum', 'tf.abs', 'weights2', 'loss', '=', 'tf.add', 'base_loss', 'scale', '*', 'reg_losses', 'name=', \"''\", 'loss', \"''\", 'However', 'many', 'layers', 'approach', 'convenient', '.', 'Fortunately', 'TensorFlow', 'provides', 'better', 'option', '.', 'Many', 'functions', 'create', 'variables', 'get_variable', 'fully_connected', 'accept', '*_regularizer', 'argument', 'created', 'variable', 'e.g.', 'weights_regularizer', '.', 'You', 'pass', 'function', 'takes', 'weights', 'argument', 'returns', 'corresponding', 'regularization', 'loss', '.', 'The', 'l1_regularizer', 'l2_regularizer', 'l1_l2_regularizer', 'functions', 'return', 'functions', '.', 'The', 'following', 'code', 'puts', 'together', 'arg_scope', 'fully_connected', 'Avoiding', 'Over•tting', 'Through', 'Regularization', '|', '303', '20ƒImproving', 'neural', 'networks', 'preventing', 'co-adaptation', 'feature', 'detectors', '⁄', 'G.', 'Hinton', 'et', 'al', '.', '2012', '.', '21ƒDropout', 'A', 'Simple', 'Way', 'Prevent', 'Neural', 'Networks', 'Overfitting', '⁄', 'N.', 'Srivastava', 'et', 'al', '.', '2014', '.', 'weights_regularizer=tf.contrib.layers.l1_regularizer', 'scale=0.01', 'hidden1', '=', 'fully_connected', 'X', 'n_hidden1', 'scope=', \"''\", 'hidden1', \"''\", 'hidden2', '=', 'fully_connected', 'hidden1', 'n_hidden2', 'scope=', \"''\", 'hidden2', \"''\", 'logits', '=', 'fully_connected', 'hidden2', 'n_outputs', 'activation_fn=None', 'scope=', \"''\", \"''\", 'This', 'code', 'creates', 'neural', 'network', 'two', 'hidden', 'layers', 'one', 'output', 'layer', 'also', 'creates', 'nodes', 'graph', 'compute', '—', '1', 'regularization', 'loss', 'corresponding', 'layer‡s', 'weights', '.', 'TensorFlow', 'automatically', 'adds', 'nodes', 'special', 'collec…', 'tion', 'containing', 'regularization', 'losses', '.', 'You', 'need', 'add', 'regularization', 'losses', 'overall', 'loss', 'like', 'reg_losses', '=', 'tf.get_collection', 'tf.GraphKeys.REGULARIZATION_LOSSES', 'loss', '=', 'tf.add_n', 'base_loss', '+', 'reg_losses', 'name=', \"''\", 'loss', \"''\", 'Don‡t', 'forget', 'add', 'regularization', 'losses', 'overall', 'loss', 'else', 'simply', 'ignored', '.', 'DropoutThe', 'popular', 'regularization', 'technique', 'deep', 'neural', 'networks', 'arguably', 'dropout', '.', 'It', 'proposed20', 'G.', 'E.', 'Hinton', '2012', 'detailed', 'paper', '21', 'Nitish', 'Srivastava', 'et', 'al.', 'proven', 'highly', 'successful', 'even', 'state-of-', 'the-art', 'neural', 'networks', 'got', '1–2', '%', 'accuracy', 'boost', 'simply', 'adding', 'dropout', '.', 'This', 'may', 'sound', 'like', 'lot', 'model', 'already', '95', '%', 'accuracy', 'getting', '2', '%', 'accuracy', 'boost', 'means', 'dropping', 'error', 'rate', 'almost', '40', '%', 'going', '5', '%', 'error', 'roughly', '3', '%', '.It', 'fairly', 'simple', 'algorithm', 'every', 'training', 'step', 'every', 'neuron', 'including', 'input', 'neurons', 'excluding', 'output', 'neurons', 'probability', 'p', 'tem…porarily', 'ƒdropped', '⁄', 'meaning', 'entirely', 'ignored', 'training', 'step', 'may', 'active', 'next', 'step', 'see', 'Figure', '11-9', '.', 'The', 'hyperparameter', 'p', 'iscalled', 'dropout', 'rate', 'typically', 'set', '50', '%', '.', 'After', 'training', 'neurons', 'don‡t', 'get', 'dropped', 'anymore', '.', 'And', 'that‡s', 'except', 'technical', 'detail', 'discuss', 'momen…', 'tarily', '.304', '|', 'Chapter', '11', 'Training', 'Deep', 'Neural', 'Nets', 'Figure', '11-9', '.', 'Dropout', 'regularization', 'It', 'quite', 'surprising', 'first', 'rather', 'brutal', 'technique', 'works', '.', 'Would', 'company', 'perform', 'better', 'employees', 'told', 'toss', 'coin', 'every', 'morning', 'decide', 'whether', 'go', 'work', '?', 'Well', 'knows', 'perhaps', 'would', '!', 'The', 'com…', 'pany', 'would', 'obviously', 'forced', 'adapt', 'organization', 'could', 'rely', 'sin…', 'gle', 'person', 'fill', 'coffee', 'machine', 'perform', 'critical', 'tasks', 'expertise', 'would', 'spread', 'across', 'several', 'people', '.', 'Employees', 'would', 'learn', 'cooperate', 'many', 'coworkers', 'handful', '.', 'The', 'company', 'would', 'become', 'much', 'resilient', '.', 'If', 'one', 'person', 'quit', 'wouldn‡t', 'make', 'much', 'difference', '.', 'It‡s', 'unclear', 'whether', 'idea', 'would', 'actually', 'work', 'compa…', 'nies', 'certainly', 'neural', 'networks', '.', 'Neurons', 'trained', 'dropout', 'co-adapt', 'neighboring', 'neurons', 'useful', 'possible', '.', 'They', 'also', 'rely', 'excessively', 'input', 'neurons', 'must', 'pay', 'attention', 'input', 'neurons', '.', 'They', 'end', 'less', 'sensitive', 'slight', 'changes', 'inputs', '.', 'In', 'end', 'get', 'robust', 'network', 'generalizes', 'bet…', 'ter', '.', 'Another', 'way', 'understand', 'power', 'dropout', 'realize', 'unique', 'neural', 'network', 'generated', 'training', 'step', '.', 'Since', 'neuron', 'either', 'present', 'absent', 'total', '2', 'N', 'possible', 'networks', 'N', 'total', 'number', 'drop…', 'pable', 'neurons', '.', 'This', 'huge', 'number', 'virtually', 'impossible', 'neural', 'network', 'sampled', 'twice', '.', 'Once', 'run', '10,000', 'training', 'steps', 'essentially', 'trained', '10,000', 'different', 'neural', 'networks', 'one', 'training', 'instance', '.', 'These', 'neural', 'networks', 'obviously', 'independent', 'since', 'share', 'many', 'weights', 'nevertheless', 'different', '.', 'The', 'resulting', 'neural', 'network', 'seen', 'averaging', 'ensemble', 'smaller', 'neural', 'networks', '.', 'There', 'one', 'small', 'important', 'technical', 'detail', '.', 'Suppose', 'p', '=', '50', 'case', 'dur…ing', 'testing', 'neuron', 'connected', 'twice', 'many', 'input', 'neurons', 'average', 'training', '.', 'To', 'compensate', 'fact', 'need', 'multiply', 'neu…', 'Avoiding', 'Over•tting', 'Through', 'Regularization', '|', '305', 'ron‡s', 'input', 'connection', 'weights', '0.5', 'training', '.', 'If', 'don‡t', 'neuron', 'get', 'total', 'input', 'signal', 'roughly', 'twice', 'large', 'network', 'trained', 'unlikely', 'perform', 'well', '.', 'More', 'generally', 'need', 'multiply', 'input', 'connection', 'weight', 'keep', 'probability', '1', '–', 'p', 'training', '.', 'Alternatively', 'divide', 'neuron‡s', 'output', 'keep', 'probability', 'training', 'alternatives', 'perfectly', 'equivalent', 'work', 'equally', 'well', '.', 'To', 'implement', 'dropout', 'using', 'TensorFlow', 'simply', 'apply', 'dropout', 'func…tion', 'input', 'layer', 'output', 'every', 'hidden', 'layer', '.', 'During', 'training', 'function', 'randomly', 'drops', 'items', 'setting', '0', 'divides', 'remainingitems', 'keep', 'probability', '.', 'After', 'training', 'function', 'nothing', '.', 'The', 'following', 'code', 'applies', 'dropout', 'regularization', 'three-layer', 'neural', 'network', 'tensorflow.contrib.layers', 'import', 'dropout', '...', 'is_training', '=', 'tf.placeholder', 'tf.bool', 'shape=', 'name=•is_training•', 'keep_prob', '=', '0.5X_drop', '=', 'dropout', 'X', 'keep_prob', 'is_training=is_training', 'hidden1', '=', 'fully_connected', 'X_drop', 'n_hidden1', 'scope=', \"''\", 'hidden1', \"''\", 'hidden1_drop', '=', 'dropout', 'hidden1', 'keep_prob', 'is_training=is_training', 'hidden2', '=', 'fully_connected', 'hidden1_drop', 'n_hidden2', 'scope=', \"''\", 'hidden2', \"''\", 'hidden2_drop', '=', 'dropout', 'hidden2', 'keep_prob', 'is_training=is_training', 'logits', '=', 'fully_connected', 'hidden2_drop', 'n_outputs', 'activation_fn=None', 'scope=', \"''\", 'outputs', \"''\", 'You', 'want', 'use', 'dropout', 'function', 'tensorflow.contrib.layers', 'one', 'tensorflow.nn', '.', 'The', 'first', 'one', 'turns', 'no-op', 'training', 'want', 'sec…', 'ond', 'one', 'not.Of', 'course', 'like', 'earlier', 'Batch', 'Normalization', 'need', 'set', 'is_training', 'True', 'training', 'False', 'testing.If', 'observe', 'model', 'overfitting', 'increase', 'dropout', 'rate', 'i.e.', 'reduce', 'keep_prob', 'hyperparameter', '.', 'Conversely', 'try', 'decreasing', 'dropout', 'rate', 'i.e.', 'increasing', 'keep_prob', 'model', 'underfits', 'training', 'set', '.', 'It', 'also', 'help', 'increase', 'dropout', 'rate', 'large', 'layers', 'reduce', 'small', 'ones', '.', 'Dropout', 'tend', 'significantly', 'slow', 'convergence', 'usually', 'results', 'much', 'better', 'model', 'tuned', 'properly', '.', 'So', 'generally', 'well', 'worth', 'extra', 'time', 'effort.306', '|', 'Chapter', '11', 'Training', 'Deep', 'Neural', 'Nets', 'Dropconnect', 'variant', 'dropout', 'individual', 'connections', 'dropped', 'randomly', 'rather', 'whole', 'neurons', '.', 'In', 'general', 'drop…', 'performs', 'better', '.', 'Max-Norm', 'RegularizationAnother', 'regularization', 'technique', 'quite', 'popular', 'neural', 'networks', 'called', 'max-norm', 'regularization', 'neuron', 'constrains', 'weights', 'w', 'incom…ing', 'connections', 'w', '2', 'Ž', 'r', 'r', 'max-norm', 'hyperparameter', '’', '2', '—2', 'norm.We', 'typically', 'implement', 'constraint', 'computing', 'w2', 'training', 'stepand', 'clipping', 'w', 'needed', 'r', '2', '.Reducing', 'r', 'increases', 'amount', 'regularization', 'helps', 'reduce', 'overfitting', '.', 'Max-', 'norm', 'regularization', 'also', 'help', 'alleviate', 'vanishing/exploding', 'gradients', 'prob…', 'lems', 'using', 'Batch', 'Normalization', '.', 'TensorFlow', 'provide', 'off-the-shelf', 'max-norm', 'regularizer', 'hard', 'implement', '.', 'The', 'following', 'code', 'creates', 'node', 'clip_weights', 'clip', 'weights', 'variable', 'along', 'second', 'axis', 'row', 'vector', 'maximum', 'norm', '1.0', 'threshold', '=', '1.0clipped_weights', '=', 'tf.clip_by_norm', 'weights', 'clip_norm=threshold', 'axes=1', 'clip_weights', '=', 'tf.assign', 'weights', 'clipped_weights', 'You', 'would', 'apply', 'operation', 'training', 'step', 'like', 'tf.Session', 'sess', '...', 'epoch', 'range', 'n_epochs', '...', 'X_batch', 'y_batch', 'zip', 'X_batches', 'y_batches', 'sess.run', 'training_op', 'feed_dict=', '{', 'X', 'X_batch', 'y_batch', '}', 'clip_weights.eval', 'You', 'may', 'wonder', 'get', 'access', 'weights', 'variable', 'layer', '.', 'For', 'simply', 'use', 'variable', 'scope', 'like', 'hidden1', '=', 'fully_connected', 'X', 'n_hidden1', 'scope=', \"''\", 'hidden1', \"''\", 'tf.variable_scope', '``', 'hidden1', \"''\", 'reuse=True', 'weights1', '=', 'tf.get_variable', '``', 'weights', \"''\", 'Alternatively', 'use', 'root', 'variable', 'scope', 'hidden1', '=', 'fully_connected', 'X', 'n_hidden1', 'scope=', \"''\", 'hidden1', \"''\", 'hidden2', '=', 'fully_connected', 'hidden1', 'n_hidden2', 'scope=', \"''\", 'hidden2', \"''\", 'Avoiding', 'Over•tting', 'Through', 'Regularization', '|', '307', '...', 'tf.variable_scope', '``', \"''\", 'default_name=', \"''\", \"''\", 'reuse=True', '#', 'root', 'scope', 'weights1', '=', 'tf.get_variable', '``', 'hidden1/weights', \"''\", 'weights2', '=', 'tf.get_variable', '``', 'hidden2/weights', \"''\", 'If', 'don‡t', 'know', 'name', 'variable', 'either', 'use', 'TensorBoard', 'find', 'simply', 'use', 'global_variables', 'function', 'print', 'variable', 'names', 'variable', 'tf.global_variables', 'print', 'variable.name', 'Although', 'preceding', 'solution', 'work', 'fine', 'bit', 'messy', '.', 'A', 'cleaner', 'solution', 'create', 'max_norm_regularizer', 'function', 'use', 'like', 'earlier', 'l1_regularizer', 'function', 'def', 'max_norm_regularizer', 'threshold', 'axes=1', 'name=', \"''\", 'max_norm', \"''\", 'collection=', \"''\", 'max_norm', \"''\", 'def', 'max_norm', 'weights', 'clipped', '=', 'tf.clip_by_norm', 'weights', 'clip_norm=threshold', 'axes=axes', 'clip_weights', '=', 'tf.assign', 'weights', 'clipped', 'name=name', 'tf.add_to_collection', 'collection', 'clip_weights', 'return', 'None', '#', 'regularization', 'loss', 'term', 'return', 'max_normThis', 'function', 'returns', 'parametrized', 'max_norm', 'function', 'use', 'like', 'regularizer', 'max_norm_reg', '=', 'max_norm_regularizer', 'threshold=1.0', 'hidden1', '=', 'fully_connected', 'X', 'n_hidden1', 'scope=', \"''\", 'hidden1', \"''\", 'weights_regularizer=max_norm_reg', 'Note', 'max-norm', 'regularization', 'require', 'adding', 'regularization', 'loss', 'term', 'overall', 'loss', 'function', 'max_norm', 'function', 'returns', 'None', '.', 'But', 'stillneed', 'able', 'run', 'clip_weights', 'operation', 'training', 'step', 'need', 'able', 'get', 'handle', '.', 'This', 'max_norm', 'function', 'adds', 'theclip_weights', 'node', 'collection', 'max-norm', 'clipping', 'operations', '.', 'You', 'need', 'fetch', 'clipping', 'operations', 'run', 'training', 'step', 'clip_all_weights', '=', 'tf.get_collection', '``', 'max_norm', \"''\", 'tf.Session', 'sess', '...', 'epoch', 'range', 'n_epochs', '...', 'X_batch', 'y_batch', 'zip', 'X_batches', 'y_batches', 'sess.run', 'training_op', 'feed_dict=', '{', 'X', 'X_batch', 'y_batch', '}', 'sess.run', 'clip_all_weights', 'Much', 'cleaner', 'code', 'isn‡t', '?', '308', '|', 'Chapter', '11', 'Training', 'Deep', 'Neural', 'Nets', 'Data', 'AugmentationOne', 'last', 'regularization', 'technique', 'data', 'augmentation', 'consists', 'generating', 'new', 'training', 'instances', 'existing', 'ones', 'artificially', 'boosting', 'size', 'training', 'set.This', 'reduce', 'overfitting', 'making', 'regularization', 'technique', '.', 'The', 'trick', 'generate', 'realistic', 'training', 'instances', 'ideally', 'human', 'able', 'tell', 'instances', 'generated', 'ones', '.', 'Moreover', 'simply', 'adding', 'white', 'noise', 'help', 'modifications', 'apply', 'learnable', 'white', 'noise', '.For', 'example', 'model', 'meant', 'classify', 'pictures', 'mushrooms', 'slightly', 'shift', 'rotate', 'resize', 'every', 'picture', 'training', 'set', 'various', 'amounts', 'add', 'resulting', 'pictures', 'training', 'set', 'see', 'Figure', '11-10', '.', 'This', 'forces', 'themodel', 'tolerant', 'position', 'orientation', 'size', 'mushrooms', 'picture', '.', 'If', 'want', 'model', 'tolerant', 'lighting', 'conditions', 'similarly', 'generate', 'many', 'images', 'various', 'contrasts', '.', 'Assuming', 'mushrooms', 'symmetrical', 'also', 'flip', 'pictures', 'horizontally', '.', 'By', 'combining', 'transfor…', 'mations', 'greatly', 'increase', 'size', 'training', 'set', '.', 'Figure', '11-10', '.', 'Generating', 'new', 'training', 'instances', 'existing', 'ones', 'It', 'often', 'preferable', 'generate', 'training', 'instances', 'fly', 'training', 'rather', 'wasting', 'storage', 'space', 'network', 'bandwidth', '.', 'TensorFlow', 'offers', 'several', 'image', 'manipulation', 'operations', 'transposing', 'shifting', 'rotating', 'resizing', 'flipping', 'cropping', 'well', 'adjusting', 'brightness', 'contrast', 'saturation', 'hue', 'see', 'Avoiding', 'Over•tting', 'Through', 'Regularization', '|', '309', 'API', 'documentation', 'details', '.', 'This', 'makes', 'easy', 'implement', 'data', 'aug…', 'mentation', 'image', 'datasets', '.', 'Another', 'powerful', 'technique', 'train', 'deep', 'neural', 'networks', 'add', 'skip', 'connections', 'skip', 'connection', 'add', 'theinput', 'layer', 'output', 'higher', 'layer', '.', 'We', 'explore', 'idea', 'Chapter', '13', 'talk', 'deep', 'residual', 'networks.Practical', 'GuidelinesIn', 'chapter', 'covered', 'wide', 'range', 'techniques', 'may', 'wonder…', 'ing', 'ones', 'use', '.', 'The', 'configuration', 'Table', '11-2', 'work', 'fine', 'inmost', 'cases.Table', '11-2', '.', 'Default', 'DNN', 'con†gurationInitializationHe', 'initialization', 'Activation', 'function', 'ELUNormalizationBatch', 'Normalization', 'RegularizationDropoutOptimizerAdamLearning', 'rate', 'schedule', 'NoneOf', 'course', 'try', 'reuse', 'parts', 'pretrained', 'neural', 'network', 'find', 'one', 'solves', 'similar', 'problem', '.', 'This', 'default', 'configuration', 'may', 'need', 'tweaked', '‹If', 'can‡t', 'find', 'good', 'learning', 'rate', 'convergence', 'slow', 'increased', 'training', 'rate', 'convergence', 'fast', 'network‡s', 'accuracy', 'sub…', 'optimal', 'try', 'adding', 'learning', 'schedule', 'exponential', 'decay', '.', '‹If', 'training', 'set', 'bit', 'small', 'implement', 'data', 'augmentation', '.', '‹If', 'need', 'sparse', 'model', 'add', '—1', 'regularization', 'mix', 'optionally', 'zero', 'tiny', 'weights', 'training', '.', 'If', 'need', 'even', 'sparser', 'model', 'try', 'using', 'FTRL', 'instead', 'Adam', 'optimization', 'along', '—', '1', 'reg…ularization', '.', '‹If', 'need', 'lightning-fast', 'model', 'runtime', 'may', 'want', 'drop', 'Batch', 'Nor…', 'malization', 'possibly', 'replace', 'ELU', 'activation', 'function', 'leaky', 'ReLU', '.', 'Having', 'sparse', 'model', 'also', 'help', '.', 'With', 'guidelines', 'ready', 'train', 'deep', 'nets›well', 'patient', '!', 'If', 'use', 'single', 'machine', 'may', 'wait', 'days', '310', '|', 'Chapter', '11', 'Training', 'Deep', 'Neural', 'Nets', 'even', 'months', 'training', 'complete', '.', 'In', 'next', 'chapter', 'discuss', 'use', 'distributed', 'TensorFlow', 'train', 'run', 'models', 'across', 'many', 'servers', 'GPUs', '.', 'Exercises1.Is', 'okay', 'initialize', 'weights', 'value', 'long', 'value', 'selected', 'randomly', 'using', 'He', 'initialization', '?', '2.Is', 'okay', 'initialize', 'bias', 'terms', '0', '?', '3.Name', 'three', 'advantages', 'ELU', 'activation', 'function', 'ReLU', '.', '4.In', 'cases', 'would', 'want', 'use', 'following', 'activation', 'functions', 'ELU', 'leaky', 'ReLU', 'variants', 'ReLU', 'tanh', 'logistic', 'softmax', '?', '5.What', 'may', 'happen', 'set', 'momentum', 'hyperparameter', 'close', '1', 'e.g.', '0.99999', 'using', 'MomentumOptimizer', '?', '6.Name', 'three', 'ways', 'produce', 'sparse', 'model', '.', '7.Does', 'dropout', 'slow', 'training', '?', 'Does', 'slow', 'inference', 'i.e.', 'makingpredictions', 'new', 'instances', '?', '8.Deep', 'Learning.a.Build', 'DNN', 'five', 'hidden', 'layers', '100', 'neurons', 'He', 'initialization', 'ELU', 'activation', 'function', '.', 'b', '.', 'Using', 'Adam', 'optimization', 'early', 'stopping', 'try', 'training', 'MNIST', 'digits', '0', '4', 'use', 'transfer', 'learning', 'digits', '5', '9', 'thenext', 'exercise', '.', 'You', 'need', 'softmax', 'output', 'layer', 'five', 'neurons', 'always', 'make', 'sure', 'save', 'checkpoints', 'regular', 'intervals', 'save', 'final', 'model', 'reuse', 'later', '.', 'c.Tune', 'hyperparameters', 'using', 'cross-validation', 'see', 'precision', 'achieve.d.Now', 'try', 'adding', 'Batch', 'Normalization', 'compare', 'learning', 'curves', 'converging', 'faster', '?', 'Does', 'produce', 'better', 'model', '?', 'e.Is', 'model', 'overfitting', 'training', 'set', '?', 'Try', 'adding', 'dropout', 'every', 'layer', 'try', '.', 'Does', 'help', '?', '9.Transfer', 'learning', '.', 'a.Create', 'new', 'DNN', 'reuses', 'pretrained', 'hidden', 'layers', 'previous', 'model', 'freezes', 'replaces', 'softmax', 'output', 'layer', 'fresh', 'new', 'one.b', '.', 'Train', 'new', 'DNN', 'digits', '5', '9', 'using', '100', 'images', 'per', 'digit', 'time', 'long', 'takes', '.', 'Despite', 'small', 'number', 'examples', 'achieve', 'high', 'precision', '?', 'Exercises', '|', '311', 'c.Try', 'caching', 'frozen', 'layers', 'train', 'model', 'much', 'faster', '?', 'd.Try', 'reusing', 'four', 'hidden', 'layers', 'instead', 'five', '.', 'Can', 'achieve', 'higher', 'precision', '?', 'e.Now', 'unfreeze', 'top', 'two', 'hidden', 'layers', 'continue', 'training', 'get', 'model', 'perform', 'even', 'better', '?', '10.Pretraining', 'auxiliary', 'task', '.', 'a.In', 'exercise', 'build', 'DNN', 'compares', 'two', 'MNIST', 'digit', 'images', 'predicts', 'whether', 'represent', 'digit', '.', 'Then', 'reuse', 'lower', 'layers', 'network', 'train', 'MNIST', 'classifier', 'using', 'little', 'training', 'data', '.', 'Start', 'building', 'two', 'DNNs', 'let‡s', 'call', 'DNN', 'A', 'B', 'similar', 'one', 'built', 'earlier', 'without', 'output', 'layer', 'DNN', 'five', 'hidden', 'layers', '100', 'neurons', 'He', 'initialization', 'ELU', 'activation', '.', 'Next', 'add', 'single', 'output', 'layer', 'top', 'DNNs', '.', 'You', 'use', 'TensorFlow‡s', 'concat', 'function', 'axis=1', 'concatenate', 'outputs', 'DNNs', 'along', 'horizontal', 'axis', 'feed', 'result', 'output', 'layer', '.', 'This', 'output', 'layer', 'contain', 'single', 'neuron', 'using', 'logistic', 'acti…', 'vation', 'function', '.', 'b', '.', 'Split', 'MNIST', 'training', 'set', 'two', 'sets', 'split', '#', '1', 'containing', '55,000', 'images', 'split', '#', '2', 'contain', 'contain', '5,000', 'images', '.', 'Create', 'function', 'generates', 'training', 'batch', 'instance', 'pair', 'MNIST', 'images', 'picked', 'split', '#', '1', '.', 'Half', 'training', 'instances', 'pairs', 'images', 'belong', 'class', 'half', 'images', 'dif…', 'ferent', 'classes', '.', 'For', 'pair', 'training', 'label', '0', 'images', 'class', '1', 'different', 'classes', '.', 'c.Train', 'DNN', 'training', 'set', '.', 'For', 'image', 'pair', 'simultane…', 'ously', 'feed', 'first', 'image', 'DNN', 'A', 'second', 'image', 'DNN', 'B', '.', 'The', 'whole', 'network', 'gradually', 'learn', 'tell', 'whether', 'two', 'images', 'belong', 'thesame', 'class', 'not.d.Now', 'create', 'new', 'DNN', 'reusing', 'freezing', 'hidden', 'layers', 'DNN', 'A', 'adding', 'softmax', 'output', 'layer', '10', 'neurons', '.', 'Train', 'network', 'split', '#', '2', 'see', 'achieve', 'high', 'performance', 'despite', '500', 'images', 'per', 'class.Solutions', 'exercises', 'available', 'Appendix', 'A', '.312', '|', 'Chapter', '11', 'Training', 'Deep', 'Neural', 'Nets', 'CHAPTER', '12Distributing', 'TensorFlow', 'AcrossDevices', 'ServersIn', 'Chapter', '11', 'discussed', 'several', 'techniques', 'considerably', 'speed', 'train…', 'ing', 'better', 'weight', 'initialization', 'Batch', 'Normalization', 'sophisticated', 'optimizers', '.', 'However', 'even', 'techniques', 'training', 'large', 'neural', 'network', 'single', 'machine', 'single', 'CPU', 'take', 'days', 'even', 'weeks', '.', 'In', 'chapter', 'see', 'use', 'TensorFlow', 'distribute', 'computations', 'across', 'multiple', 'devices', 'CPUs', 'GPUs', 'run', 'parallel', 'see', 'Figure', '12-1', '.', 'Firstwe', 'distribute', 'computations', 'across', 'multiple', 'devices', 'one', 'machine', 'multiple', 'devices', 'across', 'multiple', 'machines', '.', 'Figure', '12-1', '.', 'Executing', 'TensorFlow', 'graph', 'across', 'multiple', 'devices', 'parallel', '313TensorFlow‡s', 'support', 'distributed', 'computing', 'one', 'main', 'highlights', 'com…', 'pared', 'neural', 'network', 'frameworks', '.', 'It', 'gives', 'full', 'control', 'split', 'replicate', 'computation', 'graph', 'across', 'devices', 'servers', 'lets', 'par…', 'allelize', 'synchronize', 'operations', 'flexible', 'ways', 'choose', 'sorts', 'parallelization', 'approaches', '.', 'We', 'look', 'popular', 'approaches', 'parallelizing', 'execution', 'training', 'neural', 'network', '.', 'Instead', 'waiting', 'weeks', 'training', 'algo…rithm', 'complete', 'may', 'end', 'waiting', 'hours', '.', 'Not', 'save', 'enormous', 'amount', 'time', 'also', 'means', 'experiment', 'vari…', 'ous', 'models', 'much', 'easily', 'frequently', 'retrain', 'models', 'fresh', 'data', '.', 'Other', 'great', 'use', 'cases', 'parallelization', 'include', 'exploring', 'much', 'larger', 'hyperparame…', 'ter', 'space', 'fine-tuning', 'model', 'running', 'large', 'ensembles', 'neural', 'net…works', 'efficiently', '.', 'But', 'must', 'learn', 'walk', 'run', '.', 'Let‡s', 'start', 'parallelizing', 'simple', 'graphs', 'across', 'several', 'GPUs', 'single', 'machine', '.', 'Multiple', 'Devices', 'Single', 'MachineYou', 'often', 'get', 'major', 'performance', 'boost', 'simply', 'adding', 'GPU', 'cards', 'single', 'machine', '.', 'In', 'fact', 'many', 'cases', 'suffice', 'won‡t', 'need', 'use', 'multiple', 'machines', '.', 'For', 'example', 'typically', 'train', 'neural', 'network', 'fast', 'using', '8', 'GPUs', 'single', 'machine', 'rather', '16', 'GPUs', 'across', 'multiple', 'machines', 'due', 'extra', 'delay', 'imposed', 'network', 'communications', 'multimachine', 'setup', '.In', 'section', 'look', 'set', 'environment', 'TensorFlow', 'use', 'multiple', 'GPU', 'cards', 'one', 'machine', '.', 'Then', 'look', 'distribute', 'operations', 'across', 'available', 'devices', 'execute', 'parallel', '.', 'InstallationIn', 'order', 'run', 'TensorFlow', 'multiple', 'GPU', 'cards', 'first', 'need', 'make', 'sure', 'GPU', 'cards', 'NVidia', 'Compute', 'Capability', 'greater', 'equal', '3.0', '.', 'This', 'includes', 'Nvidia‡s', 'Titan', 'Titan', 'X', 'K20', 'K40', 'cards', 'another', 'card', 'check', 'compatibility', 'https', '//developer.nvidia.com/cuda-gpus', '.314', '|', 'Chapter', '12', 'Distributing', 'TensorFlow', 'Across', 'Devices', 'Servers', 'If', 'don‡t', 'GPU', 'cards', 'use', 'hosting', 'service', 'GPU', 'capability', 'Amazon', 'AWS', '.', 'Detailed', 'instructions', 'set', 'TensorFlow', '0.9', 'Python', '3.5', 'Amazon', 'AWS', 'GPU', 'instance', 'available', 'Àiga', 'Avsec‡s', 'helpful', 'blog', 'post', '.', 'It', 'hard', 'update', 'latest', 'version', 'TensorFlow', '.', 'Google', 'also', 'released', 'cloud', 'service', 'called', 'Cloud', 'Machine', 'Learning', 'run', 'TensorFlow', 'graphs', '.', 'In', 'May', '2016', 'announced', 'platform', 'includes', 'servers', 'equipped', 'tensor', 'processing', 'units', 'TPUs', 'processors', 'specialized', 'Machine', 'Learning', 'much', 'faster', 'GPUs', 'many', 'ML', 'tasks', '.', 'Of', 'course', 'another', 'option', 'simply', 'buy', 'GPU', 'card', '.', 'Tim', 'Dettmers', 'wrote', 'great', 'blog', 'post', 'help', 'choose', 'updates', 'fairly', 'regularly', '.', 'You', 'must', 'download', 'install', 'appropriate', 'version', 'CUDA', 'cuDNN', 'libraries', 'CUDA', '8.0', 'cuDNN', '5.1', 'using', 'binary', 'installation', 'TensorFlow', '1.0.0', 'set', 'environment', 'variables', 'TensorFlow', 'knows', 'find', 'CUDA', 'cuDNN', '.', 'The', 'detailed', 'installation', 'instructions', 'likely', 'change', 'fairly', 'quickly', 'best', 'follow', 'instructions', 'TensorFlow‡s', 'website.Nvidia‡s', 'Compute', 'Uni†ed', 'Device', 'Architecture', 'library', 'CUDA', 'allows', 'developers', 'use', 'CUDA-enabled', 'GPUs', 'sorts', 'computations', 'graphics', 'accelera…', 'tion', '.', 'Nvidia‡s', 'CUDA', 'Deep', 'Neural', 'Network', 'library', 'cuDNN', 'GPU-accelerated', 'library', 'primitives', 'DNNs', '.', 'It', 'provides', 'optimized', 'implementations', 'common', 'DNN', 'computations', 'activation', 'layers', 'normalization', 'forward', 'backward', 'convolutions', 'pooling', 'see', 'Chapter', '13', '.', 'It', 'part', 'Nvidia‡s', 'Deep', 'Learning', 'SDK', 'note', 'requires', 'creating', 'Nvidia', 'developer', 'account', 'order', 'download', '.', 'TensorFlow', 'uses', 'CUDA', 'cuDNN', 'control', 'GPU', 'cards', 'accelerate', 'com…', 'putations', 'see', 'Figure', '12-2', '.Figure', '12-2', '.', 'TensorFlow', 'uses', 'CUDA', 'cuDNN', 'control', 'GPUs', 'boost', 'DNNs', 'You', 'use', 'nvidia-smi', 'command', 'check', 'CUDA', 'properly', 'installed', '.', 'It', 'lists', 'available', 'GPU', 'cards', 'well', 'processes', 'running', 'card', 'Multiple', 'Devices', 'Single', 'Machine', '|', '315', '$', 'nvidia-smiWed', 'Sep', '16', '09:50:03', '2016+', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '+|', 'NVIDIA-SMI', '352.63', 'Driver', 'Version', '352.63', '||', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '-+', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '+', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '+|', 'GPU', 'Name', 'Persistence-M|', 'Bus-Id', 'Disp.A', '|', 'Volatile', 'Uncorr', '.', 'ECC', '||', 'Fan', 'Temp', 'Perf', 'Pwr', 'Usage/Cap|', 'Memory-Usage', '|', 'GPU-Util', 'Compute', 'M.', '||===============================+======================+======================||', '0', 'GRID', 'K520', 'Off', '|', '0000:00:03.0', 'Off', '|', 'N/A', '||', 'N/A', '27C', 'P8', '17W', '/', '125W', '|', '11MiB', '/', '4095MiB', '|', '0', '%', 'Default', '|+', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '-+', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '+', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '++', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '-+|', 'Processes', 'GPU', 'Memory', '||', 'GPU', 'PID', 'Type', 'Process', 'name', 'Usage', '||=============================================================================||', 'No', 'running', 'processes', 'found', '|+', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '-+Finally', 'must', 'install', 'TensorFlow', 'GPU', 'support', '.', 'If', 'created', 'isolated', 'environment', 'using', 'virtualenv', 'first', 'need', 'activate', '$', 'cd', '$', 'ML_PATH', '#', 'Your', 'ML', 'working', 'directory', 'e.g.', '$', 'HOME/ml', '$', 'source', 'env/bin/activateThen', 'install', 'appropriate', 'GPU-enabled', 'version', 'TensorFlow', '$', 'pip3', 'install', '--', 'upgrade', 'tensorflow-gpuNow', 'open', 'Python', 'shell', 'check', 'TensorFlow', 'detects', 'uses', 'CUDA', 'cuDNN', 'properly', 'importing', 'TensorFlow', 'creating', 'session', '>', '>', '>', 'import', 'tensorflow', 'tfI', '...', '/dso_loader.cc:108', 'successfully', 'opened', 'CUDA', 'library', 'libcublas.so', 'locallyI', '...', '/dso_loader.cc:108', 'successfully', 'opened', 'CUDA', 'library', 'libcudnn.so', 'locallyI', '...', '/dso_loader.cc:108', 'successfully', 'opened', 'CUDA', 'library', 'libcufft.so', 'locallyI', '...', '/dso_loader.cc:108', 'successfully', 'opened', 'CUDA', 'library', 'libcuda.so.1', 'locallyI', '...', '/dso_loader.cc:108', 'successfully', 'opened', 'CUDA', 'library', 'libcurand.so', 'locally', '>', '>', '>', 'sess', '=', 'tf.Session', '...', 'I', '...', '/gpu_init.cc:102', 'Found', 'device', '0', 'properties', 'name', 'GRID', 'K520major', '3', 'minor', '0', 'memoryClockRate', 'GHz', '0.797pciBusID', '0000:00:03.0Total', 'memory', '4.00GiBFree', 'memory', '3.95GiBI', '...', '/gpu_init.cc:126', 'DMA', '0I', '...', '/gpu_init.cc:136', '0', 'YI', '...', '/gpu_device.cc:839', 'Creating', 'TensorFlow', 'device', '/gpu:0', '-', '>', 'device', '0', 'name', 'GRID', 'K520', 'pci', 'bus', 'id', '0000:00:03.0', 'Looks', 'good', '!', 'TensorFlow', 'detected', 'CUDA', 'cuDNN', 'libraries', 'used', 'CUDA', 'library', 'detect', 'GPU', 'card', 'case', 'Nvidia', 'Grid', 'K520', 'card', '.', '316', '|', 'Chapter', '12', 'Distributing', 'TensorFlow', 'Across', 'Devices', 'Servers', 'Managing', 'GPU', 'RAMBy', 'default', 'TensorFlow', 'automatically', 'grabs', 'RAM', 'available', 'GPUs', 'first', 'time', 'run', 'graph', 'able', 'start', 'second', 'TensorFlow', 'program', 'first', 'one', 'still', 'running', '.', 'If', 'try', 'get', 'following', 'error', 'E', '...', '/cuda_driver.cc:965', 'failed', 'allocate', '3.66G', '3928915968', 'bytes', 'fromdevice', 'CUDA_ERROR_OUT_OF_MEMORYOne', 'solution', 'run', 'process', 'different', 'GPU', 'cards', '.', 'To', 'simplest', 'option', 'set', 'CUDA_VISIBLE_DEVICES', 'environment', 'variable', 'process', 'sees', 'appropriate', 'GPU', 'cards', '.', 'For', 'example', 'could', 'start', 'two', 'programs', 'like', '$', 'CUDA_VISIBLE_DEVICES=0,1', 'python3', 'program_1.py', '#', 'another', 'terminal', '$', 'CUDA_VISIBLE_DEVICES=3,2', 'python3', 'program_2.pyProgram', '#', '1', 'see', 'GPU', 'cards', '0', '1', 'numbered', '0', '1', 'respectively', 'program', '#', '2', 'see', 'GPU', 'cards', '2', '3', 'numbered', '1', '0', 'respectively', '.', 'Every…', 'thing', 'work', 'fine', 'see', 'Figure', '12-3', '.Figure', '12-3', '.', 'Each', 'program', 'gets', 'two', 'GPUs', 'Another', 'option', 'tell', 'TensorFlow', 'grab', 'fraction', 'memory', '.', 'For', 'example', 'make', 'TensorFlow', 'grab', '40', '%', 'GPU‡s', 'memory', 'must', 'create', 'ConfigProto', 'object', 'set', 'gpu_options.per_process_gpu_memory_fraction', 'option', '0.4', 'create', 'session', 'using', 'configuration', 'config', '=', 'tf.ConfigProto', 'config.gpu_options.per_process_gpu_memory_fraction', '=', '0.4session', '=', 'tf.Session', 'config=config', 'Now', 'two', 'programs', 'like', 'one', 'run', 'parallel', 'using', 'GPU', 'cards', 'three', 'since', '3', '‰', '0.4', '>', '1', '.', 'See', 'Figure', '12-4.Multiple', 'Devices', 'Single', 'Machine', '|', '317', '1ƒTensorFlow', 'Large-Scale', 'Machine', 'Learning', 'Heterogeneous', 'Distributed', 'Systems', '⁄', 'Google', 'Research', '2015', '.Figure', '12-4', '.', 'Each', 'program', 'gets', 'four', 'GPUs', '40', '%', 'RAM', 'If', 'run', 'nvidia-smi', 'command', 'programs', 'running', 'shouldsee', 'process', 'holds', 'roughly', '40', '%', 'total', 'RAM', 'card', '$', 'nvidia-smi', '...', '+', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '-+|', 'Processes', 'GPU', 'Memory', '||', 'GPU', 'PID', 'Type', 'Process', 'name', 'Usage', '||=============================================================================||', '0', '5231', 'C', 'python', '1677MiB', '||', '0', '5262', 'C', 'python', '1677MiB', '||', '1', '5231', 'C', 'python', '1677MiB', '||', '1', '5262', 'C', 'python', '1677MiB', '|', '...', 'Yet', 'another', 'option', 'tell', 'TensorFlow', 'grab', 'memory', 'needs', '.', 'To', 'must', 'set', 'config.gpu_options.allow_growth', 'True', '.', 'However', 'TensorFlow', 'never', 'releases', 'memory', 'grabbed', 'avoid', 'memory', 'fragmentation', 'may', 'still', 'run', 'memory', '.', 'It', 'may', 'harder', 'guarantee', 'deter…', 'ministic', 'behavior', 'using', 'option', 'general', 'probably', 'want', 'stick', 'one', 'previous', 'options.Okay', 'working', 'GPU-enabled', 'TensorFlow', 'installation', '.', 'Let‡s', 'see', 'use', '!', 'Placing', 'Operations', 'DevicesThe', 'TensorFlow', 'whitepaper', '1', 'presents', 'friendly', 'dynamic', 'placer', 'algorithm', 'auto…', 'magically', 'distributes', 'operations', 'across', 'available', 'devices', 'taking', 'account', 'things', 'like', 'measured', 'computation', 'time', 'previous', 'runs', 'graph', 'estimations', 'size', 'input', 'output', 'tensors', 'operation', 'amount', 'RAM', 'available', 'device', 'communication', 'delay', 'transferring', 'data', '318', '|', 'Chapter', '12', 'Distributing', 'TensorFlow', 'Across', 'Devices', 'Servers', 'devices', 'hints', 'constraints', 'user', '.', 'Unfortunately', 'sophistica…', 'ted', 'algorithm', 'internal', 'Google', 'released', 'open', 'source', 'version', 'TensorFlow', '.', 'The', 'reason', 'left', 'seems', 'practice', 'small', 'set', 'place…', 'ment', 'rules', 'specified', 'user', 'actually', 'results', 'efficient', 'placement', 'dynamic', 'placer', 'capable', '.', 'However', 'TensorFlow', 'team', 'working', 'improving', 'dynamic', 'placer', 'perhaps', 'eventually', 'good', 'enough', 'released.Until', 'TensorFlow', 'relies', 'simple', 'placer', 'name', 'suggests', 'basic.Simple', 'placementWhenever', 'run', 'graph', 'TensorFlow', 'needs', 'evaluate', 'node', 'placed', 'device', 'yet', 'uses', 'simple', 'placer', 'place', 'along', 'nodes', 'placed', 'yet', '.', 'The', 'simple', 'placer', 'respects', 'following', 'rules', '‹If', 'node', 'already', 'placed', 'device', 'previous', 'run', 'graph', 'left', 'device', '.', '‹Else', 'user', 'pinned', 'node', 'device', 'described', 'next', 'placer', 'places', 'onthat', 'device', '.', '‹Else', 'defaults', 'GPU', '#', '0', 'CPU', 'GPU', '.', 'As', 'see', 'placing', 'operations', 'appropriate', 'device', 'mostly', '.', 'If', 'don‡t', 'anything', 'whole', 'graph', 'placed', 'default', 'device', '.', 'To', 'pin', 'nodes', 'onto', 'device', 'must', 'create', 'device', 'block', 'using', 'device', 'function', '.', 'For', 'example', 'following', 'code', 'pins', 'variable', 'constant', 'b', 'CPU', 'multiplication', 'node', 'c', 'pinned', 'device', 'placed', 'default', 'device', 'tf.device', '``', '/cpu:0', \"''\", '=', 'tf.Variable', '3.0', 'b', '=', 'tf.constant', '4.0', 'c', '=', '*', 'bThe', '``', '/cpu:0', \"''\", 'device', 'aggregates', 'CPUs', 'multi-CPU', 'system', '.', 'There', 'currently', 'way', 'pin', 'nodes', 'specific', 'CPUs', 'use', 'subset', 'CPUs', '.', 'Multiple', 'Devices', 'Single', 'Machine', '|', '319', 'Logging', 'placementsLet‡s', 'check', 'simple', 'placer', 'respects', 'placement', 'constraints', 'defined', '.', 'For', 'set', 'log_device_placement', 'option', 'True', 'tells', 'theplacer', 'log', 'message', 'whenever', 'places', 'node', '.', 'For', 'example', '>', '>', '>', 'config', '=', 'tf.ConfigProto', '>', '>', '>', 'config.log_device_placement', '=', 'True', '>', '>', '>', 'sess', '=', 'tf.Session', 'config=config', 'I', '...', 'Creating', 'TensorFlow', 'device', '/gpu:0', '-', '>', 'device', '0', 'name', 'GRID', 'K520', 'pci', 'bus', 'id', '0000:00:03.0', '...', '>', '>', '>', 'x.initializer.run', 'session=sess', 'I', '...', '/job', 'localhost/replica:0/task:0/cpu:0I', '...', 'a/read', '/job', 'localhost/replica:0/task:0/cpu:0I', '...', 'mul', '/job', 'localhost/replica:0/task:0/gpu:0I', '...', 'a/Assign', '/job', 'localhost/replica:0/task:0/cpu:0I', '...', 'b', '/job', 'localhost/replica:0/task:0/cpu:0I', '...', 'a/initial_value', '/job', 'localhost/replica:0/task:0/cpu:0', '>', '>', '>', 'sess.run', 'c', '12The', 'lines', 'starting', '``', 'I', \"''\", 'Info', 'log', 'messages', '.', 'When', 'create', 'session', 'TensorFlow', 'logs', 'message', 'tell', 'us', 'found', 'GPU', 'card', 'case', 'Grid', 'K520', 'card', '.', 'Then', 'first', 'time', 'run', 'graph', 'case', 'initializing', 'variable', 'simple', 'placer', 'run', 'places', 'node', 'device', 'assigned', '.', 'As', 'expected', 'log', 'messages', 'show', 'nodes', 'placed', \"''\", '/cpu:0', \"''\", 'except', 'multiplication', 'node', 'ends', 'default', 'device', \"''\", '/gpu:0', \"''\", 'youcan', 'safely', 'ignore', 'prefix', '/job', 'localhost/replica:0/task:0', 'talk', 'moment', '.', 'Notice', 'second', 'time', 'run', 'graph', 'compute', 'c', 'placer', 'used', 'since', 'nodes', 'TensorFlow', 'needs', 'compute', 'c', 'alreadyplaced.Dynamic', 'placement', 'functionWhen', 'create', 'device', 'block', 'specify', 'function', 'instead', 'device', 'name', '.', 'TensorFlow', 'call', 'function', 'operation', 'needs', 'place', 'device', 'block', 'function', 'must', 'return', 'name', 'device', 'pin', 'operation', '.', 'For', 'example', 'following', 'code', 'pins', 'variable', 'nodes', \"''\", '/cpu:0', \"''\", 'casejust', 'variable', 'nodes', '``', '/gpu:0', \"''\", 'def', 'variables_on_cpu', 'op', 'op.type', '==', '``', 'Variable', \"''\", 'return', '``', '/cpu:0', \"''\", 'else', 'return', '``', '/gpu:0', \"''\", 'tf.device', 'variables_on_cpu', '=', 'tf.Variable', '3.0', '320', '|', 'Chapter', '12', 'Distributing', 'TensorFlow', 'Across', 'Devices', 'Servers', 'b', '=', 'tf.constant', '4.0', 'c', '=', '*', 'bYou', 'easily', 'implement', 'complex', 'algorithms', 'pinning', 'variables', 'across', 'GPUs', 'round-robin', 'fashion', '.', 'Operations', 'kernelsFor', 'TensorFlow', 'operation', 'run', 'device', 'needs', 'implementation', 'device', 'called', 'kernel', '.', 'Many', 'operations', 'kernels', 'CPUs', 'GPUs', '.', 'For', 'example', 'TensorFlow', 'GPU', 'kernel', 'integer', 'variables', 'following', 'code', 'fail', 'TensorFlow', 'tries', 'place', 'variable', 'GPU', '#', '0', '>', '>', '>', 'tf.device', '``', '/gpu:0', \"''\", '...', '=', 'tf.Variable', '3', '...', '>', '>', '>', 'sess.run', 'i.initializer', 'Traceback', 'recent', 'call', 'last', '...', 'tensorflow.python.framework.errors.InvalidArgumentError', 'Can', 'assign', 'deviceto', 'node', '•Variable•', 'Could', 'satisfy', 'explicit', 'device', 'specificationNote', 'TensorFlow', 'infers', 'variable', 'must', 'type', 'int32', 'since', 'initiali…zation', 'value', 'integer', '.', 'If', 'change', 'initialization', 'value', '3.0', 'instead', '3', 'orif', 'explicitly', 'set', 'dtype=tf.float32', 'creating', 'variable', 'everything', 'work', 'fine.Soft', 'placementBy', 'default', 'try', 'pin', 'operation', 'device', 'operation', 'kernel', 'get', 'exception', 'shown', 'earlier', 'TensorFlow', 'tries', 'place', 'opera…', 'tion', 'device', '.', 'If', 'prefer', 'TensorFlow', 'fall', 'back', 'CPU', 'instead', 'set', 'allow_soft_placement', 'configuration', 'option', 'True', 'tf.device', '``', '/gpu:0', \"''\", '=', 'tf.Variable', '3', 'config', '=', 'tf.ConfigProto', 'config.allow_soft_placement', '=', 'Truesess', '=', 'tf.Session', 'config=config', 'sess.run', 'i.initializer', '#', 'placer', 'runs', 'falls', 'back', '/cpu:0So', 'far', 'discussed', 'place', 'nodes', 'different', 'devices', '.', 'Now', 'let‡s', 'see', 'TensorFlow', 'run', 'nodes', 'parallel', '.', 'Parallel', 'ExecutionWhen', 'TensorFlow', 'runs', 'graph', 'starts', 'finding', 'list', 'nodes', 'need', 'evaluated', 'counts', 'many', 'dependencies', '.', 'TensorFlow', 'Multiple', 'Devices', 'Single', 'Machine', '|', '321', 'starts', 'evaluating', 'nodes', 'zero', 'dependencies', 'i.e.', 'source', 'nodes', '.', 'If', 'nodes', 'placed', 'separate', 'devices', 'obviously', 'get', 'evaluated', 'parallel', '.', 'If', 'placed', 'device', 'get', 'evaluated', 'different', 'threads', 'may', 'run', 'parallel', 'separate', 'GPU', 'threads', 'CPU', 'cores', '.', 'TensorFlow', 'manages', 'thread', 'pool', 'device', 'parallelize', 'operations', 'see', 'Figure', '12-5', '.', 'These', 'called', 'inter-op', 'thread', 'pools', '.', 'Some', 'operations', 'multi…', 'threaded', 'kernels', 'use', 'thread', 'pools', 'one', 'per', 'device', 'called', 'intra-op', 'thread', 'pools', '.Figure', '12-5', '.', 'Parallelized', 'execution', 'TensorFlow', 'graph', 'For', 'example', 'Figure', '12-5', 'operations', 'A', 'B', 'C', 'source', 'ops', 'immediately', 'evaluated', '.', 'Operations', 'A', 'B', 'placed', 'GPU', '#', '0', 'sent', 'device‡s', 'inter-op', 'thread', 'pool', 'immediately', 'evaluated', 'parallel', '.', 'Operation', 'A', 'happens', 'multithreaded', 'kernel', 'computations', 'split', 'three', 'parts', 'executed', 'parallel', 'intra-op', 'thread', 'pool', '.', 'Operation', 'C', 'goes', 'GPU', '#', '1‡s', 'inter-op', 'thread', 'pool', '.', 'As', 'soon', 'operation', 'C', 'finishes', 'dependency', 'counters', 'operations', 'D', 'E', 'decremented', 'reach', '0', 'operations', 'sent', 'inter-op', 'thread', 'pool', 'executed.You', 'control', 'number', 'threads', 'per', 'inter-op', 'pool', 'setting', 'inter_op_parallelism_threads', 'option', '.', 'Note', 'first', 'ses…', 'sion', 'start', 'creates', 'inter-op', 'thread', 'pools', '.', 'All', 'sessions', 'reuse', 'unless', 'set', 'use_per_session_threadsoption', 'True', '.', 'You', 'control', 'number', 'threads', 'per', 'intra-op', 'pool', 'setting', 'intra_op_parallelism_threads', 'option.322', '|', 'Chapter', '12', 'Distributing', 'TensorFlow', 'Across', 'Devices', 'Servers', 'Control', 'DependenciesIn', 'cases', 'may', 'wise', 'postpone', 'evaluation', 'operation', 'even', 'though', 'operations', 'depends', 'executed', '.', 'For', 'example', 'uses', 'lot', 'memory', 'value', 'needed', 'much', 'graph', 'would', 'best', 'evaluate', 'last', 'moment', 'avoid', 'needlessly', 'occupying', 'RAM', 'opera…', 'tions', 'may', 'need', '.', 'Another', 'example', 'set', 'operations', 'depend', 'data', 'located', 'outside', 'device', '.', 'If', 'run', 'time', 'may', 'saturate', 'device‡s', 'communication', 'bandwidth', 'end', 'waiting', 'I/O', '.', 'Other', 'operations', 'need', 'communicate', 'data', 'also', 'blocked', '.', 'It', 'would', 'preferable', 'execute', 'communication-heavy', 'operations', 'sequentially', 'allowing', 'device', 'perform', 'operations', 'parallel', '.', 'To', 'postpone', 'evaluation', 'nodes', 'simple', 'solution', 'add', 'control', 'dependen…', 'cies', '.', 'For', 'example', 'following', 'code', 'tells', 'TensorFlow', 'evaluate', 'x', 'aand', 'b', 'evaluated', '=', 'tf.constant', '1.0', 'b', '=', '+', '2.0with', 'tf.control_dependencies', 'b', 'x', '=', 'tf.constant', '3.0', '=', 'tf.constant', '4.0', 'z', '=', 'x', '+', 'yObviously', 'since', 'z', 'depends', 'x', 'evaluating', 'z', 'also', 'implies', 'waiting', 'b', 'evaluated', 'even', 'though', 'explicitly', 'control_dependencies', 'block.Also', 'since', 'b', 'depends', 'could', 'simplify', 'preceding', 'code', 'creating', 'control', 'dependency', 'b', 'instead', 'b', 'cases', 'ƒexplicit', 'better', 'implicit.⁄', 'Great', '!', 'Now', 'know', '‹How', 'place', 'operations', 'multiple', 'devices', 'way', 'please', '‹How', 'operations', 'get', 'executed', 'parallel', '‹How', 'create', 'control', 'dependencies', 'optimize', 'parallel', 'execution', 'It‡s', 'time', 'distribute', 'computations', 'across', 'multiple', 'servers', '!', 'Multiple', 'Devices', 'Across', 'Multiple', 'ServersTo', 'run', 'graph', 'across', 'multiple', 'servers', 'first', 'need', 'define', 'cluster', '.', 'A', 'cluster', 'iscomposed', 'one', 'TensorFlow', 'servers', 'called', 'tasks', 'typically', 'spread', 'acrossseveral', 'machines', 'see', 'Figure', '12-6', '.', 'Each', 'task', 'belongs', 'job', '.', 'A', 'job', 'namedgroup', 'tasks', 'typically', 'common', 'role', 'keeping', 'track', 'model', 'Multiple', 'Devices', 'Across', 'Multiple', 'Servers', '|', '323', 'parameters', 'job', 'usually', 'named', '``', 'ps', \"''\", 'parameter', 'server', 'performingcomputations', 'job', 'usually', 'named', \"''\", 'worker', \"''\", '.Figure', '12-6', '.', 'TensorFlow', 'cluster', 'The', 'following', 'cluster', 'speci†cation', 'defines', 'two', 'jobs', '``', 'ps', \"''\", '``', 'worker', \"''\", 'containing', 'one', 'task', 'two', 'tasks', 'respectively', '.', 'In', 'example', 'machine', 'A', 'hosts', 'two', 'Tensor…', 'Flow', 'servers', 'i.e.', 'tasks', 'listening', 'different', 'ports', 'one', 'part', \"''\", 'ps', \"''\", 'job', 'part', '``', 'worker', \"''\", 'job', '.', 'Machine', 'B', 'hosts', 'one', 'TensorFlow', 'server', 'part', '``', 'worker', \"''\", 'job', '.', 'cluster_spec', '=', 'tf.train.ClusterSpec', '{', '``', 'ps', \"''\", '``', 'machine-a.example.com:2221', \"''\", '#', '/job', 'ps/task:0', '``', 'worker', \"''\", '``', 'machine-a.example.com:2222', \"''\", '#', '/job', 'worker/task:0', '``', 'machine-b.example.com:2222', \"''\", '#', '/job', 'worker/task:1', '}', 'To', 'start', 'TensorFlow', 'server', 'must', 'create', 'Server', 'object', 'passing', 'clusterspecification', 'communicate', 'servers', 'job', 'name', 'task', 'number', '.', 'For', 'example', 'start', 'first', 'worker', 'task', 'would', 'run', 'following', 'code', 'machine', 'A', 'server', '=', 'tf.train.Server', 'cluster_spec', 'job_name=', \"''\", 'worker', \"''\", 'task_index=0', 'It', 'usually', 'simpler', 'run', 'one', 'task', 'per', 'machine', 'previous', 'example', 'dem…', 'onstrates', 'TensorFlow', 'allows', 'run', 'multiple', 'tasks', 'machine', '324', '|', 'Chapter', '12', 'Distributing', 'TensorFlow', 'Across', 'Devices', 'Servers', '2You', 'even', 'start', 'multiple', 'tasks', 'process', '.', 'It', 'may', 'useful', 'tests', 'recommended', 'production.3It', 'next', 'version', 'Google‡s', 'internal', 'Stubby', 'service', 'Google', 'used', 'successfully', 'decade', '.', 'See', 'http', '//grpc.io/', 'details.you', 'want', '.', '2', 'If', 'several', 'servers', 'one', 'machine', 'need', 'ensure', 'don‡t', 'try', 'grab', 'RAM', 'every', 'GPU', 'explained', 'earlier', '.', 'For', 'example', 'Figure', '12-6', '``', 'ps', \"''\", 'task', 'see', 'GPU', 'devices', 'since', 'presumably', 'pro…cess', 'launched', 'CUDA_VISIBLE_DEVICES=', \"''\", \"''\", '.', 'Note', 'CPU', 'shared', 'tasks', 'located', 'machine', '.', 'If', 'want', 'process', 'nothing', 'run', 'TensorFlow', 'server', 'block', 'main', 'thread', 'telling', 'wait', 'server', 'finish', 'using', 'join', 'method', 'otherwise', 'server', 'killed', 'soon', 'main', 'thread', 'exits', '.', 'Since', 'currently', 'way', 'stop', 'server', 'actually', 'block', 'forever', 'server.join', '#', 'blocks', 'server', 'stops', 'i.e.', 'never', 'Opening', 'SessionOnce', 'tasks', 'running', 'nothing', 'yet', 'open', 'session', 'onany', 'servers', 'client', 'located', 'process', 'machine', 'even', 'process', 'running', 'one', 'tasks', 'use', 'session', 'like', 'regular', 'local', 'session', '.', 'For', 'example', '=', 'tf.constant', '1.0', 'b', '=', '+', '2c', '=', '*', '3with', 'tf.Session', '``', 'grpc', '//machine-b.example.com:2222', \"''\", 'sess', 'print', 'c.eval', '#', '9.0This', 'client', 'code', 'first', 'creates', 'simple', 'graph', 'opens', 'session', 'TensorFlow', 'server', 'located', 'machine', 'B', 'call', 'master', 'instructs', 'evalu…ate', 'c.', 'The', 'master', 'starts', 'placing', 'operations', 'appropriate', 'devices', '.', 'In', 'example', 'since', 'pin', 'operation', 'device', 'master', 'simply', 'places', 'default', 'device›in', 'case', 'machine', 'B‡s', 'GPU', 'device', '.', 'Then', 'evaluates', 'c', 'instructed', 'client', 'returns', 'result', '.', 'The', 'Master', 'Worker', 'ServicesThe', 'client', 'uses', 'gRPC', 'protocol', 'Google', 'Remote', 'Procedure', 'Call', 'communicate', 'server', '.', 'This', 'efficient', 'open', 'source', 'framework', 'call', 'remote', 'functions', 'get', 'outputs', 'across', 'variety', 'platforms', 'languages', '.', '3', 'It', 'based', 'HTTP2', 'opens', 'connection', 'leaves', 'open', 'whole', 'session', 'allowing', 'efficient', 'bidirectional', 'communication', 'connection', 'established', '.', 'Multiple', 'Devices', 'Across', 'Multiple', 'Servers', '|', '325', 'Data', 'transmitted', 'form', 'protocol', 'bu›ers', 'another', 'open', 'source', 'Google', 'tech…nology', '.', 'This', 'lightweight', 'binary', 'data', 'interchange', 'format', '.', 'All', 'servers', 'TensorFlow', 'cluster', 'may', 'communicate', 'server', 'cluster', 'make', 'sure', 'open', 'appropriate', 'ports', 'firewall.Every', 'TensorFlow', 'server', 'provides', 'two', 'services', 'master', 'service', 'worker', 'ser…', 'vice', '.', 'The', 'master', 'service', 'allows', 'clients', 'open', 'sessions', 'use', 'run', 'graphs', '.', 'It', 'coordinates', 'computations', 'across', 'tasks', 'relying', 'worker', 'service', 'actually', 'execute', 'computations', 'tasks', 'get', 'results', '.', 'This', 'architecture', 'gives', 'lot', 'flexibility', '.', 'One', 'client', 'connect', 'multiple', 'servers', 'opening', 'multiple', 'sessions', 'different', 'threads', '.', 'One', 'server', 'handle', 'mul…', 'tiple', 'sessions', 'simultaneously', 'one', 'clients', '.', 'You', 'run', 'one', 'client', 'per', 'task', 'typically', 'within', 'process', 'one', 'client', 'control', 'tasks', '.', 'All', 'options', 'open.Pinning', 'Operations', 'Across', 'TasksYou', 'use', 'device', 'blocks', 'pin', 'operations', 'device', 'managed', 'task', 'specifying', 'job', 'name', 'task', 'index', 'device', 'type', 'device', 'index', '.', 'For', 'example', 'following', 'code', 'pins', 'CPU', 'first', 'task', '``', 'ps', \"''\", 'job', 'that‡s', 'CPU', 'machine', 'A', 'pins', 'b', 'second', 'GPU', 'managed', 'first', 'task', \"''\", 'worker', \"''\", 'job', 'that‡s', 'GPU', '#', '1', 'machine', 'A', '.', 'Finally', 'c', 'pinned', 'device', 'master', 'places', 'default', 'device', 'machine', 'B‡s', 'GPU', '#', '0', 'device', '.', 'tf.device', '``', '/job', 'ps/task:0/cpu:0', \"''\", '=', 'tf.constant', '1.0', 'tf.device', '``', '/job', 'worker/task:0/gpu:1', \"''\", 'b', '=', '+', '2c', '=', '+', 'bAs', 'earlier', 'omit', 'device', 'type', 'index', 'TensorFlow', 'default', 'task‡s', 'default', 'device', 'example', 'pinning', 'operation', \"''\", '/job', 'ps/task:0', \"''\", 'place', 'iton', 'default', 'device', 'first', 'task', \"''\", 'ps', \"''\", 'job', 'machine', 'A‡s', 'CPU', '.', 'If', 'also', 'omit', 'task', 'index', 'e.g.', '``', '/job', 'ps', \"''\", 'TensorFlow', 'defaults', \"''\", '/task:0', \"''\", '.', 'If', 'omitthe', 'job', 'name', 'task', 'index', 'TensorFlow', 'defaults', 'session‡s', 'master', 'task', '.', '326', '|', 'Chapter', '12', 'Distributing', 'TensorFlow', 'Across', 'Devices', 'Servers', 'Sharding', 'Variables', 'Across', 'Multiple', 'Parameter', 'ServersAs', 'see', 'shortly', 'common', 'pattern', 'training', 'neural', 'network', 'dis…', 'tributed', 'setup', 'store', 'model', 'parameters', 'set', 'parameter', 'servers', 'i.e.', 'tasks', '``', 'ps', \"''\", 'job', 'tasks', 'focus', 'computations', 'i.e.', 'tasks', \"''\", 'worker', \"''\", 'job', '.', 'For', 'large', 'models', 'millions', 'parameters', 'useful', 'shard', 'parameters', 'across', 'multiple', 'parameter', 'servers', 'reduce', 'risk', 'saturating', 'single', 'parameter', 'server‡s', 'network', 'card', '.', 'If', 'manually', 'pin', 'every', 'variable', 'different', 'parameter', 'server', 'would', 'quite', 'tedious', '.', 'Fortunately', 'TensorFlow', 'pro…', 'vides', 'replica_device_setter', 'function', 'distributes', 'variables', 'across', 'allthe', '``', 'ps', \"''\", 'tasks', 'round-robin', 'fashion', '.', 'For', 'example', 'following', 'code', 'pins', 'five', 'variables', 'two', 'parameter', 'servers', 'tf.device', 'tf.train.replica_device_setter', 'ps_tasks=2', 'v1', '=', 'tf.Variable', '1.0', '#', 'pinned', '/job', 'ps/task:0', 'v2', '=', 'tf.Variable', '2.0', '#', 'pinned', '/job', 'ps/task:1', 'v3', '=', 'tf.Variable', '3.0', '#', 'pinned', '/job', 'ps/task:0', 'v4', '=', 'tf.Variable', '4.0', '#', 'pinned', '/job', 'ps/task:1', 'v5', '=', 'tf.Variable', '5.0', '#', 'pinned', '/job', 'ps/task:0Instead', 'passing', 'number', 'ps_tasks', 'pass', 'cluster', 'spec', 'cluster=cluster_spec', 'TensorFlow', 'simply', 'count', 'number', 'tasks', \"''\", 'ps', \"''\", 'job', '.', 'If', 'create', 'operations', 'block', 'beyond', 'variables', 'TensorFlow', 'auto…', 'matically', 'pins', \"''\", '/job', 'worker', \"''\", 'default', 'first', 'device', 'managed', 'first', 'task', '``', 'worker', \"''\", 'job', '.', 'You', 'pin', 'another', 'device', 'setting', 'worker_device', 'parameter', 'better', 'approach', 'use', 'embedded', 'device', 'blocks', '.', 'An', 'inner', 'device', 'block', 'override', 'job', 'task', 'device', 'defined', 'outer', 'block', '.', 'For', 'example', 'tf.device', 'tf.train.replica_device_setter', 'ps_tasks=2', 'v1', '=', 'tf.Variable', '1.0', '#', 'pinned', '/job', 'ps/task:0', '+', 'defaults', '/cpu:0', 'v2', '=', 'tf.Variable', '2.0', '#', 'pinned', '/job', 'ps/task:1', '+', 'defaults', '/cpu:0', 'v3', '=', 'tf.Variable', '3.0', '#', 'pinned', '/job', 'ps/task:0', '+', 'defaults', '/cpu:0', '...', '=', 'v1', '+', 'v2', '#', 'pinned', '/job', 'worker', '+', 'defaults', 'task:0/gpu:0', 'tf.device', '``', '/gpu:1', \"''\", 'p1', '=', '2', '*', '#', 'pinned', '/job', 'worker/gpu:1', '+', 'defaults', '/task:0', 'tf.device', '``', '/task:1', \"''\", 'p2', '=', '3', '*', '#', 'pinned', '/job', 'worker/task:1/gpu:1This', 'example', 'assumes', 'parameter', 'servers', 'CPU-only', 'typically', 'case', 'since', 'need', 'store', 'com…municate', 'parameters', 'perform', 'intensive', 'computations', '.', 'Multiple', 'Devices', 'Across', 'Multiple', 'Servers', '|', '327', 'Sharing', 'State', 'Across', 'Sessions', 'Using', 'Resource', 'ContainersWhen', 'using', 'plain', 'local', 'session', 'distributed', 'kind', 'variable‡s', 'state', 'managed', 'session', 'soon', 'ends', 'variable', 'values', 'lost', '.', 'Moreover', 'multiple', 'local', 'sessions', 'share', 'state', 'even', 'run', 'graph', 'session', 'copy', 'every', 'variable', 'discussed', 'Chap…', 'ter', '9', '.', 'In', 'contrast', 'using', 'distributed', 'sessions', 'variable', 'state', 'managed', 'resource', 'containers', 'located', 'cluster', 'sessions', '.', 'So', 'create', 'variable', 'named', 'x', 'using', 'one', 'client', 'session', 'automatically', 'available', 'session', 'cluster', 'even', 'sessions', 'connected', 'different', 'server', '.', 'For', 'example', 'consider', 'following', 'client', 'code', '#', 'simple_client.pyimport', 'tensorflow', 'tfimport', 'sysx', '=', 'tf.Variable', '0.0', 'name=', \"''\", 'x', \"''\", 'increment_x', '=', 'tf.assign', 'x', 'x', '+', '1', 'tf.Session', 'sys.argv', '1', 'sess', 'sys.argv', '2', '==', '``', 'init', \"''\", 'sess.run', 'x.initializer', 'sess.run', 'increment_x', 'print', 'x.eval', 'Let‡s', 'suppose', 'TensorFlow', 'cluster', 'running', 'machines', 'A', 'B', 'port', '2222', '.', 'You', 'could', 'launch', 'client', 'open', 'session', 'server', 'machine', 'A', 'tell', 'initialize', 'variable', 'increment', 'print', 'value', 'launching', 'following', 'command', '$', 'python3', 'simple_client.py', 'grpc', '//machine-a.example.com:2222', 'init1.0Now', 'launch', 'client', 'following', 'command', 'connect', 'server', 'machine', 'B', 'magically', 'reuse', 'variable', 'x', 'time', 'don‡t', 'ask', 'server', 'initialize', 'variable', '$', 'python3', 'simple_client.py', 'grpc', '//machine-b.example.com:22222.0This', 'feature', 'cuts', 'ways', 'it‡s', 'great', 'want', 'share', 'variables', 'across', 'multiple', 'sessions', 'want', 'run', 'completely', 'independent', 'computations', 'cluster', 'careful', 'use', 'variable', 'names', 'accident', '.', 'One', 'way', 'ensure', 'won‡t', 'name', 'clashes', 'wrap', 'construc…', 'tion', 'phase', 'inside', 'variable', 'scope', 'unique', 'name', 'computation', 'example', 'tf.variable_scope', '``', 'my_problem_1', \"''\", '...', '#', 'Construction', 'phase', 'problem', '1328', '|', 'Chapter', '12', 'Distributing', 'TensorFlow', 'Across', 'Devices', 'Servers', 'A', 'better', 'option', 'use', 'container', 'block', 'tf.container', '``', 'my_problem_1', \"''\", '...', '#', 'Construction', 'phase', 'problem', '1This', 'use', 'container', 'dedicated', 'problem', '#', '1', 'instead', 'default', 'one', 'whose', 'name', 'empty', 'string', \"''\", \"''\", '.', 'One', 'advantage', 'variable', 'names', 'remain', 'nice', 'short', '.', 'Another', 'advantage', 'easily', 'reset', 'named', 'container', '.', 'For', 'example', 'following', 'command', 'connect', 'server', 'machine', 'A', 'ask', 'reset', 'container', 'named', \"''\", 'my_problem_1', \"''\", 'free', 'resources', 'container', 'used', 'also', 'close', 'sessions', 'open', 'server', '.', 'Any', 'variable', 'managed', 'container', 'must', 'initialized', 'use', 'tf.Session.reset', '``', 'grpc', '//machine-a.example.com:2222', \"''\", '``', 'my_problem_1', \"''\", 'Resource', 'containers', 'make', 'easy', 'share', 'variables', 'across', 'sessions', 'flexible', 'ways', '.', 'For', 'example', 'Figure', '12-7', 'shows', 'four', 'clients', 'running', 'different', 'graphs', 'cluster', 'sharing', 'variables', '.', 'Clients', 'A', 'B', 'share', 'variable', 'x', 'man…aged', 'default', 'container', 'clients', 'C', 'D', 'share', 'another', 'variable', 'named', 'xmanaged', 'container', 'named', \"''\", 'my_problem_1', \"''\", '.', 'Note', 'client', 'C', 'even', 'uses', 'vari…', 'ables', 'containers', '.', 'Figure', '12-7', '.', 'Resource', 'containers', 'Resource', 'containers', 'also', 'take', 'care', 'preserving', 'state', 'stateful', 'operations', 'namely', 'queues', 'readers', '.', 'Let‡s', 'take', 'look', 'queues', 'first', '.', 'Asynchronous', 'Communication', 'Using', 'TensorFlow', 'QueuesQueues', 'another', 'great', 'way', 'exchange', 'data', 'multiple', 'sessions', 'exam…', 'ple', 'one', 'common', 'use', 'case', 'client', 'create', 'graph', 'loads', 'training', 'data', 'pushes', 'queue', 'another', 'client', 'creates', 'graph', 'pulls', 'data', 'queue', 'trains', 'model', 'see', 'Figure', '12-8', '.', 'This', 'speed', 'training', 'con…Multiple', 'Devices', 'Across', 'Multiple', 'Servers', '|', '329', 'siderably', 'training', 'operations', 'don‡t', 'wait', 'next', 'mini-batch', 'every', 'step', '.', 'Figure', '12-8', '.', 'Using', 'queues', 'load', 'training', 'data', 'asynchronously', 'TensorFlow', 'provides', 'various', 'kinds', 'queues', '.', 'The', 'simplest', 'kind', '†rst-in', '†rst-out', 'FIFO', 'queue', '.', 'For', 'example', 'following', 'code', 'creates', 'FIFO', 'queue', 'store', '10', 'tensors', 'containing', 'two', 'float', 'values', 'q', '=', 'tf.FIFOQueue', 'capacity=10', 'dtypes=', 'tf.float32', 'shapes=', '2', 'name=', \"''\", 'q', \"''\", 'shared_name=', \"''\", 'shared_q', \"''\", 'To', 'share', 'variables', 'across', 'sessions', 'specify', 'name', 'container', 'ends', '.', 'With', 'queues', 'Tensor…', 'Flow', 'use', 'name', 'attribute', 'instead', 'uses', 'shared_name', 'important', 'specify', 'even', 'name', '.And', 'course', 'use', 'container', '.', 'Enqueuing', 'dataTo', 'push', 'data', 'queue', 'must', 'create', 'enqueue', 'operation', '.', 'For', 'example', 'fol…', 'lowing', 'code', 'pushes', 'three', 'training', 'instances', 'queue', '#', 'training_data_loader.pyimport', 'tensorflow', 'tfq', '=', '...', 'training_instance', '=', 'tf.placeholder', 'tf.float32', 'shape=', '2', 'enqueue', '=', 'q.enqueue', 'training_instance', 'tf.Session', '``', 'grpc', '//machine-a.example.com:2222', \"''\", 'sess', 'sess.run', 'enqueue', 'feed_dict=', '{', 'training_instance', '1.', '2', '.', '}', 'sess.run', 'enqueue', 'feed_dict=', '{', 'training_instance', '3.', '4', '.', '}', 'sess.run', 'enqueue', 'feed_dict=', '{', 'training_instance', '5.', '6', '.', '}', '330', '|', 'Chapter', '12', 'Distributing', 'TensorFlow', 'Across', 'Devices', 'Servers', 'Instead', 'enqueuing', 'instances', 'one', 'one', 'enqueue', 'several', 'time', 'using', 'enqueue_many', 'operation', '...', 'training_instances', '=', 'tf.placeholder', 'tf.float32', 'shape=', 'None', '2', 'enqueue_many', '=', 'q.enqueue', 'training_instances', 'tf.Session', '``', 'grpc', '//machine-a.example.com:2222', \"''\", 'sess', 'sess.run', 'enqueue_many', 'feed_dict=', '{', 'training_instances', '1.', '2', '.', '3.', '4', '.', '5.', '6', '.', '}', 'Both', 'examples', 'enqueue', 'three', 'tensors', 'queue', '.', 'Dequeuing', 'dataTo', 'pull', 'instances', 'queue', 'end', 'need', 'use', 'dequeueoperation', '#', 'trainer.pyimport', 'tensorflow', 'tfq', '=', '...', 'dequeue', '=', 'q.dequeue', 'tf.Session', '``', 'grpc', '//machine-a.example.com:2222', \"''\", 'sess', 'print', 'sess.run', 'dequeue', '#', '1.', '2', '.', 'print', 'sess.run', 'dequeue', '#', '3.', '4', '.', 'print', 'sess.run', 'dequeue', '#', '5.', '6', '.', 'In', 'general', 'want', 'pull', 'whole', 'mini-batch', 'instead', 'pulling', 'one', 'instance', 'time', '.', 'To', 'must', 'use', 'dequeue_many', 'operation', 'specifying', 'mini-batch', 'size', '...', 'batch_size', '=', '2dequeue_mini_batch=', 'q.dequeue_many', 'batch_size', 'tf.Session', '``', 'grpc', '//machine-a.example.com:2222', \"''\", 'sess', 'print', 'sess.run', 'dequeue_mini_batch', '#', '1.', '2', '.', '4.', '5', '.', 'print', 'sess.run', 'dequeue_mini_batch', '#', 'blocked', 'waiting', 'another', 'instanceWhen', 'queue', 'full', 'enqueue', 'operation', 'block', 'items', 'pulled', 'dequeue', 'operation', '.', 'Similarly', 'queue', 'empty', 'using', 'dequeue_many', 'fewer', 'items', 'mini-batch', 'size', 'dequeue', 'operation', 'block', 'enough', 'items', 'pushed', 'queue', 'using', 'enqueue', 'operation', '.', 'Multiple', 'Devices', 'Across', 'Multiple', 'Servers', '|', '331', 'Queues', 'tuplesEach', 'item', 'queue', 'tuple', 'tensors', 'various', 'types', 'shapes', 'instead', 'single', 'tensor', '.', 'For', 'example', 'following', 'queue', 'stores', 'pairs', 'tensors', 'one', 'type', 'int32', 'shape', 'type', 'float32', 'shape', '3,2', 'q', '=', 'tf.FIFOQueue', 'capacity=10', 'dtypes=', 'tf.int32', 'tf.float32', 'shapes=', '3,2', 'name=', \"''\", 'q', \"''\", 'shared_name=', \"''\", 'shared_q', \"''\", 'The', 'enqueue', 'operation', 'must', 'given', 'pairs', 'tensors', 'note', 'pair', 'represents', 'one', 'item', 'queue', '=', 'tf.placeholder', 'tf.int32', 'shape=', 'b', '=', 'tf.placeholder', 'tf.float32', 'shape=', '3', '2', 'enqueue', '=', 'q.enqueue', 'b', 'tf.Session', '...', 'sess', 'sess.run', 'enqueue', 'feed_dict=', '{', '10', 'b', '1.', '2', '.', '3.', '4', '.', '5.', '6', '.', '}', 'sess.run', 'enqueue', 'feed_dict=', '{', '11', 'b', '2.', '4', '.', '6.', '8', '.', '0.', '2', '.', '}', 'sess.run', 'enqueue', 'feed_dict=', '{', '12', 'b', '3.', '6', '.', '9.', '2', '.', '5.', '8', '.', '}', 'On', 'end', 'dequeue', 'function', 'creates', 'pair', 'dequeue', 'operations', 'dequeue_a', 'dequeue_b', '=', 'q.dequeue', 'In', 'general', 'run', 'operations', 'together', 'tf.Session', '...', 'sess', 'a_val', 'b_val', '=', 'sess.run', 'dequeue_a', 'dequeue_b', 'print', 'a_val', '#', '10', 'print', 'b_val', '#', '1.', '2', '.', '3.', '4', '.', '5.', '6', '.', 'If', 'run', 'dequeue_a', 'dequeue', 'pair', 'returnonly', 'first', 'element', 'second', 'element', 'lost', 'simi…', 'larly', 'run', 'dequeue_b', 'first', 'element', 'lost', '.The', 'dequeue_many', 'function', 'also', 'returns', 'pair', 'operations', 'batch_size', '=', '2dequeue_as', 'dequeue_bs', '=', 'q.dequeue_many', 'batch_size', 'You', 'use', 'would', 'expect', 'tf.Session', '...', 'sess', 'b', '=', 'sess.run', 'dequeue_a', 'dequeue_b', 'print', '#', '10', '11', 'print', 'b', '#', '1.', '2', '.', '3.', '4', '.', '5.', '6', '.', '2.', '4', '.', '6.', '8', '.', '0.', '2', '.', 'b', '=', 'sess.run', 'dequeue_a', 'dequeue_b', '#', 'blocked', 'waiting', 'another', 'pair332', '|', 'Chapter', '12', 'Distributing', 'TensorFlow', 'Across', 'Devices', 'Servers', 'Closing', 'queueIt', 'possible', 'close', 'queue', 'signal', 'sessions', 'data', 'enqueued', 'close_q', '=', 'q.close', 'tf.Session', '...', 'sess', '...', 'sess.run', 'close_q', 'Subsequent', 'executions', 'enqueue', 'enqueue_many', 'operations', 'raise', 'excep…', 'tion', '.', 'By', 'default', 'pending', 'enqueue', 'request', 'honored', 'unless', 'call', 'q.close', 'cancel_pending_enqueues=True', '.Subsequent', 'executions', 'dequeue', 'dequeue_many', 'operations', 'continue', 'suc…', 'ceed', 'long', 'items', 'queue', 'fail', 'notenough', 'items', 'left', 'queue', '.', 'If', 'using', 'dequeue_many', 'operation', 'instances', 'left', 'queue', 'fewer', 'mini-batch', 'size', 'lost', '.', 'You', 'may', 'prefer', 'use', 'dequeue_up_to', 'operation', 'instead', 'behaves', 'exactly', 'like', 'dequeue_many', 'except', 'queue', 'closed', 'fewer', 'batch_sizeinstances', 'left', 'queue', 'case', 'returns', 'them.RandomShuƒeQueueTensorFlow', 'also', 'supports', 'couple', 'types', 'queues', 'including', 'RandomShuffleQueue', 'used', 'like', 'FIFOQueue', 'except', 'items', 'dequeued', 'random', 'order', '.', 'This', 'useful', 'shuffle', 'training', 'instances', 'epoch', 'training', '.', 'First', 'let‡s', 'create', 'queue', 'q', '=', 'tf.RandomShuffleQueue', 'capacity=50', 'min_after_dequeue=10', 'dtypes=', 'tf.float32', 'shapes=', 'name=', \"''\", 'q', \"''\", 'shared_name=', \"''\", 'shared_q', \"''\", 'The', 'min_after_dequeue', 'specifies', 'minimum', 'number', 'items', 'must', 'remain', 'queue', 'dequeue', 'operation', '.', 'This', 'ensures', 'enough', 'instances', 'queue', 'enough', 'randomness', 'queue', 'closed', 'min_after_dequeue', 'limit', 'ignored', '.', 'Now', 'suppose', 'enqueued', '22', 'items', 'queue', 'floats', '1.', '22.', '.', 'Here', 'could', 'dequeue', 'dequeue', '=', 'q.dequeue_many', '5', 'tf.Session', '...', 'sess', 'print', 'sess.run', 'dequeue', '#', '20', '.', '15', '.', '11', '.', '12', '.', '4', '.', '17', 'items', 'left', 'print', 'sess.run', 'dequeue', '#', '5', '.', '13', '.', '6', '.', '0', '.', '17', '.', '12', 'items', 'left', 'print', 'sess.run', 'dequeue', '#', '12', '-', '5', '<', '10', 'blocked', 'waiting', '3', 'instancesMultiple', 'Devices', 'Across', 'Multiple', 'Servers', '|', '333', 'PaddingFifoQueueA', 'PaddingFIFOQueue', 'also', 'used', 'like', 'FIFOQueue', 'except', 'accepts', 'ten…', 'sors', 'variable', 'sizes', 'along', 'dimension', 'fixed', 'rank', '.', 'When', 'dequeuing', 'dequeue_many', 'dequeue_up_to', 'operation', 'tensor', 'padded', 'zeros', 'along', 'every', 'variable', 'dimension', 'make', 'size', 'largest', 'tensor', 'mini-batch', '.', 'For', 'example', 'could', 'enqueue', '2D', 'tensors', 'matri…', 'ces', 'arbitrary', 'sizes', 'q', '=', 'tf.PaddingFIFOQueue', 'capacity=50', 'dtypes=', 'tf.float32', 'shapes=', 'None', 'None', 'name=', \"''\", 'q', \"''\", 'shared_name=', \"''\", 'shared_q', \"''\", 'v', '=', 'tf.placeholder', 'tf.float32', 'shape=', 'None', 'None', 'enqueue', '=', 'q.enqueue', 'v', 'tf.Session', '...', 'sess', 'sess.run', 'enqueue', 'feed_dict=', '{', 'v', '1.', '2', '.', '3.', '4', '.', '5.', '6', '.', '}', '#', '3x2', 'sess.run', 'enqueue', 'feed_dict=', '{', 'v', '1', '.', '}', '#', '1x1', 'sess.run', 'enqueue', 'feed_dict=', '{', 'v', '7.', '8.', '9.', '5', '.', '6.', '7.', '8.', '9', '.', '}', '#', '2x4If', 'dequeue', 'one', 'item', 'time', 'get', 'exact', 'tensors', 'enqueued', '.', 'But', 'dequeue', 'several', 'items', 'time', 'using', 'dequeue_many', 'ordequeue_up_to', 'queue', 'automatically', 'pads', 'tensors', 'appropriately', '.', 'For', 'exam…', 'ple', 'dequeue', 'three', 'items', 'tensors', 'padded', 'zeros', 'become', '3', '‰', '4', 'tensors', 'since', 'maximum', 'size', 'first', 'dimension', '3', 'first', 'item', 'maximum', 'size', 'second', 'dimension', '4', 'third', 'item', '>', '>', '>', 'q', '=', '...', '>', '>', '>', 'dequeue', '=', 'q.dequeue_many', '3', '>', '>', '>', 'tf.Session', '...', 'sess', '...', 'print', 'sess.run', 'dequeue', '1', '.', '2', '.', '0', '.', '0', '.', '3', '.', '4', '.', '0', '.', '0', '.', '5', '.', '6', '.', '0', '.', '0', '.', '1', '.', '0', '.', '0', '.', '0', '.', '0', '.', '0', '.', '0', '.', '0', '.', '0', '.', '0', '.', '0', '.', '0', '.', '7', '.', '8', '.', '9', '.', '5', '.', '6', '.', '7', '.', '8', '.', '9', '.', '0', '.', '0', '.', '0', '.', '0', '.', 'This', 'type', 'queue', 'useful', 'dealing', 'variable', 'length', 'inputs', 'sequences', 'words', 'see', 'Chapter', '14', '.Okay', 'let‡s', 'pause', 'second', 'far', 'learned', 'distribute', 'computations', 'across', 'multiple', 'devices', 'servers', 'share', 'variables', 'across', 'sessions', 'communicate', 'asynchronously', 'using', 'queues', '.', 'Before', 'start', 'training', 'neural', 'networks', 'though', 'there‡s', 'one', 'last', 'topic', 'need', 'discuss', 'efficiently', 'load', 'training', 'data', '.', '334', '|', 'Chapter', '12', 'Distributing', 'TensorFlow', 'Across', 'Devices', 'Servers', 'Loading', 'Data', 'Directly', 'GraphSo', 'far', 'assumed', 'clients', 'would', 'load', 'training', 'data', 'feed', 'cluster', 'using', 'placeholders', '.', 'This', 'simple', 'works', 'quite', 'well', 'simple', 'setups', 'rather', 'inefficient', 'since', 'transfers', 'training', 'data', 'several', 'times', '1.From', 'filesystem', 'client', '2.From', 'client', 'master', 'task', '3.Possibly', 'master', 'task', 'tasks', 'data', 'needed', 'It', 'gets', 'worse', 'several', 'clients', 'training', 'various', 'neural', 'networks', 'using', 'training', 'data', 'example', 'hyperparameter', 'tuning', 'every', 'client', 'loads', 'data', 'simultaneously', 'may', 'end', 'even', 'saturating', 'file', 'server', 'network‡s', 'bandwidth.Preload', 'data', 'variableFor', 'datasets', 'fit', 'memory', 'better', 'option', 'load', 'training', 'data', 'assign', 'variable', 'use', 'variable', 'graph', '.', 'This', 'calledpreloading', 'training', 'set', '.', 'This', 'way', 'data', 'transferred', 'client', 'cluster', 'may', 'still', 'need', 'moved', 'around', 'task', 'task', 'depending', 'operations', 'need', '.', 'The', 'following', 'code', 'shows', 'load', 'full', 'training', 'set', 'variable', 'training_set_init', '=', 'tf.placeholder', 'tf.float32', 'shape=', 'None', 'n_features', 'training_set', '=', 'tf.Variable', 'training_set_init', 'trainable=False', 'collections=', 'name=', \"''\", 'training_set', \"''\", 'tf.Session', '...', 'sess', 'data', '=', '...', '#', 'load', 'training', 'data', 'datastore', 'sess.run', 'training_set.initializer', 'feed_dict=', '{', 'training_set_init', 'data', '}', 'You', 'must', 'set', 'trainable=False', 'optimizers', 'don‡t', 'try', 'tweak', 'variable', '.', 'You', 'also', 'set', 'collections=', 'ensure', 'variable', 'won‡t', 'get', 'added', 'GraphKeys.GLOBAL_VARIABLES', 'collection', 'used', 'saving', 'restoring', 'checkpoints', '.', 'This', 'example', 'assumes', 'training', 'set', 'including', 'labels', 'consists', 'float32', 'values', '.', 'If', 'that‡s', 'case', 'need', 'one', 'variable', 'per', 'type.Reading', 'training', 'data', 'directly', 'graphIf', 'training', 'set', 'fit', 'memory', 'good', 'solution', 'use', 'reader', 'operations', 'operations', 'capable', 'reading', 'data', 'directly', 'filesystem', '.', 'This', 'way', 'Multiple', 'Devices', 'Across', 'Multiple', 'Servers', '|', '335', 'training', 'data', 'never', 'needs', 'flow', 'clients', '.', 'TensorFlow', 'provides', 'read…', 'ers', 'various', 'file', 'formats', '‹CSV‹Fixed-length', 'binary', 'records', '‹TensorFlow‡s', 'TFRecords', 'format', 'based', 'protocol', 'buffers', 'Let‡s', 'look', 'simple', 'example', 'reading', 'CSV', 'file', 'formats', 'please', 'check', 'API', 'documentation', '.', 'Suppose', 'file', 'named', 'my_test.csv', 'contains', 'training', 'instances', 'want', 'create', 'operations', 'read', '.', 'Suppose', 'following', 'content', 'two', 'float', 'features', 'x1', 'x2', 'one', 'integer', 'targetrepresenting', 'binary', 'class', 'x1', 'x2', 'target1', '.', '2.', '04.', '5', '17.', '0First', 'let‡s', 'create', 'TextLineReader', 'read', 'file', '.', 'A', 'TextLineReader', 'opens', 'file', 'tell', 'one', 'open', 'reads', 'lines', 'one', 'one', '.', 'It', 'stateful', 'opera…', 'tion', 'like', 'variables', 'queues', 'preserves', 'state', 'across', 'multiple', 'runs', 'graph', 'keeping', 'track', 'file', 'currently', 'reading', 'current', 'position', 'file.reader', '=', 'tf.TextLineReader', 'skip_header_lines=1', 'Next', 'create', 'queue', 'reader', 'pull', 'know', 'file', 'read', 'next', '.', 'We', 'also', 'create', 'enqueue', 'operation', 'placeholder', 'push', 'filename', 'want', 'queue', 'create', 'operation', 'close', 'queue', 'files', 'read', 'filename_queue', '=', 'tf.FIFOQueue', 'capacity=10', 'dtypes=', 'tf.string', 'shapes=', 'filename', '=', 'tf.placeholder', 'tf.string', 'enqueue_filename', '=', 'filename_queue.enqueue', 'filename', 'close_filename_queue', '=', 'filename_queue.close', 'Now', 'ready', 'create', 'read', 'operation', 'read', 'one', 'record', 'i.e.', 'line', 'time', 'return', 'key/value', 'pair', '.', 'The', 'key', 'record‡s', 'unique', 'identifier›a', 'string', 'composed', 'filename', 'colon', 'line', 'number›and', 'value', 'simply', 'string', 'containing', 'content', 'line', 'key', 'value', '=', 'reader.read', 'filename_queue', 'We', 'need', 'read', 'file', 'line', 'line', '!', 'But', 'quite', 'done', 'yet›we', 'need', 'parse', 'string', 'get', 'features', 'target', 'x1', 'x2', 'target', '=', 'tf.decode_csv', 'value', 'record_defaults=', '-1', '.', '-1', '.', '-1', 'features', '=', 'tf.stack', 'x1', 'x2', '336', '|', 'Chapter', '12', 'Distributing', 'TensorFlow', 'Across', 'Devices', 'Servers', 'The', 'first', 'line', 'uses', 'TensorFlow‡s', 'CSV', 'parser', 'extract', 'values', 'current', 'line', '.', 'The', 'default', 'values', 'used', 'field', 'missing', 'example', 'third', 'training', 'instance‡s', 'x2', 'feature', 'also', 'used', 'determine', 'type', 'field', 'case', 'two', 'floats', 'one', 'integer', '.', 'Finally', 'push', 'training', 'instance', 'target', 'RandomShuffleQueuethat', 'share', 'training', 'graph', 'pull', 'mini-batches', 'create', 'operation', 'close', 'queue', 'done', 'pushing', 'instances', 'instance_queue', '=', 'tf.RandomShuffleQueue', 'capacity=10', 'min_after_dequeue=2', 'dtypes=', 'tf.float32', 'tf.int32', 'shapes=', '2', 'name=', \"''\", 'instance_q', \"''\", 'shared_name=', \"''\", 'shared_instance_q', \"''\", 'enqueue_instance', '=', 'instance_queue.enqueue', 'features', 'target', 'close_instance_queue', '=', 'instance_queue.close', 'Wow', '!', 'That', 'lot', 'work', 'read', 'file', '.', 'Plus', 'created', 'graph', 'need', 'run', 'tf.Session', '...', 'sess', 'sess.run', 'enqueue_filename', 'feed_dict=', '{', 'filename', '``', 'my_test.csv', \"''\", '}', 'sess.run', 'close_filename_queue', 'try', 'True', 'sess.run', 'enqueue_instance', 'except', 'tf.errors.OutOfRangeError', 'ex', 'pass', '#', 'records', 'current', 'file', 'files', 'read', 'sess.run', 'close_instance_queue', 'First', 'open', 'session', 'enqueue', 'filename', '``', 'my_test.csv', \"''\", 'andimmediately', 'close', 'queue', 'since', 'enqueue', 'filenames', '.', 'Then', 'run', 'infinite', 'loop', 'enqueue', 'instances', 'one', 'one', '.', 'The', 'enqueue_instancedepends', 'reader', 'reading', 'next', 'line', 'every', 'iteration', 'new', 'record', 'read', 'reaches', 'end', 'file', '.', 'At', 'point', 'tries', 'read', 'filename', 'queue', 'know', 'file', 'read', 'next', 'since', 'queue', 'closed', 'throws', 'OutOfRangeError', 'exception', 'close', 'queue', 'would', 'remain', 'blocked', 'pushed', 'another', 'filename', 'closed', 'queue', '.', 'Lastly', 'close', 'instance', 'queue', 'training', 'operations', 'pulling', 'won‡t', 'get', 'blocked', 'forever', '.', 'Figure', '12-9summarizes', 'learned', 'represents', 'typical', 'graph', 'reading', 'training', 'instances', 'set', 'CSV', 'files.Multiple', 'Devices', 'Across', 'Multiple', 'Servers', '|', '337', 'Figure', '12-9', '.', 'A', 'graph', 'dedicated', 'reading', 'training', 'instances', 'CSV', '†lesIn', 'training', 'graph', 'need', 'create', 'shared', 'instance', 'queue', 'simply', 'dequeue', 'mini-batches', 'instance_queue', '=', 'tf.RandomShuffleQueue', '...', 'shared_name=', \"''\", 'shared_instance_q', \"''\", 'mini_batch_instances', 'mini_batch_targets', '=', 'instance_queue.dequeue_up_to', '2', '...', '#', 'use', 'mini_batch', 'instances', 'targets', 'build', 'training', 'graphtraining_op', '=', '...', 'tf.Session', '...', 'sess', 'try', 'step', 'range', 'max_steps', 'sess.run', 'training_op', 'except', 'tf.errors.OutOfRangeError', 'ex', 'pass', '#', 'training', 'instancesIn', 'example', 'first', 'mini-batch', 'contain', 'first', 'two', 'instances', 'CSV', 'file', 'second', 'mini-batch', 'contain', 'last', 'instance', '.', 'TensorFlow', 'queues', 'don‡t', 'handle', 'sparse', 'tensors', 'well', 'training', 'instances', 'sparse', 'parse', 'records', 'theinstance', 'queue.This', 'architecture', 'use', 'one', 'thread', 'read', 'records', 'push', 'theinstance', 'queue', '.', 'You', 'get', 'much', 'higher', 'throughput', 'multiple', 'threads', 'read', 'simultaneously', 'multiple', 'files', 'using', 'multiple', 'readers', '.', 'Let‡s', 'see', '.', 'Multithreaded', 'readers', 'using', 'Coordinator', 'QueueRunnerTo', 'multiple', 'threads', 'read', 'instances', 'simultaneously', 'could', 'create', 'Pythonthreads', 'using', 'threading', 'module', 'manage', '.', 'However', 'Tensor…', 'Flow', 'provides', 'tools', 'make', 'simpler', 'Coordinator', 'class', 'QueueRunner', 'class.338', '|', 'Chapter', '12', 'Distributing', 'TensorFlow', 'Across', 'Devices', 'Servers', 'A', 'Coordinator', 'simple', 'object', 'whose', 'sole', 'purpose', 'coordinate', 'stopping', 'multiple', 'threads', '.', 'First', 'create', 'Coordinator', 'coord', '=', 'tf.train.Coordinator', 'Then', 'give', 'threads', 'need', 'stop', 'jointly', 'main', 'loop', 'looks', 'like', 'coord.should_stop', '...', '#', 'somethingAny', 'thread', 'request', 'every', 'thread', 'stop', 'calling', 'Coordinator‡s', 'request_stop', 'method', 'coord.request_stop', 'Every', 'thread', 'stop', 'soon', 'finishes', 'current', 'iteration', '.', 'You', 'wait', 'threads', 'finish', 'calling', 'Coordinator‡s', 'join', 'method', 'passing', 'list', 'ofthreads', 'coord.join', 'list_of_threads', 'A', 'QueueRunner', 'runs', 'multiple', 'threads', 'run', 'enqueue', 'operation', 'repeatedly', 'filling', 'queue', 'fast', 'possible', '.', 'As', 'soon', 'queue', 'closed', 'next', 'threadthat', 'tries', 'push', 'item', 'queue', 'get', 'OutOfRangeError', 'threadcatches', 'error', 'immediately', 'tells', 'threads', 'stop', 'using', 'Coordinator.The', 'following', 'code', 'shows', 'use', 'QueueRunner', 'five', 'threads', 'read…', 'ing', 'instances', 'simultaneously', 'pushing', 'instance', 'queue', '...', '#', 'construction', 'phase', 'earlierqueue_runner', '=', 'tf.train.QueueRunner', 'instance_queue', 'enqueue_instance', '*', '5', 'tf.Session', 'sess', 'sess.run', 'enqueue_filename', 'feed_dict=', '{', 'filename', '``', 'my_test.csv', \"''\", '}', 'sess.run', 'close_filename_queue', 'coord', '=', 'tf.train.Coordinator', 'enqueue_threads', '=', 'queue_runner.create_threads', 'sess', 'coord=coord', 'start=True', 'The', 'first', 'line', 'creates', 'QueueRunner', 'tells', 'run', 'five', 'threads', 'running', 'thesame', 'enqueue_instance', 'operation', 'repeatedly', '.', 'Then', 'start', 'session', 'enqueue', 'name', 'files', 'read', 'case', '``', 'my_test.csv', \"''\", '.', 'Next', 'cre…', 'ate', 'Coordinator', 'QueueRunner', 'use', 'stop', 'gracefully', 'explained', '.', 'Finally', 'tell', 'QueueRunner', 'create', 'threads', 'start', '.', 'The', 'threads', 'read', 'training', 'instances', 'push', 'instance', 'queue', 'allstop', 'gracefully', '.', 'This', 'bit', 'efficient', 'earlier', 'better', '.', 'Currently', 'threads', 'reading', 'file', '.', 'We', 'make', 'read', 'simultaneously', 'separate', 'files', 'instead', 'assuming', 'training', 'data', 'sharded', 'across', 'multiple', 'CSV', 'files', 'creating', 'multiple', 'readers', 'see', 'Figure', '12-10', '.Multiple', 'Devices', 'Across', 'Multiple', 'Servers', '|', '339', 'Figure', '12-10', '.', 'Reading', 'simultaneously', 'multiple', '†lesFor', 'need', 'write', 'small', 'function', 'create', 'reader', 'nodes', 'read', 'push', 'one', 'instance', 'instance', 'queue', 'def', 'read_and_push_instance', 'filename_queue', 'instance_queue', 'reader', '=', 'tf.TextLineReader', 'skip_header_lines=1', 'key', 'value', '=', 'reader.read', 'filename_queue', 'x1', 'x2', 'target', '=', 'tf.decode_csv', 'value', 'record_defaults=', '-1', '.', '-1', '.', '-1', 'features', '=', 'tf.stack', 'x1', 'x2', 'enqueue_instance', '=', 'instance_queue.enqueue', 'features', 'target', 'return', 'enqueue_instanceNext', 'define', 'queues', 'filename_queue', '=', 'tf.FIFOQueue', 'capacity=10', 'dtypes=', 'tf.string', 'shapes=', 'filename', '=', 'tf.placeholder', 'tf.string', 'enqueue_filename', '=', 'filename_queue.enqueue', 'filename', 'close_filename_queue', '=', 'filename_queue.close', 'instance_queue', '=', 'tf.RandomShuffleQueue', '...', 'And', 'finally', 'create', 'QueueRunner', 'time', 'give', 'list', 'different', 'enqueue', 'operations', '.', 'Each', 'operation', 'use', 'different', 'reader', 'threads', 'simultaneously', 'read', 'different', 'files', 'read_and_enqueue_ops', '=', 'read_and_push_instance', 'filename_queue', 'instance_queue', 'range', '5', 'queue_runner', '=', 'tf.train.QueueRunner', 'instance_queue', 'read_and_enqueue_ops', 'The', 'execution', 'phase', 'first', 'push', 'names', 'files', 'toread', 'create', 'Coordinator', 'create', 'start', 'QueueRunner', 'threads', '.', 'Thistime', 'threads', 'read', 'different', 'files', 'simultaneously', 'files', 'read', 'entirely', 'QueueRunner', 'close', 'instance', 'queue', 'ops', 'pulling', 'don‡t', 'get', 'blocked', '.', '340', '|', 'Chapter', '12', 'Distributing', 'TensorFlow', 'Across', 'Devices', 'Servers', 'Other', 'convenience', 'functionsTensorFlow', 'also', 'offers', 'convenience', 'functions', 'simplify', 'common', 'tasks', 'reading', 'training', 'instances', '.', 'We', 'go', 'see', 'API', 'documenta…', 'tion', 'full', 'list', '.The', 'string_input_producer', 'takes', '1D', 'tensor', 'containing', 'list', 'filenames', 'cre…', 'ates', 'thread', 'pushes', 'one', 'filename', 'time', 'filename', 'queue', 'closes', 'queue', '.', 'If', 'specify', 'number', 'epochs', 'cycle', 'file…', 'names', 'per', 'epoch', 'closing', 'queue', '.', 'By', 'default', 'shuffles', 'filenames', 'epoch', '.', 'It', 'creates', 'QueueRunner', 'manage', 'thread', 'adds', 'GraphKeys.QUEUE_RUNNERS', 'collection', '.', 'To', 'start', 'every', 'QueueRunner', 'collection', 'call', 'tf.train.start_queue_runners', 'function', '.', 'Note', 'forget', 'start', 'QueueRunner', 'filename', 'queue', 'open', 'empty', 'readers', 'blocked', 'forever', '.', 'There', 'producer', 'functions', 'similarly', 'create', 'queue', 'corre…', 'sponding', 'QueueRunner', 'running', 'enqueue', 'operation', 'e.g.', 'input_producer', 'range_input_producer', 'slice_input_producer', '.The', 'shuffle_batch', 'function', 'takes', 'list', 'tensors', 'e.g.', 'features', 'target', 'andcreates', '‹A', 'RandomShuffleQueue‹A', 'QueueRunner', 'enqueue', 'tensors', 'queue', 'added', 'GraphKeys.QUEUE_RUNNERS', 'collection', '‹A', 'dequeue_many', 'operation', 'extract', 'mini-batch', 'queue', 'This', 'makes', 'easy', 'manage', 'single', 'process', 'multithreaded', 'input', 'pipeline', 'feed…', 'ing', 'queue', 'training', 'pipeline', 'reading', 'mini-batches', 'queue', '.', 'Also', 'check', 'batch', 'batch_join', 'shuffle_batch_join', 'functions', 'providesimilar', 'functionality', '.', 'Okay', '!', 'You', 'tools', 'need', 'start', 'training', 'running', 'neural', 'net…', 'works', 'efficiently', 'across', 'multiple', 'devices', 'servers', 'TensorFlow', 'cluster', '.', 'Let‡s', 'review', 'learned', '‹Using', 'multiple', 'GPU', 'devices', '‹Setting', 'starting', 'TensorFlow', 'cluster', '‹Distributing', 'computations', 'across', 'multiple', 'devices', 'servers', '‹Sharing', 'variables', 'stateful', 'ops', 'queues', 'readers', 'across', 'ses…', 'sions', 'using', 'containers', '‹Coordinating', 'multiple', 'graphs', 'working', 'asynchronously', 'using', 'queues', 'Multiple', 'Devices', 'Across', 'Multiple', 'Servers', '|', '341', '4Not', '100', '%', 'linear', 'wait', 'devices', 'finish', 'since', 'total', 'time', 'time', 'taken', 'slowest', 'device.‹Reading', 'inputs', 'efficiently', 'using', 'readers', 'queue', 'runners', 'coordinators', 'Now', 'let‡s', 'use', 'parallelize', 'neural', 'networks', '!', 'Parallelizing', 'Neural', 'Networks', 'TensorFlow', 'ClusterIn', 'section', 'first', 'look', 'parallelize', 'several', 'neural', 'networks', 'sim…', 'ply', 'placing', 'one', 'different', 'device', '.', 'Then', 'look', 'much', 'trickier', 'problem', 'training', 'single', 'neural', 'network', 'across', 'multiple', 'devices', 'servers', '.', 'One', 'Neural', 'Network', 'per', 'DeviceThe', 'trivial', 'way', 'train', 'run', 'neural', 'networks', 'TensorFlow', 'cluster', 'take', 'exact', 'code', 'would', 'use', 'single', 'device', 'single', 'machine', 'andspecify', 'master', 'server‡s', 'address', 'creating', 'session', '.', 'That‡s', 'it›you‡re', 'done', '!', 'Your', 'code', 'running', 'server‡s', 'default', 'device', '.', 'You', 'change', 'device', 'run', 'graph', 'simply', 'putting', 'code‡s', 'construction', 'phase', 'within', 'device', 'block.By', 'running', 'several', 'client', 'sessions', 'parallel', 'different', 'threads', 'different', 'pro…', 'cesses', 'connecting', 'different', 'servers', 'configuring', 'use', 'different', 'devices', 'quite', 'easily', 'train', 'run', 'many', 'neural', 'networks', 'parallel', 'across', 'devices', 'machines', 'cluster', 'see', 'Figure', '12-11', '.', 'The', 'speedup', 'almostlinear', '.', '4', 'Training', '100', 'neural', 'networks', 'across', '50', 'servers', '2', 'GPUs', 'take', 'much', 'longer', 'training', '1', 'neural', 'network', '1', 'GPU', '.', 'Figure', '12-11', '.', 'Training', 'one', 'neural', 'network', 'per', 'device', '342', '|', 'Chapter', '12', 'Distributing', 'TensorFlow', 'Across', 'Devices', 'Servers', 'This', 'solution', 'perfect', 'hyperparameter', 'tuning', 'device', 'cluster', 'train', 'different', 'model', 'set', 'hyperparameters', '.', 'The', 'computing', 'power', 'larger', 'hyperparameter', 'space', 'explore', '.', 'It', 'also', 'works', 'perfectly', 'host', 'web', 'service', 'receives', 'large', 'number', 'queries', 'per', 'second', 'QPS', 'need', 'neural', 'network', 'make', 'prediction', 'eachquery', '.', 'Simply', 'replicate', 'neural', 'network', 'across', 'devices', 'cluster', 'dis…', 'patch', 'queries', 'across', 'devices', '.', 'By', 'adding', 'servers', 'handle', 'unlimited', 'number', 'QPS', 'however', 'reduce', 'time', 'takes', 'process', 'single', 'request', 'since', 'still', 'wait', 'neural', 'network', 'make', 'prediction', '.', 'Another', 'option', 'serve', 'neural', 'networks', 'using', 'TensorFlow', 'Serving', '.', 'It', 'open', 'source', 'system', 'released', 'Google', 'Febru…', 'ary', '2016', 'designed', 'serve', 'high', 'volume', 'queries', 'Machine', 'Learning', 'models', 'typically', 'built', 'TensorFlow', '.', 'It', 'handles', 'model', 'versioning', 'easily', 'deploy', 'new', 'version', 'yournetwork', 'production', 'experiment', 'various', 'algorithms', 'without', 'interrupting', 'service', 'sustain', 'heavy', 'load', 'adding', 'servers', '.', 'For', 'details', 'check', 'https', '//tensor', '‡ow.github.io/serving/.In-Graph', 'Versus', 'Between-Graph', 'ReplicationYou', 'also', 'parallelize', 'training', 'large', 'ensemble', 'neural', 'networks', 'simply', 'placing', 'every', 'neural', 'network', 'different', 'device', 'ensembles', 'introduced', 'Chapter', '7', '.', 'However', 'want', 'run', 'ensemble', 'need', 'aggregate', 'individual', 'predictions', 'made', 'neural', 'network', 'produce', 'ensemble‡s', 'prediction', 'requires', 'bit', 'coordination', '.', 'There', 'two', 'major', 'approaches', 'handling', 'neural', 'network', 'ensemble', 'graph', 'contains', 'large', 'chunks', 'independent', 'computations', '‹You', 'create', 'one', 'big', 'graph', 'containing', 'every', 'neural', 'network', 'pinned', 'different', 'device', 'plus', 'computations', 'needed', 'aggregate', 'individual', 'pre…', 'dictions', 'neural', 'networks', 'see', 'Figure', '12-12', '.', 'Then', 'create', 'one', 'session', 'server', 'cluster', 'let', 'take', 'care', 'everything', 'includ…', 'ing', 'waiting', 'individual', 'predictions', 'available', 'aggregating', '.', 'This', 'approach', 'called', 'in-graph', 'replication', '.Parallelizing', 'Neural', 'Networks', 'TensorFlow', 'Cluster', '|', '343', 'Figure', '12-12', '.', 'In-graph', 'replication', '‹Alternatively', 'create', 'one', 'separate', 'graph', 'neural', 'network', 'handle', 'synchronization', 'graphs', '.', 'This', 'approach', 'calledbetween-graph', 'replication', '.', 'One', 'typical', 'implementation', 'coordinate', 'exe…', 'cution', 'graphs', 'using', 'queues', 'see', 'Figure', '12-13', '.', 'A', 'set', 'clients', 'handles', 'one', 'neural', 'network', 'reading', 'dedicated', 'input', 'queue', 'writing', 'dedicated', 'prediction', 'queue', '.', 'Another', 'client', 'charge', 'reading', 'inputs', 'pushing', 'input', 'queues', 'copying', 'inputs', 'every', 'queue', '.', 'Finally', 'one', 'last', 'client', 'charge', 'reading', 'one', 'prediction', 'prediction', 'queue', 'aggregating', 'produce', 'ensemble‡s', 'prediction', '.', 'Figure', '12-13', '.', 'Between-graph', 'replication', '344', '|', 'Chapter', '12', 'Distributing', 'TensorFlow', 'Across', 'Devices', 'Servers', 'These', 'solutions', 'pros', 'cons', '.', 'In-graph', 'replication', 'somewhat', 'simpler', 'implement', 'since', 'don‡t', 'manage', 'multiple', 'clients', 'multiple', 'queues', '.', 'However', 'between-graph', 'replication', 'bit', 'easier', 'organize', 'well-bounded', 'easy-to-test', 'modules', '.', 'Moreover', 'gives', 'flexibility', '.', 'For', 'example', 'could', 'add', 'dequeue', 'timeout', 'aggregator', 'client', 'ensemble', 'would', 'fail', 'even', 'one', 'neural', 'network', 'clients', 'crashes', 'one', 'neural', 'network', 'takes', 'long', 'produce', 'prediction', '.', 'TensorFlow', 'lets', 'specify', 'timeout', 'calling', 'therun', 'function', 'passing', 'RunOptions', 'timeout_in_ms', 'tf.Session', '...', 'sess', '...', 'run_options', '=', 'tf.RunOptions', 'run_options.timeout_in_ms', '=', '1000', '#', '1s', 'timeout', 'try', 'pred', '=', 'sess.run', 'dequeue_prediction', 'options=run_options', 'except', 'tf.errors.DeadlineExceededError', 'ex', '...', '#', 'dequeue', 'operation', 'timed', '1sAnother', 'way', 'specify', 'timeout', 'set', 'session‡s', 'operation_timeout_in_ms', 'configuration', 'option', 'case', 'run', 'function', 'times', 'operation', 'takes', 'longer', 'timeout', 'delay', 'config', '=', 'tf.ConfigProto', 'config.operation_timeout_in_ms', '=', '1000', '#', '1s', 'timeout', 'every', 'operationwith', 'tf.Session', '...', 'config=config', 'sess', '...', 'try', 'pred', '=', 'sess.run', 'dequeue_prediction', 'except', 'tf.errors.DeadlineExceededError', 'ex', '...', '#', 'dequeue', 'operation', 'timed', '1sModel', 'ParallelismSo', 'far', 'run', 'neural', 'network', 'single', 'device', '.', 'What', 'want', 'run', 'single', 'neural', 'network', 'across', 'multiple', 'devices', '?', 'This', 'requires', 'chopping', 'model', 'separate', 'chunks', 'running', 'chunk', 'different', 'device', '.', 'This', 'called', 'model', 'parallelism', '.', 'Unfortunately', 'model', 'parallelism', 'turns', 'pretty', 'tricky', 'really', 'depends', 'architecture', 'neural', 'network', '.', 'For', 'fully', 'connected', 'net…', 'works', 'generally', 'much', 'gained', 'approach', 'see', 'Figure', '12-14', '.', 'Intuitively', 'may', 'seem', 'easy', 'way', 'split', 'model', 'place', 'layer', 'different', 'device', 'work', 'since', 'layer', 'needs', 'wait', 'output', 'previous', 'layer', 'anything', '.', 'So', 'perhaps', 'slice', 'vertically›for', 'example', 'left', 'half', 'layer', 'one', 'device', 'right', 'part', 'another', 'device', '?', 'This', 'slightly', 'better', 'since', 'halves', 'layer', 'indeed', 'work', 'parallel', 'problem', 'half', 'next', 'layer', 'requires', 'output', 'halves', 'lot', 'cross-device', 'communication', 'repre…', 'Parallelizing', 'Neural', 'Networks', 'TensorFlow', 'Cluster', '|', '345', 'sented', 'dashed', 'arrows', '.', 'This', 'likely', 'completely', 'cancel', 'benefit', 'parallel', 'computation', 'since', 'cross-device', 'communication', 'slow', 'especially', 'across', 'separate', 'machines', '.', 'Figure', '12-14', '.', 'Splitting', 'fully', 'connected', 'neural', 'network', 'However', 'see', 'Chapter', '13', 'neural', 'network', 'architectures', 'asconvolutional', 'neural', 'networks', 'contain', 'layers', 'partially', 'connected', 'lower', 'layers', 'much', 'easier', 'distribute', 'chunks', 'across', 'devices', 'efficient', 'way', '.', 'Figure', '12-15', '.', 'Splitting', 'partially', 'connected', 'neural', 'network', 'Moreover', 'see', 'Chapter', '14', 'deep', 'recurrent', 'neural', 'networks', 'composed', 'several', 'layers', 'memory', 'cells', 'see', 'left', 'side', 'Figure', '12-16', '.', 'A', 'cell‡s', 'output', 'time', 'fed', 'back', 'input', 'time', '+', '1', 'see', 'clearly', 'onthe', 'right', 'side', 'Figure', '12-16', '.', 'If', 'split', 'network', 'horizontally', 'placing', 'layer', 'different', 'device', 'first', 'step', 'one', 'device', 'active', 'second', 'step', 'two', 'active', 'time', 'signal', 'propagates', 'output', 'layer', 'devices', 'active', 'simultaneously', '.', 'There', 'still', 'lot', 'cross-device', 'com…', 'munication', 'going', 'since', 'cell', 'may', 'fairly', 'complex', 'benefit', 'run…', 'ning', 'multiple', 'cells', 'parallel', 'often', 'outweighs', 'communication', 'penalty', '.', '346', '|', 'Chapter', '12', 'Distributing', 'TensorFlow', 'Across', 'Devices', 'Servers', 'Figure', '12-16', '.', 'Splitting', 'deep', 'recurrent', 'neural', 'network', 'In', 'short', 'model', 'parallelism', 'speed', 'running', 'training', 'types', 'neuralnetworks', 'requires', 'special', 'care', 'tuning', 'making', 'surethat', 'devices', 'need', 'communicate', 'run', 'machine', '.', 'Data', 'ParallelismAnother', 'way', 'parallelize', 'training', 'neural', 'network', 'replicate', 'device', 'run', 'training', 'step', 'simultaneously', 'replicas', 'using', 'different', 'mini-batch', 'aggregate', 'gradients', 'update', 'model', 'parameters', '.', 'This', 'called', 'data', 'parallelism', 'see', 'Figure', '12-17', '.Figure', '12-17', '.', 'Data', 'parallelism', 'There', 'two', 'variants', 'approach', 'synchronous', 'updates', 'asynchronous', 'updates', '.Parallelizing', 'Neural', 'Networks', 'TensorFlow', 'Cluster', '|', '347', '5This', 'name', 'slightly', 'confusing', 'since', 'sounds', 'like', 'replicas', 'special', 'nothing', '.', 'In', 'reality', 'rep…', 'licas', 'equivalent', 'work', 'hard', 'among', 'fastest', 'training', 'step', 'losers', 'vary', 'every', 'step', 'unless', 'devices', 'really', 'slower', 'others', '.', 'Synchronous', 'updatesWith', 'synchronous', 'updates', 'aggregator', 'waits', 'gradients', 'available', 'computing', 'average', 'applying', 'result', 'i.e.', 'using', 'aggregated', 'gradients', 'update', 'model', 'parameters', '.', 'Once', 'replica', 'finished', 'computing', 'gradients', 'must', 'wait', 'parameters', 'updated', 'proceed', 'next', 'mini-', 'batch', '.', 'The', 'downside', 'devices', 'may', 'slower', 'others', 'devices', 'wait', 'every', 'step', '.', 'Moreover', 'parameters', 'copied', 'every', 'device', 'almost', 'time', 'immediately', 'gradients', 'applied', 'may', 'saturate', 'parameter', 'servers‡', 'bandwidth', '.', 'To', 'reduce', 'waiting', 'time', 'step', 'could', 'ignore', 'gradi…', 'ents', 'slowest', 'replicas', 'typically', '~10', '%', '.', 'For', 'example', 'could', 'run', '20', 'replicas', 'aggregate', 'gradients', 'fastest', '18', 'replicas', 'step', 'ignore', 'gradients', 'last', '2', '.', 'As', 'soon', 'parameters', 'updated', 'first', '18', 'replicas', 'start', 'working', 'immediately', 'without', 'wait', '2', 'slowest', 'replicas', '.', 'This', 'setup', 'generally', 'described', 'ashaving', '18', 'replicas', 'plus', '2', 'spare', 'replicas', '.5Asynchronous', 'updatesWith', 'asynchronous', 'updates', 'whenever', 'replica', 'finished', 'computing', 'gradi…', 'ents', 'immediately', 'uses', 'update', 'model', 'parameters', '.', 'There', 'aggrega…', 'tion', 'remove', 'ƒmean⁄', 'step', 'Figure', '12-17', 'synchronization', '.', 'Replicas', 'work', 'independently', 'replicas', '.', 'Since', 'waiting', 'repli…', 'cas', 'approach', 'runs', 'training', 'steps', 'per', 'minute', '.', 'Moreover', 'although', 'parameters', 'still', 'need', 'copied', 'every', 'device', 'every', 'step', 'happens', 'differ…', 'ent', 'times', 'replica', 'risk', 'bandwidth', 'saturation', 'reduced', '.', 'Data', 'parallelism', 'asynchronous', 'updates', 'attractive', 'choice', 'simplicity', 'absence', 'synchronization', 'delay', 'better', 'use', 'bandwidth', '.', 'However', 'although', 'works', 'reasonably', 'well', 'practice', 'almost', 'surprising', 'works', '!', 'Indeed', 'time', 'replica', 'finished', 'computing', 'gradients', 'based', 'parameter', 'values', 'parameters', 'updated', 'several', 'times', 'replicas', 'average', 'N', '–', '1', 'times', 'N', 'replicas', 'guaran…tee', 'computed', 'gradients', 'still', 'pointing', 'right', 'direction', 'see', 'Figure', '12-18', '.', 'When', 'gradients', 'severely', 'out-of-date', 'called', 'stale', 'gradients', 'slow', 'convergence', 'introducing', 'noise', 'wobble', 'effects', 'learning', '348', '|', 'Chapter', '12', 'Distributing', 'TensorFlow', 'Across', 'Devices', 'Servers', 'curve', 'may', 'contain', 'temporary', 'oscillations', 'even', 'make', 'training', 'algo…', 'rithm', 'diverge.Figure', '12-18', '.', 'Stale', 'gradients', 'using', 'asynchronous', 'updates', 'There', 'ways', 'reduce', 'effect', 'stale', 'gradients', '‹Reduce', 'learning', 'rate', '.', '‹Drop', 'stale', 'gradients', 'scale', '.', '‹Adjust', 'mini-batch', 'size', '.', '‹Start', 'first', 'epochs', 'using', 'one', 'replica', 'called', 'warmup', 'phase', '.Stale', 'gradients', 'tend', 'damaging', 'beginning', 'training', 'gra…', 'dients', 'typically', 'large', 'parameters', 'settled', 'valley', 'cost', 'function', 'yet', 'different', 'replicas', 'may', 'push', 'parameters', 'quite', 'different', 'directions.A', 'paper', 'published', 'Google', 'Brain', 'team', 'April', '2016', 'benchmarked', 'variousapproaches', 'found', 'data', 'parallelism', 'synchronous', 'updates', 'using', 'spare', 'replicas', 'efficient', 'converging', 'faster', 'also', 'producing', 'better', 'model', '.', 'However', 'still', 'active', 'area', 'research', 'rule', 'asynchronous', 'updates', 'quite', 'yet', '.', 'Bandwidth', 'saturationWhether', 'use', 'synchronous', 'asynchronous', 'updates', 'data', 'parallelism', 'still', 'requires', 'communicating', 'model', 'parameters', 'parameter', 'servers', 'every', 'replica', 'beginning', 'every', 'training', 'step', 'gradients', 'direction', 'end', 'training', 'step', '.', 'Unfortunately', 'means', 'always', 'comes', 'point', 'adding', 'extra', 'GPU', 'improve', 'performance', 'Parallelizing', 'Neural', 'Networks', 'TensorFlow', 'Cluster', '|', '349', 'time', 'spent', 'moving', 'data', 'GPU', 'RAM', 'possibly', 'across', 'net…', 'work', 'outweigh', 'speedup', 'obtained', 'splitting', 'computation', 'load', '.', 'At', 'point', 'adding', 'GPUs', 'increase', 'saturation', 'slow', 'training', '.', 'For', 'models', 'typically', 'relatively', 'small', 'trained', 'large', 'training', 'set', 'often', 'better', 'training', 'model', 'asingle', 'machine', 'single', 'GPU', '.', 'Saturation', 'severe', 'large', 'dense', 'models', 'since', 'lot', 'parameters', 'gradients', 'transfer', '.', 'It', 'less', 'severe', 'small', 'models', 'parallelization', 'gain', 'small', 'also', 'large', 'sparse', 'models', 'since', 'gradients', 'typically', 'mostly', 'zeros', 'communicated', 'efficiently', '.', 'Jeff', 'Dean', 'initiator', 'lead', 'Google', 'Brain', 'project', 'reported', 'typical', 'speedups', '25–40x', 'distributing', 'compu…', 'tations', 'across', '50', 'GPUs', 'dense', 'models', '300x', 'speedup', 'sparser', 'models', 'trained', 'across', '500', 'GPUs', '.', 'As', 'see', 'sparse', 'models', 'really', 'scale', 'better', '.', 'Here', 'concrete', 'examples', '‹Neural', 'Machine', 'Translation', '6x', 'speedup', '8', 'GPUs', '‹Inception/ImageNet', '32x', 'speedup', '50', 'GPUs', '‹RankBrain', '300x', 'speedup', '500', 'GPUs', 'These', 'numbers', 'represent', 'state', 'art', 'Q1', '2016', '.', 'Beyond', 'dozen', 'GPUs', 'dense', 'model', 'hundred', 'GPUs', 'sparse', 'model', 'saturation', 'kicks', 'performance', 'degrades', '.', 'There', 'plenty', 'research', 'going', 'solve', 'problem', 'exploring', 'peer-to-peer', 'architectures', 'rather', 'centralized', 'parameter', 'servers', 'using', 'lossy', 'model', 'compression', 'optimizing', 'replicas', 'need', 'communi…', 'cate', 'likely', 'lot', 'progress', 'parallelizing', 'neural', 'net…', 'works', 'next', 'years.In', 'meantime', 'simple', 'steps', 'take', 'reduce', 'saturation', 'problem', '‹Group', 'GPUs', 'servers', 'rather', 'scattering', 'across', 'many', 'servers', '.', 'This', 'avoid', 'unnecessary', 'network', 'hops', '.', '‹Shard', 'parameters', 'across', 'multiple', 'parameter', 'servers', 'discussed', 'earlier', '.', '‹Drop', 'model', 'parameters‡', 'float', 'precision', '32', 'bits', 'tf.float32', '16', 'bits', 'tf.bfloat16', '.', 'This', 'cut', 'half', 'amount', 'data', 'transfer', 'without', 'much', 'impact', 'convergence', 'rate', 'model‡s', 'performance', '.', '350', '|', 'Chapter', '12', 'Distributing', 'TensorFlow', 'Across', 'Devices', 'Servers', 'Although', '16-bit', 'precision', 'minimum', 'training', 'neural', 'net…', 'work', 'actually', 'drop', '8-bit', 'precision', 'trainingto', 'reduce', 'size', 'model', 'speed', 'computations', '.', 'This', 'called', 'quantizing', 'neural', 'network', '.', 'It', 'particularly', 'useful', 'deploying', 'running', 'pretrained', 'models', 'mobile', 'phones', '.', 'SeePete', 'Warden‡s', 'great', 'post', 'subject.TensorFlow', 'implementationTo', 'implement', 'data', 'parallelism', 'using', 'TensorFlow', 'first', 'need', 'choose', 'whether', 'want', 'in-graph', 'replication', 'between-graph', 'replication', 'whether', 'want', 'synchronous', 'updates', 'asynchronous', 'updates', '.', 'Let‡s', 'look', 'would', 'imple…', 'ment', 'combination', 'see', 'exercises', 'Jupyter', 'notebooks', 'complete', 'code', 'examples', '.', 'With', 'in-graph', 'replication', '+', 'synchronous', 'updates', 'build', 'one', 'big', 'graph', 'contain…', 'ing', 'model', 'replicas', 'placed', 'different', 'devices', 'nodes', 'aggregate', 'gradients', 'feed', 'optimizer', '.', 'Your', 'code', 'opens', 'session', 'cluster', 'simply', 'runs', 'training', 'operation', 'repeatedly', '.', 'With', 'in-graph', 'replication', '+', 'asynchronous', 'updates', 'also', 'create', 'one', 'big', 'graph', 'one', 'optimizer', 'per', 'replica', 'run', 'one', 'thread', 'per', 'replica', 'repeatedly', 'run…', 'ning', 'replica‡s', 'optimizer', '.', 'With', 'between-graph', 'replication', '+', 'asynchronous', 'updates', 'run', 'multiple', 'inde…', 'pendent', 'clients', 'typically', 'separate', 'processes', 'training', 'model', 'replica', 'alone', 'world', 'parameters', 'actually', 'shared', 'replicas', 'using', 'resource', 'container', '.', 'With', 'between-graph', 'replication', '+', 'synchronous', 'updates', 'run', 'multiple', 'clients', 'training', 'model', 'replica', 'based', 'shared', 'parameters', 'time', 'wrap', 'optimizer', 'e.g.', 'MomentumOptimizer', 'within', 'SyncReplicasOptimizer.Each', 'replica', 'uses', 'optimizer', 'would', 'use', 'optimizer', 'hood', 'optimizer', 'sends', 'gradients', 'set', 'queues', 'one', 'per', 'variable', 'read', 'one', 'replica‡s', 'SyncReplicasOptimizer', 'called', 'chief', '.', 'The', 'chief', 'aggre…gates', 'gradients', 'applies', 'writes', 'token', 'token', 'queue', 'eachreplica', 'signaling', 'go', 'ahead', 'compute', 'next', 'gradients', '.', 'This', 'approach', 'supports', 'spare', 'replicas', '.If', 'go', 'exercises', 'implement', 'four', 'solutions', '.', 'You', 'easily', 'able', 'apply', 'learned', 'train', 'large', 'deep', 'neural', 'networks', 'across', 'dozens', 'servers', 'GPUs', '!', 'In', 'following', 'chapters', 'go', 'important', 'neural', 'network', 'architectures', 'tackle', 'Reinforcement', 'Learning.Parallelizing', 'Neural', 'Networks', 'TensorFlow', 'Cluster', '|', '351', 'Exercises1.If', 'get', 'CUDA_ERROR_OUT_OF_MEMORY', 'starting', 'TensorFlow', 'pro…', 'gram', 'probably', 'going', '?', 'What', '?', '2.What', 'difference', 'pinning', 'operation', 'device', 'placing', 'operation', 'device', '?', '3.If', 'running', 'GPU-enabled', 'TensorFlow', 'installation', 'use', 'default', 'placement', 'operations', 'placed', 'first', 'GPU', '?', '4.If', 'pin', 'variable', '``', '/gpu:0', \"''\", 'used', 'operations', 'placed', '/gpu:1', '?', 'Or', 'operations', 'placed', \"''\", '/cpu:0', \"''\", '?', 'Or', 'operations', 'pinned', 'devices', 'loca…', 'ted', 'servers', '?', '5.Can', 'two', 'operations', 'placed', 'device', 'run', 'parallel', '?', '6.What', 'control', 'dependency', 'would', 'want', 'use', 'one', '?', '7.Suppose', 'train', 'DNN', 'days', 'TensorFlow', 'cluster', 'immediately', 'training', 'program', 'ends', 'realize', 'forgot', 'save', 'model', 'using', 'Saver', '.', 'Is', 'trained', 'model', 'lost', '?', '8.Train', 'several', 'DNNs', 'parallel', 'TensorFlow', 'cluster', 'using', 'different', 'hyper…', 'parameter', 'values', '.', 'This', 'could', 'DNNs', 'MNIST', 'classification', 'task', 'interested', '.', 'The', 'simplest', 'option', 'write', 'single', 'client', 'program', 'trains', 'one', 'DNN', 'run', 'program', 'multiple', 'processes', 'parallel', 'different', 'hyperparameter', 'values', 'client', '.', 'The', 'program', 'command-line', 'options', 'control', 'server', 'device', 'DNN', 'placed', 'resource', 'container', 'hyperparameter', 'values', 'use', 'make', 'sure', 'use', 'different', 'resource', 'container', 'DNN', '.', 'Use', 'validation', 'set', 'cross-validation', 'select', 'top', 'three', 'models', '.', '9.Create', 'ensemble', 'using', 'top', 'three', 'models', 'previous', 'exercise', '.', 'Define', 'single', 'graph', 'ensuring', 'DNN', 'runs', 'different', 'device', '.', 'Evaluate', 'validation', 'set', 'ensemble', 'perform', 'better', 'indi…', 'vidual', 'DNNs', '?', '10.Train', 'DNN', 'using', 'between-graph', 'replication', 'data', 'parallelism', 'asyn…', 'chronous', 'updates', 'timing', 'long', 'takes', 'reach', 'satisfying', 'performance', '.', 'Next', 'try', 'using', 'synchronous', 'updates', '.', 'Do', 'synchronous', 'updates', 'produce', 'better', 'model', '?', 'Is', 'training', 'faster', '?', 'Split', 'DNN', 'vertically', 'place', 'vertical', 'slice', 'different', 'device', 'train', 'model', '.', 'Is', 'training', 'faster', '?', 'Is', 'theperformance', 'different', '?', 'Solutions', 'exercises', 'available', 'Appendix', 'A', '.352', '|', 'Chapter', '12', 'Distributing', 'TensorFlow', 'Across', 'Devices', 'Servers', 'CHAPTER', '13Convolutional', 'Neural', 'NetworksAlthough', 'IBM‡s', 'Deep', 'Blue', 'supercomputer', 'beat', 'chess', 'world', 'champion', 'Garry', 'Kas…', 'parov', 'back', '1996', 'quite', 'recently', 'computers', 'unable', 'reliably', 'perform', 'seemingly', 'trivial', 'tasks', 'detecting', 'puppy', 'picture', 'recognizing', 'spokenwords', '.', 'Why', 'tasks', 'effortless', 'us', 'humans', '?', 'The', 'answer', 'lies', 'fact', 'perception', 'largely', 'takes', 'place', 'outside', 'realm', 'consciousness', 'within', 'special…ized', 'visual', 'auditory', 'sensory', 'modules', 'brains', '.', 'By', 'time', 'sensory', 'information', 'reaches', 'consciousness', 'already', 'adorned', 'high-level', 'features', 'example', 'look', 'picture', 'cute', 'puppy', 'choose', 'see', 'puppy', 'notice', 'cuteness', '.', 'Nor', 'explain', 'recognize', 'cute', 'puppy', 'it‡s', 'obvious', '.', 'Thus', 'trust', 'subjective', 'experience', 'per…', 'ception', 'trivial', 'understand', 'must', 'look', 'sensory', 'modules', 'work.Convolutional', 'neural', 'networks', 'CNNs', 'emerged', 'study', 'brain‡s', 'visual', 'cortex', 'used', 'image', 'recognition', 'since', '1980s', '.', 'In', 'last', 'years', 'thanks', 'increase', 'computational', 'power', 'amount', 'available', 'training', 'data', 'tricks', 'presented', 'Chapter', '11', 'training', 'deep', 'nets', 'CNNs', 'man…', 'aged', 'achieve', 'superhuman', 'performance', 'complex', 'visual', 'tasks', '.', 'They', 'power', 'image', 'search', 'services', 'self-driving', 'cars', 'automatic', 'video', 'classification', 'systems', '.', 'Moreover', 'CNNs', 'restricted', 'visual', 'perception', 'also', 'successful', 'tasks', 'voice', 'recognition', 'natural', 'language', 'processing', 'NLP', 'however', 'focus', 'visual', 'applications', '.', 'In', 'chapter', 'present', 'CNNs', 'came', 'building', 'blocks', 'look', 'like', 'implement', 'using', 'TensorFlow', '.', 'Then', 'present', 'best', 'CNN', 'architectures.3531ƒSingle', 'Unit', 'Activity', 'Striate', 'Cortex', 'Unrestrained', 'Cats', '⁄', 'D.', 'Hubel', 'T.', 'Wiesel', '1958', '.', '2ƒReceptive', 'Fields', 'Single', 'Neurones', 'Cat‡s', 'Striate', 'Cortex', '⁄', 'D.', 'Hubel', 'T.', 'Wiesel', '1959', '.', '3ƒReceptive', 'Fields', 'Functional', 'Architecture', 'Monkey', 'Striate', 'Cortex', '⁄', 'D.', 'Hubel', 'T.', 'Wiesel', '1968', '.', '4ƒNeocognitron', 'A', 'Self-organizing', 'Neural', 'Network', 'Model', 'Mechanism', 'Pattern', 'Recognition', 'Unaffected', 'Shift', 'Position', '⁄', 'K.', 'Fukushima', '1980', '.', '5ƒGradient-Based', 'Learning', 'Applied', 'Document', 'Recognition', '⁄', 'Y.', 'LeCun', 'et', 'al', '.', '1998', '.', 'The', 'Architecture', 'Visual', 'CortexDavid', 'H.', 'Hubel', 'Torsten', 'Wiesel', 'performed', 'series', 'experiments', 'cats', '19581', '19592', 'years', 'later', 'monkeys', '3', 'giving', 'crucial', 'insights', 'structure', 'visual', 'cortex', 'authors', 'received', 'Nobel', 'Prize', 'Physiology', 'Medicine', '1981', 'work', '.', 'In', 'particular', 'showed', 'many', 'neurons', 'visual', 'cortex', 'small', 'local', 'receptive', '†eld', 'meaning', 'react', 'visualstimuli', 'located', 'limited', 'region', 'visual', 'field', 'see', 'Figure', '13-1', 'thelocal', 'receptive', 'fields', 'five', 'neurons', 'represented', 'dashed', 'circles', '.', 'The', 'receptive', 'fields', 'different', 'neurons', 'may', 'overlap', 'together', 'tile', 'whole', 'visual', 'field', '.', 'Moreover', 'authors', 'showed', 'neurons', 'react', 'images', 'horizontal', 'lines', 'others', 'react', 'lines', 'different', 'orientations', 'two', 'neurons', 'may', 'receptive', 'field', 'react', 'different', 'line', 'orientations', '.', 'They', 'also', 'noticed', 'neurons', 'larger', 'receptive', 'fields', 'react', 'com…', 'plex', 'patterns', 'combinations', 'lower-level', 'patterns', '.', 'These', 'observations', 'led', 'idea', 'higher-level', 'neurons', 'based', 'outputs', 'neighboring', 'lower-level', 'neurons', 'Figure', '13-1', 'notice', 'neuron', 'connected', 'neurons', 'previous', 'layer', '.', 'This', 'powerful', 'architecture', 'able', 'detect', 'sorts', 'complex', 'patterns', 'area', 'visual', 'field', '.', 'Figure', '13-1', '.', 'Local', 'receptive', '†elds', 'visual', 'cortex', 'These', 'studies', 'visual', 'cortex', 'inspired', 'neocognitron', 'introduced', '1980', ',4which', 'gradually', 'evolved', 'call', 'convolutional', 'neural', 'networks', '.', 'Animportant', 'milestone', '1998', 'paper', '5', 'Yann', 'LeCun', 'L•on', 'Bottou', 'Yoshua', 'Bengio', '354', '|', 'Chapter', '13', 'Convolutional', 'Neural', 'Networks', '6A', 'convolution', 'mathematical', 'operation', 'slides', 'one', 'function', 'another', 'measures', 'integral', 'pointwise', 'multiplication', '.', 'It', 'deep', 'connections', 'Fourier', 'transform', 'Laplace', 'transform', 'heavily', 'used', 'signal', 'processing', '.', 'Convolutional', 'layers', 'actually', 'use', 'cross-correlations', 'similar', 'convolutions', 'see', 'http', '//goo.gl/HAfxXd', 'details', '.and', 'Patrick', 'Haffner', 'introduced', 'famous', 'LeNet-5', 'architecture', 'widely', 'usedto', 'recognize', 'handwritten', 'check', 'numbers', '.', 'This', 'architecture', 'building', 'blocks', 'already', 'know', 'fully', 'connected', 'layers', 'sigmoid', 'activation', 'func…', 'tions', 'also', 'introduces', 'two', 'new', 'building', 'blocks', 'convolutional', 'layers', 'pooling', 'layers', '.', 'Let‡s', 'look', '.', 'Why', 'simply', 'use', 'regular', 'deep', 'neural', 'network', 'fully', 'con…', 'nected', 'layers', 'image', 'recognition', 'tasks', '?', 'Unfortunately', 'although', 'works', 'fine', 'small', 'images', 'e.g.', 'MNIST', 'breaks', 'forlarger', 'images', 'huge', 'number', 'parameters', 'requires', '.', 'For', 'example', '100', '‰', '100', 'image', '10,000', 'pixels', 'first', 'layer', '1,000', 'neurons', 'already', 'severely', 'restricts', 'amount', 'information', 'transmitted', 'next', 'layer', 'means', 'total', '10', 'million', 'connections', '.', 'And', 'that‡s', 'first', 'layer', '.', 'CNNs', 'solve', 'problem', 'using', 'partially', 'connected', 'layers', '.', 'Convolutional', 'LayerThe', 'important', 'building', 'block', 'CNN', 'convolutional', 'layer', ':6', 'neurons', 'inthe', 'first', 'convolutional', 'layer', 'connected', 'every', 'single', 'pixel', 'input', 'image', 'like', 'previous', 'chapters', 'pixels', 'receptive', 'fields', 'see', 'Figure', '13-2', '.', 'In', 'turn', 'neuron', 'second', 'convolutional', 'layer', 'connected', 'neurons', 'located', 'within', 'small', 'rectangle', 'first', 'layer', '.', 'This', 'architecture', 'allows', 'network', 'concentrate', 'low-level', 'features', 'first', 'hidden', 'layer', 'assemble', 'higher-level', 'features', 'next', 'hidden', 'layer', '.', 'This', 'hierarchical', 'structure', 'common', 'real-world', 'images', 'one', 'reasonswhy', 'CNNs', 'work', 'well', 'image', 'recognition', '.', 'Convolutional', 'Layer', '|', '355', 'Figure', '13-2', '.', 'CNN', 'layers', 'rectangular', 'local', 'receptive', '†eldsUntil', 'multilayer', 'neural', 'networks', 'looked', 'layers', 'composed', 'long', 'line', 'neurons', 'flatten', 'input', 'images', '1D', 'feeding', 'neural', 'network', '.', 'Now', 'layer', 'represented', '2D', 'makes', 'easier', 'match', 'neurons', 'corresponding', 'inputs', '.', 'A', 'neuron', 'located', 'row', 'column', 'j', 'given', 'layer', 'connected', 'outputs', 'neurons', 'previous', 'layer', 'located', 'rows', '+', 'fh', '–', '1', 'columns', 'j', 'j', '+', 'fw', '–', '1', 'fh', 'fw', 'height', 'width', 'receptive', 'field', 'see', 'Figure', '13-3', '.', 'Inorder', 'layer', 'height', 'width', 'previous', 'layer', 'com…', 'mon', 'add', 'zeros', 'around', 'inputs', 'shown', 'diagram', '.', 'This', 'called', 'zero', 'pad…', 'ding', '.Figure', '13-3', '.', 'Connections', 'layers', 'zero', 'padding', '356', '|', 'Chapter', '13', 'Convolutional', 'Neural', 'Networks', 'It', 'also', 'possible', 'connect', 'large', 'input', 'layer', 'much', 'smaller', 'layer', 'spacing', 'receptive', 'fields', 'shown', 'Figure', '13-4', '.', 'The', 'distance', 'two', 'consecutivereceptive', 'fields', 'called', 'stride', '.', 'In', 'diagram', '5', '‰', '7', 'input', 'layer', 'plus', 'zero', 'pad…', 'ding', 'connected', '3', '‰', '4', 'layer', 'using', '3', '‰', '3', 'receptive', 'fields', 'stride', '2', 'example', 'stride', 'directions', '.', 'A', 'neuron', 'located', 'row', 'column', 'j', 'upper', 'layer', 'connected', 'outputs', 'neurons', 'previous', 'layer', 'located', 'rows', '‰', 'sh', '‰', 'sh', '+', 'fh', '–', '1', 'columns', 'j', '‰', 'sw', '+fw', '–', '1', 'sh', 'sw', 'vertical', 'horizontal', 'strides', '.', 'Figure', '13-4', '.', 'Reducing', 'dimensionality', 'using', 'stride', 'FiltersA', 'neuron‡s', 'weights', 'represented', 'small', 'image', 'size', 'receptive', 'field', '.', 'For', 'example', 'Figure', '13-5', 'shows', 'two', 'possible', 'sets', 'weights', 'called', '†lters', 'convolu…', 'tion', 'kernels', '.', 'The', 'first', 'one', 'represented', 'black', 'square', 'vertical', 'white', 'line', 'middle', '7', '‰', '7', 'matrix', 'full', '0s', 'except', 'central', 'column', 'full', '1s', 'neurons', 'using', 'weights', 'ignore', 'everything', 'receptive', 'field', 'except', 'central', 'vertical', 'line', 'since', 'inputs', 'get', 'multiplied', '0', 'except', 'ones', 'located', 'central', 'vertical', 'line', '.', 'The', 'second', 'filter', 'black', 'square', 'horizontal', 'white', 'line', 'middle', '.', 'Once', 'neurons', 'using', 'weights', 'ignore', 'everything', 'receptive', 'field', 'except', 'central', 'horizontal', 'line', '.', 'Now', 'neurons', 'layer', 'use', 'vertical', 'line', 'filter', 'bias', 'term', 'feed', 'network', 'input', 'image', 'shown', 'Figure', '13-5', 'bottom', 'image', 'thelayer', 'output', 'top-left', 'image', '.', 'Notice', 'vertical', 'white', 'lines', 'get', 'enhanced', 'rest', 'gets', 'blurred', '.', 'Similarly', 'upper-right', 'image', 'get', 'neu…', 'rons', 'use', 'horizontal', 'line', 'filter', 'notice', 'horizontal', 'white', 'lines', 'get', 'enhanced', 'rest', 'blurred', '.', 'Thus', 'layer', 'full', 'neurons', 'using', 'filter', 'gives', 'Convolutional', 'Layer', '|', '357', 'feature', 'map', 'highlights', 'areas', 'image', 'similar', 'filter', '.', 'During', 'training', 'CNN', 'finds', 'useful', 'filters', 'task', 'learns', 'combine', 'complex', 'patterns', 'e.g.', 'cross', 'area', 'image', 'vertical', 'filter', 'horizontal', 'filter', 'active', '.', 'Figure', '13-5', '.', 'Applying', 'two', 'di›erent', '†lters', 'get', 'two', 'feature', 'maps', 'Stacking', 'Multiple', 'Feature', 'MapsUp', 'simplicity', 'represented', 'convolutional', 'layer', 'thin', '2D', 'layer', 'reality', 'composed', 'several', 'feature', 'maps', 'equal', 'sizes', 'accurately', 'represented', '3D', 'see', 'Figure', '13-6', '.', 'Within', 'one', 'feature', 'map', 'neurons', 'share', 'parameters', 'weights', 'bias', 'term', 'different', 'feature', 'maps', 'may', 'different', 'parameters', '.', 'A', 'neuron‡s', 'receptive', 'field', 'described', 'earlier', 'extends', 'across', 'previous', 'layers‡', 'feature', 'maps', '.', 'In', 'short', 'convolutional', 'layer', 'simultaneously', 'applies', 'multiple', 'filters', 'inputs', 'making', 'capable', 'detect…', 'ing', 'multiple', 'features', 'anywhere', 'inputs', '.', 'The', 'fact', 'neurons', 'feature', 'map', 'share', 'parame…', 'ters', 'dramatically', 'reduces', 'number', 'parameters', 'model', 'importantly', 'means', 'CNN', 'learned', 'recognize', 'pattern', 'one', 'location', 'recognize', 'location', '.', 'In', 'contrast', 'regular', 'DNN', 'learned', 'recognize', 'pattern', 'one', 'location', 'recognize', 'particular', 'location', '.', '358', '|', 'Chapter', '13', 'Convolutional', 'Neural', 'Networks', 'Moreover', 'input', 'images', 'also', 'composed', 'multiple', 'sublayers', 'one', 'per', 'color', 'chan…', 'nel', '.', 'There', 'typically', 'three', 'red', 'green', 'blue', 'RGB', '.', 'Grayscale', 'images', 'one', 'channel', 'images', 'may', 'much', 'more›for', 'example', 'satellite', 'images', 'capture', 'extra', 'light', 'frequencies', 'infrared', '.', 'Figure', '13-6', '.', 'Convolution', 'layers', 'multiple', 'feature', 'maps', 'images', 'three', 'channels', 'Specifically', 'neuron', 'located', 'row', 'column', 'j', 'feature', 'map', 'k', 'given', 'convo…', 'lutional', 'layer', 'l', 'connected', 'outputs', 'neurons', 'previous', 'layer', 'l', '–', '1', 'located', 'rows', '‰', 'sw', '‰', 'sw', '+', 'fw', '–', '1', 'columns', 'j', '‰', 'sh', 'j', '‰', 'sh', '+', 'fh', '–', '1', 'across', 'allfeature', 'maps', 'layer', 'l', '–', '1', '.', 'Note', 'neurons', 'located', 'row', 'col…umn', 'j', 'different', 'feature', 'maps', 'connected', 'outputs', 'exact', 'neurons', 'previous', 'layer', '.', 'Equation', '13-1', 'summarizes', 'preceding', 'explanations', 'one', 'big', 'mathematical', 'equa…', 'tion', 'shows', 'compute', 'output', 'given', 'neuron', 'convolutional', 'layer', '.', 'Convolutional', 'Layer', '|', '359', 'It', 'bit', 'ugly', 'due', 'different', 'indices', 'calculate', 'weighted', 'sum', 'inputs', 'plus', 'bias', 'term', '.', 'Equation', '13-1', '.', 'Computing', 'output', 'neuron', 'convolutional', 'layer', 'zi', 'j', 'k=bk+', '“', 'u=1', 'fh', '“', 'v=1', 'fw', '“', 'k=1', 'fnxi', 'j', 'k.wu', 'v', 'k', 'kwithi=u.sh+fh', '”', '1', 'j=v.sw+fw', '”', '1', '‹zi', 'j', 'k', 'output', 'neuron', 'located', 'row', 'column', 'j', 'feature', 'map', 'k', 'convolutional', 'layer', 'layer', 'l', '.‹As', 'explained', 'earlier', 'sh', 'sw', 'vertical', 'horizontal', 'strides', 'fh', 'fw', 'arethe', 'height', 'width', 'receptive', 'field', 'fn', 'number', 'feature', 'maps', 'previous', 'layer', 'layer', 'l', '–', '1', '.‹xi', 'j', 'k', 'output', 'neuron', 'located', 'layer', 'l', '–', '1', 'row', 'column', 'j', 'feature', 'map', 'k', 'channel', 'kif', 'previous', 'layer', 'input', 'layer', '.', '‹bk', 'bias', 'term', 'feature', 'map', 'k', 'layer', 'l', '.', 'You', 'think', 'knob', 'tweaks', 'overall', 'brightness', 'feature', 'map', 'k.‹wu', 'v', 'k', 'k', 'connection', 'weight', 'neuron', 'feature', 'map', 'k', 'layer', 'l', 'input', 'located', 'row', 'u', 'column', 'v', 'relative', 'neuron‡s', 'receptive', 'field', 'feature', 'map', 'k.TensorFlow', 'ImplementationIn', 'TensorFlow', 'input', 'image', 'typically', 'represented', '3D', 'tensor', 'shape', 'height', 'width', 'channels', '.', 'A', 'mini-batch', 'represented', '4D', 'tensor', 'shape', 'mini-batch', 'size', 'height', 'width', 'channels', '.', 'The', 'weights', 'convolutional', 'layer', 'represented', '4D', 'tensor', 'shape', 'fh', 'fw', 'fn', 'fn', '.', 'The', 'bias', 'terms', 'convo…', 'lutional', 'layer', 'simply', 'represented', '1D', 'tensor', 'shape', 'fn', '.Let‡s', 'look', 'simple', 'example', '.', 'The', 'following', 'code', 'loads', 'two', 'sample', 'images', 'using', 'Scikit-Learn‡s', 'load_sample_images', 'loads', 'two', 'color', 'images', 'one', 'Chi…nese', 'temple', 'flower', '.', 'Then', 'creates', 'two', '7', '‰', '7', 'filters', 'one', 'vertical', 'white', 'line', 'middle', 'horizontal', 'white', 'line', 'applies', 'images', 'using', 'convolutional', 'layer', 'built', 'using', 'TensorFlow‡s', 'conv2d', 'function', 'zero', 'padding', 'stride', '2', '.', 'Finally', 'plots', 'one', 'resulting', 'feature', 'maps', 'similar', 'top-right', 'image', 'Figure', '13-5', '.360', '|', 'Chapter', '13', 'Convolutional', 'Neural', 'Networks', 'import', 'numpy', 'npfrom', 'sklearn.datasets', 'import', 'load_sample_images', '#', 'Load', 'sample', 'imagesdataset', '=', 'np.array', 'load_sample_images', '.images', 'dtype=np.float32', 'batch_size', 'height', 'width', 'channels', '=', 'dataset.shape', '#', 'Create', '2', 'filtersfilters_test', '=', 'np.zeros', 'shape=', '7', '7', 'channels', '2', 'dtype=np.float32', 'filters_test', '3', '0', '=', '1', '#', 'vertical', 'linefilters_test', '3', '1', '=', '1', '#', 'horizontal', 'line', '#', 'Create', 'graph', 'input', 'X', 'plus', 'convolutional', 'layer', 'applying', '2', 'filtersX', '=', 'tf.placeholder', 'tf.float32', 'shape=', 'None', 'height', 'width', 'channels', 'convolution', '=', 'tf.nn.conv2d', 'X', 'filters', 'strides=', '1,2,2,1', 'padding=', \"''\", 'SAME', \"''\", 'tf.Session', 'sess', 'output', '=', 'sess.run', 'convolution', 'feed_dict=', '{', 'X', 'dataset', '}', 'plt.imshow', 'output', '0', '1', '#', 'plot', '1st', 'image•s', '2nd', 'feature', 'mapplt.show', 'Most', 'code', 'self-explanatory', 'conv2d', 'line', 'deserves', 'bit', 'explana…', 'tion', '‹X', 'input', 'mini-batch', '4D', 'tensor', 'explained', 'earlier', '.', '‹filters', 'set', 'filters', 'apply', 'also', '4D', 'tensor', 'explained', 'earlier', '.', '‹strides', 'four-element', '1D', 'array', 'two', 'central', 'elements', 'verti…', 'cal', 'horizontal', 'strides', 'sh', 'sw', '.', 'The', 'first', 'last', 'elements', 'must', 'currently', 'equal', '1', '.', 'They', 'may', 'one', 'day', 'used', 'specify', 'batch', 'stride', 'skip', 'instances', 'channel', 'stride', 'skip', 'previous', 'layer‡s', 'feature', 'maps', 'channels', '.‹padding', 'must', 'either', \"''\", 'VALID', \"''\", '``', 'SAME', \"''\", '›If', 'set', '``', 'VALID', \"''\", 'convolutional', 'layer', 'use', 'zero', 'padding', 'may', 'ignore', 'rows', 'columns', 'bottom', 'right', 'input', 'image', 'depending', 'stride', 'shown', 'Figure', '13-7', 'simplicity', 'hor…', 'izontal', 'dimension', 'shown', 'course', 'logic', 'applies', 'vertical', 'dimension', '.›If', 'set', '``', 'SAME', \"''\", 'convolutional', 'layer', 'uses', 'zero', 'padding', 'necessary', '.', 'In', 'case', 'number', 'output', 'neurons', 'equal', 'number', 'input', 'neurons', 'divided', 'stride', 'rounded', 'example', 'ceil', '13', '/', '5', '=', '3', '.', 'Then', 'zeros', 'added', 'evenly', 'possible', 'around', 'inputs', '.', 'Convolutional', 'Layer', '|', '361', '7A', 'fully', 'connected', 'layer', '150', '‰', '100', 'neurons', 'connected', '150', '‰', '100', '‰', '3', 'inputs', 'would', '150', '2‰', '1002', '‰', '3', '=', '675', 'million', 'parameters', '!', 'Figure', '13-7', '.', 'Padding', 'optionsŠinput', 'width', '13', '†lter', 'width', '6', 'stride', '5', 'Unfortunately', 'convolutional', 'layers', 'quite', 'hyperparameters', 'must', 'choose', 'number', 'filters', 'height', 'width', 'strides', 'padding', 'type', '.', 'As', 'always', 'use', 'cross-validation', 'find', 'right', 'hyperparameter', 'values', 'time-consuming', '.', 'We', 'discuss', 'common', 'CNN', 'architectures', 'later', 'give', 'idea', 'hyperparameter', 'values', 'work', 'best', 'practice', '.', 'Memory', 'RequirementsAnother', 'problem', 'CNNs', 'convolutional', 'layers', 'require', 'huge', 'amount', 'RAM', 'especially', 'training', 'reverse', 'pass', 'backpropagation', 'requires', 'intermediate', 'values', 'computed', 'forward', 'pass', '.', 'For', 'example', 'consider', 'convolutional', 'layer', '5', '‰', '5', 'filters', 'outputting', '200', 'feature', 'maps', 'size', '150', '‰', '100', 'stride', '1', 'SAME', 'padding', '.', 'If', 'input', '150', '‰', '100', 'RGB', 'image', 'three', 'channels', 'number', 'parameters', '5', '‰', '5', '‰', '3', '+', '1', '‰', '200', '=', '15,200', '+1', 'corresponds', 'bias', 'terms', 'fairly', 'small', 'compared', 'fully', 'connected', 'layer', '.', '7', 'However', '200', 'feature', 'maps', 'contains', '150', '‰', '100', 'neu…', 'rons', 'neurons', 'needs', 'compute', 'weighted', 'sum', '5', '‰', '5', '‰', '3', '=', '75', 'inputs', 'that‡s', 'total', '225', 'million', 'float', 'multiplications', '.', 'Not', 'bad', 'fully', 'con…', '362', '|', 'Chapter', '13', 'Convolutional', 'Neural', 'Networks', '81', 'MB', '=', '1,024', 'kB', '=', '1,024', '‰', '1,024', 'bytes', '=', '1,024', '‰', '1,024', '‰', '8', 'bits.nected', 'layer', 'still', 'quite', 'computationally', 'intensive', '.', 'Moreover', 'feature', 'maps', 'represented', 'using', '32-bit', 'floats', 'convolutional', 'layer‡s', 'output', 'occupy', '200', '‰', '150', '‰', '100', '‰', '32', '=', '96', 'million', 'bits', '11.4', 'MB', 'RAM.8', 'And', 'that‡s', 'one', 'instance', '!', 'If', 'training', 'batch', 'contains', '100', 'instances', 'layer', 'use', '1', 'GB', 'RAM', '!', 'During', 'inference', 'i.e.', 'making', 'prediction', 'new', 'instance', 'RAM', 'occu…pied', 'one', 'layer', 'released', 'soon', 'next', 'layer', 'computed', 'need', 'much', 'RAM', 'required', 'two', 'consecutive', 'layers', '.', 'But', 'training', 'everything', 'computed', 'forward', 'pass', 'needs', 'preserved', 'reverse', 'pass', 'amount', 'RAM', 'needed', 'least', 'total', 'amount', 'RAM', 'required', 'layers', '.', 'If', 'training', 'crashes', 'out-of-memory', 'error', 'try', 'reducing', 'mini-batch', 'size', '.', 'Alternatively', 'try', 'reducing', 'dimensionality', 'using', 'stride', 'removing', 'layers', '.', 'Or', 'try', 'using', '16-bit', 'floats', 'instead', '32-bit', 'floats', '.', 'Or', 'could', 'distrib…', 'ute', 'CNN', 'across', 'multiple', 'devices', '.', 'Now', 'let‡s', 'look', 'second', 'common', 'building', 'block', 'CNNs', 'pooling', 'layer', '.Pooling', 'LayerOnce', 'understand', 'convolutional', 'layers', 'work', 'pooling', 'layers', 'quite', 'easy', 'grasp', '.', 'Their', 'goal', 'subsample', 'i.e.', 'shrink', 'input', 'image', 'order', 'reduce', 'computational', 'load', 'memory', 'usage', 'number', 'parameters', 'thereby', 'limiting', 'risk', 'overfitting', '.', 'Reducing', 'input', 'image', 'size', 'also', 'makes', 'neural', 'network', 'tolerate', 'little', 'bit', 'image', 'shift', 'location', 'invariance', '.Just', 'like', 'convolutional', 'layers', 'neuron', 'pooling', 'layer', 'connected', 'outputs', 'limited', 'number', 'neurons', 'previous', 'layer', 'located', 'within', 'small', 'rectangular', 'receptive', 'field', '.', 'You', 'must', 'define', 'size', 'stride', 'padding', 'type', 'like', '.', 'However', 'pooling', 'neuron', 'weights', 'aggregate', 'inputs', 'using', 'aggregation', 'function', 'max', 'mean', '.', 'Figure', '13-8', 'shows', 'amax', 'pooling', 'layer', 'common', 'type', 'pooling', 'layer', '.', 'In', 'example', 'use', '2', '‰', '2', 'pooling', 'kernel', 'stride', '2', 'padding', '.', 'Note', 'max', 'input', 'value', 'kernel', 'makes', 'next', 'layer', '.', 'The', 'inputs', 'dropped', '.', 'Pooling', 'Layer', '|', '363', 'Figure', '13-8', '.', 'Max', 'pooling', 'layer', '2', 'Ÿ', '2', 'pooling', 'kernel', 'stride', '2', 'padding', 'This', 'obviously', 'destructive', 'kind', 'layer', 'even', 'tiny', '2', '‰', '2', 'kernel', 'stride', '2', 'output', 'two', 'times', 'smaller', 'directions', 'area', 'befour', 'times', 'smaller', 'simply', 'dropping', '75', '%', 'input', 'values', '.', 'A', 'pooling', 'layer', 'typically', 'works', 'every', 'input', 'channel', 'independently', 'output', 'depth', 'input', 'depth', '.', 'You', 'may', 'alternatively', 'pool', 'depth', 'dimension', 'see', 'next', 'case', 'image‡s', 'spatial', 'dimensions', 'height', 'width', 'remain', 'unchanged', 'number', 'channels', 'reduced', '.', 'Implementing', 'max', 'pooling', 'layer', 'TensorFlow', 'quite', 'easy', '.', 'The', 'following', 'code', 'creates', 'max', 'pooling', 'layer', 'using', '2', '‰', '2', 'kernel', 'stride', '2', 'padding', 'applies', 'images', 'dataset', '...', '#', 'load', 'image', 'dataset', 'like', '#', 'Create', 'graph', 'input', 'X', 'plus', 'max', 'pooling', 'layerX', '=', 'tf.placeholder', 'tf.float32', 'shape=', 'None', 'height', 'width', 'channels', 'max_pool', '=', 'tf.nn.max_pool', 'X', 'ksize=', '1,2,2,1', 'strides=', '1,2,2,1', 'padding=', \"''\", 'VALID', \"''\", 'tf.Session', 'sess', 'output', '=', 'sess.run', 'max_pool', 'feed_dict=', '{', 'X', 'dataset', '}', 'plt.imshow', 'output', '0', '.astype', 'np.uint8', '#', 'plot', 'output', '1st', 'imageplt.show', 'The', 'ksize', 'argument', 'contains', 'kernel', 'shape', 'along', 'four', 'dimensions', 'input', 'tensor', 'batch', 'size', 'height', 'width', 'channels', '.', 'TensorFlow', 'currently', 'support', 'pooling', 'multiple', 'instances', 'first', 'element', 'ksize', 'must', 'equal', '1', '.', 'Moreover', 'support', 'pooling', 'spatial', 'dimensions', 'height', 'width', 'depth', 'dimension', 'either', 'ksize', '1', 'ksize', '2', 'must', 'equal', '1', 'ksize', '3', 'must', 'equal', '1', '.', 'To', 'create', 'average', 'pooling', 'layer', 'use', 'avg_pool', 'function', 'instead', 'ofmax_pool', '.364', '|', 'Chapter', '13', 'Convolutional', 'Neural', 'Networks', 'Now', 'know', 'building', 'blocks', 'create', 'convolutional', 'neural', 'network', '.', 'Let‡s', 'see', 'assemble', 'them.CNN', 'ArchitecturesTypical', 'CNN', 'architectures', 'stack', 'convolutional', 'layers', 'one', 'generally', 'fol…', 'lowed', 'ReLU', 'layer', 'pooling', 'layer', 'another', 'convolutional', 'layers', '+ReLU', 'another', 'pooling', 'layer', '.', 'The', 'image', 'gets', 'smaller', 'smaller', 'progresses', 'network', 'also', 'typically', 'gets', 'deeper', 'deeper', 'i.e.', 'feature', 'maps', 'thanks', 'convolutional', 'layers', 'see', 'Figure', '13-9', '.', 'At', 'top', 'stack', 'regular', 'feedforward', 'neural', 'network', 'added', 'composed', 'fully', 'connected', 'layers', '+ReLUs', 'final', 'layer', 'outputs', 'prediction', 'e.g.', 'softmax', 'layer', 'outputs', 'estimated', 'class', 'probabilities', '.', 'Figure', '13-9', '.', 'Typical', 'CNN', 'architecture', 'A', 'common', 'mistake', 'use', 'convolution', 'kernels', 'large', '.', 'You', 'often', 'get', 'effect', '9', '‰', '9', 'kernel', 'stacking', 'two', '3', '‰', '3', 'kernels', 'top', 'lot', 'less', 'compute', '.', 'Over', 'years', 'variants', 'fundamental', 'architecture', 'developed', 'lead…', 'ing', 'amazing', 'advances', 'field', '.', 'A', 'good', 'measure', 'progress', 'error', 'rate', 'competitions', 'ILSVRC', 'ImageNet', 'challenge', '.', 'In', 'competition', 'top-5', 'error', 'rate', 'image', 'classification', 'fell', '26', '%', 'barely', '3', '%', 'five', 'years', '.', 'The', 'top-five', 'error', 'rate', 'number', 'test', 'images', 'system‡s', 'top', '5', 'predictions', 'include', 'correct', 'answer', '.', 'The', 'images', 'large', '256', 'pixels', 'high', '1,000', 'classes', 'really', 'subtle', 'try', 'distinguishing', '120', 'dog', 'breeds', '.', 'Looking', 'evolution', 'winning', 'entries', 'good', 'way', 'understand', 'CNNs', 'work', '.', 'We', 'first', 'look', 'classical', 'LeNet-5', 'architecture', '1998', 'three', 'win…', 'ners', 'ILSVRC', 'challenge', 'AlexNet', '2012', 'GoogLeNet', '2014', 'ResNet', '2015', '.CNN', 'Architectures', '|', '365', 'Other', 'Visual', 'TasksThere', 'stunning', 'progress', 'well', 'visual', 'tasks', 'object', 'detection', 'andlocalization', 'image', 'segmentation', '.', 'In', 'object', 'detection', 'localization', 'neural', 'network', 'typically', 'outputs', 'sequence', 'bounding', 'boxes', 'around', 'various', 'objects', 'inthe', 'image', '.', 'For', 'example', 'see', 'Maxine', 'Oquab', 'et', 'al.‡s', '2015', 'paper', 'outputs', 'heat', 'map', 'object', 'class', 'Russell', 'Stewart', 'et', 'al.‡s', '2015', 'paper', 'uses', 'combination', 'CNN', 'detect', 'faces', 'recurrent', 'neural', 'network', 'output', 'sequence', 'bound…', 'ing', 'boxes', 'around', '.', 'In', 'image', 'segmentation', 'net', 'outputs', 'image', 'usually', 'size', 'input', 'pixel', 'indicates', 'class', 'object', 'corresponding', 'input', 'pixel', 'belongs', '.', 'For', 'example', 'check', 'Evan', 'Shelhamer', 'et', 'al.‡s', '2016', 'paper', '.LeNet-5The', 'LeNet-5', 'architecture', 'perhaps', 'widely', 'known', 'CNN', 'architecture', '.', 'As', 'mentioned', 'earlier', 'created', 'Yann', 'LeCun', '1998', 'widely', 'used', 'hand…', 'written', 'digit', 'recognition', 'MNIST', '.', 'It', 'composed', 'layers', 'shown', 'Table', '13-1', '.Table', '13-1', '.', 'LeNet-5', 'architecture', 'LayerTypeMapsSizeKernel', 'size', 'StrideActivationOutFully', 'Connected', '–10––RBFF6Fully', 'Connected', '–84––tanhC5Convolution1201', '†', '1', '5', '†', '5', '1tanhS4Avg', 'Pooling', '165', '†', '5', '2', '†', '2', '2tanhC3Convolution1610', '†', '10', '5', '†', '5', '1tanhS2Avg', 'Pooling', '614', '†', '14', '2', '†', '2', '2tanhC1Convolution628', '†', '28', '5', '†', '5', '1tanhInInput132', '†', '32', '–––There', 'extra', 'details', 'noted', '‹MNIST', 'images', '28', '‰', '28', 'pixels', 'zero-padded', '32', '‰', '32', 'pixels', 'andnormalized', 'fed', 'network', '.', 'The', 'rest', 'network', 'useany', 'padding', 'size', 'keeps', 'shrinking', 'image', 'progresses', 'network.‹The', 'average', 'pooling', 'layers', 'slightly', 'complex', 'usual', 'neuron', 'computes', 'mean', 'inputs', 'multiplies', 'result', 'learnable', 'coeffi…', 'cient', 'one', 'per', 'map', 'adds', 'learnable', 'bias', 'term', 'one', 'per', 'map', 'finally', 'applies', 'activation', 'function', '.', '366', '|', 'Chapter', '13', 'Convolutional', 'Neural', 'Networks', '9ƒImageNet', 'Classification', 'Deep', 'Convolutional', 'Neural', 'Networks', '⁄', 'A.', 'Krizhevsky', 'et', 'al', '.', '2012', '.', '‹Most', 'neurons', 'C3', 'maps', 'connected', 'neurons', 'three', 'four', 'S2', 'maps', 'instead', 'six', 'S2', 'maps', '.', 'See', 'table', '1', 'original', 'paper', 'details', '.', '‹The', 'output', 'layer', 'bit', 'special', 'instead', 'computing', 'dot', 'product', 'inputs', 'weight', 'vector', 'neuron', 'outputs', 'square', 'Euclidian', 'distance', 'input', 'vector', 'weight', 'vector', '.', 'Each', 'output', 'measures', 'much', 'image', 'belongs', 'particular', 'digit', 'class', '.', 'The', 'cross', 'entropy', 'costfunction', 'preferred', 'penalizes', 'bad', 'predictions', 'much', 'producing', 'larger', 'gradients', 'thus', 'converging', 'faster', '.', 'Yann', 'LeCun‡s', 'website', 'ƒLENET⁄', 'section', 'features', 'great', 'demos', 'LeNet-5', 'classifying', 'digits.AlexNetThe', 'AlexNet', 'CNN', 'architecture', '9', '2012', 'ImageNet', 'ILSVRC', 'challenge', 'large', 'margin', 'achieved', '17', '%', 'top-5', 'error', 'rate', 'second', 'best', 'achieved', '26', '%', '!', 'It', 'developed', 'Alex', 'Krizhevsky', 'hence', 'name', 'Ilya', 'Sutskever', 'Geoffrey', 'Hinton', '.', 'It', 'quite', 'similar', 'LeNet-5', 'much', 'larger', 'deeper', 'first', 'stack', 'convolutional', 'layers', 'directly', 'top', 'instead', 'stacking', 'pooling', 'layer', 'top', 'convolutional', 'layer', '.', 'Table', '13-2', 'presents', 'architecture', '.', 'Table', '13-2', '.', 'AlexNet', 'architecture', 'LayerTypeMapsSizeKernel', 'size', 'StridePaddingActivationOutFully', 'Connected', '–1,000–––SoftmaxF9Fully', 'Connected', '–4,096–––ReLUF8Fully', 'Connected', '–4,096–––ReLUC7Convolution25613', '†', '13', '3', '†', '3', '1SAMEReLUC6Convolution38413', '†', '13', '3', '†', '3', '1SAMEReLUC5Convolution38413', '†', '13', '3', '†', '3', '1SAMEReLUS4Max', 'Pooling', '25613', '†', '13', '3', '†', '3', '2VALID–C3Convolution25627', '†', '27', '5', '†', '5', '1SAMEReLUS2Max', 'Pooling', '9627', '†', '27', '3', '†', '3', '2VALID–C1Convolution9655', '†', '55', '11', '†', '11', '4SAMEReLUInInput3', 'RGB', '224', '†', '224', '––––To', 'reduce', 'overfitting', 'authors', 'used', 'two', 'regularization', 'techniques', 'discussed', 'previous', 'chapters', 'first', 'applied', 'dropout', '50', '%', 'dropout', 'rate', 'train…', 'ing', 'outputs', 'layers', 'F8', 'F9', '.', 'Second', 'performed', 'data', 'augmentation', 'CNN', 'Architectures', '|', '367', '10ƒGoing', 'Deeper', 'Convolutions', '⁄', 'C.', 'Szegedy', 'et', 'al', '.', '2015', '.', 'randomly', 'shifting', 'training', 'images', 'various', 'offsets', 'flipping', 'horizontally', 'changing', 'lighting', 'conditions', '.', 'AlexNet', 'also', 'uses', 'competitive', 'normalization', 'step', 'immediately', 'ReLU', 'step', 'layers', 'C1', 'C3', 'called', 'local', 'response', 'normalization', '.', 'This', 'form', 'normalization', 'makes', 'neurons', 'strongly', 'activate', 'inhibit', 'neurons', 'location', 'neighboring', 'feature', 'maps', 'competitive', 'activation', 'observed', 'biological', 'neurons', '.', 'This', 'encourages', 'different', 'feature', 'maps', 'specialize', 'pushing', 'apart', 'forcing', 'explore', 'wider', 'range', 'features', 'ultimately', 'improv…', 'ing', 'generalization', '.', 'Equation', '13-2', 'shows', 'apply', 'LRN', '.', 'Equation', '13-2', '.', 'Local', 'response', 'normalization', 'bi=aik+‰', '“', 'j=jlowjhighaj2', '”', 'Łwithjhigh=min', 'i+r2', 'fn', '”', '1', 'jlow=max', '0', '”', 'r2‹bi', 'normalized', 'output', 'neuron', 'located', 'feature', 'map', 'row', 'uand', 'column', 'v', 'note', 'equation', 'consider', 'neurons', 'located', 'row', 'column', 'u', 'v', 'shown', '.‹ai', 'activation', 'neuron', 'ReLU', 'step', 'normalization', '.', '‹k', '‰', 'Ł', 'r', 'hyperparameters', '.', 'k', 'called', 'bias', 'r', 'called', 'depth', 'radius', '.‹fn', 'number', 'feature', 'maps', '.', 'For', 'example', 'r', '=', '2', 'neuron', 'strong', 'activation', 'inhibit', 'activation', 'neurons', 'located', 'feature', 'maps', 'immediately', '.', 'In', 'AlexNet', 'hyperparameters', 'set', 'follows', 'r', '=', '2', '‰', '=', '0.00002', 'Ł', '=', '0.75', 'k=', '1', '.', 'This', 'step', 'implemented', 'using', 'TensorFlow‡s', 'local_response_normalization', 'operation', '.', 'A', 'variant', 'AlexNet', 'called', 'ZF', 'Net', 'developed', 'Matthew', 'Zeiler', 'Rob', 'Fergus', '2013', 'ILSVRC', 'challenge', '.', 'It', 'essentially', 'AlexNet', 'tweaked', 'hyperparameters', 'number', 'feature', 'maps', 'kernel', 'size', 'stride', 'etc.', '.', 'GoogLeNetThe', 'GoogLeNet', 'architecture', 'developed', 'Christian', 'Szegedy', 'et', 'al', '.', 'GoogleResearch,10', 'ILSVRC', '2014', 'challenge', 'pushing', 'top-5', 'error', 'rate', '368', '|', 'Chapter', '13', 'Convolutional', 'Neural', 'Networks', '11In', '2010', 'movie', 'Inception', 'characters', 'keep', 'going', 'deeper', 'deeper', 'multiple', 'layers', 'dreams', 'hence', 'name', 'modules.below', '7', '%', '.', 'This', 'great', 'performance', 'came', 'large', 'part', 'fact', 'network', 'much', 'deeper', 'previous', 'CNNs', 'see', 'Figure', '13-11', '.', 'This', 'made', 'possible', 'bysub-networks', 'called', 'inception', 'modules', ',11', 'allow', 'GoogLeNet', 'use', 'parameters', 'much', 'efficiently', 'previous', 'architectures', 'GoogLeNet', 'actually', '10', 'times', 'fewer', 'parameters', 'AlexNet', 'roughly', '6', 'million', 'instead', '60', 'million', '.', 'Figure', '13-10', 'shows', 'architecture', 'inception', 'module', '.', 'The', 'notation', 'ƒ3', '‰', '3', '+', '2', 'S', '⁄', 'means', 'layer', 'uses', '3', '‰', '3', 'kernel', 'stride', '2', 'SAME', 'padding', '.', 'The', 'input', 'signal', 'first', 'copied', 'fed', 'four', 'different', 'layers', '.', 'All', 'convolutional', 'layers', 'use', 'ReLU', 'activation', 'function', '.', 'Note', 'second', 'set', 'convolutional', 'layers', 'uses', 'differ…', 'ent', 'kernel', 'sizes', '1', '‰', '1', '3', '‰', '3', '5', '‰', '5', 'allowing', 'capture', 'patterns', 'different', 'scales', '.', 'Also', 'note', 'every', 'single', 'layer', 'uses', 'stride', '1', 'SAME', 'padding', 'even', 'max', 'pooling', 'layer', 'outputs', 'height', 'width', 'inputs', '.', 'This', 'makes', 'possible', 'concatenate', 'outputs', 'along', 'depth', 'dimen…', 'sion', 'final', 'depth', 'concat', 'layer', 'i.e.', 'stack', 'feature', 'maps', 'four', 'top', 'con…', 'volutional', 'layers', '.', 'This', 'concatenation', 'layer', 'implemented', 'TensorFlow', 'using', 'concat', 'operation', 'axis=3', 'axis', '3', 'depth', '.Figure', '13-10', '.', 'Inception', 'module', 'You', 'may', 'wonder', 'inception', 'modules', 'convolutional', 'layers', '1', '‰', '1', 'ker…', 'nels', '.', 'Surely', 'layers', 'capture', 'features', 'since', 'look', 'one', 'pixel', 'time', '?', 'In', 'fact', 'layers', 'serve', 'two', 'purposes', '‹First', 'configured', 'output', 'many', 'fewer', 'feature', 'maps', 'inputs', 'serve', 'bottleneck', 'layers', 'meaning', 'reduce', 'dimensionality', '.', 'This', 'par…', 'CNN', 'Architectures', '|', '369', 'ticularly', 'useful', '3', '‰', '3', '5', '‰', '5', 'convolutions', 'since', 'com…', 'putationally', 'expensive', 'layers', '.', '‹Second', 'pair', 'convolutional', 'layers', '1', '‰', '1', '3', '‰', '3', '1', '‰', '1', '5', '‰', '5', 'acts', 'like', 'single', 'powerful', 'convolutional', 'layer', 'capable', 'capturing', 'complex', 'patterns', '.', 'Indeed', 'instead', 'sweeping', 'simple', 'linear', 'classifier', 'across', 'image', 'single', 'convolutional', 'layer', 'pair', 'convolutional', 'layers', 'sweeps', 'two-layer', 'neural', 'network', 'across', 'image', '.', 'In', 'short', 'think', 'whole', 'inception', 'module', 'convolutional', 'layer', 'steroids', 'able', 'output', 'feature', 'maps', 'capture', 'complex', 'patterns', 'various', 'scales', '.', 'The', 'number', 'convolutional', 'kernels', 'convolutional', 'layer', 'hyperparameter', '.', 'Unfortunately', 'means', 'six', 'hyperparameters', 'tweak', 'every', 'inception', 'layer', 'add', '.', 'Now', 'let‡s', 'look', 'architecture', 'GoogLeNet', 'CNN', 'see', 'Figure', '13-11', '.', 'It', 'deep', 'represent', 'three', 'columns', 'GoogLeNet', 'actually', 'one', 'tall', 'stack', 'including', 'nine', 'inception', 'modules', 'boxes', 'spinning', 'tops', 'actually', 'contain', 'three', 'layers', '.', 'The', 'number', 'feature', 'maps', 'output', 'convo…', 'lutional', 'layer', 'pooling', 'layer', 'shown', 'kernel', 'size', '.', 'The', 'six', 'numbers', 'inception', 'modules', 'represent', 'number', 'feature', 'maps', 'output', 'con…', 'volutional', 'layer', 'module', 'order', 'Figure', '13-10', '.', 'Note', 'convolutional', 'layers', 'use', 'ReLU', 'activation', 'function', '.', '370', '|', 'Chapter', '13', 'Convolutional', 'Neural', 'Networks', 'Figure', '13-11', '.', 'GoogLeNet', 'architecture', 'Let‡s', 'go', 'network', '‹The', 'first', 'two', 'layers', 'divide', 'image‡s', 'height', 'width', '4', 'area', 'divided', '16', 'reduce', 'computational', 'load', '.', '‹Then', 'local', 'response', 'normalization', 'layer', 'ensures', 'previous', 'layers', 'learn', 'wide', 'variety', 'features', 'discussed', 'earlier', '.', '‹Two', 'convolutional', 'layers', 'follow', 'first', 'acts', 'like', 'bottleneck', 'layer', '.', 'Asexplained', 'earlier', 'think', 'pair', 'single', 'smarter', 'convolutional', 'layer', '.', '‹Again', 'local', 'response', 'normalization', 'layer', 'ensures', 'previous', 'layers', 'cap…', 'ture', 'wide', 'variety', 'patterns', '.', '‹Next', 'max', 'pooling', 'layer', 'reduces', 'image', 'height', 'width', '2', 'speed', 'computations', '.', 'CNN', 'Architectures', '|', '371', '12ƒDeep', 'Residual', 'Learning', 'Image', 'Recognition', '⁄', 'K.', 'He', '2015', '.', '‹Then', 'comes', 'tall', 'stack', 'nine', 'inception', 'modules', 'interleaved', 'couple', 'max', 'pooling', 'layers', 'reduce', 'dimensionality', 'speed', 'net', '.', '‹Next', 'average', 'pooling', 'layer', 'uses', 'kernel', 'size', 'feature', 'maps', 'VALID', 'padding', 'outputting', '1', '‰', '1', 'feature', 'maps', 'surprising', 'strategy', 'calledglobal', 'average', 'pooling', '.', 'It', 'effectively', 'forces', 'previous', 'layers', 'produce', 'feature', 'maps', 'actually', 'confidence', 'maps', 'target', 'class', 'since', 'kinds', 'features', 'would', 'destroyed', 'averaging', 'step', '.', 'This', 'makes', 'unnecessary', 'several', 'fully', 'connected', 'layers', 'top', 'CNN', 'like', 'AlexNet', 'con…', 'siderably', 'reducing', 'number', 'parameters', 'network', 'limiting', 'risk', 'overfitting.‹The', 'last', 'layers', 'self-explanatory', 'dropout', 'regularization', 'fully', 'con…', 'nected', 'layer', 'softmax', 'activation', 'function', 'output', 'estimated', 'class', 'proba…', 'bilities.This', 'diagram', 'slightly', 'simplified', 'original', 'GoogLeNet', 'architecture', 'also', 'included', 'two', 'auxiliary', 'classifiers', 'plugged', 'top', 'third', 'sixth', 'inception', 'modules', '.', 'They', 'composed', 'one', 'average', 'pooling', 'layer', 'one', 'convolutional', 'layer', 'two', 'fully', 'connected', 'layers', 'softmax', 'activation', 'layer', '.', 'During', 'training', 'loss', 'scaled', '70', '%', 'added', 'overall', 'loss', '.', 'The', 'goal', 'fight', 'vanish…', 'ing', 'gradients', 'problem', 'regularize', 'network', '.', 'However', 'shown', 'effect', 'relatively', 'minor', '.', 'ResNetLast', 'least', 'winner', 'ILSVRC', '2015', 'challenge', 'Residual', 'Network', 'ResNet', 'developed', 'Kaiming', 'He', 'et', 'al.', '12', 'delivered', 'astounding', 'top-5error', 'rate', '3.6', '%', 'using', 'extremely', 'deep', 'CNN', 'composed', '152', 'layers', '.', 'The', 'key', 'able', 'train', 'deep', 'network', 'use', 'skip', 'connections', 'also', 'calledshortcut', 'connections', 'signal', 'feeding', 'layer', 'also', 'added', 'output', 'layer', 'located', 'bit', 'higher', 'stack', '.', 'Let‡s', 'see', 'useful', '.', 'When', 'training', 'neural', 'network', 'goal', 'make', 'model', 'target', 'function', 'h', 'x', '.If', 'add', 'input', 'x', 'output', 'network', 'i.e.', 'add', 'skip', 'connection', 'network', 'forced', 'model', 'f', 'x', '=', 'h', 'x', '–', 'x', 'rather', 'h', 'x', '.', 'This', 'iscalled', 'residual', 'learning', 'see', 'Figure', '13-12', '.372', '|', 'Chapter', '13', 'Convolutional', 'Neural', 'Networks', 'Figure', '13-12', '.', 'Residual', 'learning', 'When', 'initialize', 'regular', 'neural', 'network', 'weights', 'close', 'zero', 'net…', 'work', 'outputs', 'values', 'close', 'zero', '.', 'If', 'add', 'skip', 'connection', 'resulting', 'net…', 'work', 'outputs', 'copy', 'inputs', 'words', 'initially', 'models', 'identity', 'function', '.', 'If', 'target', 'function', 'fairly', 'close', 'identity', 'function', 'often', 'case', 'speed', 'training', 'considerably', '.', 'Moreover', 'add', 'many', 'skip', 'connections', 'network', 'start', 'making', 'progress', 'even', 'several', 'layers', 'started', 'learning', 'yet', 'see', 'Figure', '13-13', '.', 'Thanks', 'skipconnections', 'signal', 'easily', 'make', 'way', 'across', 'whole', 'network', '.', 'The', 'deep', 'residual', 'network', 'seen', 'stack', 'residual', 'units', 'residual', 'unit', 'asmall', 'neural', 'network', 'skip', 'connection.Figure', '13-13', '.', 'Regular', 'deep', 'neural', 'network', 'le', '“', 'deep', 'residual', 'network', 'right', 'CNN', 'Architectures', '|', '373', 'Now', 'let‡s', 'look', 'ResNet‡s', 'architecture', 'see', 'Figure', '13-14', '.', 'It', 'actually', 'surprisingly', 'simple', '.', 'It', 'starts', 'ends', 'exactly', 'like', 'GoogLeNet', 'except', 'without', 'dropout', 'layer', 'deep', 'stack', 'simple', 'residual', 'units', '.', 'Each', 'residual', 'unit', 'composed', 'two', 'convolutional', 'layers', 'Batch', 'Normalization', 'BN', 'ReLU', 'activation', 'using', '3', '‰', '3', 'kernels', 'preserving', 'spatial', 'dimensions', 'stride', '1', 'SAME', 'padding', '.Figure', '13-14', '.', 'ResNet', 'architecture', 'Note', 'number', 'feature', 'maps', 'doubled', 'every', 'residual', 'units', 'time', 'height', 'width', 'halved', 'using', 'convolutional', 'layer', 'stride', '2', '.', 'When', 'happens', 'inputs', 'added', 'directly', 'outputs', 'residual', 'unit', 'since', 'don‡t', 'shape', 'example', 'problem', 'affects', 'skip', 'connection', 'represented', 'dashed', 'arrow', 'Figure', '13-14', '.', 'To', 'solve', 'problem', 'inputs', 'passed', '1', '‰', '1', 'convolutional', 'layer', 'stride', '2', 'right', 'number', 'output', 'feature', 'maps', 'see', 'Figure', '13-15', '.Figure', '13-15', '.', 'Skip', 'connection', 'changing', 'feature', 'map', 'size', 'depth', '374', '|', 'Chapter', '13', 'Convolutional', 'Neural', 'Networks', '13ƒVery', 'Deep', 'Convolutional', 'Networks', 'Large-Scale', 'Image', 'Recognition', '⁄', 'K.', 'Simonyan', 'A.', 'Zisserman', '2015', '.14ƒInception-v4', 'Inception-ResNet', 'Impact', 'Residual', 'Connections', 'Learning', '⁄', 'C.', 'Szegedy', 'et', 'al', '.', '2016', '.ResNet-34', 'ResNet', '34', 'layers', 'counting', 'convolutional', 'layers', 'fully', 'connected', 'layer', 'containing', 'three', 'residual', 'units', 'output', '64', 'feature', 'maps', '4', 'RUs', '128', 'maps', '6', 'RUs', '256', 'maps', '3', 'RUs', '512', 'maps', '.', 'ResNets', 'deeper', 'ResNet-152', 'use', 'slightly', 'different', 'residual', 'units', '.', 'Instead', 'two', '3', '‰', '3', 'convolutional', 'layers', 'say', '256', 'feature', 'maps', 'use', 'three', 'convolutional', 'layers', 'first', '1', '‰', '1', 'convolutional', 'layer', '64', 'feature', 'maps', '4', 'times', 'less', 'acts', 'bottleneck', 'layer', 'discussed', 'already', '3', '‰', '3', 'layer', '64', 'feature', 'maps', 'finally', 'another', '1', '‰', '1', 'convolutional', 'layer', '256', 'feature', 'maps', '4', 'times', '64', 'restores', 'original', 'depth', '.', 'ResNet-152', 'contains', 'three', 'RUs', 'output', '256', 'maps', '8', 'RUs', '512', 'maps', 'whopping', '36', 'RUs', '1,024', 'maps', 'finally', '3', 'RUs', '2,048', 'maps', '.', 'As', 'see', 'field', 'moving', 'rapidly', 'sorts', 'architectures', 'popping', 'every', 'year', '.', 'One', 'clear', 'trend', 'CNNs', 'keep', 'getting', 'deeper', 'deeper', '.', 'They', 'also', 'getting', 'lighter', 'requiring', 'fewer', 'fewer', 'parameters', '.', 'At', 'present', 'ResNet', 'architecture', 'powerful', 'arguably', 'simplest', 'really', 'one', 'probably', 'use', 'keep', 'looking', 'ILSVRC', 'challenge', 'every', 'year', '.', 'The', '2016', 'winners', 'Trimps-Soushen', 'team', 'China', 'astounding', '2.99', '%', 'error', 'rate', '.', 'To', 'achieve', 'trained', 'combinations', 'previ…', 'ous', 'models', 'joined', 'ensemble', '.', 'Depending', 'task', 'reduced', 'error', 'rate', 'may', 'may', 'worth', 'extra', 'complexity', '.', 'There', 'architectures', 'may', 'want', 'look', 'particular', 'VGGNet', '13', 'runner-up', 'ILSVRC', '2014', 'challenge', 'Inception-v414', 'whichmerges', 'ideas', 'GoogLeNet', 'ResNet', 'achieves', 'close', '3', '%', 'top-5', 'error', 'rate', 'ImageNet', 'classification', '.', 'There', 'really', 'nothing', 'special', 'implementing', 'various', 'CNN', 'architectures', 'discussed', '.', 'We', 'saw', 'earlier', 'build', 'individual', 'building', 'blocks', 'need', 'assem…ble', 'create', 'desired', 'architecture', '.', 'We', 'build', 'ResNet-34', 'upcoming', 'exercises', 'find', 'full', 'working', 'code', 'inthe', 'Jupyter', 'notebooks', '.', 'CNN', 'Architectures', '|', '375', '15This', 'name', 'quite', 'misleading', 'since', 'layer', 'perform', 'deconvolution', 'well-defined', 'mathematical', 'operation', 'inverse', 'convolution', '.', 'TensorFlow', 'Convolution', 'OperationsTensorFlow', 'also', 'offers', 'kinds', 'convolutional', 'layers', '‹conv1d', 'creates', 'convolutional', 'layer', '1D', 'inputs', '.', 'This', 'useful', 'example', 'natural', 'language', 'processing', 'sentence', 'may', 'represented', '1D', 'array', 'words', 'receptive', 'field', 'covers', 'neighboring', 'words', '.', '‹conv3d', 'creates', 'convolutional', 'layer', '3D', 'inputs', '3D', 'PET', 'scan', '.', '‹atrous_conv2d', 'creates', 'atrous', 'convolutional', 'layer', 'ƒÉ', 'trous⁄', 'French', 'ƒwith', 'holes⁄', '.', 'This', 'equivalent', 'using', 'regular', 'convolutional', 'layer', 'fil…', 'ter', 'dilated', 'inserting', 'rows', 'columns', 'zeros', 'i.e.', 'holes', '.', 'For', 'example', '1', '‰', '3', 'filter', 'equal', '1,2,3', 'may', 'dilated', 'dilation', 'rate', '4', 'resulting', 'adilated', '†lter', '1', '0', '0', '0', '2', '0', '0', '0', '3', '.', 'This', 'allows', 'convolutional', 'layer', 'larger', 'receptive', 'field', 'computational', 'price', 'using', 'extra', 'parameters.‹conv2d_transpose', 'creates', 'transpose', 'convolutional', 'layer', 'sometimes', 'called', 'adeconvolutional', 'layer', ',15', 'upsamples', 'image', '.', 'It', 'inserting', 'zeros', 'inputs', 'think', 'regular', 'convolutional', 'layer', 'using', 'fractional', 'stride', '.', 'Upsampling', 'useful', 'example', 'image', 'segmentation', 'typical', 'CNN', 'feature', 'maps', 'get', 'smaller', 'smaller', 'progress', 'network', 'want', 'output', 'image', 'size', 'input', 'need', 'upsampling', 'layer', '.', '‹depthwise_conv2d', 'creates', 'depthwise', 'convolutional', 'layer', 'applies', 'every', 'fil…', 'ter', 'every', 'individual', 'input', 'channel', 'independently', '.', 'Thus', 'fn', 'filtersand', 'fn', 'input', 'channels', 'output', 'fn', '‰', 'fn', 'feature', 'maps', '.', '‹separable_conv2d', 'creates', 'separable', 'convolutional', 'layer', 'first', 'acts', 'like', 'depthwise', 'convolutional', 'layer', 'applies', '1', '‰', '1', 'convolutional', 'layer', 'resulting', 'feature', 'maps', '.', 'This', 'makes', 'possible', 'apply', 'filters', 'arbitrary', 'sets', 'inputs', 'channels', '.', 'Exercises1.What', 'advantages', 'CNN', 'fully', 'connected', 'DNN', 'image', 'classi…', 'fication', '?', '2.Consider', 'CNN', 'composed', 'three', 'convolutional', 'layers', '3', '‰', '3', 'kernels', 'stride', '2', 'SAME', 'padding', '.', 'The', 'lowest', 'layer', 'outputs', '100', 'feature', 'maps', '376', '|', 'Chapter', '13', 'Convolutional', 'Neural', 'Networks', 'middle', 'one', 'outputs', '200', 'top', 'one', 'outputs', '400', '.', 'The', 'input', 'images', 'RGB', 'images', '200', '‰', '300', 'pixels', '.', 'What', 'total', 'number', 'parameters', 'CNN', '?', 'If', 'using', '32-bit', 'floats', 'least', 'much', 'RAM', 'network', 'require', 'making', 'prediction', 'single', 'instance', '?', 'What', 'training', 'mini-batch', '50', 'images', '?', '3.If', 'GPU', 'runs', 'memory', 'training', 'CNN', 'five', 'things', 'could', 'try', 'solve', 'problem', '?', '4.Why', 'would', 'want', 'add', 'max', 'pooling', 'layer', 'rather', 'convolutional', 'layer', 'stride', '?', '5.When', 'would', 'want', 'add', 'local', 'response', 'normalization', 'layer', '?', '6.Can', 'name', 'main', 'innovations', 'AlexNet', 'compared', 'LeNet-5', '?', 'What', 'main', 'innovations', 'GoogLeNet', 'ResNet', '?', '7.Build', 'CNN', 'try', 'achieve', 'highest', 'possible', 'accuracy', 'MNIST', '.', '8.Classifying', 'large', 'images', 'using', 'Inception', 'v3', '.', 'a.Download', 'images', 'various', 'animals', '.', 'Load', 'Python', 'example', 'using', 'matplotlib.image.mpimg.imread', 'function', '.', 'Resize', 'and/or', 'cropthem', '299', '‰', '299', 'pixels', 'ensure', 'three', 'channels', 'RGB', 'transparency', 'channel.b', '.', 'Download', 'latest', 'pretrained', 'Inception', 'v3', 'model', 'checkpoint', 'avail…', 'able', 'https', '//goo.gl/nxSQvl', '.c.Create', 'Inception', 'v3', 'model', 'calling', 'inception_v3', 'function', 'asshown', '.', 'This', 'must', 'done', 'within', 'argument', 'scope', 'created', 'inception_v3_arg_scope', 'function', '.', 'Also', 'must', 'set', 'is_training=Falseand', 'num_classes=1001', 'like', 'tensorflow.contrib.slim.nets', 'import', 'inceptionimport', 'tensorflow.contrib.slim', 'slimX', '=', 'tf.placeholder', 'tf.float32', 'shape=', 'None', '299', '299', '3', 'slim.arg_scope', 'inception.inception_v3_arg_scope', 'logits', 'end_points', '=', 'inception.inception_v3', 'X', 'num_classes=1001', 'is_training=False', 'predictions', '=', 'end_points', '``', 'Predictions', \"''\", 'saver', '=', 'tf.train.Saver', 'd.Open', 'session', 'use', 'Saver', 'restore', 'pretrained', 'model', 'checkpoint', 'downloaded', 'earlier', '.', 'e.Run', 'model', 'classify', 'images', 'prepared', '.', 'Display', 'top', 'five', 'pre…', 'dictions', 'image', 'along', 'estimated', 'probability', 'list', 'class', 'names', 'available', 'https', '//goo.gl/brXRtZ', '.', 'How', 'accurate', 'model', '?', '9.Transfer', 'learning', 'large', 'image', 'classification', '.', 'Exercises', '|', '377', 'a.Create', 'training', 'set', 'containing', 'least', '100', 'images', 'per', 'class', '.', 'For', 'example', 'could', 'classify', 'pictures', 'based', 'location', 'beach', 'mountain', 'city', 'etc', '.', 'alternatively', 'use', 'existing', 'dataset', 'flowersdataset', 'MIT‡s', 'places', 'dataset', 'requires', 'registration', 'huge', '.', 'b', '.', 'Write', 'preprocessing', 'step', 'resize', 'crop', 'image', '299', '‰', '299', 'randomness', 'data', 'augmentation', '.', 'c.Using', 'pretrained', 'Inception', 'v3', 'model', 'previous', 'exercise', 'freeze', 'layers', 'bottleneck', 'layer', 'i.e.', 'last', 'layer', 'output', 'layer', 'replace', 'output', 'layer', 'appropriate', 'number', 'outputs', 'new', 'classification', 'task', 'e.g.', 'flowers', 'dataset', 'five', 'mutually', 'exclusive', 'classes', 'output', 'layer', 'must', 'five', 'neurons', 'use', 'softmax', 'activa…', 'tion', 'function', '.d.Split', 'dataset', 'training', 'set', 'test', 'set', '.', 'Train', 'model', 'training', 'set', 'evaluate', 'test', 'set', '.', '10.Go', 'TensorFlow‡s', 'DeepDream', 'tutorial', '.', 'It', 'fun', 'way', 'familiarize', 'your…', 'self', 'various', 'ways', 'visualizing', 'patterns', 'learned', 'CNN', 'gener…', 'ate', 'art', 'using', 'Deep', 'Learning', '.', 'Solutions', 'exercises', 'available', 'Appendix', 'A', '.378', '|', 'Chapter', '13', 'Convolutional', 'Neural', 'Networks', 'CHAPTER', '14Recurrent', 'Neural', 'NetworksThe', 'batter', 'hits', 'ball', '.', 'You', 'immediately', 'start', 'running', 'anticipating', 'ball‡s', 'trajec…', 'tory', '.', 'You', 'track', 'adapt', 'movements', 'finally', 'catch', 'thunder', 'applause', '.', 'Predicting', 'future', 'time', 'whether', 'finishing', 'friend‡s', 'sentence', 'anticipating', 'smell', 'coffee', 'breakfast', '.', 'In', 'chapter', 'going', 'discuss', 'recurrent', 'neural', 'networks', 'RNN', 'class', 'nets', 'predict', 'future', 'well', 'point', 'course', '.', 'They', 'analyze', 'time', 'series', 'data', 'stock', 'prices', 'tell', 'buy', 'sell', '.', 'In', 'autonomous', 'driving', 'systems', 'anticipate', 'car', 'trajectories', 'help', 'avoid', 'accidents', '.', 'More', 'generally', 'work', 'sequences', 'arbitrary', 'lengths', 'rather', 'fixed-sized', 'inputs', 'like', 'nets', 'discussed', 'far', '.', 'For', 'example', 'take', 'sentences', 'documents', 'audio', 'samples', 'input', 'making', 'extremely', 'useful', 'natural', 'language', 'processing', 'NLP', 'systems', 'automatic', 'translation', 'speech-to-text', 'sentiment', 'analysis', 'e.g.', 'reading', 'movie', 'reviews', 'extracting', 'rater‡s', 'feeling', 'movie', '.', 'Moreover', 'RNNs‡', 'ability', 'anticipate', 'also', 'makes', 'capable', 'surprising', 'creativ…', 'ity', '.', 'You', 'ask', 'predict', 'likely', 'next', 'notes', 'melody', 'randomly', 'pick', 'one', 'notes', 'play', '.', 'Then', 'ask', 'net', 'next', 'likely', 'notes', 'play', 'repeat', 'process', '.', 'Before', 'know', 'net', 'compose', 'melody', 'one', 'produced', 'Google‡s', 'Magenta', 'project', '.', 'Simi…larly', 'RNNs', 'generate', 'sentences', 'image', 'captions', 'much', '.', 'The', 'result', 'exactly', 'Shakespeare', 'Mozart', 'yet', 'knows', 'produce', 'years', '?', 'In', 'chapter', 'look', 'fundamental', 'concepts', 'underlying', 'RNNs', 'main', 'problem', 'face', 'namely', 'vanishing/exploding', 'gradients', 'discussed', 'Chapter', '11', 'solutions', 'widely', 'used', 'fight', 'LSTM', 'GRU', 'cells', '.', 'Along', 'way', 'always', 'show', 'implement', 'RNNs', 'using', 'TensorFlow', '.', 'Finally', 'take', 'look', 'architecture', 'machine', 'translation', 'system', '.', '379Recurrent', 'NeuronsUp', 'mostly', 'looked', 'feedforward', 'neural', 'networks', 'activa…', 'tions', 'flow', 'one', 'direction', 'input', 'layer', 'output', 'layer', 'except', 'networks', 'Appendix', 'E', '.', 'A', 'recurrent', 'neural', 'network', 'looks', 'much', 'like', 'feedforward', 'neural', 'network', 'except', 'also', 'connections', 'pointing', 'backward', '.', 'Let‡s', 'look', 'simplest', 'possible', 'RNN', 'composed', 'one', 'neuron', 'receiving', 'inputs', 'producing', 'output', 'sending', 'output', 'back', 'shown', 'Figure', '14-1', 'left', '.', 'At', 'time', 'step', 'also', 'called', 'frame', 'recurrent', 'neuron', 'receives', 'inputs', 'x', 'well', 'output', 'previous', 'time', 'step', 't–1', '.', 'We', 'represent', 'tiny', 'network', 'time', 'axis', 'shown', 'Figure', '14-1', 'right', '.', 'This', 'called', 'unrolling', 'network', 'time', '.Figure', '14-1', '.', 'A', 'recurrent', 'neuron', 'le', '“', 'unrolled', 'time', 'right', 'You', 'easily', 'create', 'layer', 'recurrent', 'neurons', '.', 'At', 'time', 'step', 'every', 'neuron', 'receives', 'input', 'vector', 'x', 'output', 'vector', 'previous', 'time', 'stepy', 't–1', 'shown', 'Figure', '14-2', '.', 'Note', 'inputs', 'outputs', 'vectors', 'single', 'neuron', 'output', 'scalar', '.Figure', '14-2', '.', 'A', 'layer', 'recurrent', 'neurons', 'le', '“', 'unrolled', 'time', 'right', 'Each', 'recurrent', 'neuron', 'two', 'sets', 'weights', 'one', 'inputs', 'x', 'forthe', 'outputs', 'previous', 'time', 'step', 't–1', '.', 'Let‡s', 'call', 'weight', 'vectors', 'wx', 'wy.380', '|', 'Chapter', '14', 'Recurrent', 'Neural', 'Networks', '1Note', 'many', 'researchers', 'prefer', 'use', 'hyperbolic', 'tangent', 'tanh', 'activation', 'function', 'RNNs', 'rather', 'ReLU', 'activation', 'function', '.', 'For', 'example', 'take', 'look', 'Vu', 'Pham', 'et', 'al.‡s', 'paper', 'ƒDropout', 'Improves', 'Recurrent', 'Neural', 'Networks', 'Handwriting', 'Recognition⁄', '.', 'However', 'ReLU-based', 'RNNs', 'also', 'possible', 'shown', 'Quoc', 'V.', 'Le', 'et', 'al.‡s', 'paper', 'ƒA', 'Simple', 'Way', 'Initialize', 'Recurrent', 'Networks', 'Rectified', 'Linear', 'Units⁄', '.The', 'output', 'single', 'recurrent', 'neuron', 'computed', 'pretty', 'much', 'might', 'expect', 'shown', 'Equation', '14-1', 'b', 'bias', 'term', 'Ì', '’', 'activation', 'func…', 'tion', 'e.g.', 'ReLU', '1', '.Equation', '14-1', '.', 'Output', 'single', 'recurrent', 'neuron', 'single', 'instance', 't=‚tT', '’', 'x+t', '”', '1', 'T', '’', 'y+bJust', 'like', 'feedforward', 'neural', 'networks', 'compute', 'whole', 'layer‡s', 'output', 'one', 'shot', 'whole', 'mini-batch', 'using', 'vectorized', 'form', 'previous', 'equation', 'see', 'Equation', '14-2', '.Equation', '14-2', '.', 'Outputs', 'layer', 'recurrent', 'neurons', 'instances', 'mini-', 'batch', 't=‚t', '’', 'x+t', '”', '1', '’', 'y+=‚tt', '”', '1', '’', '+with=xy‹Y', '‰', 'nneurons', 'matrix', 'containing', 'layer‡s', 'outputs', 'time', 'step', 'eachinstance', 'mini-batch', 'number', 'instances', 'mini-batch', 'nneurons', 'number', 'neurons', '.', '‹X', '‰', 'ninputs', 'matrix', 'containing', 'inputs', 'instances', 'ninputs', 'thenumber', 'input', 'features', '.', '‹Wx', 'ninputs', '‰', 'nneurons', 'matrix', 'containing', 'connection', 'weights', 'inputs', 'current', 'time', 'step', '.', '‹Wy', 'nneurons', '‰', 'nneurons', 'matrix', 'containing', 'connection', 'weights', 'out…', 'puts', 'previous', 'time', 'step', '.', '‹The', 'weight', 'matrices', 'Wx', 'Wy', 'often', 'concatenated', 'single', 'weight', 'matrix', 'W', 'shape', 'ninputs', '+', 'nneurons', '‰', 'nneurons', 'see', 'second', 'line', 'Equation', '14-2', '.‹b', 'vector', 'size', 'nneurons', 'containing', 'neuron‡s', 'bias', 'term', '.', 'Recurrent', 'Neurons', '|', '381', 'Notice', 'Y', 'function', 'X', 'Y', 't–1', 'function', 'X', 't–1', 'Y', 't–2', 'function', 'X', 't–2', 'Y', 't–3', '.', 'This', 'makes', 'Y', 'function', 'theinputs', 'since', 'time', '=', '0', 'X', '0', 'X', '1', 'µ', 'X', '.', 'At', 'first', 'time', 'step', '=', '0', 'areno', 'previous', 'outputs', 'typically', 'assumed', 'zeros.Memory', 'CellsSince', 'output', 'recurrent', 'neuron', 'time', 'step', 'function', 'inputs', 'previous', 'time', 'steps', 'could', 'say', 'form', 'memory', '.', 'A', 'part', 'neuralnetwork', 'preserves', 'state', 'across', 'time', 'steps', 'called', 'memory', 'cell', 'simply', 'cell', '.', 'A', 'single', 'recurrent', 'neuron', 'layer', 'recurrent', 'neurons', 'basic', 'cell', 'later', 'chapter', 'look', 'complex', 'powerful', 'types', 'cells.In', 'general', 'cell‡s', 'state', 'time', 'step', 'denoted', 'h', 'ƒh⁄', 'stands', 'ƒhidden⁄', 'function', 'inputs', 'time', 'step', 'state', 'previous', 'time', 'step', 'h', '=f', 'h', 't–1', 'x', '.', 'Its', 'output', 'time', 'step', 'denoted', 'also', 'function', 'previousstate', 'current', 'inputs', '.', 'In', 'case', 'basic', 'cells', 'discussed', 'far', 'output', 'simply', 'equal', 'state', 'complex', 'cells', 'always', 'case', 'shown', 'Figure', '14-3.Figure', '14-3', '.', 'A', 'cell‹s', 'hidden', 'state', 'output', 'may', 'di›erentInput', 'Output', 'SequencesAn', 'RNN', 'simultaneously', 'take', 'sequence', 'inputs', 'produce', 'sequence', 'outputs', 'see', 'Figure', '14-4', 'top-left', 'network', '.', 'For', 'example', 'type', 'network', 'use…', 'ful', 'predicting', 'time', 'series', 'stock', 'prices', 'feed', 'prices', 'last', 'Ndays', 'must', 'output', 'prices', 'shifted', 'one', 'day', 'future', 'i.e.', 'N', '–', '1', 'days', 'ago', 'tomorrow', '.', 'Alternatively', 'could', 'feed', 'network', 'sequence', 'inputs', 'ignore', 'outputs', 'except', 'last', 'one', 'see', 'top-right', 'network', '.', 'In', 'words', 'sequence-', 'to-vector', 'network', '.', 'For', 'example', 'could', 'feed', 'network', 'sequence', 'words', 'cor…', '382', '|', 'Chapter', '14', 'Recurrent', 'Neural', 'Networks', 'responding', 'movie', 'review', 'network', 'would', 'output', 'sentiment', 'score', 'e.g.', '–1', 'hate', '+1', 'love', '.', 'Conversely', 'could', 'feed', 'network', 'single', 'input', 'first', 'time', 'step', 'zeros', 'time', 'steps', 'let', 'output', 'sequence', 'see', 'bottom-left', 'network', '.This', 'vector-to-sequence', 'network', '.', 'For', 'example', 'input', 'could', 'image', 'output', 'could', 'caption', 'image', '.', 'Lastly', 'could', 'sequence-to-vector', 'network', 'called', 'encoder', 'followed', 'avector-to-sequence', 'network', 'called', 'decoder', 'see', 'bottom-right', 'network', '.', 'For', 'example', 'used', 'translating', 'sentence', 'one', 'language', 'another', '.', 'You', 'would', 'feed', 'network', 'sentence', 'one', 'language', 'encoder', 'would', 'convert', 'sentence', 'single', 'vector', 'representation', 'decoder', 'would', 'decode', 'vector', 'sentence', 'another', 'language', '.', 'This', 'two-step', 'model', 'called', 'Encoder–Decoder', 'works', 'much', 'better', 'trying', 'translate', 'fly', 'single', 'sequence-to-sequence', 'RNN', 'like', 'one', 'represented', 'top', 'left', 'since', 'last', 'words', 'sentence', 'affect', 'first', 'words', 'translation', 'need', 'wait', 'heard', 'whole', 'sentence', 'translating', '.', 'Figure', '14-4', '.', 'Seq', 'seq', 'top', 'le', '“', 'seq', 'vector', 'top', 'right', 'vector', 'seq', 'bottom', 'le', '“', 'delayed', 'seq', 'seq', 'bottom', 'right', 'Sounds', 'promising', 'let‡s', 'start', 'coding', '!', 'Recurrent', 'Neurons', '|', '383', 'Basic', 'RNNs', 'TensorFlowFirst', 'let‡s', 'implement', 'simple', 'RNN', 'model', 'without', 'using', 'TensorFlow‡s', 'RNN', 'operations', 'better', 'understand', 'goes', 'hood', '.', 'We', 'create', 'RNN', 'composed', 'layer', 'five', 'recurrent', 'neurons', 'like', 'RNN', 'represented', 'Figure', '14-2', 'using', 'tanh', 'activation', 'function', '.', 'We', 'assume', 'RNN', 'runs', 'two', 'time', 'steps', 'taking', 'input', 'vectors', 'size', '3', 'time', 'step', '.', 'The', 'follow…', 'ing', 'code', 'builds', 'RNN', 'unrolled', 'two', 'time', 'steps', 'n_inputs', '=', '3n_neurons', '=', '5X0', '=', 'tf.placeholder', 'tf.float32', 'None', 'n_inputs', 'X1', '=', 'tf.placeholder', 'tf.float32', 'None', 'n_inputs', 'Wx', '=', 'tf.Variable', 'tf.random_normal', 'shape=', 'n_inputs', 'n_neurons', 'dtype=tf.float32', 'Wy', '=', 'tf.Variable', 'tf.random_normal', 'shape=', 'n_neurons', 'n_neurons', 'dtype=tf.float32', 'b', '=', 'tf.Variable', 'tf.zeros', '1', 'n_neurons', 'dtype=tf.float32', 'Y0', '=', 'tf.tanh', 'tf.matmul', 'X0', 'Wx', '+', 'b', 'Y1', '=', 'tf.tanh', 'tf.matmul', 'Y0', 'Wy', '+', 'tf.matmul', 'X1', 'Wx', '+', 'b', 'init', '=', 'tf.global_variables_initializer', 'This', 'network', 'looks', 'much', 'like', 'two-layer', 'feedforward', 'neural', 'network', 'twists', 'first', 'weights', 'bias', 'terms', 'shared', 'layers', 'second', 'feed', 'inputs', 'layer', 'get', 'outputs', 'layer', '.', 'To', 'run', 'model', 'need', 'feed', 'inputs', 'time', 'steps', 'like', 'import', 'numpy', 'np', '#', 'Mini-batch', 'instance', '0', 'instance', '1', 'instance', '2', 'instance', '3X0_batch', '=', 'np.array', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', '1', '#', '=', '0X1_batch', '=', 'np.array', '9', '8', '7', '0', '0', '0', '6', '5', '4', '3', '2', '1', '#', '=', '1with', 'tf.Session', 'sess', 'init.run', 'Y0_val', 'Y1_val', '=', 'sess.run', 'Y0', 'Y1', 'feed_dict=', '{', 'X0', 'X0_batch', 'X1', 'X1_batch', '}', 'This', 'mini-batch', 'contains', 'four', 'instances', 'input', 'sequence', 'composed', 'exactly', 'two', 'inputs', '.', 'At', 'end', 'Y0_val', 'Y1_val', 'contain', 'outputs', 'network', 'time', 'steps', 'neurons', 'instances', 'mini-batch', '>', '>', '>', 'print', 'Y0_val', '#', 'output', '=', '0', '-0.2964572', '0.82874775', '-0.34216955', '-0.75720584', '0.19011548', '#', 'instance', '0', '-0.12842922', '0.99981797', '0.84704727', '-0.99570125', '0.38665548', '#', 'instance', '1', '0.04731077', '0.99999976', '0.99330056', '-0.999933', '0.55339795', '#', 'instance', '2', '0.70323634', '0.99309105', '0.99909431', '-0.85363263', '0.7472108', '#', 'instance', '3', '>', '>', '>', 'print', 'Y1_val', '#', 'output', '=', '1', '0.51955646', '1', '.', '0.99999022', '-0.99984968', '-0.24616946', '#', 'instance', '0', '-0.70553327', '-0.11918639', '0.48885304', '0.08917919', '-0.26579669', '#', 'instance', '1384', '|', 'Chapter', '14', 'Recurrent', 'Neural', 'Networks', '-0.32477224', '0.99996376', '0.99933046', '-0.99711186', '0.10981458', '#', 'instance', '2', '-0.43738723', '0.91517633', '0.97817528', '-0.91763324', '0.11047263', '#', 'instance', '3That', 'wasn‡t', 'hard', 'course', 'want', 'able', 'run', 'RNN', '100', 'time', 'steps', 'graph', 'going', 'pretty', 'big', '.', 'Now', 'let‡s', 'look', 'create', 'model', 'using', 'TensorFlow‡s', 'RNN', 'operations', '.', 'Static', 'Unrolling', 'Through', 'Time', 'The', 'static_rnn', 'function', 'creates', 'unrolled', 'RNN', 'network', 'chaining', 'cells', '.', 'The', 'following', 'code', 'creates', 'exact', 'model', 'previous', 'one', 'X0', '=', 'tf.placeholder', 'tf.float32', 'None', 'n_inputs', 'X1', '=', 'tf.placeholder', 'tf.float32', 'None', 'n_inputs', 'basic_cell', '=', 'tf.contrib.rnn.BasicRNNCell', 'num_units=n_neurons', 'output_seqs', 'states', '=', 'tf.contrib.rnn.static_rnn', 'basic_cell', 'X0', 'X1', 'dtype=tf.float32', 'Y0', 'Y1', '=', 'output_seqsFirst', 'create', 'input', 'placeholders', '.', 'Then', 'create', 'BasicRNNCell', 'think', 'factory', 'creates', 'copies', 'cell', 'build', 'unrolled', 'RNN', 'one', 'time', 'step', '.', 'Then', 'call', 'static_rnn', 'giving', 'cell', 'factory', 'input', 'tensors', 'telling', 'data', 'type', 'inputs', 'used', 'create', 'initial', 'state', 'matrix', 'default', 'full', 'zeros', '.', 'The', 'static_rnn', 'functioncalls', 'cell', 'factory‡s', '__call__', 'function', 'per', 'input', 'creating', 'two', 'copies', 'cell', 'containing', 'layer', 'five', 'recurrent', 'neurons', 'shared', 'weights', 'bias', 'terms', 'chains', 'like', 'earlier', '.', 'The', 'static_rnn', 'function', 'returns', 'two', 'objects', '.', 'The', 'first', 'Python', 'list', 'containing', 'output', 'tensors', 'time', 'step', '.', 'The', 'second', 'tensor', 'containing', 'final', 'states', 'network', '.', 'When', 'using', 'basic', 'cells', 'final', 'state', 'simply', 'equal', 'last', 'output', '.', 'If', '50', 'time', 'steps', 'would', 'convenient', 'define', '50', 'input', 'placeholders', '50', 'output', 'tensors', '.', 'Moreover', 'execution', 'time', 'would', 'feed', '50', 'placeholders', 'manipulate', '50', 'outputs', '.', 'Let‡s', 'simplify', '.', 'The', 'following', 'code', 'builds', 'RNN', 'time', 'takes', 'single', 'input', 'placeholder', 'shape', 'None', 'n_steps', 'n_inputs', 'first', 'dimension', 'themini-batch', 'size', '.', 'Then', 'extracts', 'list', 'input', 'sequences', 'time', 'step', '.', 'X_seqsis', 'Python', 'list', 'n_steps', 'tensors', 'shape', 'None', 'n_inputs', 'thefirst', 'dimension', 'mini-batch', 'size', '.', 'To', 'first', 'swap', 'first', 'two', 'dimen…', 'sions', 'using', 'transpose', 'function', 'time', 'steps', 'first', 'dimen…', 'sion', '.', 'Then', 'extract', 'Python', 'list', 'tensors', 'along', 'first', 'dimension', 'i.e.', 'onetensor', 'per', 'time', 'step', 'using', 'unstack', 'function', '.', 'The', 'next', 'two', 'lines', 'sameas', '.', 'Finally', 'merge', 'output', 'tensors', 'single', 'tensor', 'using', 'stack', 'function', 'swap', 'first', 'two', 'dimensions', 'get', 'final', 'outputs', 'tensor', 'Basic', 'RNNs', 'TensorFlow', '|', '385', 'shape', 'None', 'n_steps', 'n_neurons', 'first', 'dimension', 'mini-batch', 'size', '.X', '=', 'tf.placeholder', 'tf.float32', 'None', 'n_steps', 'n_inputs', 'X_seqs', '=', 'tf.unstack', 'tf.transpose', 'X', 'perm=', '1', '0', '2', 'basic_cell', '=', 'tf.contrib.rnn.BasicRNNCell', 'num_units=n_neurons', 'output_seqs', 'states', '=', 'tf.contrib.rnn.static_rnn', 'basic_cell', 'X_seqs', 'dtype=tf.float32', 'outputs', '=', 'tf.transpose', 'tf.stack', 'output_seqs', 'perm=', '1', '0', '2', 'Now', 'run', 'network', 'feeding', 'single', 'tensor', 'contains', 'mini-', 'batch', 'sequences', 'X_batch', '=', 'np.array', '#', '=', '0', '=', '1', '0', '1', '2', '9', '8', '7', '#', 'instance', '0', '3', '4', '5', '0', '0', '0', '#', 'instance', '1', '6', '7', '8', '6', '5', '4', '#', 'instance', '2', '9', '0', '1', '3', '2', '1', '#', 'instance', '3', 'tf.Session', 'sess', 'init.run', 'outputs_val', '=', 'outputs.eval', 'feed_dict=', '{', 'X', 'X_batch', '}', 'And', 'get', 'single', 'outputs_val', 'tensor', 'instances', 'time', 'steps', 'neu…rons', '>', '>', '>', 'print', 'outputs_val', '-0.2964572', '0.82874775', '-0.34216955', '-0.75720584', '0.19011548', '0.51955646', '1', '.', '0.99999022', '-0.99984968', '-0.24616946', '-0.12842922', '0.99981797', '0.84704727', '-0.99570125', '0.38665548', '-0.70553327', '-0.11918639', '0.48885304', '0.08917919', '-0.26579669', '0.04731077', '0.99999976', '0.99330056', '-0.999933', '0.55339795', '-0.32477224', '0.99996376', '0.99933046', '-0.99711186', '0.10981458', '0.70323634', '0.99309105', '0.99909431', '-0.85363263', '0.7472108', '-0.43738723', '0.91517633', '0.97817528', '-0.91763324', '0.11047263', 'However', 'approach', 'still', 'builds', 'graph', 'containing', 'one', 'cell', 'per', 'time', 'step', '.', 'If', '50', 'time', 'steps', 'graph', 'would', 'look', 'pretty', 'ugly', '.', 'It', 'bit', 'like', 'writing', 'program', 'without', 'ever', 'using', 'loops', 'e.g.', 'Y0=f', '0', 'X0', 'Y1=f', 'Y0', 'X1', 'Y2=f', 'Y1', 'X2', '...', 'Y50=f', 'Y49', 'X50', '.', 'With', 'large', 'graph', 'may', 'even', 'get', 'out-of-memory', 'OOM', 'errors', 'backpropagation', 'especially', 'limited', 'memory', 'GPU', 'cards', 'since', 'must', 'store', 'tensor', 'values', 'forward', 'pass', 'use', 'compute', 'gradients', 'reverse', 'pass', '.', 'Fortunately', 'better', 'solution', 'dynamic_rnn', 'function.386', '|', 'Chapter', '14', 'Recurrent', 'Neural', 'Networks', 'Dynamic', 'Unrolling', 'Through', 'Time', 'The', 'dynamic_rnn', 'function', 'uses', 'while_loop', 'operation', 'run', 'cell', 'appropriate', 'number', 'times', 'set', 'swap_memory=True', 'want', 'swap', 'GPU‡s', 'memory', 'CPU‡s', 'memory', 'backpropagation', 'avoid', 'OOM', 'errors', '.', 'Conveniently', 'also', 'accepts', 'single', 'tensor', 'inputs', 'every', 'time', 'step', 'shape', 'None', 'n_steps', 'n_inputs', 'outputs', 'single', 'tensor', 'out…puts', 'every', 'time', 'step', 'shape', 'None', 'n_steps', 'n_neurons', 'need', 'tostack', 'unstack', 'transpose', '.', 'The', 'following', 'code', 'creates', 'RNN', 'earlier', 'using', 'dynamic_rnn', 'function', '.', 'It‡s', 'much', 'nicer', '!', 'X', '=', 'tf.placeholder', 'tf.float32', 'None', 'n_steps', 'n_inputs', 'basic_cell', '=', 'tf.contrib.rnn.BasicRNNCell', 'num_units=n_neurons', 'outputs', 'states', '=', 'tf.nn.dynamic_rnn', 'basic_cell', 'X', 'dtype=tf.float32', 'During', 'backpropagation', 'while_loop', 'operation', 'appropriate', 'magic', 'stores', 'tensor', 'values', 'iteration', 'dur…', 'ing', 'forward', 'pass', 'use', 'compute', 'gradients', 'dur…', 'ing', 'reverse', 'pass.Handling', 'Variable', 'Length', 'Input', 'SequencesSo', 'far', 'used', 'fixed-size', 'input', 'sequences', 'exactly', 'two', 'steps', 'long', '.', 'What', 'input', 'sequences', 'variable', 'lengths', 'e.g.', 'like', 'sentences', '?', 'In', 'case', 'set', 'sequence_length', 'parameter', 'calling', 'dynamic_rnn', 'orstatic_rnn', 'function', 'must', '1D', 'tensor', 'indicating', 'length', 'input', 'sequence', 'instance', '.', 'For', 'example', 'seq_length', '=', 'tf.placeholder', 'tf.int32', 'None', '...', 'outputs', 'states', '=', 'tf.nn.dynamic_rnn', 'basic_cell', 'X', 'dtype=tf.float32', 'sequence_length=seq_length', 'For', 'example', 'suppose', 'second', 'input', 'sequence', 'contains', 'one', 'input', 'instead', 'two', '.', 'It', 'must', 'padded', 'zero', 'vector', 'order', 'fit', 'input', 'tensor', 'X', 'input', 'tensor‡s', 'second', 'dimension', 'size', 'longest', 'sequence›i.e.', '2', '.', 'X_batch', '=', 'np.array', '#', 'step', '0', 'step', '1', '0', '1', '2', '9', '8', '7', '#', 'instance', '0', '3', '4', '5', '0', '0', '0', '#', 'instance', '1', 'padded', 'zero', 'vector', '6', '7', '8', '6', '5', '4', '#', 'instance', '2', '9', '0', '1', '3', '2', '1', '#', 'instance', '3', 'seq_length_batch', '=', 'np.array', '2', '1', '2', '2', 'Basic', 'RNNs', 'TensorFlow', '|', '387', 'Of', 'course', 'need', 'feed', 'values', 'placeholders', 'X', 'seq_length', 'tf.Session', 'sess', 'init.run', 'outputs_val', 'states_val', '=', 'sess.run', 'outputs', 'states', 'feed_dict=', '{', 'X', 'X_batch', 'seq_length', 'seq_length_batch', '}', 'Now', 'RNN', 'outputs', 'zero', 'vectors', 'every', 'time', 'step', 'past', 'input', 'sequence', 'length', 'look', 'second', 'instance‡s', 'output', 'second', 'time', 'step', '>', '>', '>', 'print', 'outputs_val', '-0.2964572', '0.82874775', '-0.34216955', '-0.75720584', '0.19011548', '0.51955646', '1', '.', '0.99999022', '-0.99984968', '-0.24616946', '#', 'final', 'state', '-0.12842922', '0.99981797', '0.84704727', '-0.99570125', '0.38665548', '#', 'final', 'state', '0', '.', '0', '.', '0', '.', '0', '.', '0', '.', '#', 'zero', 'vector', '0.04731077', '0.99999976', '0.99330056', '-0.999933', '0.55339795', '-0.32477224', '0.99996376', '0.99933046', '-0.99711186', '0.10981458', '#', 'final', 'state', '0.70323634', '0.99309105', '0.99909431', '-0.85363263', '0.7472108', '-0.43738723', '0.91517633', '0.97817528', '-0.91763324', '0.11047263', '#', 'final', 'stateMoreover', 'states', 'tensor', 'contains', 'final', 'state', 'cell', 'excluding', 'zero', 'vectors', '>', '>', '>', 'print', 'states_val', '0.51955646', '1', '.', '0.99999022', '-0.99984968', '-0.24616946', '#', '=', '1', '-0.12842922', '0.99981797', '0.84704727', '-0.99570125', '0.38665548', '#', '=', '0', '!', '!', '!', '-0.32477224', '0.99996376', '0.99933046', '-0.99711186', '0.10981458', '#', '=', '1', '-0.43738723', '0.91517633', '0.97817528', '-0.91763324', '0.11047263', '#', '=', '1Handling', 'Variable-Length', 'Output', 'SequencesWhat', 'output', 'sequences', 'variable', 'lengths', 'well', '?', 'If', 'know', 'advance', 'length', 'sequence', 'example', 'know', 'length', 'input', 'sequence', 'set', 'sequence_length', 'parameter', 'asdescribed', '.', 'Unfortunately', 'general', 'possible', 'example', 'length', 'translated', 'sentence', 'generally', 'different', 'length', 'input', 'sen…', 'tence', '.', 'In', 'case', 'common', 'solution', 'define', 'special', 'output', 'called', 'anend-of-sequence', 'token', 'EOS', 'token', '.', 'Any', 'output', 'past', 'EOS', 'ignored', 'discuss', 'later', 'chapter', '.', 'Okay', 'know', 'build', 'RNN', 'network', 'precisely', 'RNN', 'net…', 'work', 'unrolled', 'time', '.', 'But', 'train', '?', '388', '|', 'Chapter', '14', 'Recurrent', 'Neural', 'Networks', 'Training', 'RNNsTo', 'train', 'RNN', 'trick', 'unroll', 'time', 'like', 'simply', 'use', 'regular', 'backpropagation', 'see', 'Figure', '14-5', '.', 'This', 'strategy', 'called', 'backpro…', 'pagation', 'time', 'BPTT', '.', 'Figure', '14-5', '.', 'Backpropagation', 'time', 'Just', 'like', 'regular', 'backpropagation', 'first', 'forward', 'pass', 'unrolled', 'network', 'represented', 'dashed', 'arrows', 'output', 'sequence', 'evaluated', 'using', 'cost', 'function', 'Ctmin', 'tmin+1', ',tmax', 'tmin', 'tmax', 'first', 'last', 'output', 'time', 'steps', 'counting', 'ignored', 'outputs', 'gradients', 'cost', 'function', 'propagated', 'backward', 'unrolled', 'network', 'repre…', 'sented', 'solid', 'arrows', 'finally', 'model', 'parameters', 'updated', 'using', 'gradients', 'computed', 'BPTT', '.', 'Note', 'gradients', 'flow', 'backward', 'outputs', 'used', 'cost', 'function', 'final', 'output', 'example', 'Figure', '14-5', 'cost', 'function', 'computed', 'using', 'last', 'three', 'outputs', 'net…', 'work', 'Y', '2', 'Y', '3', 'Y', '4', 'gradients', 'flow', 'three', 'outputs', 'Y', '0', 'Y', '1', '.', 'Moreover', 'since', 'parameters', 'W', 'b', 'used', 'time', 'step', 'backpropagation', 'right', 'thing', 'sum', 'time', 'steps', '.', 'Training', 'Sequence', 'Classi•erLet‡s', 'train', 'RNN', 'classify', 'MNIST', 'images', '.', 'A', 'convolutional', 'neural', 'network', 'would', 'better', 'suited', 'image', 'classification', 'see', 'Chapter', '13', 'makes', 'simple', 'example', 'already', 'familiar', '.', 'We', 'treat', 'image', 'sequence', '28', 'rows', '28', 'pixels', 'since', 'MNIST', 'image', '28', '‰', '28', 'pixels', '.', 'We', 'use', 'cells', '150', 'recurrent', 'neurons', 'plus', 'fully', 'connected', 'layer', 'containing', '10', 'neurons', 'Training', 'RNNs', '|', '389', 'one', 'per', 'class', 'connected', 'output', 'last', 'time', 'step', 'followed', 'softmax', 'layer', 'see', 'Figure', '14-6', '.Figure', '14-6', '.', 'Sequence', 'classi†erThe', 'construction', 'phase', 'quite', 'straightforward', 'it‡s', 'pretty', 'much', 'MNIST', 'classifier', 'built', 'Chapter', '10', 'except', 'unrolled', 'RNN', 'replaces', 'hidden', 'layers', '.', 'Note', 'fully', 'connected', 'layer', 'connected', 'states', 'tensor', 'contains', 'final', 'state', 'RNN', 'i.e.', '28', 'th', 'output', '.', 'Also', 'note', 'yis', 'placeholder', 'target', 'classes.from', 'tensorflow.contrib.layers', 'import', 'fully_connectedn_steps', '=', '28n_inputs', '=', '28n_neurons', '=', '150n_outputs', '=', '10learning_rate', '=', '0.001X', '=', 'tf.placeholder', 'tf.float32', 'None', 'n_steps', 'n_inputs', '=', 'tf.placeholder', 'tf.int32', 'None', 'basic_cell', '=', 'tf.contrib.rnn.BasicRNNCell', 'num_units=n_neurons', 'outputs', 'states', '=', 'tf.nn.dynamic_rnn', 'basic_cell', 'X', 'dtype=tf.float32', 'logits', '=', 'fully_connected', 'states', 'n_outputs', 'activation_fn=None', 'xentropy', '=', 'tf.nn.sparse_softmax_cross_entropy_with_logits', 'labels=y', 'logits=logits', 'loss', '=', 'tf.reduce_mean', 'xentropy', 'optimizer', '=', 'tf.train.AdamOptimizer', 'learning_rate=learning_rate', 'training_op', '=', 'optimizer.minimize', 'loss', 'correct', '=', 'tf.nn.in_top_k', 'logits', '1', 'accuracy', '=', 'tf.reduce_mean', 'tf.cast', 'correct', 'tf.float32', '390', '|', 'Chapter', '14', 'Recurrent', 'Neural', 'Networks', 'init', '=', 'tf.global_variables_initializer', 'Now', 'let‡s', 'load', 'MNIST', 'data', 'reshape', 'test', 'data', 'batch_size', 'n_steps', 'n_inputs', 'expected', 'network', '.', 'We', 'take', 'care', 'reshaping', 'training', 'data', 'moment', '.', 'tensorflow.examples.tutorials.mnist', 'import', 'input_datamnist', '=', 'input_data.read_data_sets', '``', '/tmp/data/', \"''\", 'X_test', '=', 'mnist.test.images.reshape', '-1', 'n_steps', 'n_inputs', 'y_test', '=', 'mnist.test.labelsNow', 'ready', 'train', 'RNN', '.', 'The', 'execution', 'phase', 'exactly', 'MNIST', 'classifier', 'Chapter', '10', 'except', 'reshape', 'training', 'batch', 'feeding', 'network.n_epochs', '=', '100batch_size', '=', '150with', 'tf.Session', 'sess', 'init.run', 'epoch', 'range', 'n_epochs', 'iteration', 'range', 'mnist.train.num_examples', '//', 'batch_size', 'X_batch', 'y_batch', '=', 'mnist.train.next_batch', 'batch_size', 'X_batch', '=', 'X_batch.reshape', '-1', 'n_steps', 'n_inputs', 'sess.run', 'training_op', 'feed_dict=', '{', 'X', 'X_batch', 'y_batch', '}', 'acc_train', '=', 'accuracy.eval', 'feed_dict=', '{', 'X', 'X_batch', 'y_batch', '}', 'acc_test', '=', 'accuracy.eval', 'feed_dict=', '{', 'X', 'X_test', 'y_test', '}', 'print', 'epoch', '``', 'Train', 'accuracy', \"''\", 'acc_train', '``', 'Test', 'accuracy', \"''\", 'acc_test', 'The', 'output', 'look', 'like', 'this:0', 'Train', 'accuracy', '0.713333', 'Test', 'accuracy', '0.72991', 'Train', 'accuracy', '0.766667', 'Test', 'accuracy', '0.7977', '...', '98', 'Train', 'accuracy', '0.986667', 'Test', 'accuracy', '0.977799', 'Train', 'accuracy', '0.986667', 'Test', 'accuracy', '0.9809We', 'get', '98', '%', 'accuracy›not', 'bad', '!', 'Plus', 'would', 'certainly', 'get', 'better', 'result', 'tuning', 'hyperparameters', 'initializing', 'RNN', 'weights', 'using', 'He', 'initialization', 'training', 'longer', 'adding', 'bit', 'regularization', 'e.g.', 'dropout', '.', 'You', 'specify', 'initializer', 'RNN', 'wrapping', 'construction', 'code', 'variable', 'scope', 'e.g.', 'usevariable_scope', '``', 'rnn', \"''\", 'initializer=variance_scaling_initializer', 'use', 'He', 'initialization', '.', 'Training', 'RNNs', '|', '391', 'Training', 'Predict', 'Time', 'Series', 'Now', 'let‡s', 'take', 'look', 'handle', 'time', 'series', 'stock', 'prices', 'air', 'tempera…', 'ture', 'brain', 'wave', 'patterns', '.', 'In', 'section', 'train', 'RNN', 'predict', 'next', 'value', 'generated', 'time', 'series', '.', 'Each', 'training', 'instance', 'randomly', 'selected', 'sequence', '20', 'consecutive', 'values', 'time', 'series', 'targetsequence', 'input', 'sequence', 'except', 'shifted', 'one', 'time', 'step', 'future', 'see', 'Figure', '14-7', '.Figure', '14-7', '.', 'Time', 'series', 'le', '“', 'training', 'instance', 'series', 'right', 'First', 'let‡s', 'create', 'RNN', '.', 'It', 'contain', '100', 'recurrent', 'neurons', 'unroll', '20', 'time', 'steps', 'since', 'training', 'instance', '20', 'inputs', 'long', '.', 'Each', 'input', 'contain', 'one', 'feature', 'value', 'time', '.', 'The', 'targets', 'also', 'sequences', '20', 'inputs', 'containing', 'single', 'value', '.', 'The', 'code', 'almost', 'earlier', 'n_steps', '=', '20n_inputs', '=', '1n_neurons', '=', '100n_outputs', '=', '1X', '=', 'tf.placeholder', 'tf.float32', 'None', 'n_steps', 'n_inputs', '=', 'tf.placeholder', 'tf.float32', 'None', 'n_steps', 'n_outputs', 'cell', '=', 'tf.contrib.rnn.BasicRNNCell', 'num_units=n_neurons', 'activation=tf.nn.relu', 'outputs', 'states', '=', 'tf.nn.dynamic_rnn', 'cell', 'X', 'dtype=tf.float32', 'In', 'general', 'would', 'one', 'input', 'feature', '.', 'For', 'example', 'trying', 'predict', 'stock', 'prices', 'would', 'likely', 'many', 'input', 'features', 'time', 'step', 'pri…', 'ces', 'competing', 'stocks', 'ratings', 'analysts', 'feature', 'might', 'help', 'system', 'make', 'predictions', '.', 'At', 'time', 'step', 'output', 'vector', 'size', '100', '.', 'But', 'actually', 'want', 'single', 'output', 'value', 'time', 'step', '.', 'The', 'simplest', 'solution', 'wrap', 'cell', 'OutputProjectionWrapper', '.', 'A', 'cell', 'wrapper', 'acts', 'like', 'normal', 'cell', 'proxying', '392', '|', 'Chapter', '14', 'Recurrent', 'Neural', 'Networks', 'every', 'method', 'call', 'underlying', 'cell', 'also', 'adds', 'functionality', '.', 'The', 'OutputProjectionWrapper', 'adds', 'fully', 'connected', 'layer', 'linear', 'neurons', 'i.e.', 'without', 'activation', 'function', 'top', 'output', 'affect', 'cell', 'state', '.', 'All', 'fully', 'connected', 'layers', 'share', 'trainable', 'weights', 'bias', 'terms', '.', 'The', 'resulting', 'RNN', 'represented', 'Figure', '14-8.Figure', '14-8', '.', 'RNN', 'cells', 'using', 'output', 'projections', 'Wrapping', 'cell', 'quite', 'easy', '.', 'Let‡s', 'tweak', 'preceding', 'code', 'wrapping', 'BasicRNNCell', 'OutputProjectionWrapper', 'cell', '=', 'tf.contrib.rnn.OutputProjectionWrapper', 'tf.contrib.rnn.BasicRNNCell', 'num_units=n_neurons', 'activation=tf.nn.relu', 'output_size=n_outputs', 'So', 'far', 'good', '.', 'Now', 'need', 'define', 'cost', 'function', '.', 'We', 'use', 'Mean', 'Squared', 'Error', 'MSE', 'previous', 'regression', 'tasks', '.', 'Next', 'create', 'Adam', 'optimizer', 'training', 'op', 'variable', 'initialization', 'op', 'usual', 'learning_rate', '=', '0.001loss', '=', 'tf.reduce_mean', 'tf.square', 'outputs', '-', 'optimizer', '=', 'tf.train.AdamOptimizer', 'learning_rate=learning_rate', 'training_op', '=', 'optimizer.minimize', 'loss', 'init', '=', 'tf.global_variables_initializer', 'Now', 'execution', 'phase', 'n_iterations', '=', '10000batch_size', '=', '50with', 'tf.Session', 'sess', 'Training', 'RNNs', '|', '393', 'init.run', 'iteration', 'range', 'n_iterations', 'X_batch', 'y_batch', '=', '...', '#', 'fetch', 'next', 'training', 'batch', 'sess.run', 'training_op', 'feed_dict=', '{', 'X', 'X_batch', 'y_batch', '}', 'iteration', '%', '100', '==', '0', 'mse', '=', 'loss.eval', 'feed_dict=', '{', 'X', 'X_batch', 'y_batch', '}', 'print', 'iteration', '``', '\\\\tMSE', \"''\", 'mse', 'The', 'program‡s', 'output', 'look', 'like', '0', 'MSE', '379.586100', 'MSE', '14.58426200', 'MSE', '7.14066300', 'MSE', '3.98528400', 'MSE', '2.00254', '...', 'Once', 'model', 'trained', 'make', 'predictions', 'X_new', '=', '...', '#', 'New', 'sequencesy_pred', '=', 'sess.run', 'outputs', 'feed_dict=', '{', 'X', 'X_new', '}', 'Figure', '14-9', 'shows', 'predicted', 'sequence', 'instance', 'looked', 'earlier', 'Figure', '14-7', '1,000', 'training', 'iterations', '.', 'Figure', '14-9', '.', 'Time', 'series', 'prediction', 'Although', 'using', 'OutputProjectionWrapper', 'simplest', 'solution', 'reduce', 'dimensionality', 'RNN‡s', 'output', 'sequences', 'one', 'value', 'per', 'time', 'step', 'per', 'instance', 'efficient', '.', 'There', 'trickier', 'efficient', 'solu…', 'tion', 'reshape', 'RNN', 'outputs', 'batch_size', 'n_steps', 'n_neurons', 'batch_size', '*', 'n_steps', 'n_neurons', 'apply', 'single', 'fully', 'connected', 'layer', 'appropriate', 'output', 'size', 'case', '1', 'result', 'output', 'tensor', 'shape', 'batch_size', '*', 'n_steps', 'n_outputs', 'reshape', 'tensor', '394', '|', 'Chapter', '14', 'Recurrent', 'Neural', 'Networks', 'batch_size', 'n_steps', 'n_outputs', '.', 'These', 'operations', 'represented', 'Figure', '14-10.Figure', '14-10', '.', 'Stack', 'outputs', 'apply', 'projection', 'unstack', 'result', 'To', 'implement', 'solution', 'first', 'revert', 'basic', 'cell', 'without', 'OutputProjectionWrapper', 'cell', '=', 'tf.contrib.rnn.BasicRNNCell', 'num_units=n_neurons', 'activation=tf.nn.relu', 'rnn_outputs', 'states', '=', 'tf.nn.dynamic_rnn', 'cell', 'X', 'dtype=tf.float32', 'Then', 'stack', 'outputs', 'using', 'reshape', 'operation', 'apply', 'fully', 'connec…', 'ted', 'linear', 'layer', 'without', 'using', 'activation', 'function', 'projection', 'finally', 'unstack', 'outputs', 'using', 'reshape', 'stacked_rnn_outputs', '=', 'tf.reshape', 'rnn_outputs', '-1', 'n_neurons', 'stacked_outputs', '=', 'fully_connected', 'stacked_rnn_outputs', 'n_outputs', 'activation_fn=None', 'outputs', '=', 'tf.reshape', 'stacked_outputs', '-1', 'n_steps', 'n_outputs', 'Training', 'RNNs', '|', '395', 'The', 'rest', 'code', 'earlier', '.', 'This', 'provide', 'significant', 'speed', 'boost', 'since', 'one', 'fully', 'connected', 'layer', 'instead', 'one', 'per', 'time', 'step', '.', 'Creative', 'RNNNow', 'model', 'predict', 'future', 'use', 'generate', 'creative', 'sequences', 'explained', 'beginning', 'chapter', '.', 'All', 'need', 'pro…', 'vide', 'seed', 'sequence', 'containing', 'n_steps', 'values', 'e.g.', 'full', 'zeros', 'use', 'model', 'predict', 'next', 'value', 'append', 'predicted', 'value', 'sequence', 'feed', 'last', 'n_steps', 'values', 'model', 'predict', 'next', 'value', '.', 'This', 'process', 'gener…ates', 'new', 'sequence', 'resemblance', 'original', 'time', 'series', 'see', 'Figure', '14-11', '.sequence', '=', '0', '.', '*', 'n_stepsfor', 'iteration', 'range', '300', 'X_batch', '=', 'np.array', 'sequence', '-n_steps', '.reshape', '1', 'n_steps', '1', 'y_pred', '=', 'sess.run', 'outputs', 'feed_dict=', '{', 'X', 'X_batch', '}', 'sequence.append', 'y_pred', '0', '-1', '0', 'Figure', '14-11', '.', 'Creative', 'sequences', 'seeded', 'zeros', 'le', '“', 'instance', 'right', 'Now', 'try', 'feed', 'John', 'Lennon', 'albums', 'RNN', 'see', 'generate', 'next', 'ƒImagine.⁄', 'However', 'probably', 'need', 'much', 'powerful', 'RNN', 'neurons', 'also', 'much', 'deeper', '.', 'Let‡s', 'look', 'deep', 'RNNs', '.', 'Deep', 'RNNsIt', 'quite', 'common', 'stack', 'multiple', 'layers', 'cells', 'shown', 'Figure', '14-12', '.', 'Thisgives', 'deep', 'RNN', '.396', '|', 'Chapter', '14', 'Recurrent', 'Neural', 'Networks', 'Figure', '14-12', '.', 'Deep', 'RNN', 'le', '“', 'unrolled', 'time', 'right', 'To', 'implement', 'deep', 'RNN', 'TensorFlow', 'create', 'several', 'cells', 'stack', 'MultiRNNCell', '.', 'In', 'following', 'code', 'stack', 'three', 'identical', 'cells', 'could', 'well', 'use', 'various', 'kinds', 'cells', 'different', 'number', 'neurons', 'n_neurons', '=', '100n_layers', '=', '3basic_cell', '=', 'tf.contrib.rnn.BasicRNNCell', 'num_units=n_neurons', 'multi_layer_cell', '=', 'tf.contrib.rnn.MultiRNNCell', 'basic_cell', '*', 'n_layers', 'outputs', 'states', '=', 'tf.nn.dynamic_rnn', 'multi_layer_cell', 'X', 'dtype=tf.float32', 'That‡s', '!', 'The', 'states', 'variable', 'tuple', 'containing', 'one', 'tensor', 'per', 'layer', 'representing', 'final', 'state', 'layer‡s', 'cell', 'shape', 'batch_size', 'n_neurons', '.', 'If', 'set', 'state_is_tuple=False', 'creating', 'MultiRNNCell', 'thenstates', 'becomes', 'single', 'tensor', 'containing', 'states', 'every', 'layer', 'concatenated', 'along', 'column', 'axis', 'i.e.', 'shape', 'batch_size', 'n_layers', '*', 'n_neurons', '.Note', 'TensorFlow', '0.11.0', 'behavior', 'default', '.', 'Distributing', 'Deep', 'RNN', 'Across', 'Multiple', 'GPUsChapter', '12', 'pointed', 'efficiently', 'distribute', 'deep', 'RNNs', 'across', 'multiple', 'GPUs', 'pinning', 'layer', 'different', 'GPU', 'see', 'Figure', '12-16', '.', 'However', 'try', 'create', 'cell', 'different', 'device', 'block', 'work', 'tf.device', '``', '/gpu:0', \"''\", '#', 'BAD', '!', 'This', 'ignored', '.', 'layer1', '=', 'tf.contrib.rnn.BasicRNNCell', 'num_units=n_neurons', 'tf.device', '``', '/gpu:1', \"''\", '#', 'BAD', '!', 'Ignored', '.', 'layer2', '=', 'tf.contrib.rnn.BasicRNNCell', 'num_units=n_neurons', 'This', 'fails', 'BasicRNNCell', 'cell', 'factory', 'cell', 'per', 'se', 'mentioned', 'ear…', 'lier', 'cells', 'get', 'created', 'create', 'factory', 'thus', 'variables', 'either', '.', 'Deep', 'RNNs', '|', '397', '2This', 'uses', 'decorator', 'design', 'pattern', '.', 'The', 'device', 'block', 'simply', 'ignored', '.', 'The', 'cells', 'actually', 'get', 'created', 'later', '.', 'When', 'call', 'dynamic_rnn', 'calls', 'MultiRNNCell', 'calls', 'individual', 'BasicRNNCell', 'create', 'actual', 'cells', 'including', 'variables', '.', 'Unfortunately', 'none', 'classes', 'provide', 'way', 'control', 'devices', 'variables', 'get', 'created', '.', 'If', 'try', 'put', 'dynamic_rnn', 'call', 'within', 'device', 'block', 'whole', 'RNN', 'gets', 'pin…ned', 'single', 'device', '.', 'So', 'stuck', '?', 'Fortunately', '!', 'The', 'trick', 'create', 'cell', 'wrapper', 'import', 'tensorflow', 'tfclass', 'DeviceCellWrapper', 'tf.contrib.rnn.RNNCell', 'def', '__init__', 'self', 'device', 'cell', 'self._cell', '=', 'cell', 'self._device', '=', 'device', '@', 'property', 'def', 'state_size', 'self', 'return', 'self._cell.state_size', '@', 'property', 'def', 'output_size', 'self', 'return', 'self._cell.output_size', 'def', '__call__', 'self', 'inputs', 'state', 'scope=None', 'tf.device', 'self._device', 'return', 'self._cell', 'inputs', 'state', 'scope', 'This', 'wrapper', 'simply', 'proxies', 'every', 'method', 'call', 'another', 'cell', 'except', 'wraps', '__call__', 'function', 'within', 'device', 'block.2', 'Now', 'distribute', 'layer', 'different', 'GPU', 'devices', '=', '``', '/gpu:0', \"''\", '``', '/gpu:1', \"''\", '``', '/gpu:2', \"''\", 'cells', '=', 'DeviceCellWrapper', 'dev', 'tf.contrib.rnn.BasicRNNCell', 'num_units=n_neurons', 'dev', 'devices', 'multi_layer_cell', '=', 'tf.contrib.rnn.MultiRNNCell', 'cells', 'outputs', 'states', '=', 'tf.nn.dynamic_rnn', 'multi_layer_cell', 'X', 'dtype=tf.float32', 'Do', 'set', 'state_is_tuple=False', 'MultiRNNCell', 'con…catenate', 'cell', 'states', 'single', 'tensor', 'single', 'GPU', '.', '398', '|', 'Chapter', '14', 'Recurrent', 'Neural', 'Networks', 'Applying', 'DropoutIf', 'build', 'deep', 'RNN', 'may', 'end', 'overfitting', 'training', 'set', '.', 'To', 'prevent', 'common', 'technique', 'apply', 'dropout', 'introduced', 'Chapter', '11', '.', 'You', 'simply', 'add', 'dropout', 'layer', 'RNN', 'usual', 'also', 'want', 'apply', 'dropout', 'RNN', 'layers', 'need', 'use', 'DropoutWrapper', '.', 'The', 'fol…lowing', 'code', 'applies', 'dropout', 'inputs', 'layer', 'RNN', 'dropping', 'input', '50', '%', 'probability', 'keep_prob', '=', '0.5cell', '=', 'tf.contrib.rnn.BasicRNNCell', 'num_units=n_neurons', 'cell_drop', '=', 'tf.contrib.rnn.DropoutWrapper', 'cell', 'input_keep_prob=keep_prob', 'multi_layer_cell', '=', 'tf.contrib.rnn.MultiRNNCell', 'cell_drop', '*', 'n_layers', 'rnn_outputs', 'states', '=', 'tf.nn.dynamic_rnn', 'multi_layer_cell', 'X', 'dtype=tf.float32', 'Note', 'also', 'possible', 'apply', 'dropout', 'outputs', 'setting', 'output_keep_prob.The', 'main', 'problem', 'code', 'apply', 'dropout', 'train…', 'ing', 'also', 'testing', 'want', 'recall', 'dropout', 'applied', 'training', '.', 'Unfortunately', 'DropoutWrapper', 'supportan', 'is_training', 'placeholder', 'yet', '?', 'must', 'either', 'write', 'dropout', 'wrap…', 'per', 'class', 'two', 'different', 'graphs', 'one', 'training', 'testing', '.', 'The', 'second', 'option', 'looks', 'like', 'import', 'sysis_training', '=', 'sys.argv', '-1', '==', '``', 'train', \"''\", 'X', '=', 'tf.placeholder', 'tf.float32', 'None', 'n_steps', 'n_inputs', '=', 'tf.placeholder', 'tf.float32', 'None', 'n_steps', 'n_outputs', 'cell', '=', 'tf.contrib.rnn.BasicRNNCell', 'num_units=n_neurons', 'is_training', 'cell', '=', 'tf.contrib.rnn.DropoutWrapper', 'cell', 'input_keep_prob=keep_prob', 'multi_layer_cell', '=', 'tf.contrib.rnn.MultiRNNCell', 'cell', '*', 'n_layers', 'rnn_outputs', 'states', '=', 'tf.nn.dynamic_rnn', 'multi_layer_cell', 'X', 'dtype=tf.float32', '...', '#', 'build', 'rest', 'graphinit', '=', 'tf.global_variables_initializer', 'saver', '=', 'tf.train.Saver', 'tf.Session', 'sess', 'is_training', 'init.run', 'iteration', 'range', 'n_iterations', '...', '#', 'train', 'model', 'save_path', '=', 'saver.save', 'sess', '``', '/tmp/my_model.ckpt', \"''\", 'else', 'saver.restore', 'sess', '``', '/tmp/my_model.ckpt', \"''\", '...', '#', 'use', 'modelDeep', 'RNNs', '|', '399', 'With', 'able', 'train', 'sorts', 'RNNs', '!', 'Unfortunately', 'want', 'train', 'RNN', 'long', 'sequences', 'things', 'get', 'bit', 'harder', '.', 'Let‡s', 'see', 'it.The', 'Di…culty', 'Training', 'Many', 'Time', 'Steps', 'To', 'train', 'RNN', 'long', 'sequences', 'need', 'run', 'many', 'time', 'steps', 'making', 'unrolled', 'RNN', 'deep', 'network', '.', 'Just', 'like', 'deep', 'neural', 'network', 'may', 'suffer', 'vanishing/exploding', 'gradients', 'problem', 'discussed', 'Chap…', 'ter', '11', 'take', 'forever', 'train', '.', 'Many', 'tricks', 'discussed', 'alleviate', 'problem', 'used', 'deep', 'unrolled', 'RNNs', 'well', 'good', 'parameter', 'initialization', 'nonsaturating', 'activation', 'functions', 'e.g.', 'ReLU', 'Batch', 'Normalization', 'Gradient', 'Clip…', 'ping', 'faster', 'optimizers', '.', 'However', 'RNN', 'needs', 'handle', 'even', 'moderately', 'long', 'sequences', 'e.g.', '100', 'inputs', 'training', 'still', 'slow', '.', 'The', 'simplest', 'common', 'solution', 'problem', 'unroll', 'RNN', 'limited', 'number', 'time', 'steps', 'training', '.', 'This', 'called', 'truncated', 'backpro…', 'pagation', 'time', '.', 'In', 'TensorFlow', 'implement', 'simply', 'truncating', 'input', 'sequences', '.', 'For', 'example', 'time', 'series', 'prediction', 'problem', 'would', 'sim…', 'ply', 'reduce', 'n_steps', 'training', '.', 'The', 'problem', 'course', 'model', 'able', 'learn', 'long-term', 'patterns', '.', 'One', 'workaround', 'could', 'make', 'sure', 'shortened', 'sequences', 'contain', 'old', 'recent', 'data', 'model', 'learn', 'use', 'e.g.', 'sequence', 'could', 'contain', 'monthly', 'data', 'last', 'five', 'months', 'weekly', 'data', 'last', 'five', 'weeks', 'daily', 'data', 'last', 'five', 'days', '.', 'But', 'workaround', 'limits', 'fine-grained', 'data', 'last', 'year', 'actually', 'useful', '?', 'What', 'brief', 'significant', 'event', 'absolutely', 'must', 'taken', 'account', 'even', 'years', 'later', 'e.g.', 'result', 'election', '?', 'Besides', 'long', 'training', 'time', 'second', 'problem', 'faced', 'long-running', 'RNNs', 'fact', 'memory', 'first', 'inputs', 'gradually', 'fades', 'away', '.', 'Indeed', 'due', 'trans…', 'formations', 'data', 'goes', 'traversing', 'RNN', 'information', 'lost', 'time', 'step', '.', 'After', 'RNN‡s', 'state', 'contains', 'virtually', 'trace', 'first', 'inputs', '.', 'This', 'showstopper', '.', 'For', 'example', 'say', 'want', 'perform', 'sentiment', 'analysis', 'long', 'review', 'starts', 'four', 'words', 'ƒI', 'loved', 'movie', '⁄', 'rest', 'review', 'lists', 'many', 'things', 'could', 'made', 'movie', 'even', 'better', '.', 'If', 'RNN', 'gradually', 'forgets', 'first', 'four', 'words', 'completely', 'misinterpret', 'review', '.', 'To', 'solve', 'problem', 'various', 'types', 'cells', 'long-term', 'memory', 'introduced', '.', 'They', 'proved', 'successful', 'basic', 'cells', 'much', 'used', 'anymore', '.', 'Let‡s', 'first', 'look', 'popular', 'long', 'memory', 'cells', 'LSTM', 'cell.400', '|', 'Chapter', '14', 'Recurrent', 'Neural', 'Networks', '3ƒLong', 'Short-Term', 'Memory', '⁄', 'S.', 'Hochreiter', 'J.', 'Schmidhuber', '1997', '.', '4ƒLong', 'Short-Term', 'Memory', 'Recurrent', 'Neural', 'Network', 'Architectures', 'Large', 'Scale', 'Acoustic', 'Modeling', '⁄', 'H.', 'Sak', 'et', 'al', '.', '2014', '.5ƒRecurrent', 'Neural', 'Network', 'Regularization', '⁄', 'W.', 'Zaremba', 'et', 'al', '.', '2015', '.', 'LSTM', 'CellThe', 'Long', 'Short-Term', 'Memory', 'LSTM', 'cell', 'proposed', '19973', 'Sepp', 'Hochreiter', 'J™rgen', 'Schmidhuber', 'gradually', 'improved', 'years', 'several', 'researchers', 'Alex', 'Graves', 'HaÑim', 'Sak', ',4', 'Wojciech', 'Zaremba', ',5', 'many', '.', 'If', 'consider', 'LSTM', 'cell', 'black', 'box', 'used', 'much', 'like', 'basic', 'cell', 'except', 'perform', 'much', 'better', 'training', 'converge', 'faster', 'detect', 'long-term', 'dependencies', 'data', '.', 'In', 'TensorFlow', 'simply', 'use', 'BasicLSTMCell', 'instead', 'BasicRNNCell', 'lstm_cell', '=', 'tf.contrib.rnn.BasicLSTMCell', 'num_units=n_neurons', 'LSTM', 'cells', 'manage', 'two', 'state', 'vectors', 'performance', 'reasons', 'kept', 'separate', 'default', '.', 'You', 'change', 'default', 'behavior', 'setting', 'state_is_tuple=False', 'creating', 'BasicLSTMCell.So', 'LSTM', 'cell', 'work', '?', 'The', 'architecture', 'basic', 'LSTM', 'cell', 'shown', 'inFigure', '14-13.Figure', '14-13', '.', 'LSTM', 'cell', 'LSTM', 'Cell', '|', '401', 'If', 'don‡t', 'look', 'what‡s', 'inside', 'box', 'LSTM', 'cell', 'looks', 'exactly', 'like', 'regular', 'cell', 'except', 'state', 'split', 'two', 'vectors', 'h', 'c', 'ƒc⁄', 'stands', 'ƒcell⁄', '.', 'You', 'think', 'h', 'short-term', 'state', 'c', 'long-term', 'state', '.', 'Now', 'let‡s', 'open', 'box', '!', 'The', 'key', 'idea', 'network', 'learn', 'store', 'long-term', 'state', 'throw', 'away', 'read', '.', 'As', 'long-term', 'state', 'c', 't–1', 'traverses', 'network', 'left', 'right', 'see', 'first', 'goes', 'aforget', 'gate', 'dropping', 'memories', 'adds', 'new', 'memories', 'via', 'theaddition', 'operation', 'adds', 'memories', 'selected', 'input', 'gate', '.The', 'result', 'c', 'sent', 'straight', 'without', 'transformation', '.', 'So', 'time', 'step', 'memories', 'dropped', 'memories', 'added', '.', 'Moreover', 'addition', 'operation', 'long-term', 'state', 'copied', 'passed', 'tanh', 'func…', 'tion', 'result', 'filtered', 'output', 'gate', '.', 'This', 'produces', 'short-termstate', 'h', 'equal', 'cell‡s', 'output', 'time', 'step', '.', 'Now', 'let‡s', 'look', 'new', 'memories', 'come', 'gates', 'work', '.', 'First', 'current', 'input', 'vector', 'x', 'previous', 'short-term', 'state', 'h', 't–1', 'fed', 'tofour', 'different', 'fully', 'connected', 'layers', '.', 'They', 'serve', 'different', 'purpose', '‹The', 'main', 'layer', 'one', 'outputs', 'g', '.', 'It', 'usual', 'role', 'analyzing', 'current', 'inputs', 'x', 'previous', 'short-term', 'state', 'h', 't–1', '.', 'In', 'basic', 'cell', 'isnothing', 'else', 'layer', 'output', 'goes', 'straight', 'h', '.', 'In', 'con…trast', 'LSTM', 'cell', 'layer‡s', 'output', 'go', 'straight', 'instead', 'partially', 'stored', 'long-term', 'state', '.', '‹The', 'three', 'layers', 'gate', 'controllers', '.', 'Since', 'use', 'logistic', 'activation', 'function', 'outputs', 'range', '0', '1', '.', 'As', 'see', 'outputs', 'fed', 'toelement-wise', 'multiplication', 'operations', 'output', '0s', 'close', 'gate', 'output', '1s', 'open', '.', 'Specifically', '›The', 'forget', 'gate', 'controlled', 'f', 'controls', 'parts', 'long-term', 'state', 'erased.›The', 'input', 'gate', 'controlled', 'controls', 'parts', 'g', 'addedto', 'long-term', 'state', 'said', 'ƒpartially', 'stored⁄', '.', '›Finally', 'output', 'gate', 'controlled', 'controls', 'parts', 'long-', 'term', 'state', 'read', 'output', 'time', 'step', 'h', '.In', 'short', 'LSTM', 'cell', 'learn', 'recognize', 'important', 'input', 'that‡s', 'role', 'input', 'gate', 'store', 'long-term', 'state', 'learn', 'preserve', 'long', 'needed', 'that‡s', 'role', 'forget', 'gate', 'learn', 'extract', 'whenever', 'needed', '.', 'This', 'explains', 'amazingly', 'successful', 'capturing', 'long-term', 'pat…', 'terns', 'time', 'series', 'long', 'texts', 'audio', 'recordings', '.', '402', '|', 'Chapter', '14', 'Recurrent', 'Neural', 'Networks', '6ƒRecurrent', 'Nets', 'Time', 'Count', '⁄', 'F.', 'Gers', 'J.', 'Schmidhuber', '2000', '.', 'Equation', '14-3', 'summarizes', 'compute', 'cell‡s', 'long-term', 'state', 'short-term', 'state', 'output', 'time', 'step', 'single', 'instance', 'equations', 'whole', 'mini-batch', 'similar', '.', 'Equation', '14-3', '.', 'LSTM', 'computations', 't=„xiT', '’', 't+hiT', '’', '”', '1', '+it=„xf', 'T', '’', 't+hf', 'T', '’', '”', '1', '+ft=„xoT', '’', 't+hoT', '’', '”', '1', '+ot=tanh', 'xg', 'T', '’', 't+hg', 'T', '’', '”', '1', '+gt=t', '”', '1', '+t', 'tt=t=ttanht‹Wxi', 'Wxf', 'Wxo', 'Wxg', 'weight', 'matrices', 'four', 'layers', 'con…', 'nection', 'input', 'vector', 'x', '.‹Whi', 'Whf', 'Who', 'Whg', 'weight', 'matrices', 'four', 'layers', 'connection', 'previous', 'short-term', 'state', 'h', 't–1', '.‹bi', 'bf', 'bo', 'bg', 'bias', 'terms', 'four', 'layers', '.', 'Note', 'Tensor…', 'Flow', 'initializes', 'bf', 'vector', 'full', '1s', 'instead', '0s', '.', 'This', 'prevents', 'forgetting', 'everything', 'beginning', 'training', '.', 'Peephole', 'ConnectionsIn', 'basic', 'LSTM', 'cell', 'gate', 'controllers', 'look', 'input', 'x', 'previ…ous', 'short-term', 'state', 'h', 't–1', '.', 'It', 'may', 'good', 'idea', 'give', 'bit', 'context', 'letting', 'peek', 'long-term', 'state', 'well', '.', 'This', 'idea', 'proposed', 'Felix', 'Gers', 'J™rgen', 'Schmidhuber', '2000', '.6', 'They', 'proposed', 'LSTM', 'variant', 'extra', 'con…', 'nections', 'called', 'peephole', 'connections', 'previous', 'long-term', 'state', 'c', 't–1', 'added', 'aninput', 'controllers', 'forget', 'gate', 'input', 'gate', 'current', 'long-', 'term', 'state', 'c', 'added', 'input', 'controller', 'output', 'gate', '.', 'To', 'implement', 'peephole', 'connections', 'TensorFlow', 'must', 'use', 'LSTMCellinstead', 'BasicLSTMCell', 'set', 'use_peepholes=True', 'lstm_cell', '=', 'tf.contrib.rnn.LSTMCell', 'num_units=n_neurons', 'use_peepholes=True', 'LSTM', 'Cell', '|', '403', '7ƒLearning', 'Phrase', 'Representations', 'using', 'RNN', 'Encoder–Decoder', 'Statistical', 'Machine', 'Translation', '⁄', 'K.', 'Cho', 'et', 'al', '.', '2014', '.8A', '2015', 'paper', 'Klaus', 'Greff', 'et', 'al.', 'ƒLSTM', 'A', 'Search', 'Space', 'Odyssey', '⁄', 'seems', 'show', 'LSTM', 'variants', 'perform', 'roughly', 'same.There', 'many', 'variants', 'LSTM', 'cell', '.', 'One', 'particularly', 'popular', 'variant', 'GRU', 'cell', 'look', '.', 'GRU', 'CellThe', 'Gated', 'Recurrent', 'Unit', 'GRU', 'cell', 'see', 'Figure', '14-14', 'proposed', 'Kyunghyun', 'Cho', 'et', 'al', '.', '2014', 'paper', '7', 'also', 'introduced', 'Encoder–Decoder', 'network', 'mentioned', 'earlier', '.', 'Figure', '14-14', '.', 'GRU', 'cell', 'The', 'GRU', 'cell', 'simplified', 'version', 'LSTM', 'cell', 'seems', 'perform', 'well8', 'explains', 'growing', 'popularity', '.', 'The', 'main', 'simplifications', '‹Both', 'state', 'vectors', 'merged', 'single', 'vector', 'h', '.‹A', 'single', 'gate', 'controller', 'controls', 'forget', 'gate', 'input', 'gate', '.', 'If', 'gate', 'controller', 'outputs', '1', 'input', 'gate', 'open', 'forget', 'gate', 'closed', '.', 'If', '404', '|', 'Chapter', '14', 'Recurrent', 'Neural', 'Networks', 'outputs', '0', 'opposite', 'happens', '.', 'In', 'words', 'whenever', 'memory', 'must', 'stored', 'location', 'stored', 'erased', 'first', '.', 'This', 'actually', 'fre…', 'quent', 'variant', 'LSTM', 'cell', '.', '‹There', 'output', 'gate', 'full', 'state', 'vector', 'output', 'every', 'time', 'step', '.', 'How…', 'ever', 'new', 'gate', 'controller', 'controls', 'part', 'previous', 'state', 'shown', 'main', 'layer', '.', 'Equation', '14-4', 'summarizes', 'compute', 'cell‡s', 'state', 'time', 'step', 'sin…', 'gle', 'instance.Equation', '14-4', '.', 'GRU', 'computations', 't=„xzT', '’', 't+hzT', '’', '”', '1', 't=„xrT', '’', 't+hrT', '’', '”', '1', 't=tanh', 'xg', 'T', '’', 't+hg', 'T', '’', '”', '1', 't=1', '”', 'ttanhxg', 'T', '’', '”', '1', '+t', 'tCreating', 'GRU', 'cell', 'TensorFlow', 'trivial', 'gru_cell', '=', 'tf.contrib.rnn.GRUCell', 'num_units=n_neurons', 'LSTM', 'GRU', 'cells', 'one', 'main', 'reasons', 'behind', 'success', 'RNNs', 'recent', 'years', 'particular', 'applications', 'natural', 'language', 'processing', 'NLP', '.Natural', 'Language', 'ProcessingMost', 'state-of-the-art', 'NLP', 'applications', 'machine', 'translation', 'automatic', 'summarization', 'parsing', 'sentiment', 'analysis', 'based', 'least', 'part', 'RNNs', '.', 'In', 'last', 'section', 'take', 'quick', 'look', 'machine', 'trans…', 'lation', 'model', 'looks', 'like', '.', 'This', 'topic', 'well', 'covered', 'TensorFlow‡s', 'awesome', 'Word2Vec', 'Seq2Seq', 'tutorials', 'definitely', 'check', 'out.Word', 'EmbeddingsBefore', 'start', 'need', 'choose', 'word', 'representation', '.', 'One', 'option', 'could', 'represent', 'word', 'using', 'one-hot', 'vector', '.', 'Suppose', 'vocabulary', 'contains', '50,000', 'words', 'nth', 'word', 'would', 'represented', '50,000-dimensional', 'vector', 'full', '0s', 'except', '1', 'n', 'th', 'position', '.', 'However', 'large', 'vocabulary', 'sparse', 'representation', 'would', 'efficient', '.', 'Ideally', 'want', 'similar', 'words', 'similar', 'representations', 'making', 'easy', 'model', 'generalize', 'learns', 'word', 'similar', 'words', '.', 'For', 'example', 'model', 'told', 'ƒI', 'drink', 'milk⁄', 'valid', 'sentence', 'knows', 'ƒmilk⁄', 'close', 'ƒwater⁄', 'far', 'ƒshoes', '⁄', 'Natural', 'Language', 'Processing', '|', '405', '9For', 'details', 'check', 'Christopher', 'Olah‡s', 'great', 'post', 'Sebastian', 'Ruder‡s', 'series', 'posts.then', 'know', 'ƒI', 'drink', 'water⁄', 'probably', 'valid', 'sentence', 'well', 'ƒI', 'drink', 'shoes⁄', 'probably', '.', 'But', 'come', 'meaningful', 'rep…', 'resentation', '?', 'The', 'common', 'solution', 'represent', 'word', 'vocabulary', 'using', 'fairly', 'small', 'dense', 'vector', 'e.g.', '150', 'dimensions', 'called', 'embedding', 'let', 'theneural', 'network', 'learn', 'good', 'embedding', 'word', 'training', '.', 'At', 'begin…', 'ning', 'training', 'embeddings', 'simply', 'chosen', 'randomly', 'training', 'back…', 'propagation', 'automatically', 'moves', 'embeddings', 'around', 'way', 'helps', 'neural', 'network', 'perform', 'task', '.', 'Typically', 'means', 'similar', 'words', 'gradu…', 'ally', 'cluster', 'close', 'one', 'another', 'even', 'end', 'organized', 'rather', 'meaningful', 'way', '.', 'For', 'example', 'embeddings', 'may', 'end', 'placed', 'along', 'various', 'axes', 'represent', 'gender', 'singular/plural', 'adjective/noun', '.', 'The', 'result', 'truly', 'amazing', '.', '9In', 'TensorFlow', 'first', 'need', 'create', 'variable', 'representing', 'embeddings', 'every', 'word', 'vocabulary', 'initialized', 'randomly', 'vocabulary_size', '=', '50000embedding_size', '=', '150embeddings', '=', 'tf.Variable', 'tf.random_uniform', 'vocabulary_size', 'embedding_size', '-1.0', '1.0', 'Now', 'suppose', 'want', 'feed', 'sentence', 'ƒI', 'drink', 'milk⁄', 'neural', 'network', '.', 'You', 'first', 'preprocess', 'sentence', 'break', 'list', 'known', 'words', '.', 'For', 'example', 'may', 'remove', 'unnecessary', 'characters', 'replace', 'unknown', 'words', 'pre…', 'defined', 'token', 'word', 'ƒ', 'UNK', '⁄', 'replace', 'numerical', 'values', 'ƒ', 'NUM', '⁄', 'replace', 'URLs', 'ƒ', 'URL', '⁄', '.', 'Once', 'list', 'known', 'words', 'look', 'word‡s', 'integer', 'identifier', '0', '49999', 'dictionary', 'example', '72', '3335', '288', '.', 'At', 'point', 'ready', 'feed', 'word', 'identifiers', 'TensorFlow', 'using', 'placeholder', 'apply', 'embedding_lookup', 'function', 'get', 'correspondingembeddings', 'train_inputs', '=', 'tf.placeholder', 'tf.int32', 'shape=', 'None', '#', 'ids', '...', 'embed', '=', 'tf.nn.embedding_lookup', 'embeddings', 'train_inputs', '#', '...', 'embeddingsOnce', 'model', 'learned', 'good', 'word', 'embeddings', 'actually', 'reusedfairly', 'efficiently', 'NLP', 'application', 'ƒmilk⁄', 'still', 'close', 'ƒwater⁄', 'far', 'ƒshoes⁄', 'matter', 'application', '.', 'In', 'fact', 'instead', 'training', 'word', 'embeddings', 'may', 'want', 'download', 'pretrained', 'word', 'embeddings', '.', 'Just', 'like', 'reusing', 'pretrained', 'layers', 'see', 'Chapter', '11', 'choose', 'freeze', 'pre…trained', 'embeddings', 'e.g.', 'creating', 'embeddings', 'variable', 'using', 'trainable=False', 'let', 'backpropagation', 'tweak', 'application', '.', 'The', 'first', 'option', 'speed', 'training', 'second', 'may', 'lead', 'slightly', 'higher', 'performance', '.', '406', '|', 'Chapter', '14', 'Recurrent', 'Neural', 'Networks', '10ƒSequence', 'Sequence', 'learning', 'Neural', 'Networks', '⁄', 'I.', 'Sutskever', 'et', 'al', '.', '2014', '.', 'Embeddings', 'also', 'useful', 'representing', 'categorical', 'attributes', 'take', 'large', 'number', 'different', 'values', 'especially', 'complex', 'similarities', 'values', '.', 'For', 'example', 'con…', 'sider', 'professions', 'hobbies', 'dishes', 'species', 'brands', 'on.You', 'almost', 'tools', 'need', 'implement', 'machine', 'translation', 'sys…', 'tem', '.', 'Let‡s', 'look', '.', 'An', 'Encoder—Decoder', 'Network', 'Machine', 'TranslationLet‡s', 'take', 'look', 'simple', 'machine', 'translation', 'model', '10', 'translate', 'English', 'sentences', 'French', 'see', 'Figure', '14-15', '.Figure', '14-15', '.', 'A', 'simple', 'machine', 'translation', 'model', 'The', 'English', 'sentences', 'fed', 'encoder', 'decoder', 'outputs', 'French', 'translations', '.', 'Note', 'French', 'translations', 'also', 'used', 'inputs', 'decoder', 'pushed', 'back', 'one', 'step', '.', 'In', 'words', 'decoder', 'given', 'input', 'word', 'output', 'previous', 'step', 'regardless', 'actually', 'output', '.', 'For', 'first', 'word', 'given', 'token', 'represents', 'beginning', 'sen…', 'Natural', 'Language', 'Processing', '|', '407', 'tence', 'e.g.', 'ƒ', '<', 'go', '>', '⁄', '.', 'The', 'decoder', 'expected', 'end', 'sentence', 'end-of-', 'sequence', 'EOS', 'token', 'e.g.', 'ƒ', '<', 'eos', '>', '⁄', '.Note', 'English', 'sentences', 'reversed', 'fed', 'encoder', '.', 'For', 'example', 'ƒI', 'drink', 'milk⁄', 'reversed', 'ƒmilk', 'drink', 'I.⁄', 'This', 'ensures', 'beginning', 'English', 'sentence', 'fed', 'last', 'encoder', 'useful', 'that‡s', 'generally', 'first', 'thing', 'decoder', 'needs', 'translate', '.', 'Each', 'word', 'initially', 'represented', 'simple', 'integer', 'identifier', 'e.g.', '288', 'word', 'ƒmilk⁄', '.', 'Next', 'embedding', 'lookup', 'returns', 'word', 'embedding', 'explained', 'ear…', 'lier', 'dense', 'fairly', 'low-dimensional', 'vector', '.', 'These', 'word', 'embeddings', 'actually', 'fed', 'encoder', 'decoder', '.', 'At', 'step', 'decoder', 'outputs', 'score', 'word', 'output', 'vocabulary', 'i.e.', 'French', 'Softmax', 'layer', 'turns', 'scores', 'probabilities', '.', 'For', 'exam…', 'ple', 'first', 'step', 'word', 'ƒJe⁄', 'may', 'probability', '20', '%', 'ƒTu⁄', 'may', 'probability', '1', '%', '.', 'The', 'word', 'highest', 'probability', 'output', '.', 'This', 'isvery', 'much', 'like', 'regular', 'classification', 'task', 'train', 'model', 'using', 'softmax_cross_entropy_with_logits', 'function.Note', 'inference', 'time', 'training', 'target', 'sentence', 'feed', 'decoder', '.', 'Instead', 'simply', 'feed', 'decoder', 'word', 'output', 'previous', 'step', 'shown', 'Figure', '14-16', 'require', 'embedding', 'lookup', 'shown', 'diagram', '.Figure', '14-16', '.', 'Feeding', 'previous', 'output', 'word', 'input', 'inference', 'time', 'Okay', 'big', 'picture', '.', 'However', 'go', 'TensorFlow‡s', 'sequence-to-sequence', 'tutorial', 'look', 'code', 'rnn/translate/', 'seq2seq_model.py', 'TensorFlow', 'models', 'notice', 'important', 'differ…', 'ences:408', '|', 'Chapter', '14', 'Recurrent', 'Neural', 'Networks', '11The', 'bucket', 'sizes', 'used', 'tutorial', 'different', '.', '12ƒOn', 'Using', 'Very', 'Large', 'Target', 'Vocabulary', 'Neural', 'Machine', 'Translation', '⁄', 'S.', 'Jean', 'et', 'al', '.', '2015', '.', '13ƒNeural', 'Machine', 'Translation', 'Jointly', 'Learning', 'Align', 'Translate', '⁄', 'D.', 'Bahdanau', 'et', 'al', '.', '2014', '.', '14ƒLong', 'Short-Term', 'Memory-Networks', 'Machine', 'Reading', '⁄', 'J.', 'Cheng', '2016', '.', '15ƒShow', 'Attend', 'Tell', 'Neural', 'Image', 'Caption', 'Generation', 'Visual', 'Attention', '⁄', 'K.', 'Xu', 'et', 'al', '.', '2015', '.', '‹First', 'far', 'assumed', 'input', 'sequences', 'encoder', 'decoder', 'constant', 'length', '.', 'But', 'obviously', 'sentence', 'lengths', 'may', 'vary', '.', 'There', 'several', 'ways', 'handled›for', 'example', 'using', 'sequence_length', 'argument', 'static_rnn', 'dynamic_rnn', 'functions', 'tospecify', 'sentence‡s', 'length', 'discussed', 'earlier', '.', 'However', 'another', 'approach', 'used', 'tutorial', 'presumably', 'performance', 'reasons', 'sentences', 'grou…', 'ped', 'buckets', 'similar', 'lengths', 'e.g.', 'bucket', '1-', '6-word', 'sentences', 'another', '7-', '12-word', 'sentences', '11', 'shorter', 'sentences', 'padded', 'using', 'special', 'padding', 'token', 'e.g.', 'ƒ', '<', 'pad', '>', '⁄', '.', 'For', 'example', 'ƒI', 'drink', 'milk⁄', 'becomes', 'ƒ', '<', 'pad', '>', '<', 'pad', '>', '<', 'pad', '>', 'milk', 'drink', 'I⁄', 'translation', 'becomes', 'ƒJe', 'bois', 'du', 'lait', '<', 'eos', '>', '<', 'pad', '>', '⁄', '.', 'Of', 'course', 'want', 'ignore', 'output', 'past', 'EOS', 'token', '.', 'For', 'tutorial‡s', 'implementation', 'uses', 'target_weights', 'vector', '.', 'For', 'example', 'target', 'sentence', 'ƒJe', 'bois', 'du', 'lait', '<', 'eos', '>', '<', 'pad', '>', '⁄', 'weights', 'would', 'set', '1.0', '1.0', '1.0', '1.0', '1.0', '0.0', 'notice', 'weight', '0.0', 'corresponds', 'padding', 'token', 'target', 'sentence', '.', 'Simply', 'multiplying', 'losses', 'target', 'weights', 'zero', 'losses', 'correspond', 'words', 'past', 'EOS', 'tokens.‹Second', 'output', 'vocabulary', 'large', 'case', 'outputting', 'probability', 'every', 'possible', 'word', 'would', 'terribly', 'slow', '.', 'If', 'tar…', 'get', 'vocabulary', 'contains', 'say', '50,000', 'French', 'words', 'decoder', 'would', 'out…', 'put', '50,000-dimensional', 'vectors', 'computing', 'softmax', 'function', 'large', 'vector', 'would', 'computationally', 'intensive', '.', 'To', 'avoid', 'one', 'solution', 'let', 'decoder', 'output', 'much', 'smaller', 'vectors', '1,000-', 'dimensional', 'vectors', 'use', 'sampling', 'technique', 'estimate', 'loss', 'without', 'compute', 'every', 'single', 'word', 'target', 'vocabulary', '.', 'This', 'Sam…', 'pled', 'So', '“', 'max', 'technique', 'introduced', '2015', 'S•bastien', 'Jean', 'et', 'al', '.12', 'In', 'Ten…', 'sorFlow', 'use', 'sampled_softmax_loss', 'function.‹Third', 'tutorial‡s', 'implementation', 'uses', 'attention', 'mechanism', 'lets', 'decoder', 'peek', 'input', 'sequence', '.', 'Attention', 'augmented', 'RNNs', 'beyond', 'scope', 'book', 'interested', 'helpful', 'papers', 'machine', 'translation', ',13', 'machine', 'reading,14', 'image', 'captions', '15', 'using', 'attention', '.', '‹Finally', 'tutorial‡s', 'implementation', 'makes', 'use', 'tf.nn.legacy_seq2seqmodule', 'provides', 'tools', 'build', 'various', 'Encoder–Decoder', 'models', 'easily', '.', 'Natural', 'Language', 'Processing', '|', '409', 'For', 'example', 'embedding_rnn_seq2seq', 'function', 'creates', 'simple', 'Encoder–', 'Decoder', 'model', 'automatically', 'takes', 'care', 'word', 'embeddings', 'like', 'one', 'represented', 'Figure', '14-15', '.', 'This', 'code', 'likely', 'updated', 'quickly', 'use', 'new', 'tf.nn.seq2seq', 'module.You', 'tools', 'need', 'understand', 'sequence-to-sequence', 'tutor…', 'ial‡s', 'implementation', '.', 'Check', 'train', 'English-to-French', 'translator', '!', 'Exercises1.Can', 'think', 'applications', 'sequence-to-sequence', 'RNN', '?', 'What', 'sequence-to-vector', 'RNN', '?', 'And', 'vector-to-sequence', 'RNN', '?', '2.Why', 'people', 'use', 'encoder–decoder', 'RNNs', 'rather', 'plain', 'sequence-to-', 'sequence', 'RNNs', 'automatic', 'translation', '?', '3.How', 'could', 'combine', 'convolutional', 'neural', 'network', 'RNN', 'classify', 'videos', '?', '4.What', 'advantages', 'building', 'RNN', 'using', 'dynamic_rnn', 'rather', 'static_rnn', '?', '5.How', 'deal', 'variable-length', 'input', 'sequences', '?', 'What', 'variable-', 'length', 'output', 'sequences', '?', '6.What', 'common', 'way', 'distribute', 'training', 'execution', 'deep', 'RNN', 'across', 'multiple', 'GPUs', '?', '7.Embedded', 'Reber', 'grammars', 'used', 'Hochreiter', 'Schmidhuber', 'paper', 'LSTMs', '.', 'They', 'artificial', 'grammars', 'produce', 'strings', 'ƒBPBTSXXVPSEPE.⁄', 'Check', 'Jenny', 'Orr‡s', 'nice', 'introduction', 'topic.Choose', 'particular', 'embedded', 'Reber', 'grammar', 'one', 'represented', 'Jenny', 'Orr‡s', 'page', 'train', 'RNN', 'identify', 'whether', 'string', 'respects', 'grammar', '.', 'You', 'first', 'need', 'write', 'function', 'capable', 'generating', 'training', 'batch', 'containing', '50', '%', 'strings', 'respect', 'grammar', '50', '%', 'don‡t', '.', '8.Tackle', 'ƒHow', 'much', 'rain', '?', 'II⁄', 'Kaggle', 'competition', '.', 'This', 'time', 'seriesprediction', 'task', 'given', 'snapshots', 'polarimetric', 'radar', 'values', 'asked', 'predict', 'hourly', 'rain', 'gauge', 'total', '.', 'Luis', 'Andre', 'Dutra', 'e', 'Silva‡s', 'interview', 'givessome', 'interesting', 'insights', 'techniques', 'used', 'reach', 'second', 'place', 'competition', '.', 'In', 'particular', 'used', 'RNN', 'composed', 'two', 'LSTM', 'layers', '.', '9.Go', 'TensorFlow‡s', 'Word2Vec', 'tutorial', 'create', 'word', 'embeddings', 'go', 'Seq2Seq', 'tutorial', 'train', 'English-to-French', 'translation', 'system.Solutions', 'exercises', 'available', 'Appendix', 'A', '.410', '|', 'Chapter', '14', 'Recurrent', 'Neural', 'Networks', 'CHAPTER', '15AutoencodersAutoencoders', 'artificial', 'neural', 'networks', 'capable', 'learning', 'efficient', 'representa…', 'tions', 'input', 'data', 'called', 'codings', 'without', 'supervision', 'i.e.', 'training', 'set', 'unlabeled', '.', 'These', 'codings', 'typically', 'much', 'lower', 'dimensionality', 'input', 'data', 'making', 'autoencoders', 'useful', 'dimensionality', 'reduction', 'see', 'Chapter', '8', '.', 'More', 'importantly', 'autoencoders', 'act', 'powerful', 'feature', 'detectors', 'used', 'unsupervised', 'pretraining', 'deep', 'neural', 'networks', 'discussed', 'Chapter', '11', '.Lastly', 'capable', 'randomly', 'generating', 'new', 'data', 'looks', 'similar', 'training', 'data', 'called', 'generative', 'model', '.', 'For', 'example', 'could', 'train', 'autoencoder', 'pictures', 'faces', 'would', 'able', 'generate', 'new', 'faces', '.', 'Surprisingly', 'autoencoders', 'work', 'simply', 'learning', 'copy', 'inputs', 'out…', 'puts', '.', 'This', 'may', 'sound', 'like', 'trivial', 'task', 'see', 'constraining', 'network', 'various', 'ways', 'make', 'rather', 'difficult', '.', 'For', 'example', 'limit', 'size', 'internal', 'representation', 'add', 'noise', 'inputs', 'train', 'network', 'recover', 'original', 'inputs', '.', 'These', 'constraints', 'prevent', 'autoencoder', 'trivially', 'copying', 'inputs', 'directly', 'outputs', 'forces', 'learn', 'efficient', 'ways', 'representing', 'data', '.', 'In', 'short', 'codings', 'byproducts', 'autoencoder‡s', 'attempt', 'learn', 'identity', 'function', 'constraints', '.', 'In', 'chapter', 'explain', 'depth', 'autoencoders', 'work', 'types', 'constraints', 'imposed', 'implement', 'using', 'TensorFlow', 'whether', 'dimensionality', 'reduction', 'feature', 'extraction', 'unsupervised', 'pretraining', 'generative', 'models', '.', '4111ƒPerception', 'chess', '⁄', 'W.', 'Chase', 'H.', 'Simon', '1973', '.', 'E…cient', 'Data', 'RepresentationsWhich', 'following', 'number', 'sequences', 'find', 'easiest', 'memorize', '?', '‹40', '27', '25', '36', '81', '57', '10', '73', '19', '68‹50', '25', '76', '38', '19', '58', '29', '88', '44', '22', '11', '34', '17', '52', '26', '13', '40', '20At', 'first', 'glance', 'would', 'seem', 'first', 'sequence', 'easier', 'since', 'much', 'shorter', '.', 'However', 'look', 'carefully', 'second', 'sequence', 'may', 'notice', 'follows', 'two', 'simple', 'rules', 'even', 'numbers', 'followed', 'half', 'odd', 'numbers', 'followed', 'triple', 'plus', 'one', 'famous', 'sequence', 'known', 'hailstone', 'sequence', '.', 'Once', 'notice', 'pattern', 'second', 'sequence', 'becomes', 'much', 'easier', 'memorize', 'first', 'need', 'memorize', 'two', 'rules', 'first', 'number', 'length', 'sequence', '.', 'Note', 'could', 'quickly', 'easily', 'memorize', 'long', 'sequences', 'would', 'care', 'much', 'existence', 'pattern', 'second', 'sequence', '.', 'You', 'would', 'learn', 'every', 'number', 'heart', 'would', '.', 'It', 'fact', 'hard', 'memorize', 'long', 'sequences', 'makes', 'useful', 'recognize', 'patterns', 'hopefully', 'clarifies', 'constraining', 'autoen…', 'coder', 'training', 'pushes', 'discover', 'exploit', 'patterns', 'data', '.', 'The', 'relationship', 'memory', 'perception', 'pattern', 'matching', 'famouslystudied', 'William', 'Chase', 'Herbert', 'Simon', 'early', '1970s', '.1', 'They', 'observed', 'expert', 'chess', 'players', 'able', 'memorize', 'positions', 'pieces', 'game', 'looking', 'board', '5', 'seconds', 'task', 'people', 'would', 'find', 'impossible', '.', 'However', 'case', 'pieces', 'placed', 'realistic', 'positions', 'actual', 'games', 'pieces', 'placed', 'randomly', '.', 'Chess', 'experts', 'don‡t', 'much', 'better', 'memory', 'I', 'see', 'chess', 'patterns', 'easily', 'thanks', 'experience', 'game', '.', 'Noticing', 'patterns', 'helps', 'store', 'infor…', 'mation', 'efficiently', '.', 'Just', 'like', 'chess', 'players', 'memory', 'experiment', 'autoencoder', 'looks', 'inputs', 'converts', 'efficient', 'internal', 'representation', 'spits', 'some…', 'thing', 'hopefully', 'looks', 'close', 'inputs', '.', 'An', 'autoencoder', 'always', 'com…', 'posed', 'two', 'parts', 'encoder', 'recognition', 'network', 'converts', 'inputs', 'internal', 'representation', 'followed', 'decoder', 'generative', 'network', 'converts', 'internal', 'representation', 'outputs', 'see', 'Figure', '15-1', '.As', 'see', 'autoencoder', 'typically', 'architecture', 'Multi-Layer', 'Perceptron', 'MLP', 'see', 'Chapter', '10', 'except', 'number', 'neurons', 'output', 'layer', 'must', 'equal', 'number', 'inputs', '.', 'In', 'example', 'one', 'hidden', '412', '|', 'Chapter', '15', 'Autoencoders', 'layer', 'composed', 'two', 'neurons', 'encoder', 'one', 'output', 'layer', 'composed', 'three', 'neurons', 'decoder', '.', 'The', 'outputs', 'often', 'called', 'reconstructions', 'since', 'autoencoder', 'tries', 'reconstruct', 'inputs', 'cost', 'function', 'contains', 'recon…', 'struction', 'loss', 'penalizes', 'model', 'reconstructions', 'different', 'inputs', '.', 'Figure', '15-1', '.', '•e', 'chess', 'memory', 'experiment', 'le', '“', 'simple', 'autoencoder', 'right', 'Because', 'internal', 'representation', 'lower', 'dimensionality', 'input', 'data', '2D', 'instead', '3D', 'autoencoder', 'said', 'undercomplete', '.', 'An', 'undercomplete', 'autoencoder', 'trivially', 'copy', 'inputs', 'codings', 'yet', 'must', 'find', 'way', 'output', 'copy', 'inputs', '.', 'It', 'forced', 'learn', 'important', 'features', 'input', 'data', 'drop', 'unimportant', 'ones', '.', 'Let‡s', 'see', 'implement', 'simple', 'undercomplete', 'autoencoder', 'dimension…', 'ality', 'reduction.Performing', 'PCA', 'Undercomplete', 'LinearAutoencoderIf', 'autoencoder', 'uses', 'linear', 'activations', 'cost', 'function', 'Mean', 'Squared', 'Error', 'MSE', 'shown', 'ends', 'performing', 'Principal', 'Component', 'Analysis', 'see', 'Chapter', '8', '.The', 'following', 'code', 'builds', 'simple', 'linear', 'autoencoder', 'perform', 'PCA', '3D', 'data…', 'set', 'projecting', '2D', 'import', 'tensorflow', 'tffrom', 'tensorflow.contrib.layers', 'import', 'fully_connectedn_inputs', '=', '3', '#', '3D', 'inputsn_hidden', '=', '2', '#', '2D', 'codingsPerforming', 'PCA', 'Undercomplete', 'Linear', 'Autoencoder', '|', '413', 'n_outputs', '=', 'n_inputslearning_rate', '=', '0.01X', '=', 'tf.placeholder', 'tf.float32', 'shape=', 'None', 'n_inputs', 'hidden', '=', 'fully_connected', 'X', 'n_hidden', 'activation_fn=None', 'outputs', '=', 'fully_connected', 'hidden', 'n_outputs', 'activation_fn=None', 'reconstruction_loss', '=', 'tf.reduce_mean', 'tf.square', 'outputs', '-', 'X', '#', 'MSEoptimizer', '=', 'tf.train.AdamOptimizer', 'learning_rate', 'training_op', '=', 'optimizer.minimize', 'reconstruction_loss', 'init', '=', 'tf.global_variables_initializer', 'This', 'code', 'really', 'different', 'MLPs', 'built', 'past', 'chapters', '.', 'The', 'two', 'things', 'note', '‹The', 'number', 'outputs', 'equal', 'number', 'inputs', '.', '‹To', 'perform', 'simple', 'PCA', 'set', 'activation_fn=None', 'i.e.', 'neurons', 'linear', 'cost', 'function', 'MSE', '.', 'We', 'see', 'complex', 'autoencoders', 'shortly', '.', 'Now', 'let‡s', 'load', 'dataset', 'train', 'model', 'training', 'set', 'use', 'encode', 'test', 'set', 'i.e.', 'project', '2D', 'X_train', 'X_test', '=', '...', '#', 'load', 'datasetn_iterations', '=', '1000codings', '=', 'hidden', '#', 'output', 'hidden', 'layer', 'provides', 'codingswith', 'tf.Session', 'sess', 'init.run', 'iteration', 'range', 'n_iterations', 'training_op.run', 'feed_dict=', '{', 'X', 'X_train', '}', '#', 'labels', 'unsupervised', 'codings_val', '=', 'codings.eval', 'feed_dict=', '{', 'X', 'X_test', '}', 'Figure', '15-2', 'shows', 'original', '3D', 'dataset', 'left', 'output', 'autoen…', 'coder‡s', 'hidden', 'layer', 'i.e.', 'coding', 'layer', 'right', '.', 'As', 'see', 'autoen…', 'coder', 'found', 'best', '2D', 'plane', 'project', 'data', 'onto', 'preserving', 'much', 'variance', 'data', 'could', 'like', 'PCA', '.', '414', '|', 'Chapter', '15', 'Autoencoders', 'Figure', '15-2', '.', 'PCA', 'performed', 'undercomplete', 'linear', 'autoencoder', 'Stacked', 'AutoencodersJust', 'like', 'neural', 'networks', 'discussed', 'autoencoders', 'multiple', 'hidden', 'layers', '.', 'In', 'case', 'called', 'stacked', 'autoencoders', 'deep', 'autoencoders', '.', 'Adding', 'layers', 'helps', 'autoencoder', 'learn', 'complex', 'codings', '.', 'However', 'one', 'must', 'careful', 'make', 'autoencoder', 'powerful', '.', 'Imagine', 'encoder', 'powerful', 'learns', 'map', 'input', 'single', 'arbitrary', 'number', 'decoder', 'learns', 'reverse', 'mapping', '.', 'Obviously', 'autoencoder', 'reconstruct', 'training', 'data', 'perfectly', 'learned', 'useful', 'data', 'representation', 'process', 'unlikely', 'generalize', 'well', 'new', 'instances', '.The', 'architecture', 'stacked', 'autoencoder', 'typically', 'symmetrical', 'regards', 'central', 'hidden', 'layer', 'coding', 'layer', '.', 'To', 'put', 'simply', 'looks', 'like', 'sandwich', '.', 'For', 'example', 'autoencoder', 'MNIST', 'introduced', 'Chapter', '3', 'may', '784', 'inputs', 'followed', 'hidden', 'layer', '300', 'neurons', 'central', 'hidden', 'layer', '150', 'neu…', 'rons', 'another', 'hidden', 'layer', '300', 'neurons', 'output', 'layer', '784', 'neu…', 'rons', '.', 'This', 'stacked', 'autoencoder', 'represented', 'Figure', '15-3.Figure', '15-3', '.', 'Stacked', 'autoencoder', 'Stacked', 'Autoencoders', '|', '415', 'TensorFlow', 'ImplementationYou', 'implement', 'stacked', 'autoencoder', 'much', 'like', 'regular', 'deep', 'MLP', '.', 'In', 'par…', 'ticular', 'techniques', 'used', 'Chapter', '11', 'training', 'deep', 'nets', 'beapplied', '.', 'For', 'example', 'following', 'code', 'builds', 'stacked', 'autoencoder', 'MNIST', 'using', 'He', 'initialization', 'ELU', 'activation', 'function', '—', '2', 'regularization', '.', 'The', 'code', 'look', 'familiar', 'except', 'labels', 'n_inputs', '=', '28', '*', '28', '#', 'MNISTn_hidden1', '=', '300n_hidden2', '=', '150', '#', 'codingsn_hidden3', '=', 'n_hidden1n_outputs', '=', 'n_inputslearning_rate', '=', '0.01l2_reg', '=', '0.001X', '=', 'tf.placeholder', 'tf.float32', 'shape=', 'None', 'n_inputs', 'tf.contrib.framework.arg_scope', 'fully_connected', 'activation_fn=tf.nn.elu', 'weights_initializer=tf.contrib.layers.variance_scaling_initializer', 'weights_regularizer=tf.contrib.layers.l2_regularizer', 'l2_reg', 'hidden1', '=', 'fully_connected', 'X', 'n_hidden1', 'hidden2', '=', 'fully_connected', 'hidden1', 'n_hidden2', '#', 'codings', 'hidden3', '=', 'fully_connected', 'hidden2', 'n_hidden3', 'outputs', '=', 'fully_connected', 'hidden3', 'n_outputs', 'activation_fn=None', 'reconstruction_loss', '=', 'tf.reduce_mean', 'tf.square', 'outputs', '-', 'X', '#', 'MSEreg_losses', '=', 'tf.get_collection', 'tf.GraphKeys.REGULARIZATION_LOSSES', 'loss', '=', 'tf.add_n', 'reconstruction_loss', '+', 'reg_losses', 'optimizer', '=', 'tf.train.AdamOptimizer', 'learning_rate', 'training_op', '=', 'optimizer.minimize', 'loss', 'init', '=', 'tf.global_variables_initializer', 'You', 'train', 'model', 'normally', '.', 'Note', 'digit', 'labels', 'y_batch', 'areunused', 'n_epochs', '=', '5batch_size', '=', '150with', 'tf.Session', 'sess', 'init.run', 'epoch', 'range', 'n_epochs', 'n_batches', '=', 'mnist.train.num_examples', '//', 'batch_size', 'iteration', 'range', 'n_batches', 'X_batch', 'y_batch', '=', 'mnist.train.next_batch', 'batch_size', 'sess.run', 'training_op', 'feed_dict=', '{', 'X', 'X_batch', '}', '416', '|', 'Chapter', '15', 'Autoencoders', 'Tying', 'WeightsWhen', 'autoencoder', 'neatly', 'symmetrical', 'like', 'one', 'built', 'common', 'technique', 'tie', 'weights', 'decoder', 'layers', 'weights', 'encoder', 'lay…', 'ers', '.', 'This', 'halves', 'number', 'weights', 'model', 'speeding', 'training', 'limit…', 'ing', 'risk', 'overfitting', '.', 'Specifically', 'autoencoder', 'total', 'N', 'layers', 'counting', 'input', 'layer', 'WL', 'represents', 'connection', 'weights', 'Lth', 'layer', 'e.g.', 'layer', '1', 'first', 'hidden', 'layer', 'layer', 'N2', 'coding', 'layer', 'layer', 'N', 'theoutput', 'layer', 'decoder', 'layer', 'weights', 'defined', 'simply', 'WN', '‘', 'L+1', '=', 'WLT', 'L', '=', '1', '2', 'N2', '.Unfortunately', 'implementing', 'tied', 'weights', 'TensorFlow', 'using', 'fully_connected', 'function', 'bit', 'cumbersome', 'it‡s', 'actually', 'easier', 'define', 'layers', 'man…', 'ually', '.', 'The', 'code', 'ends', 'significantly', 'verbose', 'activation', '=', 'tf.nn.eluregularizer', '=', 'tf.contrib.layers.l2_regularizer', 'l2_reg', 'initializer', '=', 'tf.contrib.layers.variance_scaling_initializer', 'X', '=', 'tf.placeholder', 'tf.float32', 'shape=', 'None', 'n_inputs', 'weights1_init', '=', 'initializer', 'n_inputs', 'n_hidden1', 'weights2_init', '=', 'initializer', 'n_hidden1', 'n_hidden2', 'weights1', '=', 'tf.Variable', 'weights1_init', 'dtype=tf.float32', 'name=', \"''\", 'weights1', \"''\", 'weights2', '=', 'tf.Variable', 'weights2_init', 'dtype=tf.float32', 'name=', \"''\", 'weights2', \"''\", 'weights3', '=', 'tf.transpose', 'weights2', 'name=', \"''\", 'weights3', \"''\", '#', 'tied', 'weightsweights4', '=', 'tf.transpose', 'weights1', 'name=', \"''\", 'weights4', \"''\", '#', 'tied', 'weightsbiases1', '=', 'tf.Variable', 'tf.zeros', 'n_hidden1', 'name=', \"''\", 'biases1', \"''\", 'biases2', '=', 'tf.Variable', 'tf.zeros', 'n_hidden2', 'name=', \"''\", 'biases2', \"''\", 'biases3', '=', 'tf.Variable', 'tf.zeros', 'n_hidden3', 'name=', \"''\", 'biases3', \"''\", 'biases4', '=', 'tf.Variable', 'tf.zeros', 'n_outputs', 'name=', \"''\", 'biases4', \"''\", 'hidden1', '=', 'activation', 'tf.matmul', 'X', 'weights1', '+', 'biases1', 'hidden2', '=', 'activation', 'tf.matmul', 'hidden1', 'weights2', '+', 'biases2', 'hidden3', '=', 'activation', 'tf.matmul', 'hidden2', 'weights3', '+', 'biases3', 'outputs', '=', 'tf.matmul', 'hidden3', 'weights4', '+', 'biases4reconstruction_loss', '=', 'tf.reduce_mean', 'tf.square', 'outputs', '-', 'X', 'reg_loss', '=', 'regularizer', 'weights1', '+', 'regularizer', 'weights2', 'loss', '=', 'reconstruction_loss', '+', 'reg_lossoptimizer', '=', 'tf.train.AdamOptimizer', 'learning_rate', 'training_op', '=', 'optimizer.minimize', 'loss', 'init', '=', 'tf.global_variables_initializer', 'This', 'code', 'fairly', 'straightforward', 'important', 'things', 'note', 'Stacked', 'Autoencoders', '|', '417', '‹First', 'weight3', 'weights4', 'variables', 'respectively', 'transposeof', 'weights2', 'weights1', 'ƒtied⁄', '.', '‹Second', 'since', 'variables', 'it‡s', 'use', 'regularizing', 'regula…', 'rize', 'weights1', 'weights2.‹Third', 'biases', 'never', 'tied', 'never', 'regularized.Training', 'One', 'Autoencoder', 'Time', 'Rather', 'training', 'whole', 'stacked', 'autoencoder', 'one', 'go', 'like', 'often', 'much', 'faster', 'train', 'one', 'shallow', 'autoencoder', 'time', 'stack', 'single', 'stacked', 'autoencoder', 'hence', 'name', 'shown', 'Figure', '15-4', '.', 'This', 'isespecially', 'useful', 'deep', 'autoencoders', '.', 'Figure', '15-4', '.', 'Training', 'one', 'autoencoder', 'time', 'During', 'first', 'phase', 'training', 'first', 'autoencoder', 'learns', 'reconstruct', 'inputs', '.', 'During', 'second', 'phase', 'second', 'autoencoder', 'learns', 'reconstruct', 'output', 'first', 'autoencoder‡s', 'hidden', 'layer', '.', 'Finally', 'build', 'big', 'sandwich', 'using', 'autoencoders', 'shown', 'Figure', '15-4', 'i.e.', 'first', 'stack', 'hiddenlayers', 'autoencoder', 'output', 'layers', 'reverse', 'order', '.', 'This', 'gives', 'final', 'stacked', 'autoencoder', '.', 'You', 'could', 'easily', 'train', 'autoencoders', 'way', 'building', 'deep', 'stacked', 'autoencoder', '.', 'To', 'implement', 'multiphase', 'training', 'algorithm', 'simplest', 'approach', 'use', 'different', 'TensorFlow', 'graph', 'phase', '.', 'After', 'training', 'autoencoder', 'run', 'training', 'set', 'capture', 'output', 'hidden', 'layer', '.', 'This', 'output', 'serves', 'training', 'set', 'next', 'autoencoder', '.', 'Once', 'autoencoders', 'trained', 'way', 'simply', 'copy', 'weights', 'biases', 'autoencoder', 'use', 'build', 'stacked', 'autoencoder', '.', 'Implementing', 'approach', 'quite', '418', '|', 'Chapter', '15', 'Autoencoders', 'straightforward', 'won‡t', 'detail', 'please', 'check', 'code', 'Jupyter', 'notebooks', 'example', '.', 'Another', 'approach', 'use', 'single', 'graph', 'containing', 'whole', 'stacked', 'autoencoder', 'plus', 'extra', 'operations', 'perform', 'training', 'phase', 'shown', 'Figure', '15-5.Figure', '15-5', '.', 'A', 'single', 'graph', 'train', 'stacked', 'autoencoder', 'This', 'deserves', 'bit', 'explanation', '‹The', 'central', 'column', 'graph', 'full', 'stacked', 'autoencoder', '.', 'This', 'part', 'used', 'training.‹The', 'left', 'column', 'set', 'operations', 'needed', 'run', 'first', 'phase', 'training', '.', 'It', 'creates', 'output', 'layer', 'bypasses', 'hidden', 'layers', '2', '3', '.', 'This', 'output', 'layer', 'shares', 'weights', 'biases', 'stacked', 'autoencoder‡s', 'output', 'layer', '.', 'On', 'top', 'training', 'operations', 'aim', 'making', 'output', 'close', 'possible', 'inputs', '.', 'Thus', 'phase', 'train', 'weights', 'biases', 'hidden', 'layer', '1', 'output', 'layer', 'i.e.', 'first', 'autoencoder', '.', '‹The', 'right', 'column', 'graph', 'set', 'operations', 'needed', 'run', 'second', 'phase', 'training', '.', 'It', 'adds', 'training', 'operation', 'aim', 'making', 'out…', 'put', 'hidden', 'layer', '3', 'close', 'possible', 'output', 'hidden', 'layer', '1', '.', 'Note', 'must', 'freeze', 'hidden', 'layer', '1', 'running', 'phase', '2', '.', 'This', 'phase', 'train', 'weights', 'biases', 'hidden', 'layers', '2', '3', 'i.e.', 'second', 'autoencoder', '.', 'The', 'TensorFlow', 'code', 'looks', 'like', '...', '#', 'Build', 'whole', 'stacked', 'autoencoder', 'normally', '.', '#', 'In', 'example', 'weights', 'tied.Stacked', 'Autoencoders', '|', '419', 'optimizer', '=', 'tf.train.AdamOptimizer', 'learning_rate', 'tf.name_scope', '``', 'phase1', \"''\", 'phase1_outputs', '=', 'tf.matmul', 'hidden1', 'weights4', '+', 'biases4', 'phase1_reconstruction_loss', '=', 'tf.reduce_mean', 'tf.square', 'phase1_outputs', '-', 'X', 'phase1_reg_loss', '=', 'regularizer', 'weights1', '+', 'regularizer', 'weights4', 'phase1_loss', '=', 'phase1_reconstruction_loss', '+', 'phase1_reg_loss', 'phase1_training_op', '=', 'optimizer.minimize', 'phase1_loss', 'tf.name_scope', '``', 'phase2', \"''\", 'phase2_reconstruction_loss', '=', 'tf.reduce_mean', 'tf.square', 'hidden3', '-', 'hidden1', 'phase2_reg_loss', '=', 'regularizer', 'weights2', '+', 'regularizer', 'weights3', 'phase2_loss', '=', 'phase2_reconstruction_loss', '+', 'phase2_reg_loss', 'train_vars', '=', 'weights2', 'biases2', 'weights3', 'biases3', 'phase2_training_op', '=', 'optimizer.minimize', 'phase2_loss', 'var_list=train_vars', 'The', 'first', 'phase', 'rather', 'straightforward', 'create', 'output', 'layer', 'skips', 'hid…', 'den', 'layers', '2', '3', 'build', 'training', 'operations', 'minimize', 'distance', 'outputs', 'inputs', 'plus', 'regularization', '.', 'The', 'second', 'phase', 'adds', 'operations', 'needed', 'minimize', 'distance', 'output', 'hidden', 'layer', '3', 'hidden', 'layer', '1', 'also', 'regularization', '.', 'Most', 'importantly', 'provide', 'list', 'trainable', 'variables', 'minimize', 'method', 'making', 'sure', 'leave', 'weights1', 'biases1', 'effectively', 'freezes', 'hidden', 'layer', '1', 'phase', '2.During', 'execution', 'phase', 'need', 'run', 'phase', '1', 'training', 'op', 'anumber', 'epochs', 'phase', '2', 'training', 'op', 'epochs', '.', 'Since', 'hidden', 'layer', '1', 'frozen', 'phase', '2', 'output', 'always', 'given', 'training', 'instance', '.', 'To', 'avoid', 'recompute', 'output', 'hidden', 'layer', '1', 'every', 'single', 'epoch', 'compute', 'whole', 'training', 'set', 'end', 'phase', '1', 'directly', 'feed', 'cached', 'output', 'hidden', 'layer', '1', 'phase', '2', '.', 'This', 'give', 'nice', 'performance', 'boost.Visualizing', 'ReconstructionsOne', 'way', 'ensure', 'autoencoder', 'properly', 'trained', 'compare', 'inputs', 'outputs', '.', 'They', 'must', 'fairly', 'similar', 'differences', 'unimpor…', 'tant', 'details', '.', 'Let‡s', 'plot', 'two', 'random', 'digits', 'reconstructions', 'n_test_digits', '=', '2X_test', '=', 'mnist.test.images', 'n_test_digits', 'tf.Session', 'sess', '...', '#', 'Train', 'Autoencoder', 'outputs_val', '=', 'outputs.eval', 'feed_dict=', '{', 'X', 'X_test', '}', '420', '|', 'Chapter', '15', 'Autoencoders', 'def', 'plot_image', 'image', 'shape=', '28', '28', 'plt.imshow', 'image.reshape', 'shape', 'cmap=', \"''\", 'Greys', \"''\", 'interpolation=', \"''\", 'nearest', \"''\", 'plt.axis', '``', \"''\", 'digit_index', 'range', 'n_test_digits', 'plt.subplot', 'n_test_digits', '2', 'digit_index', '*', '2', '+', '1', 'plot_image', 'X_test', 'digit_index', 'plt.subplot', 'n_test_digits', '2', 'digit_index', '*', '2', '+', '2', 'plot_image', 'outputs_val', 'digit_index', 'Figure', '15-6', 'shows', 'resulting', 'images.Figure', '15-6', '.', 'Original', 'digits', 'le', '“', 'reconstructions', 'right', 'Looks', 'close', 'enough', '.', 'So', 'autoencoder', 'properly', 'learned', 'reproduce', 'inputs', 'learned', 'useful', 'features', '?', 'Let‡s', 'take', 'look', '.', 'Visualizing', 'FeaturesOnce', 'autoencoder', 'learned', 'features', 'may', 'want', 'take', 'look', '.', 'There', 'various', 'techniques', '.', 'Arguably', 'simplest', 'technique', 'consider', 'neuron', 'every', 'hidden', 'layer', 'find', 'training', 'instances', 'acti…', 'vate', '.', 'This', 'especially', 'useful', 'top', 'hidden', 'layers', 'since', 'often', 'capture', 'relatively', 'large', 'features', 'easily', 'spot', 'group', 'training', 'instan…', 'ces', 'contain', '.', 'For', 'example', 'neuron', 'strongly', 'activates', 'sees', 'cat', 'picture', 'pretty', 'obvious', 'pictures', 'activate', 'contain', 'cats', '.', 'However', 'lower', 'layers', 'technique', 'work', 'well', 'features', 'smaller', 'abstract', 'it‡s', 'often', 'hard', 'understand', 'exactly', 'neu…', 'ron', 'getting', 'excited', 'about.Let‡s', 'look', 'another', 'technique', '.', 'For', 'neuron', 'first', 'hidden', 'layer', 'cre…', 'ate', 'image', 'pixel‡s', 'intensity', 'corresponds', 'weight', 'connection', 'given', 'neuron', '.', 'For', 'example', 'following', 'code', 'plots', 'features', 'learned', 'five', 'neurons', 'first', 'hidden', 'layer', 'tf.Session', 'sess', '...', '#', 'train', 'autoencoderStacked', 'Autoencoders', '|', '421', 'weights1_val', '=', 'weights1.eval', 'range', '5', 'plt.subplot', '1', '5', '+', '1', 'plot_image', 'weights1_val.T', 'You', 'may', 'get', 'low-level', 'features', 'ones', 'shown', 'Figure', '15-7.Figure', '15-7', '.', 'Features', 'learned', '†ve', 'neurons', '†rst', 'hidden', 'layer', 'The', 'first', 'four', 'features', 'seem', 'correspond', 'small', 'patches', 'fifth', 'feature', 'seems', 'look', 'vertical', 'strokes', 'note', 'features', 'come', 'stackeddenoising', 'autoencoder', 'discuss', 'later', '.', 'Another', 'technique', 'feed', 'autoencoder', 'random', 'input', 'image', 'measure', 'activation', 'neuron', 'interested', 'perform', 'backpropagation', 'totweak', 'image', 'way', 'neuron', 'activate', 'even', '.', 'If', 'iterate', 'several', 'times', 'performing', 'gradient', 'ascent', 'image', 'gradually', 'turn', 'exciting', 'image', 'neuron', '.', 'This', 'useful', 'technique', 'visualize', 'kindsof', 'inputs', 'neuron', 'looking', '.', 'Finally', 'using', 'autoencoder', 'perform', 'unsupervised', 'pretraining›for', 'example', 'classification', 'task›a', 'simple', 'way', 'verify', 'features', 'learned', 'autoencoder', 'useful', 'measure', 'performance', 'classifier', '.', 'Unsupervised', 'Pretraining', 'Using', 'Stacked', 'AutoencodersAs', 'discussed', 'Chapter', '11', 'tackling', 'complex', 'supervised', 'task', 'lot', 'labeled', 'training', 'data', 'one', 'solution', 'find', 'neural', 'network', 'performs', 'similar', 'task', 'reuse', 'lower', 'layers', '.', 'This', 'makes', 'possible', 'train', 'high-performance', 'model', 'using', 'little', 'training', 'data', 'neural', 'net…', 'work', 'won‡t', 'learn', 'low-level', 'features', 'reuse', 'feature', 'detec…', 'tors', 'learned', 'existing', 'net.Similarly', 'large', 'dataset', 'unlabeled', 'first', 'train', 'stacked', 'autoencoder', 'using', 'data', 'reuse', 'lower', 'layers', 'create', 'neural', 'network', 'actual', 'task', 'train', 'using', 'labeled', 'data', '.', 'For', 'example', 'Figure', '15-8', 'shows', 'use', 'stacked', 'autoencoder', 'perform', 'unsupervised', 'pre…', 'training', 'classification', 'neural', 'network', '.', 'The', 'stacked', 'autoencoder', 'typically', 'trained', 'one', 'autoencoder', 'time', 'discussed', 'earlier', '.', 'When', 'training', 'classifier', '422', '|', 'Chapter', '15', 'Autoencoders', '2ƒGreedy', 'Layer-Wise', 'Training', 'Deep', 'Networks', '⁄', 'Y.', 'Bengio', 'et', 'al', '.', '2007', '.', 'really', 'don‡t', 'much', 'labeled', 'training', 'data', 'may', 'want', 'freeze', 'pre…', 'trained', 'layers', 'least', 'lower', 'ones', '.', 'Figure', '15-8', '.', 'Unsupervised', 'pretraining', 'using', 'autoencoders', 'This', 'situation', 'actually', 'quite', 'common', 'building', 'large', 'unlabeled', 'dataset', 'often', 'cheap', 'e.g.', 'simple', 'script', 'download', 'millions', 'images', 'internet', 'labeling', 'done', 'reliably', 'humans', 'e.g.', 'classifying', 'images', 'cute', '.', 'Labeling', 'instances', 'time-consuming', 'costly', 'quite', 'com…', 'mon', 'thousand', 'labeled', 'instances', '.', 'As', 'discussed', 'earlier', 'one', 'triggers', 'current', 'Deep', 'Learning', 'tsunami', 'discovery', '2006', 'Geoffrey', 'Hinton', 'et', 'al', '.', 'deep', 'neural', 'networks', 'pre…', 'trained', 'unsupervised', 'fashion', '.', 'They', 'used', 'restricted', 'Boltzmann', 'machines', 'see', 'Appendix', 'E', '2007', 'Yoshua', 'Bengio', 'et', 'al', '.', 'showed', '2', 'autoencoders', 'worked', 'well.There', 'nothing', 'special', 'TensorFlow', 'implementation', 'train', 'autoen…', 'coder', 'using', 'training', 'data', 'reuse', 'encoder', 'layers', 'create', 'new', 'neural', 'network', 'see', 'Chapter', '11', 'details', 'reuse', 'pretrained', 'layers', 'check', 'code', 'examples', 'Jupyter', 'notebooks', '.', 'Up', 'order', 'force', 'autoencoder', 'learn', 'interesting', 'features', 'limited', 'size', 'coding', 'layer', 'making', 'undercomplete', '.', 'There', 'actually', 'many', 'kinds', 'constraints', 'used', 'including', 'ones', 'allow', 'cod…', 'Unsupervised', 'Pretraining', 'Using', 'Stacked', 'Autoencoders', '|', '423', '3ƒExtracting', 'Composing', 'Robust', 'Features', 'Denoising', 'Autoencoders', '⁄', 'P.', 'Vincent', 'et', 'al', '.', '2008', '.', '4ƒStacked', 'Denoising', 'Autoencoders', 'Learning', 'Useful', 'Representations', 'Deep', 'Network', 'Local', 'Denois…', 'ing', 'Criterion', '⁄', 'P.', 'Vincent', 'et', 'al', '.', '2010', '.', 'ing', 'layer', 'large', 'inputs', 'even', 'larger', 'resulting', 'overcomplete', 'autoencoder', '.', 'Let‡s', 'look', 'approaches', '.', 'Denoising', 'AutoencodersAnother', 'way', 'force', 'autoencoder', 'learn', 'useful', 'features', 'add', 'noise', 'inputs', 'training', 'recover', 'original', 'noise-free', 'inputs', '.', 'This', 'prevents', 'autoen…', 'coder', 'trivially', 'copying', 'inputs', 'outputs', 'ends', 'find', 'pat…', 'terns', 'data', '.', 'The', 'idea', 'using', 'autoencoders', 'remove', 'noise', 'around', 'since', '1980s', 'e.g.', 'mentioned', 'Yann', 'LeCun‡s', '1987', 'master‡s', 'thesis', '.', 'In', '2008', 'paper', ',3', 'Pascal', 'Vincent', 'et', 'al', '.', 'showed', 'autoencoders', 'could', 'also', 'used', 'feature', 'extraction', '.', 'In', '2010', 'paper', ',4', 'Vincent', 'et', 'al', '.', 'introduced', 'stacked', 'denoising', 'autoencoders', '.The', 'noise', 'pure', 'Gaussian', 'noise', 'added', 'inputs', 'randomly', 'switched', 'inputs', 'like', 'dropout', 'introduced', 'Chapter', '11', '.', 'Figure', '15-9shows', 'options.Figure', '15-9', '.', 'Denoising', 'autoencoders', 'Gaussian', 'noise', 'le', '“', 'dropout', 'right', '424', '|', 'Chapter', '15', 'Autoencoders', 'TensorFlow', 'ImplementationImplementing', 'denoising', 'autoencoders', 'TensorFlow', 'hard', '.', 'Let‡s', 'start', 'Gaussian', 'noise', '.', 'It‡s', 'really', 'like', 'training', 'regular', 'autoencoder', 'except', 'add', 'noise', 'inputs', 'reconstruction', 'loss', 'calculated', 'based', 'original', 'inputs', 'X', '=', 'tf.placeholder', 'tf.float32', 'shape=', 'None', 'n_inputs', 'X_noisy', '=', 'X', '+', 'tf.random_normal', 'tf.shape', 'X', '...', 'hidden1', '=', 'activation', 'tf.matmul', 'X_noisy', 'weights1', '+', 'biases1', '...', 'reconstruction_loss', '=', 'tf.reduce_mean', 'tf.square', 'outputs', '-', 'X', '#', 'MSE', '...', 'Since', 'shape', 'X', 'partially', 'defined', 'construc…tion', 'phase', 'know', 'advance', 'shape', 'noise', 'must', 'add', 'X', '.', 'We', 'call', 'X.get_shape', 'would', 'return', 'partially', 'defined', 'shape', 'X', 'None', 'n_inputs', 'random_normal', 'expects', 'fully', 'defined', 'shape', 'would', 'raise', 'exception', '.', 'Instead', 'call', 'tf.shape', 'X', 'whichcreates', 'operation', 'return', 'shape', 'X', 'runtime', 'fully', 'defined', 'point', '.', 'Implementing', 'dropout', 'version', 'common', 'much', 'harder', 'tensorflow.contrib.layers', 'import', 'dropoutkeep_prob', '=', '0.7is_training', '=', 'tf.placeholder_with_default', 'False', 'shape=', 'name=•is_training•', 'X', '=', 'tf.placeholder', 'tf.float32', 'shape=', 'None', 'n_inputs', 'X_drop', '=', 'dropout', 'X', 'keep_prob', 'is_training=is_training', '...', 'hidden1', '=', 'activation', 'tf.matmul', 'X_drop', 'weights1', '+', 'biases1', '...', 'reconstruction_loss', '=', 'tf.reduce_mean', 'tf.square', 'outputs', '-', 'X', '#', 'MSE', '...', 'During', 'training', 'must', 'set', 'is_training', 'True', 'explained', 'Chapter', '11', 'usingthe', 'feed_dict', 'sess.run', 'training_op', 'feed_dict=', '{', 'X', 'X_batch', 'is_training', 'True', '}', 'However', 'testing', 'necessary', 'set', 'is_training', 'False', 'since', 'setthat', 'default', 'call', 'placeholder_with_default', 'function.Denoising', 'Autoencoders', '|', '425', 'Sparse', 'AutoencodersAnother', 'kind', 'constraint', 'often', 'leads', 'good', 'feature', 'extraction', 'sparsity', 'byadding', 'appropriate', 'term', 'cost', 'function', 'autoencoder', 'pushed', 'reduce', 'number', 'active', 'neurons', 'coding', 'layer', '.', 'For', 'example', 'may', 'pushed', 'average', '5', '%', 'significantly', 'active', 'neurons', 'coding', 'layer', '.', 'This', 'forces', 'autoencoder', 'represent', 'input', 'combination', 'small', 'number', 'acti…', 'vations', '.', 'As', 'result', 'neuron', 'coding', 'layer', 'typically', 'ends', 'representing', 'useful', 'feature', 'could', 'speak', 'words', 'per', 'month', 'would', 'probably', 'try', 'make', 'worth', 'listening', '.', 'In', 'order', 'favor', 'sparse', 'models', 'must', 'first', 'measure', 'actual', 'sparsity', 'cod…', 'ing', 'layer', 'training', 'iteration', '.', 'We', 'computing', 'average', 'activation', 'neuron', 'coding', 'layer', 'whole', 'training', 'batch', '.', 'The', 'batch', 'size', 'must', 'small', 'else', 'mean', 'accurate', '.', 'Once', 'mean', 'activation', 'per', 'neuron', 'want', 'penalize', 'neurons', 'active', 'adding', 'sparsity', 'loss', 'cost', 'function', '.', 'For', 'example', 'meas…', 'ure', 'neuron', 'average', 'activation', '0.3', 'target', 'sparsity', '0.1', 'must', 'penalized', 'activate', 'less', '.', 'One', 'approach', 'could', 'simply', 'adding', 'squared', 'error', '0.3', '–', '0.1', '2', 'cost', 'function', 'practice', 'better', 'approach', 'use', 'Kull…', 'back–Leibler', 'divergence', 'briefly', 'discussed', 'Chapter', '4', 'much', 'stronger', 'gradients', 'Mean', 'Squared', 'Error', 'see', 'Figure', '15-10.Figure', '15-10', '.', 'Sparsity', 'loss', '426', '|', 'Chapter', '15', 'Autoencoders', 'Given', 'two', 'discrete', 'probability', 'distributions', 'P', 'Q', 'KL', 'divergence', 'betweenthese', 'distributions', 'noted', 'DKL', 'P', 'Q', 'computed', 'using', 'Equation', '15-1', '.Equation', '15-1', '.', 'Kullback', '‘', 'Leibler', 'divergence', 'DKLPQ=', '“', 'iPilogPiQiIn', 'case', 'want', 'measure', 'divergence', 'target', 'probability', 'p', 'neuron', 'coding', 'layer', 'activate', 'actual', 'probability', 'q', 'i.e.', 'meanactivation', 'training', 'batch', '.', 'So', 'KL', 'divergence', 'simplifies', 'Equation', '15-2', '.Equation', '15-2', '.', 'KL', 'divergence', 'target', 'sparsity', 'p', 'actual', 'sparsity', 'q', 'DKLpq=plogpq+1', '”', 'plog1', '”', 'p1', '”', 'qOnce', 'computed', 'sparsity', 'loss', 'neuron', 'coding', 'layer', 'sum', 'losses', 'add', 'result', 'cost', 'function', '.', 'In', 'order', 'control', 'relative', 'importance', 'sparsity', 'loss', 'reconstruction', 'loss', 'multiply', 'sparsity', 'loss', 'sparsity', 'weight', 'hyperparameter', '.', 'If', 'weight', 'high', 'model', 'stick', 'closely', 'target', 'sparsity', 'may', 'reconstruct', 'inputs', 'properly', 'making', 'model', 'useless', '.', 'Conversely', 'low', 'model', 'mostly', 'ignore', 'sparsity', 'objective', 'learn', 'interesting', 'features', '.', 'TensorFlow', 'ImplementationWe', 'need', 'implement', 'sparse', 'autoencoder', 'using', 'TensorFlow', 'def', 'kl_divergence', 'p', 'q', 'return', 'p', '*', 'tf.log', 'p', '/', 'q', '+', '1', '-', 'p', '*', 'tf.log', '1', '-', 'p', '/', '1', '-', 'q', 'learning_rate', '=', '0.01sparsity_target', '=', '0.1sparsity_weight', '=', '0.2', '...', '#', 'Build', 'normal', 'autoencoder', 'example', 'coding', 'layer', 'hidden1', 'optimizer', '=', 'tf.train.AdamOptimizer', 'learning_rate', 'hidden1_mean', '=', 'tf.reduce_mean', 'hidden1', 'axis=0', '#', 'batch', 'meansparsity_loss', '=', 'tf.reduce_sum', 'kl_divergence', 'sparsity_target', 'hidden1_mean', 'reconstruction_loss', '=', 'tf.reduce_mean', 'tf.square', 'outputs', '-', 'X', '#', 'MSEloss', '=', 'reconstruction_loss', '+', 'sparsity_weight', '*', 'sparsity_losstraining_op', '=', 'optimizer.minimize', 'loss', 'An', 'important', 'detail', 'fact', 'activations', 'coding', 'layer', 'must', '0', '1', 'equal', '0', '1', 'else', 'KL', 'divergence', 'return', 'NaN', 'Sparse', 'Autoencoders', '|', '427', '5ƒAuto-Encoding', 'Variational', 'Bayes', '⁄', 'D.', 'Kingma', 'M.', 'Welling', '2014', '.', 'Not', 'Number', '.', 'A', 'simple', 'solution', 'use', 'logistic', 'activation', 'function', 'coding', 'layer', 'hidden1', '=', 'tf.nn.sigmoid', 'tf.matmul', 'X', 'weights1', '+', 'biases1', 'One', 'simple', 'trick', 'speed', 'convergence', 'instead', 'using', 'MSE', 'choose', 'reconstruction', 'loss', 'larger', 'gradients', '.', 'Cross', 'entropy', 'often', 'good', 'choice', '.', 'To', 'use', 'must', 'normalize', 'inputs', 'make', 'take', 'values', '0', '1', 'use', 'logistic', 'activation', 'function', 'output', 'layer', 'outputs', 'also', 'take', 'values', '0', '1', '.', 'TensorFlow‡s', 'sigmoid_cross_entropy_with_logits', 'function', 'takes', 'care', 'efficiently', 'applying', 'logistic', 'sigmoid', 'activation', 'function', 'outputs', 'computing', 'cross', 'entropy', '...', 'logits', '=', 'tf.matmul', 'hidden1', 'weights2', '+', 'biases2', 'outputs', '=', 'tf.nn.sigmoid', 'logits', 'reconstruction_loss', '=', 'tf.reduce_sum', 'tf.nn.sigmoid_cross_entropy_with_logits', 'labels=X', 'logits=logits', 'Note', 'outputs', 'operation', 'needed', 'training', 'use', 'want', 'look', 'reconstructions', '.', 'Variational', 'AutoencodersAnother', 'important', 'category', 'autoencoders', 'introduced', '2014', 'DiederikKingma', 'Max', 'Welling', '5', 'quickly', 'become', 'one', 'popular', 'types', 'ofautoencoders', 'variational', 'autoencoders', '.They', 'quite', 'different', 'autoencoders', 'discussed', 'far', 'partic…', 'ular', '‹They', 'probabilistic', 'autoencoders', 'meaning', 'outputs', 'partly', 'deter…', 'mined', 'chance', 'even', 'training', 'opposed', 'denoising', 'autoencoders', 'use', 'randomness', 'training', '.‹Most', 'importantly', 'generative', 'autoencoders', 'meaning', 'gener…', 'ate', 'new', 'instances', 'look', 'like', 'sampled', 'training', 'set', '.', 'Both', 'properties', 'make', 'rather', 'similar', 'RBMs', 'see', 'Appendix', 'E', 'theyare', 'easier', 'train', 'sampling', 'process', 'much', 'faster', 'RBMs', 'need', 'wait', 'network', 'stabilize', 'ƒthermal', 'equilibrium⁄', 'sample', 'new', 'instance', '.428', '|', 'Chapter', '15', 'Autoencoders', '6Variational', 'autoencoders', 'actually', 'general', 'codings', 'limited', 'Gaussian', 'distributions', '.', 'Let‡s', 'take', 'look', 'work', '.', 'Figure', '15-11', 'left', 'shows', 'variational', 'autoen…', 'coder', '.', 'You', 'recognize', 'course', 'basic', 'structure', 'autoencoders', 'encoder', 'followed', 'decoder', 'example', 'two', 'hidden', 'layers', 'twist', 'instead', 'directly', 'producing', 'coding', 'given', 'input', 'encoder', 'produces', 'mean', 'coding', 'ﬂ', 'standard', 'deviation', '„', '.', 'The', 'actual', 'coding', 'isthen', 'sampled', 'randomly', 'Gaussian', 'distribution', 'mean', 'ﬂ', 'standard', 'devi…', 'ation', '„', '.', 'After', 'decoder', 'decodes', 'sampled', 'coding', 'normally', '.', 'The', 'right', 'part', 'diagram', 'shows', 'training', 'instance', 'going', 'autoencoder', '.', 'First', 'encoder', 'produces', 'ﬂ', '„', 'coding', 'sampled', 'randomly', 'notice', 'exactly', 'located', 'ﬂ', 'finally', 'coding', 'decoded', 'final', 'outputresembles', 'training', 'instance.Figure', '15-11', '.', 'Variational', 'autoencoder', 'le', '“', 'instance', 'going', 'right', 'As', 'see', 'diagram', 'although', 'inputs', 'may', 'convoluted', 'dis…', 'tribution', 'variational', 'autoencoder', 'tends', 'produce', 'codings', 'look', 'though', 'sampled', 'simple', 'Gaussian', 'distribution', '6', 'training', 'costfunction', 'discussed', 'next', 'pushes', 'codings', 'gradually', 'migrate', 'within', 'codingspace', 'also', 'called', 'latent', 'space', 'occupy', 'roughly', 'hyper', 'spherical', 'region', 'looks', 'like', 'cloud', 'Gaussian', 'points', '.', 'One', 'great', 'consequence', 'training', 'Variational', 'Autoencoders', '|', '429', '7For', 'mathematical', 'details', 'check', 'original', 'paper', 'variational', 'autoencoders', 'Carl', 'Doersch‡s', 'great', 'tutorial', '2016', '.variational', 'autoencoder', 'easily', 'generate', 'new', 'instance', 'sample', 'random', 'coding', 'Gaussian', 'distribution', 'decode', 'voilÉ', '!', 'So', 'let‡s', 'look', 'cost', 'function', '.', 'It', 'composed', 'two', 'parts', '.', 'The', 'first', 'usual', 'reconstruction', 'loss', 'pushes', 'autoencoder', 'reproduce', 'inputs', 'use', 'cross', 'entropy', 'discussed', 'earlier', '.', 'The', 'second', 'latent', 'loss', 'pushes', 'autoencoder', 'codings', 'look', 'though', 'sampled', 'simple', 'Gaussian', 'distribution', 'use', 'KL', 'divergence', 'target', 'distri…', 'bution', 'Gaussian', 'distribution', 'actual', 'distribution', 'codings', '.', 'The', 'math', 'bit', 'complex', 'earlier', 'particular', 'Gaussian', 'noise', 'limits', 'amount', 'information', 'transmitted', 'coding', 'layer', 'thus', 'pushing', 'autoencoder', 'learn', 'useful', 'features', '.', 'Luckily', 'equations', 'sim…plify', 'following', 'code', 'latent', 'loss', '7eps', '=', '1e-10', '#', 'smoothing', 'term', 'avoid', 'computing', 'log', '0', 'NaNlatent_loss', '=', '0.5', '*', 'tf.reduce_sum', 'tf.square', 'hidden3_sigma', '+', 'tf.square', 'hidden3_mean', '-', '1', '-', 'tf.log', 'eps', '+', 'tf.square', 'hidden3_sigma', 'One', 'common', 'variant', 'train', 'encoder', 'output', '’', '=', 'log', '„2', 'rather', '„.Wherever', 'need', '„', 'compute', '„=exp', '’', '2', '.', 'This', 'makes', 'bit', 'easier', 'forthe', 'encoder', 'capture', 'sigmas', 'different', 'scales', 'thus', 'helps', 'speed', 'conver…', 'gence', '.', 'The', 'latent', 'loss', 'ends', 'bit', 'simpler', 'latent_loss', '=', '0.5', '*', 'tf.reduce_sum', 'tf.exp', 'hidden3_gamma', '+', 'tf.square', 'hidden3_mean', '-', '1', '-', 'hidden3_gamma', 'The', 'following', 'code', 'builds', 'variational', 'autoencoder', 'shown', 'Figure', '15-11', 'left', 'using', 'log', '„2', 'variant', 'n_inputs', '=', '28', '*', '28', '#', 'MNISTn_hidden1', '=', '500n_hidden2', '=', '500n_hidden3', '=', '20', '#', 'codingsn_hidden4', '=', 'n_hidden2n_hidden5', '=', 'n_hidden1n_outputs', '=', 'n_inputslearning_rate', '=', '0.001with', 'tf.contrib.framework.arg_scope', 'fully_connected', 'activation_fn=tf.nn.elu', 'weights_initializer=tf.contrib.layers.variance_scaling_initializer', 'X', '=', 'tf.placeholder', 'tf.float32', 'None', 'n_inputs', '430', '|', 'Chapter', '15', 'Autoencoders', 'hidden1', '=', 'fully_connected', 'X', 'n_hidden1', 'hidden2', '=', 'fully_connected', 'hidden1', 'n_hidden2', 'hidden3_mean', '=', 'fully_connected', 'hidden2', 'n_hidden3', 'activation_fn=None', 'hidden3_gamma', '=', 'fully_connected', 'hidden2', 'n_hidden3', 'activation_fn=None', 'hidden3_sigma', '=', 'tf.exp', '0.5', '*', 'hidden3_gamma', 'noise', '=', 'tf.random_normal', 'tf.shape', 'hidden3_sigma', 'dtype=tf.float32', 'hidden3', '=', 'hidden3_mean', '+', 'hidden3_sigma', '*', 'noise', 'hidden4', '=', 'fully_connected', 'hidden3', 'n_hidden4', 'hidden5', '=', 'fully_connected', 'hidden4', 'n_hidden5', 'logits', '=', 'fully_connected', 'hidden5', 'n_outputs', 'activation_fn=None', 'outputs', '=', 'tf.sigmoid', 'logits', 'reconstruction_loss', '=', 'tf.reduce_sum', 'tf.nn.sigmoid_cross_entropy_with_logits', 'labels=X', 'logits=logits', 'latent_loss', '=', '0.5', '*', 'tf.reduce_sum', 'tf.exp', 'hidden3_gamma', '+', 'tf.square', 'hidden3_mean', '-', '1', '-', 'hidden3_gamma', 'cost', '=', 'reconstruction_loss', '+', 'latent_lossoptimizer', '=', 'tf.train.AdamOptimizer', 'learning_rate=learning_rate', 'training_op', '=', 'optimizer.minimize', 'cost', 'init', '=', 'tf.global_variables_initializer', 'Generating', 'DigitsNow', 'let‡s', 'use', 'variational', 'autoencoder', 'generate', 'images', 'look', 'like', 'handwrit…', 'ten', 'digits', '.', 'All', 'need', 'train', 'model', 'sample', 'random', 'codings', 'Gaussian', 'distribution', 'decode', '.', 'import', 'numpy', 'npn_digits', '=', '60n_epochs', '=', '50batch_size', '=', '150with', 'tf.Session', 'sess', 'init.run', 'epoch', 'range', 'n_epochs', 'n_batches', '=', 'mnist.train.num_examples', '//', 'batch_size', 'iteration', 'range', 'n_batches', 'X_batch', 'y_batch', '=', 'mnist.train.next_batch', 'batch_size', 'sess.run', 'training_op', 'feed_dict=', '{', 'X', 'X_batch', '}', 'codings_rnd', '=', 'np.random.normal', 'size=', 'n_digits', 'n_hidden3', 'outputs_val', '=', 'outputs.eval', 'feed_dict=', '{', 'hidden3', 'codings_rnd', '}', 'That‡s', '.', 'Now', 'see', 'ƒhandwritten⁄', 'digits', 'produced', 'autoencoder', 'look', 'like', 'see', 'Figure', '15-12', 'iteration', 'range', 'n_digits', 'plt.subplot', 'n_digits', '10', 'iteration', '+', '1', 'plot_image', 'outputs_val', 'iteration', 'Variational', 'Autoencoders', '|', '431', '8ƒContractive', 'Auto-Encoders', 'Explicit', 'Invariance', 'During', 'Feature', 'Extraction', '⁄', 'S.', 'Rifai', 'et', 'al', '.', '2011', '.', 'Figure', '15-12', '.', 'Images', 'handwritten', 'digits', 'generated', 'variational', 'autoencoder', 'A', 'majority', 'digits', 'look', 'pretty', 'convincing', 'rather', 'ƒcreative.⁄', 'But', 'don‡t', 'harsh', 'autoencoder›it', 'started', 'learning', 'less', 'hour', 'ago', '.', 'Give', 'bit', 'training', 'time', 'digits', 'look', 'better', 'better', '.', 'Other', 'AutoencodersThe', 'amazing', 'successes', 'supervised', 'learning', 'image', 'recognition', 'speech', 'recogni…', 'tion', 'text', 'translation', 'somewhat', 'overshadowed', 'unsupervised', 'learning', 'actually', 'booming', '.', 'New', 'architectures', 'autoencoders', 'unsuper…', 'vised', 'learning', 'algorithms', 'invented', 'regularly', 'much', 'cover', 'book', '.', 'Here', 'brief', 'means', 'exhaustive', 'overview', 'types', 'autoencoders', 'may', 'want', 'check', 'Contractive', 'autoencoder', 'CAE', '8The', 'autoencoder', 'constrained', 'training', 'derivatives', 'cod…', 'ings', 'regards', 'inputs', 'small', '.', 'In', 'words', 'two', 'similar', 'inputs', 'must', 'similar', 'codings', '.', '432', '|', 'Chapter', '15', 'Autoencoders', '9ƒStacked', 'Convolutional', 'Auto-Encoders', 'Hierarchical', 'Feature', 'Extraction', '⁄', 'J.', 'Masci', 'et', 'al', '.', '2011', '.', '10ƒGSNs', 'Generative', 'Stochastic', 'Networks', '⁄', 'G.', 'Alain', 'et', 'al', '.', '2015', '.', '11ƒWinner-Take-All', 'Autoencoders', '⁄', 'A.', 'Makhzani', 'B.', 'Frey', '2015', '.', '12ƒAdversarial', 'Autoencoders', '⁄', 'A.', 'Makhzani', 'et', 'al', '.', '2016', '.', 'Stacked', 'convolutional', 'autoencoders', '9Autoencoders', 'learn', 'extract', 'visual', 'features', 'reconstructing', 'images', 'pro…', 'cessed', 'convolutional', 'layers', '.', 'Generative', 'stochastic', 'network', 'GSN', '10A', 'generalization', 'denoising', 'autoencoders', 'added', 'capability', 'generate', 'data', '.', 'Winner-take-all', 'WTA', 'autoencoder', '11During', 'training', 'computing', 'activations', 'neurons', 'coding', 'layer', 'top', 'k', '%', 'activations', 'neuron', 'training', 'batch', 'pre…', 'served', 'rest', 'set', 'zero', '.', 'Naturally', 'leads', 'sparse', 'codings', '.', 'More…', 'similar', 'WTA', 'approach', 'used', 'produce', 'sparse', 'convolutional', 'autoencoders', '.', 'Adversarial', 'autoencoders', '12One', 'network', 'trained', 'reproduce', 'inputs', 'time', 'another', 'trained', 'find', 'inputs', 'first', 'network', 'unable', 'properly', 'reconstruct', '.', 'This', 'pushes', 'first', 'autoencoder', 'learn', 'robust', 'codings', '.', 'Exercises1.What', 'main', 'tasks', 'autoencoders', 'used', '?', '2.Suppose', 'want', 'train', 'classifier', 'plenty', 'unlabeled', 'training', 'data', 'thousand', 'labeled', 'instances', '.', 'How', 'autoencoders', 'help', '?', 'How', 'would', 'proceed', '?', '3.If', 'autoencoder', 'perfectly', 'reconstructs', 'inputs', 'necessarily', 'good', 'autoencoder', '?', 'How', 'evaluate', 'performance', 'autoencoder', '?', '4.What', 'undercomplete', 'overcomplete', 'autoencoders', '?', 'What', 'main', 'risk', 'excessively', 'undercomplete', 'autoencoder', '?', 'What', 'main', 'risk', 'overcomplete', 'autoencoder', '?', '5.How', 'tie', 'weights', 'stacked', 'autoencoder', '?', 'What', 'point', '?', '6.What', 'common', 'technique', 'visualize', 'features', 'learned', 'lower', 'layer', 'stacked', 'autoencoder', '?', 'What', 'higher', 'layers', '?', '7.What', 'generative', 'model', '?', 'Can', 'name', 'type', 'generative', 'autoencoder', '?', 'Exercises', '|', '433', '13ƒSemantic', 'Hashing', '⁄', 'R.', 'Salakhutdinov', 'G.', 'Hinton', '2008', '.', '8.Let‡s', 'use', 'denoising', 'autoencoder', 'pretrain', 'image', 'classifier', '‹You', 'use', 'MNIST', 'simplest', 'another', 'large', 'set', 'images', 'CIFAR10', 'want', 'bigger', 'challenge', '.', 'If', 'choose', 'CIFAR10', 'need', 'write', 'code', 'load', 'batches', 'images', 'training', '.', 'If', 'want', 'skip', 'part', 'Tensor…', 'Flow‡s', 'model', 'zoo', 'contains', 'tools', '.‹Split', 'dataset', 'training', 'set', 'test', 'set', '.', 'Train', 'deep', 'denoising', 'autoencoder', 'full', 'training', 'set', '.', '‹Check', 'images', 'fairly', 'well', 'reconstructed', 'visualize', 'low-level', 'features', '.', 'Visualize', 'images', 'activate', 'neuron', 'coding', 'layer', '.', '‹Build', 'classification', 'deep', 'neural', 'network', 'reusing', 'lower', 'layers', 'autoencoder', '.', 'Train', 'using', '10', '%', 'training', 'set', '.', 'Can', 'get', 'per…', 'form', 'well', 'classifier', 'trained', 'full', 'training', 'set', '?', '9.Semantic', 'hashing', 'introduced', '2008', 'Ruslan', 'Salakhutdinov', 'Geoffrey', 'Hinton', ',13', 'technique', 'used', 'efficient', 'information', 'retrieval', 'document', 'e.g.', 'image', 'passed', 'system', 'typically', 'neural', 'network', 'outputs', 'afairly', 'low-dimensional', 'binary', 'vector', 'e.g.', '30', 'bits', '.', 'Two', 'similar', 'documents', 'likely', 'identical', 'similar', 'hashes', '.', 'By', 'indexing', 'document', 'using', 'hash', 'possible', 'retrieve', 'many', 'documents', 'similar', 'particular', 'docu…', 'ment', 'almost', 'instantly', 'even', 'billions', 'documents', 'compute', 'hash', 'document', 'look', 'documents', 'hash', 'hashes', 'differing', 'one', 'two', 'bits', '.', 'Let‡s', 'implement', 'semantic', 'hashing', 'using', 'slightly', 'tweaked', 'stacked', 'autoencoder', '‹Create', 'stacked', 'autoencoder', 'containing', 'two', 'hidden', 'layers', 'coding', 'layer', 'train', 'image', 'dataset', 'used', 'previous', 'exercise', '.', 'The', 'coding', 'layer', 'contain', '30', 'neurons', 'use', 'logistic', 'activation', 'function', 'output', 'values', '0', '1', '.', 'After', 'training', 'produce', 'hash', 'animage', 'simply', 'run', 'autoencoder', 'take', 'output', 'coding', 'layer', 'round', 'every', 'value', 'closest', 'integer', '0', '1', '.', '‹One', 'neat', 'trick', 'proposed', 'Salakhutdinov', 'Hinton', 'add', 'Gaussian', 'noise', 'zero', 'mean', 'inputs', 'coding', 'layer', 'training', '.', 'In', 'order', 'preserve', 'high', 'signal-to-noise', 'ratio', 'autoencoder', 'learn', 'feed', 'large', 'values', 'coding', 'layer', 'noise', 'becomes', 'negligible', '.', 'In', 'turn', 'means', 'logistic', 'function', 'coding', 'layer', 'likely', 'satu…', 'rate', '0', '1', '.', 'As', 'result', 'rounding', 'codings', '0', '1', 'won‡t', 'distort', 'much', 'improve', 'reliability', 'hashes', '.', '434', '|', 'Chapter', '15', 'Autoencoders', '14ƒCNN', 'Based', 'Hashing', 'Image', 'Retrieval', '⁄', 'J.', 'Gua', 'J.', 'Li', '2015', '.', '‹Compute', 'hash', 'every', 'image', 'see', 'images', 'identical', 'hashes', 'look', 'alike', '.', 'Since', 'MNIST', 'CIFAR10', 'labeled', 'objective', 'way', 'measure', 'performance', 'autoencoder', 'semantic', 'hashing', 'ensure', 'images', 'hash', 'generally', 'class', '.', 'One', 'way', 'measure', 'average', 'Gini', 'purity', 'introduced', 'Chapter', '6', 'sets', 'ofimages', 'identical', 'similar', 'hashes', '.', '‹Try', 'fine-tuning', 'hyperparameters', 'using', 'cross-validation', '.', '‹Note', 'labeled', 'dataset', 'another', 'approach', 'train', 'convolutional', 'neural', 'network', 'see', 'Chapter', '13', 'classification', 'use', 'layer', 'output', 'layer', 'produce', 'hashes', '.', 'See', 'Jinma', 'Gua', 'Jianmin', 'Li‡s', '2015paper', '.14', 'See', 'performs', 'better', '.', '10.Train', 'variational', 'autoencoder', 'image', 'dataset', 'used', 'previous', 'exerci…', 'ses', 'MNIST', 'CIFAR10', 'make', 'generate', 'images', '.', 'Alternatively', 'try', 'find', 'unlabeled', 'dataset', 'interested', 'see', 'generate', 'new', 'samples', '.', 'Solutions', 'exercises', 'available', 'Appendix', 'A', '.Exercises', '|', '435', '1For', 'details', 'sure', 'check', 'Richard', 'Sutton', 'Andrew', 'Barto‡s', 'book', 'RL', 'Reinforcement', 'Learn…', 'ing', 'An', 'Introduction', 'MIT', 'Press', 'David', 'Silver‡s', 'free', 'online', 'RL', 'course', 'University', 'College', 'London', '.', '2ƒPlaying', 'Atari', 'Deep', 'Reinforcement', 'Learning', '⁄', 'V.', 'Mnih', 'et', 'al', '.', '2013', '.', '3ƒHuman-level', 'control', 'deep', 'reinforcement', 'learning', '⁄', 'V.', 'Mnih', 'et', 'al', '.', '2015', '.', '4Check', 'videos', 'DeepMind‡s', 'system', 'learning', 'play', 'Space', 'Invaders', 'Breakout', 'https', '//', 'goo.gl/yTsH6X', '.CHAPTER', '16Reinforcement', 'LearningReinforcement', 'Learning', 'RL', 'one', 'exciting', 'fields', 'Machine', 'Learning', 'today', 'also', 'one', 'oldest', '.', 'It', 'around', 'since', '1950s', 'producing', 'many', 'interesting', 'applications', 'years', '1', 'particular', 'games', 'e.g.', 'TD-Gammon', 'aBackgammon', 'playing', 'program', 'machine', 'control', 'seldom', 'making', 'headline', 'news', '.', 'But', 'revolution', 'took', 'place', '2013', 'researchers', 'Englishstartup', 'called', 'DeepMind', 'demonstrated', 'system', 'could', 'learn', 'play', 'Atari', 'game', 'scratch', ',2', 'eventually', 'outperforming', 'humans', '3', 'using', 'raw', 'pixels', 'inputs', 'without', 'prior', 'knowledge', 'rules', 'games.4', 'This', 'first', 'series', 'amazing', 'feats', 'culminating', 'March', '2016', 'victory', 'system', 'AlphaGo', 'Lee', 'Sedol', 'world', 'champion', 'game', 'Go', '.', 'No', 'program', 'ever', 'come', 'close', 'beating', 'master', 'game', 'let', 'alone', 'world', 'champion', '.', 'Today', 'whole', 'field', 'RL', 'boiling', 'new', 'ideas', 'wide', 'range', 'applications', '.', 'DeepMind', 'bought', 'Google', '500', 'million', 'dollars', '2014.So', '?', 'With', 'hindsight', 'seems', 'rather', 'simple', 'applied', 'power', 'Deep', 'Learning', 'field', 'Reinforcement', 'Learning', 'worked', 'beyond', 'wildest', 'dreams', '.', 'In', 'chapter', 'first', 'explain', 'Reinforcement', 'Learning', 'good', 'present', 'two', 'important', 'techniques', 'deep', 'Reinforcement', 'Learning', 'policy', 'gradients', 'deep', 'Q-networks', 'DQN', '437including', 'discussion', 'Markov', 'decision', 'processes', 'MDP', '.', 'We', 'use', 'techni…', 'ques', 'train', 'model', 'balance', 'pole', 'moving', 'cart', 'another', 'play', 'Atari', 'games', '.', 'The', 'techniques', 'used', 'wide', 'variety', 'tasks', 'walkingrobots', 'self-driving', 'cars.Learning', 'Optimize', 'RewardsIn', 'Reinforcement', 'Learning', 'software', 'agent', 'makes', 'observations', 'takes', 'actions', 'within', 'environment', 'return', 'receives', 'rewards', '.', 'Its', 'objective', 'learn', 'act', 'way', 'maximize', 'expected', 'long-term', 'rewards', '.', 'If', 'don‡t', 'mind', 'bit', 'anthropomorphism', 'think', 'positive', 'rewards', 'pleasure', 'negative', 'rewards', 'pain', 'term', 'ƒreward⁄', 'bit', 'misleading', 'case', '.', 'In', 'short', 'agent', 'acts', 'environment', 'learns', 'trial', 'error', 'maximize', 'pleasure', 'minimize', 'pain.This', 'quite', 'broad', 'setting', 'apply', 'wide', 'variety', 'tasks', '.', 'Here', 'examples', 'see', 'Figure', '16-1', 'a.The', 'agent', 'program', 'controlling', 'walking', 'robot', '.', 'In', 'case', 'envi…', 'ronment', 'real', 'world', 'agent', 'observes', 'environment', 'set', 'sensors', 'cameras', 'touch', 'sensors', 'actions', 'consist', 'sending', 'sig…nals', 'activate', 'motors', '.', 'It', 'may', 'programmed', 'get', 'positive', 'rewards', 'whenever', 'approaches', 'target', 'destination', 'negative', 'rewards', 'whenever', 'wastes', 'time', 'goes', 'wrong', 'direction', 'falls', 'down.b', '.', 'The', 'agent', 'program', 'controlling', 'Ms.', 'Pac-Man', '.', 'In', 'case', 'environ…', 'ment', 'simulation', 'Atari', 'game', 'actions', 'nine', 'possible', 'joystick', 'positions', 'upper', 'left', 'center', 'observations', 'screenshots', 'rewards', 'game', 'points', '.', 'c.Similarly', 'agent', 'program', 'playing', 'board', 'game', 'game', 'Go', '.d.The', 'agent', 'control', 'physically', 'virtually', 'moving', 'thing', '.', 'For', 'example', 'smart', 'thermostat', 'getting', 'rewards', 'whenever', 'close', 'target', 'temperature', 'saves', 'energy', 'negative', 'rewards', 'humans', 'need', 'tweak', 'temperature', 'agent', 'must', 'learn', 'anticipate', 'human', 'needs', '.', 'e.The', 'agent', 'observe', 'stock', 'market', 'prices', 'decide', 'much', 'buy', 'sell', 'every', 'second', '.', 'Rewards', 'obviously', 'monetary', 'gains', 'losses', '.', '438', '|', 'Chapter', '16', 'Reinforcement', 'Learning', '5Images', 'c', 'reproduced', 'Wikipedia', '.', 'public', 'domain', '.', 'c', 'created', 'user', 'Stevertigo', 'released', 'Creative', 'Commons', 'BY-SA', '2.0', '.', 'b', 'screenshot', 'Ms.', 'Pac-', 'Man', 'game', 'copyright', 'Atari', 'author', 'believes', 'fair', 'use', 'chapter', '.', 'e', 'reproduced', 'Pix…', 'abay', 'released', 'Creative', 'Commons', 'CC0', '.Figure', '16-1', '.', 'Reinforcement', 'Learning', 'examples', 'walking', 'robot', 'b', 'Ms.', 'Pac-Man', 'c', 'Go', 'player', 'thermostat', 'e', 'automatic', 'trader', '5Note', 'may', 'positive', 'rewards', 'example', 'agent', 'may', 'move', 'around', 'maze', 'getting', 'negative', 'reward', 'every', 'time', 'step', 'better', 'find', 'exit', 'quickly', 'possible', '!', 'There', 'many', 'examples', 'tasks', 'Rein…', 'forcement', 'Learning', 'well', 'suited', 'self-driving', 'cars', 'placing', 'ads', 'web', 'page', 'controlling', 'image', 'classification', 'system', 'focus', 'attention', '.', 'Learning', 'Optimize', 'Rewards', '|', '439', '6It', 'often', 'better', 'give', 'poor', 'performers', 'slight', 'chance', 'survival', 'preserve', 'diversity', 'ƒgene', 'pool.⁄', 'Policy', 'SearchThe', 'algorithm', 'used', 'software', 'agent', 'determine', 'actions', 'called', 'policy', '.', 'For', 'example', 'policy', 'could', 'neural', 'network', 'taking', 'observations', 'inputs', 'outputting', 'action', 'take', 'see', 'Figure', '16-2', '.Figure', '16-2', '.', 'Reinforcement', 'Learning', 'using', 'neural', 'network', 'policy', 'The', 'policy', 'algorithm', 'think', 'even', 'deterministic', '.', 'For', 'example', 'consider', 'robotic', 'vacuum', 'cleaner', 'whose', 'reward', 'amount', 'dust', 'picks', '30', 'minutes', '.', 'Its', 'policy', 'could', 'move', 'forward', 'probability', 'p', 'every', 'second', 'randomly', 'rotate', 'left', 'right', 'probability', '1', '–', 'p.', 'The', 'rotation', 'angle', 'would', 'random', 'angle', '–r', '+r', '.', 'Since', 'policy', 'involves', 'randomness', 'called', 'stochastic', 'policy', '.', 'The', 'robot', 'erratic', 'trajectory', 'guarantees', 'eventually', 'get', 'place', 'reach', 'pick', 'dust', '.', 'The', 'question', 'much', 'dust', 'pick', '30', 'minutes', '?', 'How', 'would', 'train', 'robot', '?', 'There', 'two', 'policy', 'parameters', 'cantweak', 'probability', 'p', 'angle', 'range', 'r.', 'One', 'possible', 'learning', 'algorithm', 'couldbe', 'try', 'many', 'different', 'values', 'parameters', 'pick', 'combination', 'performs', 'best', 'see', 'Figure', '16-3', '.', 'This', 'example', 'policy', 'search', 'caseusing', 'brute', 'force', 'approach', '.', 'However', 'policy', 'space', 'large', 'isgenerally', 'case', 'finding', 'good', 'set', 'parameters', 'way', 'like', 'searching', 'needle', 'gigantic', 'haystack', '.', 'Another', 'way', 'explore', 'policy', 'space', 'use', 'genetic', 'algorithms', '.', 'For', 'example', 'could', 'randomly', 'create', 'first', 'generation', '100', 'policies', 'try', 'ƒkill⁄', '80', 'worst', 'policies6', 'make', '20', 'survivors', 'produce', '4', 'offspring', '.', 'An', 'off…', '440', '|', 'Chapter', '16', 'Reinforcement', 'Learning', '7If', 'single', 'parent', 'called', 'asexual', 'reproduction', '.', 'With', 'two', 'parents', 'called', 'sexual', 'reproduction', '.', 'An', 'offspring‡s', 'genome', 'case', 'set', 'policy', 'parameters', 'randomly', 'composed', 'parts', 'parents‡', 'genomes', '.', 'spring', 'copy', 'parent', '7', 'plus', 'random', 'variation', '.', 'The', 'surviving', 'policies', 'plus', 'offspring', 'together', 'constitute', 'second', 'generation', '.', 'You', 'continue', 'iterate', 'generations', 'way', 'find', 'good', 'policy', '.', 'Figure', '16-3', '.', 'Four', 'points', 'policy', 'space', 'agent‹s', 'corresponding', 'behavior', 'Yet', 'another', 'approach', 'use', 'optimization', 'techniques', 'evaluating', 'gradients', 'rewards', 'regards', 'policy', 'parameters', 'tweaking', 'parameters', 'byfollowing', 'gradient', 'toward', 'higher', 'rewards', 'gradient', 'ascent', '.', 'This', 'approach', 'called', 'policy', 'gradients', 'PG', 'discuss', 'detail', 'later', 'chapter', '.', 'For', 'example', 'going', 'back', 'vacuum', 'cleaner', 'robot', 'could', 'slightly', 'increase', 'pand', 'evaluate', 'whether', 'increases', 'amount', 'dust', 'picked', 'robot', '30', 'minutes', 'increase', 'p', 'else', 'reduce', 'p.', 'We', 'implement', 'popular', 'PG', 'algorithm', 'using', 'TensorFlow', 'need', 'create', 'envi…', 'ronment', 'agent', 'live', 'it‡s', 'time', 'introduce', 'OpenAI', 'gym', '.', 'Introduction', 'OpenAI', 'GymOne', 'challenges', 'Reinforcement', 'Learning', 'order', 'train', 'agent', 'first', 'need', 'working', 'environment', '.', 'If', 'want', 'program', 'agent', 'learn', 'play', 'Atari', 'game', 'need', 'Atari', 'game', 'simulator', '.', 'If', 'want', 'program', 'walking', 'robot', 'environment', 'real', 'world', 'directly', 'train', 'robot', 'environment', 'limits', 'robot', 'falls', 'cliff', 'can‡t', 'click', 'ƒundo.⁄', 'You', 'can‡t', 'speed', 'time', 'either', 'adding', 'computing', 'Introduction', 'OpenAI', 'Gym', '|', '441', '8OpenAI', 'nonprofit', 'artificial', 'intelligence', 'research', 'company', 'funded', 'part', 'Elon', 'Musk', '.', 'Its', 'stated', 'goal', 'promote', 'develop', 'friendly', 'AIs', 'benefit', 'humanity', 'rather', 'exterminate', '.', 'power', 'won‡t', 'make', 'robot', 'move', 'faster', '.', 'And', 'it‡s', 'generally', 'expensive', 'train', '1,000', 'robots', 'parallel', '.', 'In', 'short', 'training', 'hard', 'slow', 'real', 'world', 'yougenerally', 'need', 'simulated', 'environment', 'least', 'bootstrap', 'training', '.', 'OpenAI', 'gym', '8', 'toolkit', 'provides', 'wide', 'variety', 'simulated', 'environments', 'Atari', 'games', 'board', 'games', '2D', '3D', 'physical', 'simulations', 'train', 'agents', 'compare', 'develop', 'new', 'RL', 'algorithms', '.', 'Let‡s', 'install', 'OpenAI', 'gym', '.', 'For', 'minimal', 'OpenAI', 'gym', 'installation', 'simply', 'use', 'pip', '$', 'pip3', 'install', '--', 'upgrade', 'gymNext', 'open', 'Python', 'shell', 'Jupyter', 'notebook', 'create', 'first', 'environment', '>', '>', '>', 'import', 'gym', '>', '>', '>', 'env', '=', 'gym.make', '``', 'CartPole-v0', \"''\", '2016-10-14', '16:03:23,199', 'Making', 'new', 'env', 'MsPacman-v0', '>', '>', '>', 'obs', '=', 'env.reset', '>', '>', '>', 'obsarray', '-0.03799846', '-0.03288115', '0.02337094', '0.00720711', '>', '>', '>', 'env.render', 'The', 'make', 'function', 'creates', 'environment', 'case', 'CartPole', 'environment', '.', 'This', '2D', 'simulation', 'cart', 'accelerated', 'left', 'right', 'order', 'bal…', 'ance', 'pole', 'placed', 'top', 'see', 'Figure', '16-4', '.', 'After', 'environment', 'created', 'must', 'initialize', 'using', 'reset', 'method', '.', 'This', 'returns', 'first', 'observation', '.', 'Obser…', 'vations', 'depend', 'type', 'environment', '.', 'For', 'CartPole', 'environment', 'observation', '1D', 'NumPy', 'array', 'containing', 'four', 'floats', 'floats', 'represent', 'cart‡s', 'horizontal', 'position', '0.0', '=', 'center', 'velocity', 'angle', 'pole', '0.0', '=', 'verti…', 'cal', 'angular', 'velocity', '.', 'Finally', 'render', 'method', 'displays', 'environment', 'shown', 'Figure', '16-4.Figure', '16-4', '.', '•e', 'CartPole', 'environment', '442', '|', 'Chapter', '16', 'Reinforcement', 'Learning', 'If', 'want', 'render', 'return', 'rendered', 'image', 'NumPy', 'array', 'set', 'mode', 'parameter', 'rgb_array', 'note', 'environments', 'may', 'support', 'different', 'modes', '>', '>', '>', 'img', '=', 'env.render', 'mode=', \"''\", 'rgb_array', \"''\", '>', '>', '>', 'img.shape', '#', 'height', 'width', 'channels', '3=RGB', '400', '600', '3', 'Unfortunately', 'CartPole', 'environments', 'ren…', 'ders', 'image', 'screen', 'even', 'set', 'mode', \"''\", 'rgb_array', \"''\", '.', 'The', 'way', 'avoid', 'use', 'fake', 'X', 'server', 'Xvfb', 'Xdummy', '.', 'For', 'example', 'install', 'Xvfb', 'start', 'Python', 'using', 'following', 'command', 'xvfb-run', '-s', '``', '-screen', '0', '1400x900x24', \"''\", 'python', '.', 'Or', 'use', 'xvfbwrapper', 'package', '.Let‡s', 'ask', 'environment', 'actions', 'possible', '>', '>', '>', 'env.action_spaceDiscrete', '2', 'Discrete', '2', 'means', 'possible', 'actions', 'integers', '0', '1', 'represent', 'accelerating', 'left', '0', 'right', '1', '.', 'Other', 'environments', 'may', 'discrete', 'actions', 'kinds', 'actions', 'e.g.', 'continuous', '.', 'Since', 'pole', 'leaning', 'toward', 'right', 'let‡s', 'accelerate', 'cart', 'toward', 'right', '>', '>', '>', 'action', '=', '1', '#', 'accelerate', 'right', '>', '>', '>', 'obs', 'reward', 'done', 'info', '=', 'env.step', 'action', '>', '>', '>', 'obsarray', '-0.03865608', '0.16189797', '0.02351508', '-0.27801135', '>', '>', '>', 'reward1.0', '>', '>', '>', 'doneFalse', '>', '>', '>', 'info', '{', '}', 'The', 'step', 'method', 'executes', 'given', 'action', 'returns', 'four', 'values', 'obsThis', 'new', 'observation', '.', 'The', 'cart', 'moving', 'toward', 'right', 'obs', '1', '>', '0', '.The', 'pole', 'still', 'tilted', 'toward', 'right', 'obs', '2', '>', '0', 'angular', 'velocity', 'nownegative', 'obs', '3', '<', '0', 'likely', 'tilted', 'toward', 'left', 'next', 'step', '.', 'rewardIn', 'environment', 'get', 'reward', '1.0', 'every', 'step', 'matter', 'goal', 'keep', 'running', 'long', 'possible.Introduction', 'OpenAI', 'Gym', '|', '443', 'doneThis', 'value', 'True', 'episode', '.', 'This', 'happen', 'pole', 'tilts', 'much', '.', 'After', 'environment', 'must', 'reset', 'used', 'again.infoThis', 'dictionary', 'may', 'provide', 'extra', 'debug', 'information', 'environments', '.', 'This', 'data', 'used', 'training', 'would', 'cheating', '.', 'Let‡s', 'hardcode', 'simple', 'policy', 'accelerates', 'left', 'pole', 'leaning', 'toward', 'left', 'accelerates', 'right', 'pole', 'leaning', 'toward', 'right', '.', 'We', 'run', 'policy', 'see', 'average', 'rewards', 'gets', '500', 'episodes', 'def', 'basic_policy', 'obs', 'angle', '=', 'obs', '2', 'return', '0', 'angle', '<', '0', 'else', '1totals', '=', 'episode', 'range', '500', 'episode_rewards', '=', '0', 'obs', '=', 'env.reset', 'step', 'range', '1000', '#', '1000', 'steps', 'max', 'don•t', 'want', 'run', 'forever', 'action', '=', 'basic_policy', 'obs', 'obs', 'reward', 'done', 'info', '=', 'env.step', 'action', 'episode_rewards', '+=', 'reward', 'done', 'break', 'totals.append', 'episode_rewards', 'This', 'code', 'hopefully', 'self-explanatory', '.', 'Let‡s', 'look', 'result', '>', '>', '>', 'import', 'numpy', 'np', '>', '>', '>', 'np.mean', 'totals', 'np.std', 'totals', 'np.min', 'totals', 'np.max', 'totals', '42.125999999999998', '9.1237121830974033', '24.0', '68.0', 'Even', '500', 'tries', 'policy', 'never', 'managed', 'keep', 'pole', 'upright', '68', 'consecutive', 'steps', '.', 'Not', 'great', '.', 'If', 'look', 'simulation', 'Jupyter', 'note…', 'books', 'see', 'cart', 'oscillates', 'left', 'right', 'strongly', 'pole', 'tilts', 'much', '.', 'Let‡s', 'see', 'neural', 'network', 'come', 'better', 'policy', '.', 'Neural', 'Network', 'PoliciesLet‡s', 'create', 'neural', 'network', 'policy', '.', 'Just', 'like', 'policy', 'hardcoded', 'earlier', 'neural', 'network', 'take', 'observation', 'input', 'output', 'action', 'executed', '.', 'More', 'precisely', 'estimate', 'probability', 'action', 'select', 'action', 'randomly', 'according', 'estimated', 'probabilities', 'see', 'Figure', '16-5', '.', 'In', 'case', 'CartPole', 'environment', 'two', 'possible', 'actions', 'left', 'right', 'need', 'one', 'output', 'neuron', '.', 'It', 'output', 'probabil…', 'ity', 'p', 'action', '0', 'left', 'course', 'probability', 'action', '1', 'right', '1', '–', 'p.444', '|', 'Chapter', '16', 'Reinforcement', 'Learning', 'For', 'example', 'outputs', '0.7', 'pick', 'action', '0', '70', '%', 'probability', 'action', '1', '30', '%', 'probability', '.', 'Figure', '16-5', '.', 'Neural', 'network', 'policy', 'You', 'may', 'wonder', 'picking', 'random', 'action', 'based', 'probability', 'given', 'neural', 'network', 'rather', 'picking', 'action', 'highest', 'score', '.', 'This', 'approach', 'lets', 'agent', 'find', 'right', 'balance', 'exploring', 'new', 'actions', 'andexploiting', 'actions', 'known', 'work', 'well', '.', 'Here‡s', 'analogy', 'suppose', 'go', 'restaurant', 'first', 'time', 'dishes', 'look', 'equally', 'appealing', 'ran…', 'domly', 'pick', 'one', '.', 'If', 'turns', 'good', 'increase', 'probability', 'order', 'itnext', 'time', 'shouldn‡t', 'increase', 'probability', '100', '%', 'else', 'never', 'try', 'dishes', 'may', 'even', 'better', 'one', 'tried.Also', 'note', 'particular', 'environment', 'past', 'actions', 'observations', 'safely', 'ignored', 'since', 'observation', 'contains', 'environment‡s', 'full', 'state', '.', 'If', 'hidden', 'state', 'may', 'need', 'consider', 'past', 'actions', 'observations', 'well', '.', 'For', 'example', 'environment', 'revealed', 'position', 'cart', 'velocity', 'would', 'consider', 'current', 'observation', 'also', 'previous', 'observation', 'order', 'estimate', 'current', 'velocity', '.', 'Another', 'example', 'observations', 'noisy', 'case', 'generally', 'want', 'use', 'past', 'observations', 'estimate', 'likely', 'current', 'state', '.', 'The', 'CartPole', 'problem', 'thus', 'Neural', 'Network', 'Policies', '|', '445', 'simple', 'observations', 'noise-free', 'contain', 'environment‡s', 'full', 'state', '.', 'Here', 'code', 'build', 'neural', 'network', 'policy', 'using', 'TensorFlow', 'import', 'tensorflow', 'tffrom', 'tensorflow.contrib.layers', 'import', 'fully_connected', '#', '1', '.', 'Specify', 'neural', 'network', 'architecturen_inputs', '=', '4', '#', '==', 'env.observation_space.shape', '0', 'n_hidden', '=', '4', '#', 'it•s', 'simple', 'task', 'don•t', 'need', 'hidden', 'neuronsn_outputs', '=', '1', '#', 'outputs', 'probability', 'accelerating', 'leftinitializer', '=', 'tf.contrib.layers.variance_scaling_initializer', '#', '2', '.', 'Build', 'neural', 'networkX', '=', 'tf.placeholder', 'tf.float32', 'shape=', 'None', 'n_inputs', 'hidden', '=', 'fully_connected', 'X', 'n_hidden', 'activation_fn=tf.nn.elu', 'weights_initializer=initializer', 'logits', '=', 'fully_connected', 'hidden', 'n_outputs', 'activation_fn=None', 'weights_initializer=initializer', 'outputs', '=', 'tf.nn.sigmoid', 'logits', '#', '3', '.', 'Select', 'random', 'action', 'based', 'estimated', 'probabilitiesp_left_and_right', '=', 'tf.concat', 'axis=1', 'values=', 'outputs', '1', '-', 'outputs', 'action', '=', 'tf.multinomial', 'tf.log', 'p_left_and_right', 'num_samples=1', 'init', '=', 'tf.global_variables_initializer', 'Let‡s', 'go', 'code', '1.After', 'imports', 'define', 'neural', 'network', 'architecture', '.', 'The', 'number', 'inputs', 'size', 'observation', 'space', 'case', 'CartPole', 'four', 'four', 'hidden', 'units', 'need', 'one', 'output', 'probability', 'probability', 'going', 'left', '.2.Next', 'build', 'neural', 'network', '.', 'In', 'example', 'it‡s', 'vanilla', 'Multi-Layer', 'Per…', 'ceptron', 'single', 'output', '.', 'Note', 'output', 'layer', 'uses', 'logistic', 'sig…', 'moid', 'activation', 'function', 'order', 'output', 'probability', '0.0', '1.0', '.', 'If', 'two', 'possible', 'actions', 'would', 'one', 'output', 'neuron', 'peraction', 'would', 'use', 'softmax', 'activation', 'function', 'instead', '.', '3.Lastly', 'call', 'multinomial', 'function', 'pick', 'random', 'action', '.', 'This', 'func…tion', 'independently', 'samples', 'one', 'integers', 'given', 'log', 'probability', 'integer', '.', 'For', 'example', 'call', 'array', 'np.log', '0.5', 'np.log', '0.2', 'np.log', '0.3', 'num_samples=5', 'output', 'fiveintegers', '50', '%', 'probability', '0', '20', '%', '1', '30', '%', '2', '.', 'In', 'case', 'need', 'one', 'integer', 'representing', 'action', 'take', '.', 'Since', 'outputs', 'tensor', 'contains', 'probability', 'going', 'left', 'must', 'first', 'concatenate', '1-outputs', 'tensor', 'containing', 'probability', '446', '|', 'Chapter', '16', 'Reinforcement', 'Learning', 'left', 'right', 'actions', '.', 'Note', 'two', 'possible', 'actions', 'neural', 'network', 'would', 'output', 'one', 'probability', 'per', 'action', 'would', 'need', 'concatenation', 'step', '.', 'Okay', 'neural', 'network', 'policy', 'take', 'observations', 'output', 'actions', '.', 'But', 'train', '?', 'Evaluating', 'Actions', 'The', 'Credit', 'Assignment', 'ProblemIf', 'knew', 'best', 'action', 'step', 'could', 'train', 'neural', 'network', 'usual', 'minimizing', 'cross', 'entropy', 'estimated', 'probability', 'tar…', 'get', 'probability', '.', 'It', 'would', 'regular', 'supervised', 'learning', '.', 'However', 'Reinforce…', 'ment', 'Learning', 'guidance', 'agent', 'gets', 'rewards', 'rewards', 'typically', 'sparse', 'delayed', '.', 'For', 'example', 'agent', 'manages', 'balance', 'pole', '100', 'steps', 'know', '100', 'actions', 'took', 'good', 'whichof', 'bad', '?', 'All', 'knows', 'pole', 'fell', 'last', 'action', 'surely', 'last', 'action', 'entirely', 'responsible', '.', 'This', 'called', 'credit', 'assignment', 'problem', 'agent', 'gets', 'reward', 'hard', 'know', 'actions', 'get', 'credi…', 'ted', 'blamed', '.', 'Think', 'dog', 'gets', 'rewarded', 'hours', 'behaved', 'well', 'understand', 'rewarded', '?', 'To', 'tackle', 'problem', 'common', 'strategy', 'evaluate', 'action', 'based', 'sum', 'rewards', 'come', 'usually', 'applying', 'discount', 'rate', 'r', 'step', '.', 'For', 'example', 'see', 'Figure', '16-6', 'agent', 'decides', 'go', 'right', 'three', 'times', 'row', 'gets', '+10', 'reward', 'first', 'step', '0', 'second', 'step', 'finally', '–50', 'third', 'step', 'assuming', 'use', 'discount', 'rate', 'r', '=', '0.8', 'first', 'action', 'total', 'score', '10', '+', 'r', '‰', '0', '+', 'r2', '‰', '–50', '=', '–22', '.', 'If', 'discount', 'rate', 'close', '0', 'future', 'rewards', 'won‡t', 'count', 'much', 'compared', 'immediate', 'rewards', '.', 'Conversely', 'discount', 'rate', 'close', '1', 'rewards', 'far', 'future', 'count', 'almost', 'much', 'immediate', 'rewards', '.', 'Typical', 'discount', 'rates', '0.95', '0.99', '.', 'With', 'discount', 'rate', '0.95', 'rewards', '13', 'steps', 'future', 'count', 'roughly', 'half', 'much', 'immediate', 'rewards', 'since', '0.9513', 'Ÿ', '0.5', 'discount', 'rate', '0.99', 'rewards', '69', 'steps', 'future', 'count', 'half', 'much', 'immediate', 'rewards', '.', 'In', 'CartPole', 'environ…', 'ment', 'actions', 'fairly', 'short-term', 'effects', 'choosing', 'discount', 'rate', '0.95', 'seems', 'reasonable.Evaluating', 'Actions', 'The', 'Credit', 'Assignment', 'Problem', '|', '447', '9ƒSimple', 'Statistical', 'Gradient-Following', 'Algorithms', 'Connectionist', 'Reinforcement', 'Learning', '⁄', 'R.', 'Williams', '1992', '.Figure', '16-6', '.', 'Discounted', 'rewards', 'Of', 'course', 'good', 'action', 'may', 'followed', 'several', 'bad', 'actions', 'cause', 'pole', 'fall', 'quickly', 'resulting', 'good', 'action', 'getting', 'low', 'score', 'similarly', 'good', 'actor', 'may', 'sometimes', 'star', 'terrible', 'movie', '.', 'However', 'play', 'game', 'enough', 'times', 'average', 'good', 'actions', 'get', 'better', 'score', 'bad', 'ones', '.', 'So', 'get', 'fairly', 'reliable', 'action', 'scores', 'must', 'run', 'many', 'episodes', 'normalize', 'action', 'scores', 'subtracting', 'mean', 'dividing', 'standard', 'deviation', '.', 'After', 'rea…', 'sonably', 'assume', 'actions', 'negative', 'score', 'bad', 'actions', 'posi…', 'tive', 'score', 'good', '.', 'Perfect›now', 'way', 'evaluate', 'action', 'ready', 'train', 'first', 'agent', 'using', 'policy', 'gradients', '.', 'Let‡s', 'see', '.', 'Policy', 'GradientsAs', 'discussed', 'earlier', 'PG', 'algorithms', 'optimize', 'parameters', 'policy', 'following', 'gradients', 'toward', 'higher', 'rewards', '.', 'One', 'popular', 'class', 'PG', 'algorithms', 'called', 'REINFORCE', 'algorithms', 'introduced', 'back', '1992', '9', 'Ronald', 'Williams', '.', 'Here', 'one', 'common', 'variant', '1.First', 'let', 'neural', 'network', 'policy', 'play', 'game', 'several', 'times', 'step', 'compute', 'gradients', 'would', 'make', 'chosen', 'action', 'even', 'likely', 'don‡t', 'apply', 'gradients', 'yet', '.', '448', '|', 'Chapter', '16', 'Reinforcement', 'Learning', '10We', 'already', 'something', 'similar', 'Chapter', '11', 'discussed', 'Gradient', 'Clipping', 'first', 'computed', 'gradients', 'clipped', 'finally', 'applied', 'clipped', 'gradients', '.', '2.Once', 'run', 'several', 'episodes', 'compute', 'action‡s', 'score', 'using', 'method', 'described', 'previous', 'paragraph', '.', '3.If', 'action‡s', 'score', 'positive', 'means', 'action', 'good', 'want', 'apply', 'gradients', 'computed', 'earlier', 'make', 'action', 'even', 'likely', 'chosen', 'future', '.', 'However', 'score', 'negative', 'means', 'action', 'bad', 'want', 'apply', 'opposite', 'gradients', 'make', 'action', 'slightly', 'less', 'likely', 'future', '.', 'The', 'solution', 'simply', 'multiply', 'gradient', 'vector', 'corresponding', 'action‡s', 'score', '.', '4.Finally', 'compute', 'mean', 'resulting', 'gradient', 'vectors', 'use', 'per…', 'form', 'Gradient', 'Descent', 'step', '.', 'Let‡s', 'implement', 'algorithm', 'using', 'TensorFlow', '.', 'We', 'train', 'neural', 'network', 'policy', 'built', 'earlier', 'learns', 'balance', 'pole', 'cart', '.', 'Let‡s', 'start', 'completing', 'construction', 'phase', 'coded', 'earlier', 'add', 'target', 'probability', 'cost', 'function', 'training', 'operation', '.', 'Since', 'acting', 'though', 'chosen', 'action', 'best', 'possible', 'action', 'target', 'probability', 'must', '1.0', 'chosen', 'action', 'action', '0', 'left', '0.0', 'action', '1', 'right', '=', '1', '.', '-', 'tf.to_float', 'action', 'Now', 'target', 'probability', 'define', 'cost', 'function', 'cross', 'entropy', 'compute', 'gradients', 'learning_rate', '=', '0.01cross_entropy', '=', 'tf.nn.sigmoid_cross_entropy_with_logits', 'labels=y', 'logits=logits', 'optimizer', '=', 'tf.train.AdamOptimizer', 'learning_rate', 'grads_and_vars', '=', 'optimizer.compute_gradients', 'cross_entropy', 'Note', 'calling', 'optimizer‡s', 'compute_gradients', 'method', 'instead', 'theminimize', 'method', '.', 'This', 'want', 'tweak', 'gradients', 'apply', 'them.10', 'The', 'compute_gradients', 'method', 'returns', 'list', 'gradient', 'vector/variable', 'pairs', 'one', 'pair', 'per', 'trainable', 'variable', '.', 'Let‡s', 'put', 'gradients', 'list', 'make', 'convenient', 'obtain', 'values', 'gradients', '=', 'grad', 'grad', 'variable', 'grads_and_vars', 'Okay', 'comes', 'tricky', 'part', '.', 'During', 'execution', 'phase', 'algorithm', 'run', 'policy', 'step', 'evaluate', 'gradient', 'tensors', 'store', 'val…', 'ues', '.', 'After', 'number', 'episodes', 'tweak', 'gradients', 'explained', 'earlier', 'i.e.', 'multiply', 'action', 'scores', 'normalize', 'compute', 'mean', 'tweaked', 'gradients', '.', 'Next', 'need', 'feed', 'resulting', 'gradients', 'back', 'Policy', 'Gradients', '|', '449', 'optimizer', 'perform', 'optimization', 'step', '.', 'This', 'means', 'need', 'one', 'place…', 'holder', 'per', 'gradient', 'vector', '.', 'Moreover', 'must', 'create', 'operation', 'apply', 'updated', 'gradients', '.', 'For', 'call', 'optimizer‡s', 'apply_gradients', 'function', 'takes', 'list', 'gradient', 'vector/variable', 'pairs', '.', 'Instead', 'giving', 'original', 'gradient', 'vectors', 'give', 'list', 'containing', 'updated', 'gradients', 'i.e.', 'ones', 'fed', 'gradient', 'placeholders', 'gradient_placeholders', '=', 'grads_and_vars_feed', '=', 'grad', 'variable', 'grads_and_vars', 'gradient_placeholder', '=', 'tf.placeholder', 'tf.float32', 'shape=grad.get_shape', 'gradient_placeholders.append', 'gradient_placeholder', 'grads_and_vars_feed.append', 'gradient_placeholder', 'variable', 'training_op', '=', 'optimizer.apply_gradients', 'grads_and_vars_feed', 'Let‡s', 'step', 'back', 'take', 'look', 'full', 'construction', 'phase', 'n_inputs', '=', '4n_hidden', '=', '4n_outputs', '=', '1initializer', '=', 'tf.contrib.layers.variance_scaling_initializer', 'learning_rate', '=', '0.01X', '=', 'tf.placeholder', 'tf.float32', 'shape=', 'None', 'n_inputs', 'hidden', '=', 'fully_connected', 'X', 'n_hidden', 'activation_fn=tf.nn.elu', 'weights_initializer=initializer', 'logits', '=', 'fully_connected', 'hidden', 'n_outputs', 'activation_fn=None', 'weights_initializer=initializer', 'outputs', '=', 'tf.nn.sigmoid', 'logits', 'p_left_and_right', '=', 'tf.concat', 'axis=1', 'values=', 'outputs', '1', '-', 'outputs', 'action', '=', 'tf.multinomial', 'tf.log', 'p_left_and_right', 'num_samples=1', '=', '1', '.', '-', 'tf.to_float', 'action', 'cross_entropy', '=', 'tf.nn.sigmoid_cross_entropy_with_logits', 'labels=y', 'logits=logits', 'optimizer', '=', 'tf.train.AdamOptimizer', 'learning_rate', 'grads_and_vars', '=', 'optimizer.compute_gradients', 'cross_entropy', 'gradients', '=', 'grad', 'grad', 'variable', 'grads_and_vars', 'gradient_placeholders', '=', 'grads_and_vars_feed', '=', 'grad', 'variable', 'grads_and_vars', 'gradient_placeholder', '=', 'tf.placeholder', 'tf.float32', 'shape=grad.get_shape', 'gradient_placeholders.append', 'gradient_placeholder', 'grads_and_vars_feed.append', 'gradient_placeholder', 'variable', 'training_op', '=', 'optimizer.apply_gradients', 'grads_and_vars_feed', 'init', '=', 'tf.global_variables_initializer', 'saver', '=', 'tf.train.Saver', '450', '|', 'Chapter', '16', 'Reinforcement', 'Learning', 'On', 'execution', 'phase', '!', 'We', 'need', 'couple', 'functions', 'compute', 'total', 'discounted', 'rewards', 'given', 'raw', 'rewards', 'normalize', 'results', 'across', 'multi…', 'ple', 'episodes', 'def', 'discount_rewards', 'rewards', 'discount_rate', 'discounted_rewards', '=', 'np.empty', 'len', 'rewards', 'cumulative_rewards', '=', '0', 'step', 'reversed', 'range', 'len', 'rewards', 'cumulative_rewards', '=', 'rewards', 'step', '+', 'cumulative_rewards', '*', 'discount_rate', 'discounted_rewards', 'step', '=', 'cumulative_rewards', 'return', 'discounted_rewardsdef', 'discount_and_normalize_rewards', 'all_rewards', 'discount_rate', 'all_discounted_rewards', '=', 'discount_rewards', 'rewards', 'rewards', 'all_rewards', 'flat_rewards', '=', 'np.concatenate', 'all_discounted_rewards', 'reward_mean', '=', 'flat_rewards.mean', 'reward_std', '=', 'flat_rewards.std', 'return', 'discounted_rewards', '-', 'reward_mean', '/reward_std', 'discounted_rewards', 'all_discounted_rewards', 'Let‡s', 'check', 'works', '>', '>', '>', 'discount_rewards', '10', '0', '-50', 'discount_rate=0.8', 'array', '-22.', '-40.', '-50', '.', '>', '>', '>', 'discount_and_normalize_rewards', '10', '0', '-50', '10', '20', 'discount_rate=0.8', 'array', '-0.28435071', '-0.86597718', '-1.18910299', 'array', '1.26665318', '1.0727777', 'The', 'call', 'discount_rewards', 'returns', 'exactly', 'expect', 'see', 'Figure', '16-6', '.You', 'verify', 'function', 'discount_and_normalize_rewards', 'indeedreturn', 'normalized', 'scores', 'action', 'episodes', '.', 'Notice', 'first', 'episode', 'much', 'worse', 'second', 'normalized', 'scores', 'negative', 'actions', 'first', 'episode', 'would', 'considered', 'bad', 'conversely', 'actions', 'second', 'episode', 'would', 'considered', 'good.We', 'need', 'train', 'policy', 'n_iterations', '=', '250', '#', 'number', 'training', 'iterationsn_max_steps', '=', '1000', '#', 'max', 'steps', 'per', 'episoden_games_per_update', '=', '10', '#', 'train', 'policy', 'every', '10', 'episodessave_iterations', '=', '10', '#', 'save', 'model', 'every', '10', 'training', 'iterationsdiscount_rate', '=', '0.95with', 'tf.Session', 'sess', 'init.run', 'iteration', 'range', 'n_iterations', 'all_rewards', '=', '#', 'sequences', 'raw', 'rewards', 'episode', 'all_gradients', '=', '#', 'gradients', 'saved', 'step', 'episode', 'game', 'range', 'n_games_per_update', 'current_rewards', '=', '#', 'raw', 'rewards', 'current', 'episode', 'current_gradients', '=', '#', 'gradients', 'current', 'episodePolicy', 'Gradients', '|', '451', 'obs', '=', 'env.reset', 'step', 'range', 'n_max_steps', 'action_val', 'gradients_val', '=', 'sess.run', 'action', 'gradients', 'feed_dict=', '{', 'X', 'obs.reshape', '1', 'n_inputs', '}', '#', 'one', 'obs', 'obs', 'reward', 'done', 'info', '=', 'env.step', 'action_val', '0', '0', 'current_rewards.append', 'reward', 'current_gradients.append', 'gradients_val', 'done', 'break', 'all_rewards.append', 'current_rewards', 'all_gradients.append', 'current_gradients', '#', 'At', 'point', 'run', 'policy', '10', 'episodes', '#', 'ready', 'policy', 'update', 'using', 'algorithm', 'described', 'earlier', '.', 'all_rewards', '=', 'discount_and_normalize_rewards', 'all_rewards', 'feed_dict', '=', '{', '}', 'var_index', 'grad_placeholder', 'enumerate', 'gradient_placeholders', '#', 'multiply', 'gradients', 'action', 'scores', 'compute', 'mean', 'mean_gradients', '=', 'np.mean', 'reward', '*', 'all_gradients', 'game_index', 'step', 'var_index', 'game_index', 'rewards', 'enumerate', 'all_rewards', 'step', 'reward', 'enumerate', 'rewards', 'axis=0', 'feed_dict', 'grad_placeholder', '=', 'mean_gradients', 'sess.run', 'training_op', 'feed_dict=feed_dict', 'iteration', '%', 'save_iterations', '==', '0', 'saver.save', 'sess', '``', './my_policy_net_pg.ckpt', \"''\", 'Each', 'training', 'iteration', 'starts', 'running', 'policy', '10', 'episodes', 'maximum', '1,000', 'steps', 'per', 'episode', 'avoid', 'running', 'forever', '.', 'At', 'step', 'also', 'compute', 'gradients', 'pretending', 'chosen', 'action', 'best', '.', 'After', '10', 'episodes', 'run', 'compute', 'action', 'scores', 'using', 'discount_and_normalize_rewards', 'function', 'go', 'trainable', 'variable', 'across', 'episodesand', 'steps', 'multiply', 'gradient', 'vector', 'corresponding', 'action', 'score', 'compute', 'mean', 'resulting', 'gradients', '.', 'Finally', 'run', 'training', 'opera…', 'tion', 'feeding', 'mean', 'gradients', 'one', 'per', 'trainable', 'variable', '.', 'We', 'also', 'save', 'model', 'every', '10', 'training', 'operations', '.', 'And', 'we‡re', 'done', '!', 'This', 'code', 'train', 'neural', 'network', 'policy', 'success…', 'fully', 'learn', 'balance', 'pole', 'cart', 'try', 'Jupyter', 'note…', 'books', '.', 'Note', 'actually', 'two', 'ways', 'agent', 'lose', 'game', 'either', 'pole', 'tilt', 'much', 'cart', 'go', 'completely', 'screen', '.', 'With', '250', 'training', 'iterations', 'policy', 'learns', 'balance', 'pole', 'quite', 'well', 'yet', 'good', 'enough', 'avoiding', 'going', 'screen', '.', 'A', 'hundred', 'training', 'iterations', 'fix', '.', '452', '|', 'Chapter', '16', 'Reinforcement', 'Learning', 'Researchers', 'try', 'find', 'algorithms', 'work', 'well', 'even', 'agent', 'initially', 'knows', 'nothing', 'environment', '.', 'However', 'unless', 'writing', 'paper', 'inject', 'much', 'prior', 'knowledge', 'possible', 'agent', 'speed', 'training', 'dramatically', '.', 'For', 'example', 'could', 'add', 'negative', 'rewards', 'propor…', 'tional', 'distance', 'center', 'screen', 'pole‡s', 'angle', '.', 'Also', 'already', 'reasonably', 'good', 'policy', 'e.g.', 'hardcoded', 'may', 'want', 'train', 'neural', 'network', 'imitate', 'using', 'policy', 'gradients', 'improve', '.', 'Despite', 'relative', 'simplicity', 'algorithm', 'quite', 'powerful', '.', 'You', 'use', 'tackle', 'much', 'harder', 'problems', 'balancing', 'pole', 'cart', '.', 'In', 'fact', 'AlphaGo', 'based', 'similar', 'PG', 'algorithm', 'plus', 'Monte', 'Carlo', 'Tree', 'Search', 'beyond', 'scopeof', 'book', '.We', 'look', 'another', 'popular', 'family', 'algorithms', '.', 'Whereas', 'PG', 'algorithms', 'directly', 'try', 'optimize', 'policy', 'increase', 'rewards', 'algorithms', 'look', 'less', 'direct', 'agent', 'learns', 'estimate', 'expected', 'sum', 'discounted', 'future', 'rewards', 'state', 'expected', 'sum', 'discounted', 'future', 'rewards', 'action', 'state', 'uses', 'knowledge', 'decide', 'act', '.', 'To', 'understand', 'algorithms', 'must', 'first', 'introduce', 'Markov', 'decision', 'processes', 'MDP', '.Markov', 'Decision', 'ProcessesIn', 'early', '20th', 'century', 'mathematician', 'Andrey', 'Markov', 'studied', 'stochastic', 'pro…', 'cesses', 'memory', 'called', 'Markov', 'chains', '.', 'Such', 'process', 'fixed', 'number', 'states', 'randomly', 'evolves', 'one', 'state', 'another', 'step', '.', 'The', 'probability', 'evolve', 'state', 'state', 'sis', 'fixed', 'depends', 'pair', 'past', 'states', 'system', 'memory', '.', 'Figure', '16-7', 'shows', 'example', 'Markov', 'chain', 'four', 'states', '.', 'Suppose', 'process', 'starts', 'state', 's0', '70', '%', 'chance', 'remain', 'state', 'next', 'step', '.', 'Eventually', 'bound', 'leave', 'state', 'never', 'come', 'back', 'since', 'state', 'points', 'back', 's0', '.', 'If', 'goes', 'state', 's1', 'likely', 'go', 'state', 's2', '90', '%', 'probability', 'immediately', 'back', 'state', 's1', '100', '%', 'probability', '.', 'It', 'may', 'alternate', 'number', 'times', 'two', 'states', 'eventually', 'fall', 'state', 's3', 'remain', 'forever', 'terminal', 'state', '.', 'Markov', 'chains', 'different', 'dynamics', 'heavily', 'used', 'thermodynamics', 'chemistry', 'statistics', 'much', '.', 'Markov', 'Decision', 'Processes', '|', '453', '11ƒA', 'Markovian', 'Decision', 'Process', '⁄', 'R.', 'Bellman', '1957', '.', 'Figure', '16-7', '.', 'Example', 'Markov', 'chain', 'Markov', 'decision', 'processes', 'first', 'described', '1950s', 'Richard', 'Bellman', '.11They', 'resemble', 'Markov', 'chains', 'twist', 'step', 'agent', 'choose', 'one', 'several', 'possible', 'actions', 'transition', 'probabilities', 'depend', 'chosenaction', '.', 'Moreover', 'state', 'transitions', 'return', 'reward', 'positive', 'negative', 'agent‡s', 'goal', 'find', 'policy', 'maximize', 'rewards', 'time', '.', 'For', 'example', 'MDP', 'represented', 'Figure', '16-8', 'three', 'states', 'three', 'possible', 'discrete', 'actions', 'step', '.', 'If', 'starts', 'state', 's0', 'agent', 'choose', 'actions', 'a0', 'a1', 'a2', '.', 'If', 'chooses', 'action', 'a1', 'remains', 'state', 's0', 'cer…', 'tainty', 'without', 'reward', '.', 'It', 'thus', 'decide', 'stay', 'forever', 'wants', '.', 'But', 'chooses', 'action', 'a0', '70', '%', 'probability', 'gaining', 'reward', '+10', 'andremaining', 'state', 's0', '.', 'It', 'try', 'gain', 'much', 'reward', 'possi…', 'ble', '.', 'But', 'one', 'point', 'going', 'end', 'instead', 'state', 's1', '.', 'In', 'state', 's1', 'twopossible', 'actions', 'a0', 'a1', '.', 'It', 'choose', 'stay', 'put', 'repeatedly', 'choosing', 'action', 'a1', 'orit', 'choose', 'move', 'state', 's2', 'get', 'negative', 'reward', '–50', 'ouch', '.', 'In', 'state', 's3it', 'choice', 'take', 'action', 'a1', 'likely', 'lead', 'back', 'tostate', 's0', 'gaining', 'reward', '+40', 'way', '.', 'You', 'get', 'picture', '.', 'By', 'looking', 'MDP', 'guess', 'strategy', 'gain', 'reward', 'time', '?', 'In', 'state', 's0', 'itis', 'clear', 'action', 'a0', 'best', 'option', 'state', 's3', 'agent', 'choice', 'take', 'action', 'a1', 'state', 's1', 'obvious', 'whether', 'agent', 'stay', 'put', 'a0', 'orgo', 'fire', 'a2', '.454', '|', 'Chapter', '16', 'Reinforcement', 'Learning', 'Figure', '16-8', '.', 'Example', 'Markov', 'decision', 'process', 'Bellman', 'found', 'way', 'estimate', 'optimal', 'state', 'value', 'state', 'noted', 'V*', 'sum', 'discounted', 'future', 'rewards', 'agent', 'expect', 'average', 'reaches', 'state', 'assuming', 'acts', 'optimally', '.', 'He', 'showed', 'agent', 'acts', 'optimally', 'Bellman', 'Optimality', 'Equation', 'applies', 'see', 'Equation', '16-1', '.', 'Thisrecursive', 'equation', 'says', 'agent', 'acts', 'optimally', 'optimal', 'value', 'current', 'state', 'equal', 'reward', 'get', 'average', 'taking', 'one', 'optimal', 'action', 'plus', 'expected', 'optimal', 'value', 'possible', 'next', 'states', 'action', 'lead', '.', 'Equation', '16-1', '.', 'Bellman', 'Optimality', 'Equation', 'V*s=max', '“', 'sTs', 'sRs', 's+', '’', '.V*sforall', 's‹T', 'transition', 'probability', 'state', 'state', 'given', 'agent', 'chose', 'action', 'a.‹R', 'reward', 'agent', 'gets', 'goes', 'state', 'state', 'given', 'agent', 'chose', 'action', 'a.‹', '’', 'discount', 'rate', '.', 'This', 'equation', 'leads', 'directly', 'algorithm', 'precisely', 'estimate', 'optimal', 'state', 'value', 'every', 'possible', 'state', 'first', 'initialize', 'state', 'value', 'estimates', 'zero', 'iteratively', 'update', 'using', 'Value', 'Iteration', 'algorithm', 'seeEquation', '16-2', '.', 'A', 'remarkable', 'result', 'given', 'enough', 'time', 'estimates', 'Markov', 'Decision', 'Processes', '|', '455', 'guaranteed', 'converge', 'optimal', 'state', 'values', 'corresponding', 'optimal', 'pol…', 'icy', '.', 'Equation', '16-2', '.', 'Value', 'Iteration', 'algorithm', 'Vk+1', 'smaxa', '“', 'sTs', 'sRs', 's+', '’', '.Vksforall', 's‹Vk', 'estimated', 'value', 'state', 'kth', 'iteration', 'algorithm', '.', 'This', 'algorithm', 'example', 'Dynamic', 'Programming', 'whichbreaks', 'complex', 'problem', 'case', 'estimating', 'poten…', 'tially', 'infinite', 'sum', 'discounted', 'future', 'rewards', 'tractable', 'sub-', 'problems', 'tackled', 'iteratively', 'case', 'finding', 'action', 'maximizes', 'average', 'reward', 'plus', 'discounted', 'next', 'state', 'value', '.', 'Knowing', 'optimal', 'state', 'values', 'useful', 'particular', 'evaluate', 'policy', 'tell', 'agent', 'explicitly', '.', 'Luckily', 'Bellman', 'found', 'similar', 'algorithm', 'estimate', 'optimal', 'state-action', 'values', 'generally', 'called', 'Q-Values', '.', 'Theoptimal', 'Q-Value', 'state-action', 'pair', 'noted', 'Q*', 'sum', 'discounted', 'future', 'rewards', 'agent', 'expect', 'average', 'reaches', 'state', 'chooses', 'action', 'sees', 'outcome', 'action', 'assuming', 'acts', 'optimally', 'afterthat', 'action', '.', 'Here', 'works', 'start', 'initializing', 'Q-Value', 'estimates', 'zero', 'update', 'using', 'Q-Value', 'Iteration', 'algorithm', 'see', 'Equation', '16-3', '.Equation', '16-3', '.', 'Q-Value', 'Iteration', 'algorithm', 'Qk+1', '“', 'sTs', 'sRs', 's+', '’', '.max', 'aQks', 'aforall', 'aOnce', 'optimal', 'Q-Values', 'defining', 'optimal', 'policy', 'noted', 'Ž*', 'triv…ial', 'agent', 'state', 'choose', 'action', 'highest', 'Q-Value', 'state', 'Ž*s=argmax', 'aQ*s', 'a.Let‡s', 'apply', 'algorithm', 'MDP', 'represented', 'Figure', '16-8', '.', 'First', 'need', 'todefine', 'MDP', 'nan=np.nan', '#', 'represents', 'impossible', 'actionsT', '=', 'np.array', '#', 'shape=', 's•', '0.7', '0.3', '0.0', '1.0', '0.0', '0.0', '0.8', '0.2', '0.0', ',456', '|', 'Chapter', '16', 'Reinforcement', 'Learning', '0.0', '1.0', '0.0', 'nan', 'nan', 'nan', '0.0', '0.0', '1.0', 'nan', 'nan', 'nan', '0.8', '0.1', '0.1', 'nan', 'nan', 'nan', 'R', '=', 'np.array', '#', 'shape=', 's•', '10.', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '10.', '0.0', '0.0', 'nan', 'nan', 'nan', '0.0', '0.0', '-50', '.', 'nan', 'nan', 'nan', '40.', '0.0', '0.0', 'nan', 'nan', 'nan', 'possible_actions', '=', '0', '1', '2', '0', '2', '1', 'Now', 'let‡s', 'run', 'Q-Value', 'Iteration', 'algorithm', 'Q', '=', 'np.full', '3', '3', '-np.inf', '#', '-inf', 'impossible', 'actionsfor', 'state', 'actions', 'enumerate', 'possible_actions', 'Q', 'state', 'actions', '=', '0.0', '#', 'Initial', 'value', '=', '0.0', 'possible', 'actionslearning_rate', '=', '0.01discount_rate', '=', '0.95n_iterations', '=', '100for', 'iteration', 'range', 'n_iterations', 'Q_prev', '=', 'Q.copy', 'range', '3', 'possible_actions', 'Q', '=', 'np.sum', 'T', 'sp', '*', 'R', 'sp', '+', 'discount_rate', '*', 'np.max', 'Q_prev', 'sp', 'sp', 'range', '3', 'The', 'resulting', 'Q-Values', 'look', 'like', '>', '>', '>', 'Qarray', '21.89498982', '20.80024033', '16.86353093', '1.11669335', '-inf', '1.17573546', '-inf', '53.86946068', '-inf', '>', '>', '>', 'np.argmax', 'Q', 'axis=1', '#', 'optimal', 'action', 'statearray', '0', '2', '1', 'This', 'gives', 'us', 'optimal', 'policy', 'MDP', 'using', 'discount', 'rate', '0.95', 'state', 's0', 'choose', 'action', 'a0', 'state', 's1', 'choose', 'action', 'a2', 'go', 'fire', '!', 'state', 's2', 'choose', 'action', 'a1', 'possible', 'action', '.', 'Interestingly', 'reduce', 'discount', 'rate', '0.9', 'optimal', 'policy', 'changes', 'state', 's1', 'best', 'action', 'becomes', 'a0', 'stay', 'put', 'don‡t', 'go', 'fire', '.', 'It', 'makes', 'sense', 'value', 'present', 'much', 'future', 'prospect', 'future', 'rewards', 'worth', 'immediate', 'pain', '.', 'Temporal', 'Di†erence', 'Learning', 'Q-LearningReinforcement', 'Learning', 'problems', 'discrete', 'actions', 'often', 'modeled', 'Markov', 'decision', 'processes', 'agent', 'initially', 'idea', 'transition', 'probabilities', 'know', 'T', 'know', 'rewards', 'going', 'either', 'know', 'R', '.', 'It', 'must', 'experience', 'state', 'Temporal', 'Di†erence', 'Learning', 'Q-Learning', '|', '457', 'transition', 'least', 'know', 'rewards', 'must', 'experience', 'multi…', 'ple', 'times', 'reasonable', 'estimate', 'transition', 'probabilities', '.', 'The', 'Temporal', 'Di›erence', 'Learning', 'TD', 'Learning', 'algorithm', 'similar', 'Value', 'Iteration', 'algorithm', 'tweaked', 'take', 'account', 'fact', 'agent', 'partial', 'knowledge', 'MDP', '.', 'In', 'general', 'assume', 'agent', 'initially', 'knows', 'possible', 'states', 'actions', 'nothing', '.', 'The', 'agent', 'uses', 'exploration', 'policy', '›for', 'example', 'purely', 'random', 'policy›to', 'explore', 'MDP', 'progresses', 'TD', 'Learning', 'algorithm', 'updates', 'estimates', 'state', 'values', 'based', 'transitions', 'rewards', 'actually', 'observed', 'see', 'Equation', '16-4', '.Equation', '16-4', '.', 'TD', 'Learning', 'algorithm', 'Vk+1', 's1', '”', '‰Vks+‰r+', '’', '.Vks‹‰', 'learning', 'rate', 'e.g.', '0.01', '.', 'TD', 'Learning', 'many', 'similarities', 'Stochastic', 'Gradient', 'Descent', 'particular', 'fact', 'handles', 'one', 'sample', 'time', '.', 'Just', 'like', 'SGD', 'truly', 'converge', 'gradually', 'reduce', 'learning', 'rate', 'otherwise', 'keep', 'bouncing', 'around', 'opti…', 'mum', '.', 'For', 'state', 'algorithm', 'simply', 'keeps', 'track', 'running', 'average', 'imme…', 'diate', 'rewards', 'agent', 'gets', 'upon', 'leaving', 'state', 'plus', 'rewards', 'expects', 'get', 'later', 'assuming', 'acts', 'optimally', '.', 'Similarly', 'Q-Learning', 'algorithm', 'adaptation', 'Q-Value', 'Iteration', 'algo…', 'rithm', 'situation', 'transition', 'probabilities', 'rewards', 'initially', 'unknown', 'see', 'Equation', '16-5', '.Equation', '16-5', '.', 'Q-Learning', 'algorithm', 'Qk+1', 'a1', '”', '‰Qks', 'a+‰r+', '’', '.max', 'aQks', 'aFor', 'state-action', 'pair', 'algorithm', 'keeps', 'track', 'running', 'average', 'rewards', 'r', 'agent', 'gets', 'upon', 'leaving', 'state', 'action', 'plus', 'rewards', 'itexpects', 'get', 'later', '.', 'Since', 'target', 'policy', 'would', 'act', 'optimally', 'take', 'maximum', 'Q-Value', 'estimates', 'next', 'state', '.', 'Here', 'Q-Learning', 'implemented', '458', '|', 'Chapter', '16', 'Reinforcement', 'Learning', 'import', 'numpy.random', 'rndlearning_rate0', '=', '0.05learning_rate_decay', '=', '0.1n_iterations', '=', '20000s', '=', '0', '#', 'start', 'state', '0Q', '=', 'np.full', '3', '3', '-np.inf', '#', '-inf', 'impossible', 'actionsfor', 'state', 'actions', 'enumerate', 'possible_actions', 'Q', 'state', 'actions', '=', '0.0', '#', 'Initial', 'value', '=', '0.0', 'possible', 'actionsfor', 'iteration', 'range', 'n_iterations', '=', 'rnd.choice', 'possible_actions', '#', 'choose', 'action', 'randomly', 'sp', '=', 'rnd.choice', 'range', '3', 'p=T', '#', 'pick', 'next', 'state', 'using', 'T', 'reward', '=', 'R', 'sp', 'learning_rate', '=', 'learning_rate0', '/', '1', '+', 'iteration', '*', 'learning_rate_decay', 'Q', '=', 'learning_rate', '*', 'Q', '+', '1', '-', 'learning_rate', '*', 'reward', '+', 'discount_rate', '*', 'np.max', 'Q', 'sp', '=', 'sp', '#', 'move', 'next', 'stateGiven', 'enough', 'iterations', 'algorithm', 'converge', 'optimal', 'Q-Values', '.', 'This', 'called', 'o›-policy', 'algorithm', 'policy', 'trained', 'one', 'executed', '.', 'It', 'somewhat', 'surprising', 'algorithm', 'capable', 'learning', 'opti…', 'mal', 'policy', 'watching', 'agent', 'act', 'randomly', 'imagine', 'learning', 'play', 'golf', 'teacher', 'drunken', 'monkey', '.', 'Can', 'better', '?', 'Exploration', 'PoliciesOf', 'course', 'Q-Learning', 'work', 'exploration', 'policy', 'explores', 'MDP', 'thoroughly', 'enough', '.', 'Although', 'purely', 'random', 'policy', 'guaranteed', 'eventually', 'visit', 'every', 'state', 'every', 'transition', 'many', 'times', 'may', 'take', 'extremely', 'long', 'time', '.', 'Therefore', 'better', 'option', 'use', 'ı-greedy', 'policy', 'step', 'acts', 'randomly', 'probability', 'Ò', 'greedily', 'choosing', 'action', 'highest', 'Q-Value', 'probability', '1-', 'Ò', '.', 'The', 'advantage', 'Ò-greedy', 'policy', 'compared', 'completely', 'random', 'policy', 'spend', 'time', 'exploring', 'interesting', 'parts', 'environment', 'Q-Value', 'estimates', 'get', 'better', 'better', 'still', 'spending', 'time', 'visiting', 'unknown', 'regions', 'MDP', '.', 'It', 'quite', 'com…', 'mon', 'start', 'high', 'value', 'Ò', 'e.g.', '1.0', 'gradually', 'reduce', 'e.g.', 'downto', '0.05', '.Alternatively', 'rather', 'relying', 'chance', 'exploration', 'another', 'approach', 'encourage', 'exploration', 'policy', 'try', 'actions', 'tried', 'much', '.', 'This', 'implemented', 'bonus', 'added', 'Q-Value', 'estimates', 'shown', 'Equation', '16-6.Temporal', 'Di†erence', 'Learning', 'Q-Learning', '|', '459', 'Equation', '16-6', '.', 'Q-Learning', 'using', 'exploration', 'function', 'Qs', 'a1', '”', '‰Qs', 'a+‰r+', '’', '.max', '‰fQs', 'Ns', 'a‹N', 'counts', 'number', 'times', 'action', 'awas', 'chosen', 'state', 's.‹f', 'q', 'n', 'exploration', 'function', 'f', 'q', 'n', '=', 'q', '+', 'K/', '1', '+', 'n', 'K', 'acuriosity', 'hyperparameter', 'measures', 'much', 'agent', 'attracted', 'unknown.Approximate', 'Q-LearningThe', 'main', 'problem', 'Q-Learning', 'scale', 'well', 'large', 'even', 'medium', 'MDPs', 'many', 'states', 'actions', '.', 'Consider', 'trying', 'use', 'Q-Learning', 'train', 'agent', 'play', 'Ms.', 'Pac-Man', '.', 'There', '250', 'pellets', 'Ms.', 'Pac-Man', 'eat', 'present', 'absent', 'i.e.', 'already', 'eaten', '.', 'So', 'number', 'pos…', 'sible', 'states', 'greater', '2', '250', 'Ÿ', '10', '75', 'that‡s', 'considering', 'possible', 'states', 'pellets', '.', 'This', 'way', 'atoms', 'observable', 'universe', 'there‡s', 'abso…', 'lutely', 'way', 'keep', 'track', 'estimate', 'every', 'single', 'Q-Value', '.', 'The', 'solution', 'find', 'function', 'approximates', 'Q-Values', 'using', 'manageable', 'number', 'parameters', '.', 'This', 'called', 'Approximate', 'Q-Learning', '.', 'For', 'years', 'rec…', 'ommended', 'use', 'linear', 'combinations', 'hand-crafted', 'features', 'extracted', 'state', 'e.g.', 'distance', 'closest', 'ghosts', 'directions', 'estimate', 'Q-', 'Values', 'DeepMind', 'showed', 'using', 'deep', 'neural', 'networks', 'work', 'much', 'bet…', 'ter', 'especially', 'complex', 'problems', 'require', 'feature', 'engineering', '.', 'A', 'DNN', 'used', 'estimate', 'Q-Values', 'called', 'deep', 'Q-network', 'DQN', 'using', 'DQN', 'Approximate', 'Q-Learning', 'called', 'Deep', 'Q-Learning', '.In', 'rest', 'chapter', 'use', 'Deep', 'Q-Learning', 'train', 'agent', 'play', 'Ms.', 'Pac-Man', 'much', 'like', 'DeepMind', '2013', '.', 'The', 'code', 'easily', 'tweaked', 'learn', 'play', 'majority', 'Atari', 'games', 'quite', 'well', '.', 'It', 'achieve', 'superhuman', 'skill', 'action', 'games', 'good', 'games', 'long-running', 'storylines', '.', 'Learning', 'Play', 'Ms.', 'Pac-Man', 'Using', 'Deep', 'Q-LearningSince', 'using', 'Atari', 'environment', 'must', 'first', 'install', 'OpenAI', 'gym‡s', 'Atari', 'dependencies', '.', 'While', 'we‡re', 'also', 'install', 'dependencies', 'OpenAI', 'gym', 'environments', 'may', 'want', 'play', '.', 'On', 'macOS', 'assuming', 'installed', 'Homebrew', 'need', 'run', '$', 'brew', 'install', 'cmake', 'boost', 'boost-python', 'sdl2', 'swig', 'wget460', '|', 'Chapter', '16', 'Reinforcement', 'Learning', 'On', 'Ubuntu', 'type', 'following', 'command', 'replacing', 'python3', 'python', 'areusing', 'Python', '2', '$', 'apt-get', 'install', '-y', 'python3-numpy', 'python3-dev', 'cmake', 'zlib1g-dev', 'libjpeg-dev\\\\', 'xvfb', 'libav-tools', 'xorg-dev', 'python3-opengl', 'libboost-all-dev', 'libsdl2-dev', 'swigThen', 'install', 'extra', 'Python', 'modules', '$', 'pip3', 'install', '--', 'upgrade', '•gym', '•If', 'everything', 'went', 'well', 'able', 'create', 'Ms.', 'Pac-Man', 'environment', '>', '>', '>', 'env', '=', 'gym.make', '``', 'MsPacman-v0', \"''\", '>', '>', '>', 'obs', '=', 'env.reset', '>', '>', '>', 'obs.shape', '#', 'height', 'width', 'channels', '210', '160', '3', '>', '>', '>', 'env.action_spaceDiscrete', '9', 'As', 'see', 'nine', 'discrete', 'actions', 'available', 'correspond', 'nine', 'possible', 'positions', 'joystick', 'left', 'right', 'center', 'upper', 'left', 'observations', 'simply', 'screenshots', 'Atari', 'screen', 'see', 'Figure', '16-9', 'left', 'represented', '3D', 'NumPy', 'arrays', '.', 'These', 'images', 'bit', 'large', 'create', 'small', 'preprocessing', 'function', 'crop', 'image', 'shrink', '88', '‰', '80', 'pixels', 'convert', 'grayscale', 'improve', 'contrast', 'Ms.', 'Pac-Man', '.', 'This', 'reduce', 'amount', 'computations', 'required', 'DQN', 'speed', 'training', '.', 'mspacman_color', '=', 'np.array', '210', '164', '74', '.mean', 'def', 'preprocess_observation', 'obs', 'img', '=', 'obs', '1:176:2', ':2', '#', 'crop', 'downsize', 'img', '=', 'img.mean', 'axis=2', '#', 'greyscale', 'img', 'img==mspacman_color', '=', '0', '#', 'improve', 'contrast', 'img', '=', 'img', '-', '128', '/', '128', '-', '1', '#', 'normalize', '-1.', '1.', 'return', 'img.reshape', '88', '80', '1', 'The', 'result', 'preprocessing', 'shown', 'Figure', '16-9', 'right', '.', 'Learning', 'Play', 'Ms.', 'Pac-Man', 'Using', 'Deep', 'Q-Learning', '|', '461', 'Figure', '16-9', '.', 'Ms.', 'Pac-Man', 'observation', 'original', 'le', '“', '“', 'er', 'preprocessing', 'right', 'Next', 'let‡s', 'create', 'DQN', '.', 'It', 'could', 'take', 'state-action', 'pair', 'input', 'out…', 'put', 'estimate', 'corresponding', 'Q-Value', 'Q', 'since', 'actions', 'dis…crete', 'convenient', 'use', 'neural', 'network', 'takes', 'state', 'input', 'outputs', 'one', 'Q-Value', 'estimate', 'per', 'action', '.', 'The', 'DQN', 'composed', 'three', 'convolutional', 'layers', 'followed', 'two', 'fully', 'connected', 'layers', 'including', 'output', 'layer', 'see', 'Figure', '16-10', '.Figure', '16-10', '.', 'Deep', 'Q-network', 'play', 'Ms.', 'Pac-Man', '462', '|', 'Chapter', '16', 'Reinforcement', 'Learning', 'As', 'see', 'training', 'algorithm', 'use', 'requires', 'two', 'DQNs', 'architecture', 'different', 'parameters', 'one', 'used', 'drive', 'Ms.', 'Pac-Man', 'training', 'actor', 'watch', 'actor', 'learn', 'trials', 'errors', 'critic', '.', 'At', 'regular', 'intervals', 'copy', 'critic', 'actor', '.', 'Since', 'need', 'two', 'identical', 'DQNs', 'create', 'q_network', 'function', 'build', 'tensorflow.contrib.layers', 'import', 'convolution2d', 'fully_connectedinput_height', '=', '88input_width', '=', '80input_channels', '=', '1conv_n_maps', '=', '32', '64', '64', 'conv_kernel_sizes', '=', '8,8', '4,4', '3,3', 'conv_strides', '=', '4', '2', '1', 'conv_paddings', '=', '``', 'SAME', \"''\", '*3conv_activation', '=', 'tf.nn.relu', '*3n_hidden_in', '=', '64', '*', '11', '*', '10', '#', 'conv3', '64', 'maps', '11x10', 'eachn_hidden', '=', '512hidden_activation', '=', 'tf.nn.relun_outputs', '=', 'env.action_space.n', '#', '9', 'discrete', 'actions', 'availableinitializer', '=', 'tf.contrib.layers.variance_scaling_initializer', 'def', 'q_network', 'X_state', 'scope', 'prev_layer', '=', 'X_state', 'conv_layers', '=', 'tf.variable_scope', 'scope', 'scope', 'n_maps', 'kernel_size', 'stride', 'padding', 'activation', 'zip', 'conv_n_maps', 'conv_kernel_sizes', 'conv_strides', 'conv_paddings', 'conv_activation', 'prev_layer', '=', 'convolution2d', 'prev_layer', 'num_outputs=n_maps', 'kernel_size=kernel_size', 'stride=stride', 'padding=padding', 'activation_fn=activation', 'weights_initializer=initializer', 'conv_layers.append', 'prev_layer', 'last_conv_layer_flat', '=', 'tf.reshape', 'prev_layer', 'shape=', '-1', 'n_hidden_in', 'hidden', '=', 'fully_connected', 'last_conv_layer_flat', 'n_hidden', 'activation_fn=hidden_activation', 'weights_initializer=initializer', 'outputs', '=', 'fully_connected', 'hidden', 'n_outputs', 'activation_fn=None', 'weights_initializer=initializer', 'trainable_vars', '=', 'tf.get_collection', 'tf.GraphKeys.TRAINABLE_VARIABLES', 'scope=scope.name', 'trainable_vars_by_name', '=', '{', 'var.name', 'len', 'scope.name', 'var', 'var', 'trainable_vars', '}', 'return', 'outputs', 'trainable_vars_by_nameThe', 'first', 'part', 'code', 'defines', 'hyperparameters', 'DQN', 'architecture', '.', 'Then', 'q_network', 'function', 'creates', 'DQN', 'taking', 'environment‡s', 'state', 'X_state', 'input', 'name', 'variable', 'scope', '.', 'Note', 'use', 'one', 'Learning', 'Play', 'Ms.', 'Pac-Man', 'Using', 'Deep', 'Q-Learning', '|', '463', 'observation', 'represent', 'environment‡s', 'state', 'since', 'there‡s', 'almost', 'hidden', 'state', 'except', 'blinking', 'objects', 'ghosts‡', 'directions', '.', 'The', 'trainable_vars_by_name', 'dictionary', 'gathers', 'trainable', 'variables', 'DQN', '.', 'It', 'useful', 'minute', 'create', 'operations', 'copy', 'critic', 'DQN', 'actor', 'DQN', '.', 'The', 'keys', 'dictionary', 'names', 'variables', 'stripping', 'part', 'prefix', 'corresponds', 'scope‡s', 'name', '.', 'It', 'looks', 'like', '>', '>', '>', 'trainable_vars_by_name', '{', '•/Conv/biases:0•', '<', 'tensorflow.python.ops.variables.Variable', '0x121cf7b50', '>', '•/Conv/weights:0•', '<', 'tensorflow.python.ops.variables.Variable', '...', '>', '•/Conv_1/biases:0•', '<', 'tensorflow.python.ops.variables.Variable', '...', '>', '•/Conv_1/weights:0•', '<', 'tensorflow.python.ops.variables.Variable', '...', '>', '•/Conv_2/biases:0•', '<', 'tensorflow.python.ops.variables.Variable', '...', '>', '•/Conv_2/weights:0•', '<', 'tensorflow.python.ops.variables.Variable', '...', '>', '•/fully_connected/biases:0•', '<', 'tensorflow.python.ops.variables.Variable', '...', '>', '•/fully_connected/weights:0•', '<', 'tensorflow.python.ops.variables.Variable', '...', '>', '•/fully_connected_1/biases:0•', '<', 'tensorflow.python.ops.variables.Variable', '...', '>', '•/fully_connected_1/weights:0•', '<', 'tensorflow.python.ops.variables.Variable', '...', '>', '}', 'Now', 'let‡s', 'create', 'input', 'placeholder', 'two', 'DQNs', 'operation', 'copy', 'critic', 'DQN', 'actor', 'DQN', 'X_state', '=', 'tf.placeholder', 'tf.float32', 'shape=', 'None', 'input_height', 'input_width', 'input_channels', 'actor_q_values', 'actor_vars', '=', 'q_network', 'X_state', 'scope=', \"''\", 'q_networks/actor', \"''\", 'critic_q_values', 'critic_vars', '=', 'q_network', 'X_state', 'scope=', \"''\", 'q_networks/critic', \"''\", 'copy_ops', '=', 'actor_var.assign', 'critic_vars', 'var_name', 'var_name', 'actor_var', 'actor_vars.items', 'copy_critic_to_actor', '=', 'tf.group', '*copy_ops', 'Let‡s', 'step', 'back', 'second', 'two', 'DQNs', 'capable', 'taking', 'environment', 'state', 'i.e.', 'preprocessed', 'observation', 'input', 'outputting', 'estimated', 'Q-Value', 'possible', 'action', 'state', '.', 'Plus', 'operation', 'called', 'copy_critic_to_actor', 'copy', 'trainable', 'variables', 'critic', 'DQN', 'actor', 'DQN', '.', 'We', 'use', 'TensorFlow‡s', 'tf.group', 'function', 'group', 'assign…ment', 'operations', 'single', 'convenient', 'operation', '.', 'The', 'actor', 'DQN', 'used', 'play', 'Ms.', 'Pac-Man', 'initially', 'badly', '.', 'As', 'discussed', 'earlier', 'want', 'explore', 'game', 'thoroughly', 'enough', 'generally', 'want', 'combine', 'ı-greedy', 'policy', 'another', 'exploration', 'strategy', '.', 'But', 'critic', 'DQN', '?', 'How', 'learn', 'play', 'game', '?', 'The', 'short', 'answer', 'try', 'make', 'Q-Value', 'predictions', 'match', 'Q-Values', 'estimated', 'actor', 'experience', 'game', '.', 'Specifically', 'let', 'actor', 'play', 'storing', 'experiences', 'replay', 'memory', '.', 'Each', 'memory', '5-tuple', 'state', 'action', 'next', 'state', 'reward', 'continue', 'ƒcontinue⁄', 'item', 'equal', '0.0', 'game', '1.0', 'otherwise', '.', 'Next', 'regular', 'intervals', 'sample', '464', '|', 'Chapter', '16', 'Reinforcement', 'Learning', 'batch', 'memories', 'replay', 'memory', 'estimate', 'Q-Values', 'memories', '.', 'Finally', 'train', 'critic', 'DQN', 'predict', 'Q-Values', 'using', 'regular', 'supervised', 'learning', 'techniques', '.', 'Once', 'every', 'training', 'iterations', 'copy', 'critic', 'DQN', 'actor', 'DQN', '.', 'And', 'that‡s', '!', 'Equation', '16-7', 'shows', 'costfunction', 'used', 'train', 'critic', 'DQN', 'Equation', '16-7', '.', 'Deep', 'Q-Learning', 'cost', 'function', 'J–critic=1m', '“', 'i=1', 'myi', '”', 'Qsi', 'ai', '–critic2withyi=ri+', '’', '.max', 'aQsi', '–actor‹s', 'r', 'respectively', 'state', 'action', 'reward', 'next', 'state', 'ith', 'memory', 'sampled', 'replay', 'memory', '.', '‹m', 'size', 'memory', 'batch', '.', '‹–critic', '–actor', 'critic', 'actor‡s', 'parameters', '.', '‹Q', '–critic', 'critic', 'DQN‡s', 'prediction', 'th', 'memorized', 'state-action‡s', 'Q-', 'Value', '.', '‹Q', '–actor', 'actor', 'DQN‡s', 'prediction', 'Q-Value', 'expect', 'next', 'state', 'chooses', 'action', 'a.‹y', 'target', 'Q-Value', 'th', 'memory', '.', 'Note', 'equal', 'reward', 'actually', 'observed', 'actor', 'plus', 'actor‡s', 'prediction', 'future', 'rewards', 'expect', 'play', 'optimally', 'far', 'knows', '.', '‹J', '–critic', 'cost', 'function', 'used', 'train', 'critic', 'DQN', '.', 'As', 'see', 'Mean', 'Squared', 'Error', 'target', 'Q-Values', 'estimated', 'actor', 'DQN', 'critic', 'DQN‡s', 'predictions', 'Q-Values', '.', 'The', 'replay', 'memory', 'optional', 'highly', 'recommended', '.', 'Without', 'would', 'train', 'critic', 'DQN', 'using', 'consecutive', 'experiences', 'may', 'correlated', '.', 'This', 'would', 'introduce', 'lot', 'bias', 'slow', 'training', 'algorithm‡s', 'convergence', '.', 'By', 'using', 'replay', 'memory', 'ensure', 'memories', 'fed', 'training', 'algorithm', 'fairly', 'uncorrelated', '.', 'Let‡s', 'add', 'critic', 'DQN‡s', 'training', 'operations', '.', 'First', 'need', 'able', 'compute', 'predicted', 'Q-Values', 'state-action', 'memory', 'batch', '.', 'Since', 'DQN', 'out…', 'puts', 'one', 'Q-Value', 'every', 'possible', 'action', 'need', 'keep', 'Q-Value', 'corresponds', 'action', 'actually', 'chosen', 'memory', '.', 'For', 'convert', 'action', 'one-hot', 'vector', 'recall', 'vector', 'full', '0s', 'except', 'Learning', 'Play', 'Ms.', 'Pac-Man', 'Using', 'Deep', 'Q-Learning', '|', '465', '1', 'th', 'index', 'multiply', 'Q-Values', 'zero', 'Q-Values', 'except', 'one', 'corresponding', 'memorized', 'action', '.', 'Then', 'sum', 'thefirst', 'axis', 'obtain', 'desired', 'Q-Value', 'prediction', 'memory', '.', 'X_action', '=', 'tf.placeholder', 'tf.int32', 'shape=', 'None', 'q_value', '=', 'tf.reduce_sum', 'critic_q_values', '*', 'tf.one_hot', 'X_action', 'n_outputs', 'axis=1', 'keep_dims=True', 'Next', 'let‡s', 'add', 'training', 'operations', 'assuming', 'target', 'Q-Values', 'fed', 'placeholder', '.', 'We', 'also', 'create', 'nontrainable', 'variable', 'called', 'global_step', '.', 'The', 'optimizer‡s', 'minimize', 'operation', 'take', 'care', 'incrementing', '.', 'Plus', 'cre…', 'ate', 'usual', 'init', 'operation', 'Saver.y', '=', 'tf.placeholder', 'tf.float32', 'shape=', 'None', '1', 'cost', '=', 'tf.reduce_mean', 'tf.square', '-', 'q_value', 'global_step', '=', 'tf.Variable', '0', 'trainable=False', 'name=•global_step•', 'optimizer', '=', 'tf.train.AdamOptimizer', 'learning_rate', 'training_op', '=', 'optimizer.minimize', 'cost', 'global_step=global_step', 'init', '=', 'tf.global_variables_initializer', 'saver', '=', 'tf.train.Saver', 'That‡s', 'construction', 'phase', '.', 'Before', 'look', 'execution', 'phase', 'need', 'couple', 'tools', '.', 'First', 'let‡s', 'start', 'implementing', 'replay', 'memory', '.', 'We', 'use', 'deque', 'list', 'since', 'efficient', 'pushing', 'items', 'queue', 'popping', 'end', 'list', 'maximum', 'memory', 'size', 'reached', '.', 'We', 'also', 'write', 'small', 'function', 'randomly', 'sample', 'batch', 'experiences', 'replay', 'memory', 'collections', 'import', 'dequereplay_memory_size', '=', '10000replay_memory', '=', 'deque', 'maxlen=replay_memory_size', 'def', 'sample_memories', 'batch_size', 'indices', '=', 'rnd.permutation', 'len', 'replay_memory', 'batch_size', 'cols', '=', '#', 'state', 'action', 'reward', 'next_state', 'continue', 'idx', 'indices', 'memory', '=', 'replay_memory', 'idx', 'col', 'value', 'zip', 'cols', 'memory', 'col.append', 'value', 'cols', '=', 'np.array', 'col', 'col', 'cols', 'return', 'cols', '0', 'cols', '1', 'cols', '2', '.reshape', '-1', '1', 'cols', '3', 'cols', '4', '.reshape', '-1', '1', 'Next', 'need', 'actor', 'explore', 'game', '.', 'We', 'use', 'Ò-greedy', 'policy', 'gradually', 'decrease', 'Ò', '1.0', '0.05', '50,000', 'training', 'steps', 'eps_min', '=', '0.05eps_max', '=', '1.0eps_decay_steps', '=', '50000466', '|', 'Chapter', '16', 'Reinforcement', 'Learning', 'def', 'epsilon_greedy', 'q_values', 'step', 'epsilon', '=', 'max', 'eps_min', 'eps_max', '-', 'eps_max-eps_min', '*', 'step/eps_decay_steps', 'rnd.rand', '<', 'epsilon', 'return', 'rnd.randint', 'n_outputs', '#', 'random', 'action', 'else', 'return', 'np.argmax', 'q_values', '#', 'optimal', 'actionThat‡s', '!', 'We', 'need', 'start', 'training', '.', 'The', 'execution', 'phase', 'contain', 'anything', 'complex', 'bit', 'long', 'take', 'deep', 'breath', '.', 'Ready', '?', 'Let‡s', 'go', '!', 'First', 'let‡s', 'initialize', 'variables', 'n_steps', '=', '100000', '#', 'total', 'number', 'training', 'stepstraining_start', '=', '1000', '#', 'start', 'training', '1,000', 'game', 'iterationstraining_interval', '=', '3', '#', 'run', 'training', 'step', 'every', '3', 'game', 'iterationssave_steps', '=', '50', '#', 'save', 'model', 'every', '50', 'training', 'stepscopy_steps', '=', '25', '#', 'copy', 'critic', 'actor', 'every', '25', 'training', 'stepsdiscount_rate', '=', '0.95skip_start', '=', '90', '#', 'skip', 'start', 'every', 'game', 'it•s', 'waiting', 'time', 'batch_size', '=', '50iteration', '=', '0', '#', 'game', 'iterationscheckpoint_path', '=', '``', './my_dqn.ckpt', \"''\", 'done', '=', 'True', '#', 'env', 'needs', 'resetNext', 'let‡s', 'open', 'session', 'run', 'main', 'training', 'loop', 'tf.Session', 'sess', 'os.path.isfile', 'checkpoint_path', 'saver.restore', 'sess', 'checkpoint_path', 'else', 'init.run', 'True', 'step', '=', 'global_step.eval', 'step', '>', '=', 'n_steps', 'break', 'iteration', '+=', '1', 'done', '#', 'game', 'start', 'obs', '=', 'env.reset', 'skip', 'range', 'skip_start', '#', 'skip', 'start', 'game', 'obs', 'reward', 'done', 'info', '=', 'env.step', '0', 'state', '=', 'preprocess_observation', 'obs', '#', 'Actor', 'evaluates', 'q_values', '=', 'actor_q_values.eval', 'feed_dict=', '{', 'X_state', 'state', '}', 'action', '=', 'epsilon_greedy', 'q_values', 'step', '#', 'Actor', 'plays', 'obs', 'reward', 'done', 'info', '=', 'env.step', 'action', 'next_state', '=', 'preprocess_observation', 'obs', '#', 'Let•s', 'memorize', 'happened', 'replay_memory.append', 'state', 'action', 'reward', 'next_state', '1.0', '-', 'done', 'state', '=', 'next_stateLearning', 'Play', 'Ms.', 'Pac-Man', 'Using', 'Deep', 'Q-Learning', '|', '467', 'iteration', '<', 'training_start', 'iteration', '%', 'training_interval', '!', '=', '0', 'continue', '#', 'Critic', 'learns', 'X_state_val', 'X_action_val', 'rewards', 'X_next_state_val', 'continues', '=', 'sample_memories', 'batch_size', 'next_q_values', '=', 'actor_q_values.eval', 'feed_dict=', '{', 'X_state', 'X_next_state_val', '}', 'max_next_q_values', '=', 'np.max', 'next_q_values', 'axis=1', 'keepdims=True', 'y_val', '=', 'rewards', '+', 'continues', '*', 'discount_rate', '*', 'max_next_q_values', 'training_op.run', 'feed_dict=', '{', 'X_state', 'X_state_val', 'X_action', 'X_action_val', 'y_val', '}', '#', 'Regularly', 'copy', 'critic', 'actor', 'step', '%', 'copy_steps', '==', '0', 'copy_critic_to_actor.run', '#', 'And', 'save', 'regularly', 'step', '%', 'save_steps', '==', '0', 'saver.save', 'sess', 'checkpoint_path', 'We', 'start', 'restoring', 'models', 'checkpoint', 'file', 'exists', 'else', 'initialize', 'variables', 'normally', '.', 'Then', 'main', 'loop', 'starts', 'iteration', 'counts', 'total', 'number', 'game', 'steps', 'gone', 'since', 'program', 'started', 'stepcounts', 'total', 'number', 'training', 'steps', 'since', 'training', 'started', 'checkpoint', 'restored', 'global', 'step', 'restored', 'well', '.', 'Then', 'code', 'resets', 'game', 'skipsthe', 'first', 'boring', 'game', 'steps', 'nothing', 'happens', '.', 'Next', 'actor', 'evaluates', 'plays', 'game', 'experience', 'memorized', 'replay', 'memory', '.', 'Then', 'regular', 'intervals', 'warmup', 'period', 'critic', 'goes', 'training', 'step', '.', 'It', 'samples', 'batch', 'memories', 'asks', 'actor', 'estimate', 'Q-Values', 'actions', 'next', 'state', 'applies', 'Equation', '16-7', 'compute', 'target', 'Q-Value', 'y_val.The', 'tricky', 'part', 'must', 'multiply', 'next', 'state‡s', 'Q-Values', 'continues', 'vector', 'zero', 'Q-Values', 'corresponding', 'memories', 'game', '.', 'Next', 'run', 'training', 'operation', 'improve', 'critic‡s', 'ability', 'pre…', 'dict', 'Q-Values', '.', 'Finally', 'regular', 'intervals', 'copy', 'critic', 'actor', 'save', 'model.468', '|', 'Chapter', '16', 'Reinforcement', 'Learning', 'Unfortunately', 'training', 'slow', 'use', 'laptop', 'training', 'take', 'days', 'Ms.', 'Pac-Man', 'gets', 'good', 'look', 'learning', 'curve', 'measuring', 'average', 'rewards', 'per', 'episode', 'notice', 'extremely', 'noisy', '.', 'At', 'points', 'may', 'apparent', 'progress', 'long', 'time', 'sud…', 'denly', 'agent', 'learns', 'survive', 'reasonable', 'amount', 'time', '.', 'As', 'mentioned', 'earlier', 'one', 'solution', 'inject', 'much', 'prior', 'knowl…', 'edge', 'possible', 'model', 'e.g.', 'preprocessing', 'rewards', 'also', 'try', 'bootstrap', 'model', 'byfirst', 'training', 'imitate', 'basic', 'strategy', '.', 'In', 'case', 'RL', 'still', 'requires', 'quite', 'lot', 'patience', 'tweaking', 'end', 'result', 'exciting', '.', 'Exercises1.How', 'would', 'define', 'Reinforcement', 'Learning', '?', 'How', 'different', 'regular', 'supervised', 'unsupervised', 'learning', '?', '2.Can', 'think', 'three', 'possible', 'applications', 'RL', 'mentioned', 'chapter', '?', 'For', 'environment', '?', 'What', 'agent', '?', 'What', 'possible', 'actions', '?', 'What', 'rewards', '?', '3.What', 'discount', 'rate', '?', 'Can', 'optimal', 'policy', 'change', 'modify', 'dis…', 'count', 'rate', '?', '4.How', 'measure', 'performance', 'Reinforcement', 'Learning', 'agent', '?', '5.What', 'credit', 'assignment', 'problem', '?', 'When', 'occur', '?', 'How', 'allevi…', 'ate', '?', '6.What', 'point', 'using', 'replay', 'memory', '?', '7.What', 'off-policy', 'RL', 'algorithm', '?', '8.Use', 'Deep', 'Q-Learning', 'tackle', 'OpenAI', 'gym‡s', 'ƒBypedalWalker-v2.⁄', 'The', 'Q-', 'networks', 'need', 'deep', 'task', '.', '9.Use', 'policy', 'gradients', 'train', 'agent', 'play', 'Pong', 'famous', 'Atari', 'game', 'Pong-v0', 'OpenAI', 'gym', '.', 'Beware', 'individual', 'observation', 'insufficient', 'tell', 'direction', 'speed', 'ball', '.', 'One', 'solution', 'pass', 'two', 'observations', 'time', 'neural', 'network', 'policy', '.', 'To', 'reduce', 'dimensionality', 'speed', 'train…', 'ing', 'definitely', 'preprocess', 'images', 'crop', 'resize', 'convert', 'black', 'white', 'possibly', 'merge', 'single', 'image', 'e.g.', 'overlaying', '.', '10.If', '$', '100', 'spare', 'purchase', 'Raspberry', 'Pi', '3', 'plus', 'cheap', 'robotics', 'components', 'install', 'TensorFlow', 'Pi', 'go', 'wild', '!', 'For', 'example', 'check', 'fun', 'post', 'Lukas', 'Biewald', 'take', 'look', 'GoPiGo', 'BrickPi', '.', 'Why', 'try', 'build', 'real-life', 'cartpole', 'training', 'robot', 'using', 'pol…', 'Exercises', '|', '469', 'icy', 'gradients', '?', 'Or', 'build', 'robotic', 'spider', 'learns', 'walk', 'give', 'rewards', 'time', 'gets', 'closer', 'objective', 'need', 'sensors', 'measure', 'dis…tance', 'objective', '.', 'The', 'limit', 'imagination', '.', 'Solutions', 'exercises', 'available', 'Appendix', 'A', '.Thank', 'You', '!', 'Before', 'close', 'last', 'chapter', 'book', 'I', 'would', 'like', 'thank', 'reading', 'last', 'paragraph', '.', 'I', 'truly', 'hope', 'much', 'pleasure', 'reading', 'book', 'I', 'writing', 'useful', 'projects', 'big', 'small', '.', 'If', 'find', 'errors', 'please', 'send', 'feedback', '.', 'More', 'generally', 'I', 'would', 'love', 'know', 'think', 'please', 'don‡t', 'hesitate', 'contact', 'via', 'O‡Reilly', 'ageron/', 'handson-ml', 'GitHub', 'project', '.', 'Going', 'forward', 'best', 'advice', 'practice', 'practice', 'try', 'going', 'exercises', 'done', 'already', 'play', 'Jupyter', 'notebooks', 'join', 'Kaggle.com', 'ML', 'community', 'watch', 'ML', 'courses', 'read', 'papers', 'attend', 'conferences', 'meet', 'experts', '.', 'You', 'may', 'also', 'want', 'study', 'topics', 'cover', 'book', 'including', 'recommender', 'systems', 'clustering', 'algorithms', 'anomalydetection', 'algorithms', 'genetic', 'algorithms.My', 'greatest', 'hope', 'book', 'inspire', 'build', 'wonderful', 'ML', 'applica…', 'tion', 'benefit', 'us', '!', 'What', '?', 'Aurłlien', 'Głron', 'November', '26th', '2016', '470', '|', 'Chapter', '16', 'Reinforcement', 'Learning', 'APPENDIX', 'AExercise', 'SolutionsSolutions', 'coding', 'exercises', 'available', 'online', 'Jupyter', 'notebooks', 'https', '//github.com/ageron/handson-ml', '.Chapter', '1', 'The', 'Machine', 'Learning', 'Landscape1.Machine', 'Learning', 'building', 'systems', 'learn', 'data', '.', 'Learning', 'means', 'getting', 'better', 'task', 'given', 'performance', 'measure', '.', '2.Machine', 'Learning', 'great', 'complex', 'problems', 'algorith…', 'mic', 'solution', 'replace', 'long', 'lists', 'hand-tuned', 'rules', 'build', 'systems', 'adapt', 'fluctuating', 'environments', 'finally', 'help', 'humans', 'learn', 'e.g.', 'data', 'mining', '.', '3.A', 'labeled', 'training', 'set', 'training', 'set', 'contains', 'desired', 'solution', 'a.k.a', '.', 'label', 'instance.4.The', 'two', 'common', 'supervised', 'tasks', 'regression', 'classification', '.', '5.Common', 'unsupervised', 'tasks', 'include', 'clustering', 'visualization', 'dimensionality', 'reduction', 'association', 'rule', 'learning', '.', '6.Reinforcement', 'Learning', 'likely', 'perform', 'best', 'want', 'robot', 'learn', 'walk', 'various', 'unknown', 'terrains', 'since', 'typically', 'type', 'problem', 'Reinforcement', 'Learning', 'tackles', '.', 'It', 'might', 'possible', 'express', 'problem', 'supervised', 'semisupervised', 'learning', 'problem', 'would', 'less', 'natural', '.', '7.If', 'don‡t', 'know', 'define', 'groups', 'use', 'clustering', 'algo…', 'rithm', 'unsupervised', 'learning', 'segment', 'customers', 'clusters', 'similar', 'customers', '.', 'However', 'know', 'groups', 'would', 'like', '471can', 'feed', 'many', 'examples', 'group', 'classification', 'algorithm', 'supervised', 'learning', 'classify', 'customers', 'groups', '.', '8.Spam', 'detection', 'typical', 'supervised', 'learning', 'problem', 'algorithm', 'fed', 'many', 'emails', 'along', 'label', 'spam', 'spam', '.', '9.An', 'online', 'learning', 'system', 'learn', 'incrementally', 'opposed', 'batch', 'learn…', 'ing', 'system', '.', 'This', 'makes', 'capable', 'adapting', 'rapidly', 'changing', 'data', 'autonomous', 'systems', 'training', 'large', 'quantities', 'data', '.', '10.Out-of-core', 'algorithms', 'handle', 'vast', 'quantities', 'data', 'fit', 'computer‡s', 'main', 'memory', '.', 'An', 'out-of-core', 'learning', 'algorithm', 'chops', 'data', 'mini-batches', 'uses', 'online', 'learning', 'techniques', 'learn', 'mini-', 'batches', '.', '11.An', 'instance-based', 'learning', 'system', 'learns', 'training', 'data', 'heart', 'given', 'new', 'instance', 'uses', 'similarity', 'measure', 'find', 'similar', 'learnedinstances', 'uses', 'make', 'predictions.12.A', 'model', 'one', 'model', 'parameters', 'determine', 'predict', 'given', 'new', 'instance', 'e.g.', 'slope', 'linear', 'model', '.', 'A', 'learning', 'algorithm', 'triesto', 'find', 'optimal', 'values', 'parameters', 'model', 'generalizes', 'well', 'new', 'instances', '.', 'A', 'hyperparameter', 'parameter', 'learning', 'algorithm', 'model', 'e.g.', 'amount', 'regularization', 'apply', '.', '13.Model-based', 'learning', 'algorithms', 'search', 'optimal', 'value', 'model', 'parameters', 'model', 'generalize', 'well', 'new', 'instances', '.', 'We', 'usually', 'train', 'systems', 'minimizing', 'cost', 'function', 'measures', 'bad', 'sys…', 'tem', 'making', 'predictions', 'training', 'data', 'plus', 'penalty', 'model', 'com…', 'plexity', 'model', 'regularized', '.', 'To', 'make', 'predictions', 'feed', 'new', 'instance‡s', 'features', 'model‡s', 'prediction', 'function', 'using', 'parameter', 'val…', 'ues', 'found', 'learning', 'algorithm.14.Some', 'main', 'challenges', 'Machine', 'Learning', 'lack', 'data', 'poor', 'data', 'quality', 'nonrepresentative', 'data', 'uninformative', 'features', 'excessively', 'simple', 'mod…', 'els', 'underfit', 'training', 'data', 'excessively', 'complex', 'models', 'overfit', 'data', '.', '15.If', 'model', 'performs', 'great', 'training', 'data', 'generalizes', 'poorly', 'new', 'instances', 'model', 'likely', 'overfitting', 'training', 'data', 'got', 'extremely', 'lucky', 'training', 'data', '.', 'Possible', 'solutions', 'overfitting', 'getting', 'data', 'simplifying', 'model', 'selecting', 'simpler', 'algorithm', 'reducing', 'number', 'parameters', 'features', 'used', 'regularizing', 'model', 'reducing', 'noise', 'training', 'data', '.', '16.A', 'test', 'set', 'used', 'estimate', 'generalization', 'error', 'model', 'make', 'new', 'instances', 'model', 'launched', 'production', '.', '472', '|', 'Appendix', 'A', 'Exercise', 'Solutions', '1If', 'draw', 'straight', 'line', 'two', 'points', 'curve', 'line', 'never', 'crosses', 'curve', '.', '17.A', 'validation', 'set', 'used', 'compare', 'models', '.', 'It', 'makes', 'possible', 'select', 'best', 'model', 'tune', 'hyperparameters', '.', '18.If', 'tune', 'hyperparameters', 'using', 'test', 'set', 'risk', 'overfitting', 'test', 'set', 'generalization', 'error', 'measure', 'optimistic', 'may', 'launch', 'model', 'performs', 'worse', 'expect', '.', '19.Cross-validation', 'technique', 'makes', 'possible', 'compare', 'models', 'model', 'selection', 'hyperparameter', 'tuning', 'without', 'need', 'separate', 'vali…', 'dation', 'set', '.', 'This', 'saves', 'precious', 'training', 'data', '.', 'Chapter', '2', 'End-to-End', 'Machine', 'Learning', 'ProjectSee', 'Jupyter', 'notebooks', 'available', 'https', '//github.com/ageron/handson-ml', '.Chapter', '3', 'Classi•cationSee', 'Jupyter', 'notebooks', 'available', 'https', '//github.com/ageron/handson-ml', '.Chapter', '4', 'Training', 'Linear', 'Models1.If', 'training', 'set', 'millions', 'features', 'use', 'Stochastic', 'Gradi…', 'ent', 'Descent', 'Mini-batch', 'Gradient', 'Descent', 'perhaps', 'Batch', 'Gradient', 'Descent', 'training', 'set', 'fits', 'memory', '.', 'But', 'use', 'Normal', 'Equa…', 'tion', 'computational', 'complexity', 'grows', 'quickly', 'quadrati…', 'cally', 'number', 'features', '.', '2.If', 'features', 'training', 'set', 'different', 'scales', 'cost', 'function', 'shape', 'elongated', 'bowl', 'Gradient', 'Descent', 'algorithms', 'take', 'long', 'time', 'converge', '.', 'To', 'solve', 'scale', 'data', 'training', 'model', '.', 'Note', 'Normal', 'Equation', 'work', 'fine', 'without', 'scaling', '.', '3.Gradient', 'Descent', 'get', 'stuck', 'local', 'minimum', 'training', 'Logistic', 'Regression', 'model', 'cost', 'function', 'convex', '.', '14.If', 'optimization', 'problem', 'convex', 'Linear', 'Regression', 'Logistic', 'Regression', 'assuming', 'learning', 'rate', 'high', 'Gradient', 'Descent', 'algorithms', 'approach', 'global', 'optimum', 'end', 'producing', 'fairly', 'similar', 'models', '.', 'However', 'unless', 'gradually', 'reduce', 'learning', 'rate', 'Stochastic', 'GD', 'Mini-batch', 'GD', 'never', 'truly', 'converge', 'instead', 'keep', 'jumping', 'back', 'forth', 'around', 'global', 'optimum', '.', 'This', 'means', 'even', 'Exercise', 'Solutions', '|', '473', '2Moreover', 'Normal', 'Equation', 'requires', 'computing', 'inverse', 'matrix', 'matrix', 'always', 'invertible', '.', 'In', 'contrast', 'matrix', 'Ridge', 'Regression', 'always', 'invertible', '.', 'let', 'run', 'long', 'time', 'Gradient', 'Descent', 'algorithms', 'produce', 'slightly', 'different', 'models', '.', '5.If', 'validation', 'error', 'consistently', 'goes', 'every', 'epoch', 'one', 'possibility', 'learning', 'rate', 'high', 'algorithm', 'diverging', '.', 'If', 'training', 'error', 'also', 'goes', 'clearly', 'problem', 'reduce', 'learning', 'rate', '.', 'However', 'training', 'error', 'going', 'model', 'overfitting', 'training', 'set', 'stop', 'training.6.Due', 'random', 'nature', 'neither', 'Stochastic', 'Gradient', 'Descent', 'Mini-batch', 'Gradient', 'Descent', 'guaranteed', 'make', 'progress', 'every', 'single', 'training', 'itera…', 'tion', '.', 'So', 'immediately', 'stop', 'training', 'validation', 'error', 'goes', 'may', 'stop', 'much', 'early', 'optimum', 'reached', '.', 'A', 'better', 'option', 'save', 'model', 'regular', 'intervals', 'improved', 'long', 'time', 'meaning', 'probably', 'never', 'beat', 'record', 'revert', 'best', 'saved', 'model.7.Stochastic', 'Gradient', 'Descent', 'fastest', 'training', 'iteration', 'since', 'considers', 'one', 'training', 'instance', 'time', 'generally', 'first', 'reach', 'vicinity', 'global', 'optimum', 'Mini-batch', 'GD', 'small', 'mini-batch', 'size', '.', 'However', 'Batch', 'Gradient', 'Descent', 'actually', 'converge', 'given', 'enough', 'training', 'time', '.', 'As', 'mentioned', 'Stochastic', 'GD', 'Mini-batch', 'GD', 'bounce', 'around', 'optimum', 'unless', 'gradually', 'reduce', 'learning', 'rate', '.', '8.If', 'validation', 'error', 'much', 'higher', 'training', 'error', 'likely', 'model', 'overfitting', 'training', 'set', '.', 'One', 'way', 'try', 'fix', 'reduce', 'polynomial', 'degree', 'model', 'fewer', 'degrees', 'freedom', 'less', 'likely', 'tooverfit', '.', 'Another', 'thing', 'try', 'regularize', 'model›for', 'example', 'adding', '—2', 'penalty', 'Ridge', '—1', 'penalty', 'Lasso', 'cost', 'function', '.', 'Thiswill', 'also', 'reduce', 'degrees', 'freedom', 'model', '.', 'Lastly', 'try', 'increase', 'size', 'training', 'set.9.If', 'training', 'error', 'validation', 'error', 'almost', 'equal', 'fairly', 'high', 'model', 'likely', 'underfitting', 'training', 'set', 'means', 'highbias', '.', 'You', 'try', 'reducing', 'regularization', 'hyperparameter', '‰.10.Let‡s', 'see', '‹A', 'model', 'regularization', 'typically', 'performs', 'better', 'model', 'without', 'regularization', 'generally', 'prefer', 'Ridge', 'Regression', 'plain', 'Linear', 'Regression.2‹Lasso', 'Regression', 'uses', '—1', 'penalty', 'tends', 'push', 'weights', 'exactly', 'zero', '.', 'This', 'leads', 'sparse', 'models', 'weights', 'zero', 'except', '474', '|', 'Appendix', 'A', 'Exercise', 'Solutions', 'important', 'weights', '.', 'This', 'way', 'perform', 'feature', 'selection', 'auto…', 'matically', 'good', 'suspect', 'features', 'actually', 'matter', '.', 'When', 'sure', 'prefer', 'Ridge', 'Regression.‹Elastic', 'Net', 'generally', 'preferred', 'Lasso', 'since', 'Lasso', 'may', 'behave', 'erratically', 'cases', 'several', 'features', 'strongly', 'correlated', 'features', 'training', 'instances', '.', 'However', 'add', 'extra', 'hyper…', 'parameter', 'tune', '.', 'If', 'want', 'Lasso', 'without', 'erratic', 'behavior', 'use', 'Elastic', 'Net', 'l1_ratio', 'close', '1.11.If', 'want', 'classify', 'pictures', 'outdoor/indoor', 'daytime/nighttime', 'since', 'exclusive', 'classes', 'i.e.', 'four', 'combinations', 'possible', 'train', 'two', 'Logistic', 'Regression', 'classifiers.12.See', 'Jupyter', 'notebooks', 'available', 'https', '//github.com/ageron/handson-ml', '.Chapter', '5', 'Support', 'Vector', 'Machines1.The', 'fundamental', 'idea', 'behind', 'Support', 'Vector', 'Machines', 'fit', 'widest', 'possi…', 'ble', 'ƒstreet⁄', 'classes', '.', 'In', 'words', 'goal', 'largest', 'pos…', 'sible', 'margin', 'decision', 'boundary', 'separates', 'two', 'classes', 'training', 'instances', '.', 'When', 'performing', 'soft', 'margin', 'classification', 'SVM', 'searches', 'compromise', 'perfectly', 'separating', 'two', 'classes', 'hav…', 'ing', 'widest', 'possible', 'street', 'i.e.', 'instances', 'may', 'end', 'street', '.', 'Another', 'key', 'idea', 'use', 'kernels', 'training', 'nonlinear', 'datasets', '.', '2.After', 'training', 'SVM', 'support', 'vector', 'instance', 'located', 'ƒstreet⁄', 'see', 'previous', 'answer', 'including', 'border', '.', 'The', 'decision', 'boundary', 'entirely', 'determined', 'support', 'vectors', '.', 'Any', 'instance', 'support', 'vector', 'i.e.', 'street', 'influence', 'whatsoever', 'could', 'remove', 'add', 'instances', 'move', 'around', 'long', 'stay', 'street', 'won‡t', 'affect', 'decision', 'boundary', '.', 'Computing', 'predictions', 'involves', 'sup…', 'port', 'vectors', 'whole', 'training', 'set.3.SVMs', 'try', 'fit', 'largest', 'possible', 'ƒstreet⁄', 'classes', 'see', 'first', 'answer', 'training', 'set', 'scaled', 'SVM', 'tend', 'neglect', 'smallfeatures', 'see', 'Figure', '5-2', '.4.An', 'SVM', 'classifier', 'output', 'distance', 'test', 'instance', 'deci…sion', 'boundary', 'use', 'confidence', 'score', '.', 'However', 'score', 'directly', 'converted', 'estimation', 'class', 'probability', '.', 'If', 'set', 'probability=True', 'creating', 'SVM', 'Scikit-Learn', 'training', 'calibrate', 'probabilities', 'using', 'Logistic', 'Regression', 'SVM‡s', 'scores', 'trained', 'additional', 'five-fold', 'cross-validation', 'training', 'data', '.', 'This', 'add', 'predict_proba', 'predict_log_proba', 'methods', 'SVM.Exercise', 'Solutions', '|', '475', '3log2', 'binary', 'log', 'log', '2', '=', 'log', '/', 'log', '2', '.5.This', 'question', 'applies', 'linear', 'SVMs', 'since', 'kernelized', 'use', 'dual', 'form', '.', 'The', 'computational', 'complexity', 'primal', 'form', 'SVM', 'problem', 'proportional', 'number', 'training', 'instances', 'computational', 'complexity', 'dual', 'form', 'proportional', 'number', 'm2', 'm3', '.', 'Soif', 'millions', 'instances', 'definitely', 'use', 'primal', 'form', 'dual', 'form', 'much', 'slow', '.', '6.If', 'SVM', 'classifier', 'trained', 'RBF', 'kernel', 'underfits', 'training', 'set', 'theremight', 'much', 'regularization', '.', 'To', 'decrease', 'need', 'increase', 'gamma', 'C', '.7.Let‡s', 'call', 'QP', 'parameters', 'hard-margin', 'problem', 'H', 'f', 'A', 'b', 'seeƒQuadratic', 'Programming⁄', 'page', '159', '.', 'The', 'QP', 'parameters', 'soft-marginproblem', 'additional', 'parameters', 'np', '=', 'n', '+', '1', '+', 'additional', 'con…straints', 'nc', '=', '2m', '.', 'They', 'defined', 'like', '‹H', 'equal', 'H', 'plus', 'columns', '0s', 'right', 'rows', '0s', 'bottom', '=000', '‹f', 'equal', 'f', 'additional', 'elements', 'equal', 'value', 'hyper…', 'parameter', 'C.‹b', 'equal', 'bwith', 'additional', 'elements', 'equal', '0', '.', '‹A', 'equal', 'A', 'extra', '‰', 'identity', 'matrix', 'Im', 'appended', 'right', '–', 'Im', 'rest', 'filled', 'zeros', '=m0', '”', 'mFor', 'solutions', 'exercises', '8', '9', '10', 'please', 'see', 'Jupyter', 'notebooks', 'available', 'https', '//github.com/ageron/handson-ml', '.Chapter', '6', 'Decision', 'Trees1.The', 'depth', 'well-balanced', 'binary', 'tree', 'containing', 'leaves', 'equal', 'log', '2', '3', 'rounded', '.', 'A', 'binary', 'Decision', 'Tree', 'one', 'makes', 'binary', 'decisions', 'case', 'trees', 'Scikit-Learn', 'end', 'less', 'well', 'balanced', 'end', 'training', 'one', 'leaf', 'per', 'training', 'instance', 'trained', 'without', 'restric…tions', '.', 'Thus', 'training', 'set', 'contains', 'one', 'million', 'instances', 'Decision', 'Tree', 'depth', 'log', '2', '106', 'Ÿ', '20', 'actually', 'bit', 'since', 'tree', 'generallynot', 'perfectly', 'well', 'balanced', '.476', '|', 'Appendix', 'A', 'Exercise', 'Solutions', '2.A', 'node‡s', 'Gini', 'impurity', 'generally', 'lower', 'parent‡s', '.', 'This', 'ensured', 'CART', 'training', 'algorithm‡s', 'cost', 'function', 'splits', 'node', 'way', 'minimizes', 'weighted', 'sum', 'children‡s', 'Gini', 'impurities', '.', 'However', 'one', 'child', 'smaller', 'possible', 'higher', 'Gini', 'impurity', 'parent', 'long', 'increase', 'compensated', 'decrease', 'child‡s', 'impurity', '.', 'For', 'example', 'consider', 'node', 'containing', 'four', 'instances', 'class', 'A', '1', 'class', 'B', '.', 'Its', 'Gini', 'impurity', '1', '”', '152', '”', '452', '=', '0.32.Now', 'suppose', 'dataset', 'one-dimensional', 'instances', 'lined', 'following', 'order', 'A', 'B', 'A', 'A', 'A', '.', 'You', 'verify', 'algorithm', 'split', 'node', 'second', 'instance', 'producing', 'one', 'child', 'node', 'instances', 'A', 'B', 'child', 'node', 'instances', 'A', 'A', 'A', '.', 'The', 'first', 'child', 'node‡s', 'Gini', 'impurity', '1', '”', '122', '”', '122', '=', '0.5', 'higher', 'parent', '.', 'This', 'compensated', 'fact', 'node', 'pure', 'overall', 'weighted', 'Gini', 'impurity', '25‰', '0.5', '+', '35‰0', '=', '0.2', 'lower', 'parent‡s', 'Gini', 'impurity', '.', '3.If', 'Decision', 'Tree', 'overfitting', 'training', 'set', 'may', 'good', 'idea', 'decrease', 'max_depth', 'since', 'constrain', 'model', 'regularizing', 'it.4.Decision', 'Trees', 'don‡t', 'care', 'whether', 'training', 'data', 'scaled', 'centered', 'that‡s', 'one', 'nice', 'things', '.', 'So', 'Decision', 'Tree', 'underfits', 'train…', 'ing', 'set', 'scaling', 'input', 'features', 'waste', 'time', '.', '5.The', 'computational', 'complexity', 'training', 'Decision', 'Tree', 'O', 'n', '‰', 'log', '.', 'Soif', 'multiply', 'training', 'set', 'size', '10', 'training', 'time', 'multiplied', 'K', '=', 'n', '‰', '10m', '‰', 'log', '10m', '/', 'n', '‰', '‰', 'log', '=', '10', '‰', 'log', '10m', '/', 'log', '.', 'If', '=106', 'K', 'Ÿ', '11.7', 'expect', 'training', 'time', 'roughly', '11.7', 'hours.6.Presorting', 'training', 'set', 'speeds', 'training', 'dataset', 'smaller', 'thousand', 'instances', '.', 'If', 'contains', '100,000', 'instances', 'setting', 'presort=Truewill', 'considerably', 'slow', 'training.For', 'solutions', 'exercises', '7', '8', 'please', 'see', 'Jupyter', 'notebooks', 'available', 'https', '//github.com/ageron/handson-ml', '.Chapter', '7', 'Ensemble', 'Learning', 'Random', 'Forests1.If', 'trained', 'five', 'different', 'models', 'achieve', '95', '%', 'precision', 'try', 'combining', 'voting', 'ensemble', 'often', 'give', 'even', 'better', 'results', '.', 'It', 'works', 'better', 'models', 'different', 'e.g.', 'SVM', 'classi…', 'fier', 'Decision', 'Tree', 'classifier', 'Logistic', 'Regression', 'classifier', '.', 'It', 'even', 'better', 'trained', 'different', 'training', 'instances', 'that‡s', 'whole', 'point', 'bagging', 'pasting', 'ensembles', 'still', 'work', 'long', 'models', 'different', '.', 'Exercise', 'Solutions', '|', '477', '2.A', 'hard', 'voting', 'classifier', 'counts', 'votes', 'classifier', 'ensemble', 'picks', 'class', 'gets', 'votes', '.', 'A', 'soft', 'voting', 'classifier', 'computes', 'average', 'estimated', 'class', 'probability', 'class', 'picks', 'class', 'highest', 'probability', '.', 'This', 'gives', 'high-confidence', 'votes', 'weight', 'often', 'per…', 'forms', 'better', 'works', 'every', 'classifier', 'able', 'estimate', 'class', 'probabil…', 'ities', 'e.g.', 'SVM', 'classifiers', 'Scikit-Learn', 'must', 'set', 'probability=True', '.3.It', 'quite', 'possible', 'speed', 'training', 'bagging', 'ensemble', 'distributing', 'across', 'multiple', 'servers', 'since', 'predictor', 'ensemble', 'independent', 'others', '.', 'The', 'goes', 'pasting', 'ensembles', 'Random', 'Forests', 'reason', '.', 'However', 'predictor', 'boosting', 'ensemble', 'built', 'based', 'previous', 'predictor', 'training', 'necessarily', 'sequential', 'gain', 'anything', 'distributing', 'training', 'across', 'multiple', 'servers', '.', 'Regarding', 'stacking', 'ensembles', 'predictors', 'given', 'layer', 'independent', 'trained', 'parallel', 'multiple', 'servers', '.', 'However', 'predictors', 'one', 'layer', 'trained', 'predictors', 'previous', 'layer', 'trained.4.With', 'out-of-bag', 'evaluation', 'predictor', 'bagging', 'ensemble', 'evaluated', 'using', 'instances', 'trained', 'held', '.', 'This', 'makes', 'pos…', 'sible', 'fairly', 'unbiased', 'evaluation', 'ensemble', 'without', 'need', 'additional', 'validation', 'set', '.', 'Thus', 'instances', 'available', 'training', 'ensemble', 'perform', 'slightly', 'better', '.', '5.When', 'growing', 'tree', 'Random', 'Forest', 'random', 'subset', 'features', 'considered', 'splitting', 'node', '.', 'This', 'true', 'well', 'Extra-', 'Trees', 'go', 'one', 'step', 'rather', 'searching', 'best', 'possible', 'thresholds', 'like', 'regular', 'Decision', 'Trees', 'use', 'random', 'thresholds', 'feature', '.', 'This', 'extra', 'randomness', 'acts', 'like', 'form', 'regularization', 'Random', 'Forest', 'overfits', 'training', 'data', 'Extra-Trees', 'might', 'perform', 'better', '.', 'Moreover', 'since', 'Extra-Trees', 'don‡t', 'search', 'best', 'possible', 'thresholds', 'much', 'faster', 'train', 'Random', 'Forests', '.', 'However', 'neither', 'faster', 'slower', 'Random', 'Forests', 'making', 'predictions', '.', '6.If', 'AdaBoost', 'ensemble', 'underfits', 'training', 'data', 'try', 'increasing', 'number', 'estimators', 'reducing', 'regularization', 'hyperparameters', 'base', 'estimator', '.', 'You', 'may', 'also', 'try', 'slightly', 'increasing', 'learning', 'rate', '.', '7.If', 'Gradient', 'Boosting', 'ensemble', 'overfits', 'training', 'set', 'try', 'decreasing', 'learning', 'rate', '.', 'You', 'could', 'also', 'use', 'early', 'stopping', 'find', 'right', 'number', 'predictors', 'probably', 'many', '.', 'For', 'solutions', 'exercises', '8', '9', 'please', 'see', 'Jupyter', 'notebooks', 'available', 'https', '//github.com/ageron/handson-ml', '.478', '|', 'Appendix', 'A', 'Exercise', 'Solutions', 'Chapter', '8', 'Dimensionality', 'Reduction1.Motivations', 'drawbacks', '‹The', 'main', 'motivations', 'dimensionality', 'reduction', '›To', 'speed', 'subsequent', 'training', 'algorithm', 'cases', 'may', 'even', 'remove', 'noise', 'redundant', 'features', 'making', 'training', 'algorithm', 'per…', 'form', 'better', '.›To', 'visualize', 'data', 'gain', 'insights', 'important', 'features', '.', '›Simply', 'save', 'space', 'compression', '.', '‹The', 'main', 'drawbacks', '›Some', 'information', 'lost', 'possibly', 'degrading', 'performance', 'subse…', 'quent', 'training', 'algorithms', '.', '›It', 'computationally', 'intensive', '.', '›It', 'adds', 'complexity', 'Machine', 'Learning', 'pipelines', '.', '›Transformed', 'features', 'often', 'hard', 'interpret', '.', '2.The', 'curse', 'dimensionality', 'refers', 'fact', 'many', 'problems', 'exist', 'low-dimensional', 'space', 'arise', 'high-dimensional', 'space', '.', 'In', 'Machine', 'Learning', 'one', 'common', 'manifestation', 'fact', 'randomly', 'sampled', 'high-', 'dimensional', 'vectors', 'generally', 'sparse', 'increasing', 'risk', 'overfitting', 'making', 'difficult', 'identify', 'patterns', 'data', 'without', 'plenty', 'training', 'data', '.', '3.Once', 'dataset‡s', 'dimensionality', 'reduced', 'using', 'one', 'algorithms', 'discussed', 'almost', 'always', 'impossible', 'perfectly', 'reverse', 'operation', 'information', 'gets', 'lost', 'dimensionality', 'reduction', '.', 'Moreover', 'algorithms', 'PCA', 'simple', 'reverse', 'transformation', 'pro…', 'cedure', 'reconstruct', 'dataset', 'relatively', 'similar', 'original', 'algo…', 'rithms', 'T-SNE', '.', '4.PCA', 'used', 'significantly', 'reduce', 'dimensionality', 'datasets', 'even', 'highly', 'nonlinear', 'least', 'get', 'rid', 'useless', 'dimensions', '.', 'However', 'useless', 'dimensions›for', 'example', 'Swiss', 'roll›then', 'reducing', 'dimensionality', 'PCA', 'lose', 'much', 'information', '.', 'You', 'want', 'unroll', 'Swiss', 'roll', 'squash', 'it.5.That‡s', 'trick', 'question', 'depends', 'dataset', '.', 'Let‡s', 'look', 'two', 'extreme', 'exam…', 'ples', '.', 'First', 'suppose', 'dataset', 'composed', 'points', 'almost', 'perfectly', 'aligned', '.', 'In', 'case', 'PCA', 'reduce', 'dataset', 'one', 'dimension', 'still', 'preserving', '95', '%', 'variance', '.', 'Now', 'imagine', 'dataset', 'com…', 'posed', 'perfectly', 'random', 'points', 'scattered', 'around', '1,000', 'dimensions', '.', 'In', 'Exercise', 'Solutions', '|', '479', 'case', '1,000', 'dimensions', 'required', 'preserve', '95', '%', 'variance', '.', 'So', 'answer', 'depends', 'dataset', 'could', 'number', '1', '1,000', '.', 'Plotting', 'explained', 'variance', 'function', 'number', 'dimensions', 'one', 'way', 'get', 'rough', 'idea', 'dataset‡s', 'intrinsic', 'dimensionality', '.', '6.Regular', 'PCA', 'default', 'works', 'dataset', 'fits', 'memory', '.', 'Incre…', 'mental', 'PCA', 'useful', 'large', 'datasets', 'don‡t', 'fit', 'memory', 'slower', 'regular', 'PCA', 'dataset', 'fits', 'memory', 'prefer', 'regular', 'PCA', '.', 'Incremental', 'PCA', 'also', 'useful', 'online', 'tasks', 'need', 'apply', 'PCA', 'fly', 'every', 'time', 'new', 'instance', 'arrives', '.', 'Randomized', 'PCA', 'useful', 'want', 'considerably', 'reduce', 'dimensionality', 'dataset', 'fits', 'memory', 'case', 'much', 'faster', 'regular', 'PCA', '.', 'Finally', 'Kernel', 'PCA', 'useful', 'nonlinear', 'datasets', '.', '7.Intuitively', 'dimensionality', 'reduction', 'algorithm', 'performs', 'well', 'eliminates', 'lot', 'dimensions', 'dataset', 'without', 'losing', 'much', 'information', '.', 'One', 'way', 'measure', 'apply', 'reverse', 'transformation', 'measure', 'reconstruction', 'error', '.', 'However', 'dimensionality', 'reduction', 'algorithms', 'pro…', 'vide', 'reverse', 'transformation', '.', 'Alternatively', 'using', 'dimensionality', 'reduction', 'preprocessing', 'step', 'another', 'Machine', 'Learning', 'algorithm', 'e.g.', 'Random', 'Forest', 'classifier', 'simply', 'measure', 'performance', 'second', 'algorithm', 'dimensionality', 'reduction', 'lose', 'much', 'information', 'algorithm', 'perform', 'well', 'using', 'original', 'dataset', '.', '8.It', 'absolutely', 'make', 'sense', 'chain', 'two', 'different', 'dimensionality', 'reduction', 'algorithms', '.', 'A', 'common', 'example', 'using', 'PCA', 'quickly', 'get', 'rid', 'large', 'num…', 'ber', 'useless', 'dimensions', 'applying', 'another', 'much', 'slower', 'dimensionality', 'reduction', 'algorithm', 'LLE', '.', 'This', 'two-step', 'approach', 'likely', 'yield', 'performance', 'using', 'LLE', 'fraction', 'time', '.', 'For', 'solutions', 'exercises', '9', '10', 'please', 'see', 'Jupyter', 'notebooks', 'available', 'https', '//github.com/ageron/handson-ml', '.Chapter', '9', 'Up', 'Running', 'TensorFlow1.Main', 'benefits', 'drawbacks', 'creating', 'computation', 'graph', 'rather', 'directly', 'executing', 'computations', '‹Main', 'benefits', '›TensorFlow', 'automatically', 'compute', 'gradients', 'using', 'reverse-mode', 'autodiff', '.', '›TensorFlow', 'take', 'care', 'running', 'operations', 'parallel', 'different', 'threads.480', '|', 'Appendix', 'A', 'Exercise', 'Solutions', '›It', 'makes', 'easier', 'run', 'model', 'across', 'different', 'devices', '.', '›It', 'simplifies', 'introspection›for', 'example', 'view', 'model', 'TensorBoard', '.', '‹Main', 'drawbacks', '›It', 'makes', 'learning', 'curve', 'steeper', '.', '›It', 'makes', 'step-by-step', 'debugging', 'harder', '.', '2.Yes', 'statement', 'a_val', '=', 'a.eval', 'session=sess', 'indeed', 'equivalent', 'a_val=', 'sess.run', '.3.No', 'statement', 'a_val', 'b_val', '=', 'a.eval', 'session=sess', 'b.eval', 'session=sess', 'equivalent', 'a_val', 'b_val', '=', 'sess.run', 'b', '.', 'Indeed', 'thefirst', 'statement', 'runs', 'graph', 'twice', 'compute', 'compute', 'b', 'second', 'statement', 'runs', 'graph', '.', 'If', 'operations', 'ops', 'depend', 'side', 'effects', 'e.g.', 'variable', 'modified', 'item', 'inserted', 'queue', 'reader', 'reads', 'file', 'effects', 'different', '.', 'If', 'don‡t', 'side', 'effects', 'statements', 'return', 'result', 'second', 'statement', 'faster', 'first', '.', '4.No', 'run', 'two', 'graphs', 'session', '.', 'You', 'would', 'merge', 'graphs', 'single', 'graph', 'first', '.', '5.In', 'local', 'TensorFlow', 'sessions', 'manage', 'variable', 'values', 'create', 'graph', 'gcontaining', 'variable', 'w', 'start', 'two', 'threads', 'open', 'local', 'session', 'eachthread', 'using', 'graph', 'g', 'session', 'copy', 'variable', 'w.', 'However', 'distributed', 'TensorFlow', 'variable', 'values', 'stored', 'containers', 'managed', 'cluster', 'sessions', 'connect', 'cluster', 'use', 'container', 'share', 'variable', 'value', 'w.6.A', 'variable', 'initialized', 'call', 'initializer', 'destroyed', 'session', 'ends', '.', 'In', 'distributed', 'TensorFlow', 'variables', 'live', 'containers', 'clus…', 'ter', 'closing', 'session', 'destroy', 'variable', '.', 'To', 'destroy', 'variable', 'need', 'clear', 'container', '.', '7.Variables', 'placeholders', 'extremely', 'different', 'beginners', 'often', 'confuse', '‹A', 'variable', 'operation', 'holds', 'value', '.', 'If', 'run', 'variable', 'returns', 'value', '.', 'Before', 'run', 'need', 'initialize', '.', 'You', 'change', 'variable‡s', 'value', 'example', 'using', 'assignment', 'operation', '.', 'It', 'stateful', 'variable', 'keeps', 'value', 'upon', 'successive', 'runs', 'graph', '.', 'It', 'typi…', 'cally', 'used', 'hold', 'model', 'parameters', 'also', 'purposes', 'e.g.', 'count', 'global', 'training', 'step', '.‹Placeholders', 'technically', 'don‡t', 'much', 'hold', 'information', 'type', 'shape', 'tensor', 'represent', 'value', '.', 'In', 'fact', 'Exercise', 'Solutions', '|', '481', 'try', 'evaluate', 'operation', 'depends', 'placeholder', 'must', 'feed', 'TensorFlow', 'value', 'placeholder', 'using', 'feed_dict', 'argument', 'else', 'get', 'exception', '.', 'Placeholders', 'typically', 'used', 'feed', 'trainingor', 'test', 'data', 'TensorFlow', 'execution', 'phase', '.', 'They', 'also', 'useful', 'pass', 'value', 'assignment', 'node', 'change', 'value', 'variable', 'e.g.', 'model', 'weights', '.', '8.If', 'run', 'graph', 'evaluate', 'operation', 'depends', 'placeholder', 'don‡t', 'feed', 'value', 'get', 'exception', '.', 'If', 'operation', 'depend', 'placeholder', 'exception', 'raised', '.', '9.When', 'run', 'graph', 'feed', 'output', 'value', 'operation', 'value', 'placeholders', '.', 'In', 'practice', 'however', 'rather', 'rare', 'useful', 'example', 'caching', 'output', 'frozen', 'layers', 'see', 'Chapter', '11', '.10.You', 'specify', 'variable‡s', 'initial', 'value', 'constructing', 'graph', 'initialized', 'later', 'run', 'variable‡s', 'initializer', 'execution', 'phase', '.', 'If', 'want', 'change', 'variable‡s', 'value', 'anything', 'want', 'execution', 'phase', 'simplest', 'option', 'create', 'assignment', 'node', 'dur…', 'ing', 'graph', 'construction', 'phase', 'using', 'tf.assign', 'function', 'passing', 'thevariable', 'placeholder', 'parameters', '.', 'During', 'execution', 'phase', 'canrun', 'assignment', 'operation', 'feed', 'variable‡s', 'new', 'value', 'using', 'place…holder', '.', 'import', 'tensorflow', 'tfx', '=', 'tf.Variable', 'tf.random_uniform', 'shape=', 'minval=0.0', 'maxval=1.0', 'x_new_val', '=', 'tf.placeholder', 'shape=', 'dtype=tf.float32', 'x_assign', '=', 'tf.assign', 'x', 'x_new_val', 'tf.Session', 'x.initializer.run', '#', 'random', 'number', 'sampled', '*now*', 'print', 'x.eval', '#', '0.646157', 'random', 'number', 'x_assign.eval', 'feed_dict=', '{', 'x_new_val', '5.0', '}', 'print', 'x.eval', '#', '5.011.Reverse-mode', 'autodiff', 'implemented', 'TensorFlow', 'needs', 'traverse', 'graph', 'twice', 'order', 'compute', 'gradients', 'cost', 'function', 'regards', 'number', 'variables', '.', 'On', 'hand', 'forward-mode', 'autodiff', 'would', 'need', 'run', 'variable', '10', 'times', 'want', 'gradients', 'regards', '10', 'different', 'variables', '.', 'As', 'symbolic', 'differentiation', 'would', 'build', 'different', 'graph', 'compute', 'gradients', 'would', 'traverse', 'original', 'graph', 'except', 'building', 'new', 'gradients', 'graph', '.', 'A', 'highly', 'optimized', 'symbolic', 'differentiation', 'system', 'could', 'potentially', 'run', 'new', 'gradients', 'graph', 'compute', 'gradients', 'regards', 'variables', 'new', 'graph', 'may', 'horribly', 'complex', 'inefficient', 'compared', 'original', 'graph', '.', '482', '|', 'Appendix', 'A', 'Exercise', 'Solutions', '12.See', 'Jupyter', 'notebooks', 'available', 'https', '//github.com/ageron/handson-ml', '.Chapter', '10', 'Introduction', 'Arti•cial', 'Neural', 'Networks1.Here', 'neural', 'network', 'based', 'original', 'artificial', 'neurons', 'computes', 'A', 'B', 'represents', 'exclusive', 'OR', 'using', 'fact', 'A', 'B', '=', 'A', '°', 'B', '°', 'A', 'B', '.', 'There', 'solutions›for', 'example', 'using', 'fact', 'A', 'B', '=', 'A', 'B', '°', 'A', 'B', 'fact', 'A', 'B', '=', 'A', 'B', '°', 'A', 'B', 'on.2.A', 'classical', 'Perceptron', 'converge', 'dataset', 'linearly', 'separable', 'won‡t', 'able', 'estimate', 'class', 'probabilities', '.', 'In', 'contrast', 'Logistic', 'Regression', 'classifier', 'converge', 'good', 'solution', 'even', 'dataset', 'linearly', 'sepa…', 'rable', 'output', 'class', 'probabilities', '.', 'If', 'change', 'Perceptron‡s', 'activa…', 'tion', 'function', 'logistic', 'activation', 'function', 'softmax', 'activation', 'function', 'multiple', 'neurons', 'train', 'using', 'Gradient', 'Descent', 'optimization', 'algorithm', 'minimizing', 'cost', 'function', 'typically', 'cross', 'entropy', 'becomes', 'equivalent', 'Logistic', 'Regression', 'classifier', '.', '3.The', 'logistic', 'activation', 'function', 'key', 'ingredient', 'training', 'first', 'MLPs', 'derivative', 'always', 'nonzero', 'Gradient', 'Descent', 'always', 'roll', 'slope', '.', 'When', 'activation', 'function', 'step', 'function', 'Gradient', 'Descent', 'move', 'slope', '.', '4.The', 'step', 'function', 'logistic', 'function', 'hyperbolic', 'tangent', 'rectified', 'lin…', 'ear', 'unit', 'see', 'Figure', '10-8', '.', 'See', 'Chapter', '11', 'examples', 'ELU', 'variants', 'ReLU', '.', '5.Considering', 'MLP', 'described', 'question', 'suppose', 'MLP', 'com…', 'posed', 'one', 'input', 'layer', '10', 'passthrough', 'neurons', 'followed', 'one', 'hidden', 'layer', '50', 'artificial', 'neurons', 'finally', 'one', 'output', 'layer', '3', 'artificial', 'neu…', 'rons', '.', 'All', 'artificial', 'neurons', 'use', 'ReLU', 'activation', 'function', '.', 'Exercise', 'Solutions', '|', '483', '4When', 'values', 'predict', 'vary', 'many', 'orders', 'magnitude', 'may', 'want', 'predict', 'loga…', 'rithm', 'target', 'value', 'rather', 'target', 'value', 'directly', '.', 'Simply', 'computing', 'exponential', 'neural', 'network‡s', 'output', 'give', 'estimated', 'value', 'since', 'exp', 'log', 'v', '=', 'v', '.‹The', 'shape', 'input', 'matrix', 'X', '‰', '10', 'represents', 'training', 'batch', 'size', '.', '‹The', 'shape', 'hidden', 'layer‡s', 'weight', 'vector', 'Wh', '10', '‰', '50', 'length', 'ofits', 'bias', 'vector', 'bh', '50.‹The', 'shape', 'output', 'layer‡s', 'weight', 'vector', 'Wo', '50', '‰', '3', 'length', 'itsbias', 'vector', 'bo', '3.‹The', 'shape', 'network‡s', 'output', 'matrix', 'Y', '‰', '3.‹Y', '=', 'X', '’', 'Wh', '+', 'bh', '’', 'Wo', '+', 'bo', '.', 'Note', 'adding', 'bias', 'vector', 'matrix', 'added', 'every', 'single', 'row', 'matrix', 'called', 'broadcast…', 'ing', '.6.To', 'classify', 'email', 'spam', 'ham', 'need', 'one', 'neuron', 'output', 'layer', 'neural', 'network›for', 'example', 'indicating', 'probability', 'email', 'spam', '.', 'You', 'would', 'typically', 'use', 'logistic', 'activation', 'function', 'output', 'layer', 'estimating', 'probability', '.', 'If', 'instead', 'want', 'tackle', 'MNIST', 'need', '10', 'neurons', 'output', 'layer', 'must', 'replace', 'logistic', 'function', 'softmax', 'activation', 'function', 'handle', 'multiple', 'classes', 'outputting', 'one', 'probability', 'per', 'class', '.', 'Now', 'want', 'neural', 'network', 'predict', 'housing', 'prices', 'like', 'Chapter', '2', 'need', 'one', 'output', 'neuron', 'using', 'activation', 'function', 'output', 'layer', '.', '47.Backpropagation', 'technique', 'used', 'train', 'artificial', 'neural', 'networks', '.', 'It', 'first', 'computes', 'gradients', 'cost', 'function', 'regards', 'every', 'model', 'parame…', 'ter', 'weights', 'biases', 'performs', 'Gradient', 'Descent', 'step', 'using', 'gradients', '.', 'This', 'backpropagation', 'step', 'typically', 'performed', 'thou…', 'sands', 'millions', 'times', 'using', 'many', 'training', 'batches', 'model', 'parame…', 'ters', 'converge', 'values', 'hopefully', 'minimize', 'cost', 'function', '.', 'To', 'compute', 'gradients', 'backpropagation', 'uses', 'reverse-mode', 'autodiff', 'although', 'wasn‡t', 'called', 'backpropagation', 'invented', 'reinvented', 'sev…', 'eral', 'times', '.', 'Reverse-mode', 'autodiff', 'performs', 'forward', 'pass', 'computa…', 'tion', 'graph', 'computing', 'every', 'node‡s', 'value', 'current', 'training', 'batch', 'performs', 'reverse', 'pass', 'computing', 'gradients', 'see', 'Appendix', 'D', 'details', '.', 'So', 'what‡s', 'difference', '?', 'Well', 'backpropagation', 'refers', 'whole', 'process', 'training', 'artificial', 'neural', 'network', 'using', 'multiple', 'backpropa…', 'gation', 'steps', 'computes', 'gradients', 'uses', 'perform', 'Gra…', 'dient', 'Descent', 'step', '.', 'In', 'contrast', 'reverse-mode', 'autodiff', 'simply', 'technique', 'compute', 'gradients', 'efficiently', 'happens', 'used', 'backpropagation', '.', '484', '|', 'Appendix', 'A', 'Exercise', 'Solutions', '5In', 'Chapter', '11', 'discuss', 'many', 'techniques', 'introduce', 'additional', 'hyperparameters', 'type', 'weight', 'initiali…', 'zation', 'activation', 'function', 'hyperparameters', 'e.g.', 'amount', 'leak', 'leaky', 'ReLU', 'Gradient', 'Clipping', 'thres…', 'hold', 'type', 'optimizer', 'hyperparameters', 'e.g.', 'momentum', 'hyperparameter', 'using', 'MomentumOptimizer', 'type', 'regularization', 'layer', 'regularization', 'hyperparameters', 'e.g.', 'drop…', 'rate', 'using', 'dropout', '.', '8.Here', 'list', 'hyperparameters', 'tweak', 'basic', 'MLP', 'num…', 'ber', 'hidden', 'layers', 'number', 'neurons', 'hidden', 'layer', 'activa…', 'tion', 'function', 'used', 'hidden', 'layer', 'output', 'layer', '.', '5', 'In', 'general', 'theReLU', 'activation', 'function', 'one', 'variants', 'see', 'Chapter', '11', 'good', 'default', 'hidden', 'layers', '.', 'For', 'output', 'layer', 'general', 'want', 'logistic', 'activation', 'function', 'binary', 'classification', 'softmax', 'activation', 'function', 'multiclass', 'classification', 'activation', 'function', 'regression', '.', 'If', 'MLP', 'overfits', 'training', 'data', 'try', 'reducing', 'number', 'hidden', 'layers', 'reducing', 'number', 'neurons', 'per', 'hidden', 'layer', '.', '9.See', 'Jupyter', 'notebooks', 'available', 'https', '//github.com/ageron/handson-ml', '.Chapter', '11', 'Training', 'Deep', 'Neural', 'Nets1.No', 'weights', 'sampled', 'independently', 'initial', 'value', '.', 'One', 'important', 'goal', 'sampling', 'weights', 'randomly', 'break', 'symmetries', 'weights', 'initial', 'value', 'even', 'value', 'zero', 'symmetry', 'broken', 'i.e.', 'neurons', 'given', 'layer', 'equiva…', 'lent', 'backpropagation', 'unable', 'break', '.', 'Concretely', 'means', 'neurons', 'given', 'layer', 'always', 'weights', '.', 'It‡s', 'like', 'hav…', 'ing', 'one', 'neuron', 'per', 'layer', 'much', 'slower', '.', 'It', 'virtually', 'impossible', 'configuration', 'converge', 'good', 'solution', '.', '2.It', 'perfectly', 'fine', 'initialize', 'bias', 'terms', 'zero', '.', 'Some', 'people', 'like', 'initialize', 'like', 'weights', 'that‡s', 'okay', 'make', 'much', 'difference', '.', '3.A', 'advantages', 'ELU', 'function', 'ReLU', 'function', '‹It', 'take', 'negative', 'values', 'average', 'output', 'neurons', 'given', 'layer', 'typically', 'closer', '0', 'using', 'ReLU', 'activation', 'func…', 'tion', 'never', 'outputs', 'negative', 'values', '.', 'This', 'helps', 'alleviate', 'vanishing', 'gradients', 'problem', '.', '‹It', 'always', 'nonzero', 'derivative', 'avoids', 'dying', 'units', 'issue', 'affect', 'ReLU', 'units', '.', 'Exercise', 'Solutions', '|', '485', '‹It', 'smooth', 'everywhere', 'whereas', 'ReLU‡s', 'slope', 'abruptly', 'jumps', '0', '1', 'z', '=', '0', '.', 'Such', 'abrupt', 'change', 'slow', 'Gradient', 'Descent', 'bounce', 'around', 'z', '=', '0.4.The', 'ELU', 'activation', 'function', 'good', 'default', '.', 'If', 'need', 'neural', 'network', 'fast', 'possible', 'use', 'one', 'leaky', 'ReLU', 'variants', 'instead', 'e.g.', 'simple', 'leaky', 'ReLU', 'using', 'default', 'hyperparameter', 'value', '.', 'The', 'simplicity', 'ReLU', 'activation', 'function', 'makes', 'many', 'people‡s', 'preferred', 'option', 'despite', 'fact', 'generally', 'outperformed', 'ELU', 'leaky', 'ReLU', '.', 'However', 'ReLU', 'activation', 'function‡s', 'capability', 'outputting', 'precisely', 'zero', 'use…', 'ful', 'cases', 'e.g.', 'see', 'Chapter', '15', '.', 'The', 'hyperbolic', 'tangent', 'tanh', 'use…', 'ful', 'output', 'layer', 'need', 'output', 'number', '–1', '1', 'nowadays', 'used', 'much', 'hidden', 'layers', '.', 'The', 'logistic', 'activation', 'function', 'also', 'useful', 'output', 'layer', 'need', 'estimate', 'probability', 'e.g.', 'binary', 'classification', 'also', 'rarely', 'used', 'hidden', 'layers', 'excep…', 'tions›for', 'example', 'coding', 'layer', 'variational', 'autoencoders', 'see', 'Chap…', 'ter', '15', '.', 'Finally', 'softmax', 'activation', 'function', 'useful', 'output', 'layer', 'output', 'probabilities', 'mutually', 'exclusive', 'classes', 'rarely', 'ever', 'used', 'hidden', 'layers', '.', '5.If', 'set', 'momentum', 'hyperparameter', 'close', '1', 'e.g.', '0.99999', 'using', 'MomentumOptimizer', 'algorithm', 'likely', 'pick', 'lot', 'speed', 'hope…fully', 'roughly', 'toward', 'global', 'minimum', 'shoot', 'right', 'past', 'minimum', 'due', 'momentum', '.', 'Then', 'slow', 'come', 'back', 'accel…', 'erate', 'overshoot', '.', 'It', 'may', 'oscillate', 'way', 'many', 'times', 'converging', 'overall', 'take', 'much', 'longer', 'converge', 'smaller', 'momentum', 'value.6.One', 'way', 'produce', 'sparse', 'model', 'i.e.', 'weights', 'equal', 'zero', 'train', 'model', 'normally', 'zero', 'tiny', 'weights', '.', 'For', 'sparsity', 'apply', '—', '1', 'regularization', 'training', 'pushes', 'optimizer', 'toward', 'spar…', 'sity', '.', 'A', 'third', 'option', 'combine', '—', '1', 'regularization', 'dual', 'averaging', 'usingTensorFlow‡s', 'FTRLOptimizer', 'class.7.Yes', 'dropout', 'slow', 'training', 'general', 'roughly', 'factor', 'two', '.', 'However', 'impact', 'inference', 'since', 'turned', 'training', '.', 'For', 'solutions', 'exercises', '8', '9', '10', 'please', 'see', 'Jupyter', 'notebooks', 'available', 'https', '//github.com/ageron/handson-ml', '.486', '|', 'Appendix', 'A', 'Exercise', 'Solutions', 'Chapter', '12', 'Distributing', 'TensorFlow', 'Across', 'Devices', 'andServers1.When', 'TensorFlow', 'process', 'starts', 'grabs', 'available', 'memory', 'GPU', 'devices', 'visible', 'get', 'CUDA_ERROR_OUT_OF_MEMORY', 'whenstarting', 'TensorFlow', 'program', 'probably', 'means', 'processes', 'running', 'already', 'grabbed', 'memory', 'least', 'one', 'visible', 'GPU', 'device', 'likely', 'another', 'TensorFlow', 'process', '.', 'To', 'fix', 'problem', 'triv…', 'ial', 'solution', 'stop', 'processes', 'try', '.', 'However', 'need', 'processes', 'run', 'simultaneously', 'simple', 'option', 'dedicate', 'different', 'devices', 'process', 'setting', 'CUDA_VISIBLE_DEVICES', 'environment', 'variable', 'appropriately', 'device', '.', 'Another', 'option', 'configure', 'TensorFlow', 'grab', 'part', 'GPU', 'memory', 'instead', 'creating', 'ConfigProto', 'set…ting', 'gpu_options.per_process_gpu_memory_fraction', 'proportion', 'ofthe', 'total', 'memory', 'grab', 'e.g.', '0.4', 'using', 'ConfigProto', 'opening', 'session', '.', 'The', 'last', 'option', 'tell', 'TensorFlow', 'grab', 'memory', 'needs', 'setting', 'gpu_options.allow_growth', 'True', '.', 'However', 'last', 'option', 'usually', 'recommended', 'memory', 'Tensor…', 'Flow', 'grabs', 'never', 'released', 'harder', 'guarantee', 'repeatable', 'behavior', 'may', 'race', 'conditions', 'depending', 'processes', 'start', 'first', 'much', 'memory', 'need', 'training', '.', '2.By', 'pinning', 'operation', 'device', 'telling', 'TensorFlow', 'would', 'like', 'operation', 'placed', '.', 'However', 'constraints', 'may', 'prevent', 'TensorFlow', 'honoring', 'request', '.', 'For', 'example', 'operation', 'may', 'implementation', 'called', 'kernel', 'particular', 'type', 'device', '.', 'In', 'case', 'TensorFlow', 'raise', 'exception', 'default', 'configure', 'fall', 'back', 'CPU', 'instead', 'called', '“', 'placement', '.', 'Another', 'example', 'operation', 'modify', 'variable', 'operation', 'variable', 'need', 'collocated', '.', 'So', 'difference', 'pinning', 'operation', 'placing', 'operation', 'pinning', 'ask', 'TensorFlow', 'ƒPlease', 'place', 'opera…', 'tion', 'GPU', '#', '1⁄', 'placement', 'TensorFlow', 'actually', 'ends', 'ƒSorry', 'falling', 'back', 'CPU⁄', '.', '3.If', 'running', 'GPU-enabled', 'TensorFlow', 'installation', 'use', 'default', 'placement', 'operations', 'GPU', 'kernel', 'i.e.', 'GPU', 'implementation', 'yes', 'placed', 'first', 'GPU', '.', 'However', 'one', 'operations', 'GPU', 'kernel', 'default', 'TensorFlow', 'raise', 'exception', '.', 'If', 'configure', 'TensorFlow', 'fall', 'back', 'CPU', 'instead', 'soft', 'placement', 'operations', 'placed', 'first', 'GPU', 'except', 'ones', 'without', 'GPU', 'kernel', 'operations', 'must', 'collocated', 'see', 'answer', 'previous', 'exercise', '.Exercise', 'Solutions', '|', '487', '4.Yes', 'pin', 'variable', \"''\", '/gpu:0', \"''\", 'used', 'operations', 'placed', '/gpu:1', '.', 'TensorFlow', 'automatically', 'take', 'care', 'adding', 'appropriate', 'operations', 'transfer', 'variable‡s', 'value', 'across', 'devices', '.', 'The', 'goes', 'devi…', 'ces', 'located', 'different', 'servers', 'long', 'part', 'cluster', '.', '5.Yes', 'two', 'operations', 'placed', 'device', 'run', 'parallel', 'TensorFlow', 'automatically', 'takes', 'care', 'running', 'operations', 'parallel', 'different', 'CPU', 'cores', 'different', 'GPU', 'threads', 'long', 'operation', 'depends', 'another', 'operation‡s', 'output', '.', 'Moreover', 'start', 'multiple', 'sessions', 'parallel', 'threads', 'processes', 'evaluate', 'operations', 'thread', '.', 'Since', 'sessions', 'inde…', 'pendent', 'TensorFlow', 'able', 'evaluate', 'operation', 'one', 'session', 'parallel', 'operation', 'another', 'session', '.', '6.Control', 'dependencies', 'used', 'want', 'postpone', 'evaluation', 'operation', 'X', 'operations', 'run', 'even', 'though', 'opera…', 'tions', 'required', 'compute', 'X', '.', 'This', 'useful', 'particular', 'X', 'would', 'occupy', 'lot', 'memory', 'need', 'later', 'computation', 'graph', 'X', 'uses', 'lot', 'I/O', 'example', 'requires', 'large', 'variable', 'value', 'located', 'different', 'device', 'server', 'don‡t', 'want', 'run', 'time', 'I/O-hungry', 'operations', 'avoid', 'saturating', 'bandwidth', '.', '7.You‡re', 'luck', '!', 'In', 'distributed', 'TensorFlow', 'variable', 'values', 'live', 'containers', 'managed', 'cluster', 'even', 'close', 'session', 'exit', 'client', 'pro…', 'gram', 'model', 'parameters', 'still', 'alive', 'well', 'cluster', '.', 'You', 'simply', 'need', 'open', 'new', 'session', 'cluster', 'save', 'model', 'make', 'sure', 'don‡t', 'call', 'variable', 'initializers', 'restore', 'previous', 'model', 'would', 'destroy', 'precious', 'new', 'model', '!', '.For', 'solutions', 'exercises', '8', '9', '10', 'please', 'see', 'Jupyter', 'notebooks', 'available', 'https', '//github.com/ageron/handson-ml', '.Chapter', '13', 'Convolutional', 'Neural', 'Networks1.These', 'main', 'advantages', 'CNN', 'fully', 'connected', 'DNN', 'image', 'classification', '‹Because', 'consecutive', 'layers', 'partially', 'connected', 'heavily', 'reuses', 'weights', 'CNN', 'many', 'fewer', 'parameters', 'fully', 'connected', 'DNN', 'makes', 'much', 'faster', 'train', 'reduces', 'risk', 'overfitting', 'requires', 'much', 'less', 'training', 'data', '.', '‹When', 'CNN', 'learned', 'kernel', 'detect', 'particular', 'feature', 'detect', 'feature', 'anywhere', 'image', '.', 'In', 'contrast', 'DNN', 'learns', 'feature', 'one', 'location', 'detect', 'particular', 'location', '.', 'Since', 'images', 'typically', 'repetitive', 'features', 'CNNs', 'able', 'generalize', '488', '|', 'Appendix', 'A', 'Exercise', 'Solutions', 'much', 'better', 'DNNs', 'image', 'processing', 'tasks', 'classification', 'using', 'fewer', 'training', 'examples', '.', '‹Finally', 'DNN', 'prior', 'knowledge', 'pixels', 'organized', 'know', 'nearby', 'pixels', 'close', '.', 'A', 'CNN‡s', 'architecture', 'embeds', 'prior', 'knowledge', '.', 'Lower', 'layers', 'typically', 'identify', 'features', 'small', 'areas', 'images', 'higher', 'layers', 'combine', 'lower-level', 'features', 'larger', 'features', '.', 'This', 'works', 'well', 'natural', 'images', 'giving', 'CNNs', 'decisive', 'head', 'start', 'com…', 'pared', 'DNNs', '.', '2.Let‡s', 'compute', 'many', 'parameters', 'CNN', '.', 'Since', 'first', 'convolutional', 'layer', '3', '‰', '3', 'kernels', 'input', 'three', 'channels', 'red', 'green', 'blue', 'feature', 'map', '3', '‰', '3', '‰', '3', 'weights', 'plus', 'bias', 'term', '.', 'That‡s', '28', 'parame…', 'ters', 'per', 'feature', 'map', '.', 'Since', 'first', 'convolutional', 'layer', '100', 'feature', 'maps', 'total', '2,800', 'parameters', '.', 'The', 'second', 'convolutional', 'layer', '3', '‰', '3', 'kernels', 'input', 'set', '100', 'feature', 'maps', 'previous', 'layer', 'feature', 'map', '3', '‰', '3', '‰', '100', '=', '900', 'weights', 'plus', 'bias', 'term', '.', 'Since', '200', 'feature', 'maps', 'layer', '901', '‰', '200', '=', '180,200', 'parameters', '.', 'Finally', 'third', 'last', 'convolutional', 'layer', 'also', '3', '‰', '3', 'kernels', 'input', 'set', '200', 'feature', 'maps', 'previous', 'layers', 'feature', 'map', '3', '‰', '3', '‰', '200', '=', '1,800', 'weights', 'plus', 'bias', 'term', '.', 'Since', '400', 'feature', 'maps', 'layer', 'total', '1,801', '‰', '400', '=', '720,400', 'parameters', '.', 'All', 'CNN', '2,800', '+', '180,200', '+', '720,400', '=903,400', 'parameters.Now', 'let‡s', 'compute', 'much', 'RAM', 'neural', 'network', 'require', 'least', 'making', 'prediction', 'single', 'instance', '.', 'First', 'let‡s', 'compute', 'feature', 'map', 'size', 'layer', '.', 'Since', 'using', 'stride', '2', 'SAME', 'padding', 'horizontal', 'vertical', 'size', 'feature', 'maps', 'divided', '2', 'layer', 'rounding', 'necessary', 'input', 'channels', '200', '‰', '300', 'pixels', 'first', 'layer‡s', 'feature', 'maps', '100', '‰', '150', 'second', 'layer‡s', 'feature', 'maps', '50', '‰', '75', 'third', 'layer‡s', 'feature', 'maps', '25', '‰', '38', '.', 'Since', '32', 'bits', '4', 'bytes', 'first', 'convolutional', 'layer', '100', 'feature', 'maps', 'first', 'layer', 'takes', '4', 'x', '100', '‰', '150', '‰', '100', '=', '6', 'million', 'bytes', '5.7', 'MB', 'considering', '1', 'MB', '=', '1,024', 'KB', '1', 'KB', '=', '1,024', 'bytes', '.', 'The', 'second', 'layer', 'takes', '4', '‰', '50', '‰', '75', '‰', '200', '=', '3', 'million', 'bytes', '2.9', 'MB', '.', 'Finally', 'third', 'layer', 'takes', '4', '‰', '25', '‰', '38', '‰', '400', '=', '1,520,000', 'bytes', '1.4', 'MB', '.', 'However', 'layer', 'computed', 'memory', 'occupied', 'previous', 'layer', 'released', 'everything', 'well', 'optimized', '6', '+', '9', '=', '15', 'million', 'bytes', '14.3', 'MB', 'RAM', 'required', 'thesecond', 'layer', 'computed', 'memory', 'occupied', 'first', 'layer', 'released', 'yet', '.', 'But', 'wait', 'also', 'need', 'add', 'memory', 'occupied', 'CNN‡s', 'parameters', '.', 'We', 'computed', 'earlier', '903,400', 'parameters', 'using', '4', 'bytes', 'adds', '3,613,600', 'bytes', '3.4', 'MB', '.', 'The', 'total', 'RAMrequired', 'least', '18,613,600', 'bytes', '17.8', 'MB', '.', 'Exercise', 'Solutions', '|', '489', 'Lastly', 'let‡s', 'compute', 'minimum', 'amount', 'RAM', 'required', 'training', 'CNN', 'mini-batch', '50', 'images', '.', 'During', 'training', 'TensorFlow', 'uses', 'backpropa…', 'gation', 'requires', 'keeping', 'values', 'computed', 'forward', 'pass', 'reverse', 'pass', 'begins', '.', 'So', 'must', 'compute', 'total', 'RAM', 'required', 'layers', 'single', 'instance', 'multiply', '50', '!', 'At', 'point', 'let‡s', 'start', 'counting', 'megabytes', 'rather', 'bytes', '.', 'We', 'computed', 'three', 'layers', 'require', 'respectively', '5.7', '2.9', '1.4', 'MB', 'instance', '.', 'That‡s', 'total', '10.0', 'MB', 'per', 'instance', '.', 'So', '50', 'instances', 'total', 'RAM', '500', 'MB', '.', 'Add', 'RAM', 'required', 'input', 'images', '50', '‰', '4', '‰', '200', '‰', '300', '‰', '3', '=', '36', 'million', 'bytes', '34.3', 'MB', 'plus', 'RAM', 'required', 'model', 'parameters', 'isabout', '3.4', 'MB', 'computed', 'earlier', 'plus', 'RAM', 'gradients', 'neglect', 'since', 'released', 'gradually', 'backpropagation', 'goes', 'layers', 'reverse', 'pass', '.', 'We', 'total', 'roughly', '500.0', '+', '34.3', '+', '3.4', '=', '537.7', 'MB', '.', 'And', 'that‡s', 'really', 'optimistic', 'bare', 'minimum', '.', '3.If', 'GPU', 'runs', 'memory', 'training', 'CNN', 'five', 'things', 'could', 'try', 'solve', 'problem', 'purchasing', 'GPU', 'RAM', '‹Reduce', 'mini-batch', 'size', '.', '‹Reduce', 'dimensionality', 'using', 'larger', 'stride', 'one', 'layers', '.', '‹Remove', 'one', 'layers', '.', '‹Use', '16-bit', 'floats', 'instead', '32-bit', 'floats', '.', '‹Distribute', 'CNN', 'across', 'multiple', 'devices', '.', '4.A', 'max', 'pooling', 'layer', 'parameters', 'whereas', 'convolutional', 'layer', 'quite', 'see', 'previous', 'questions', '.5.A', 'local', 'response', 'normalization', 'layer', 'makes', 'neurons', 'strongly', 'acti…', 'vate', 'inhibit', 'neurons', 'location', 'neighboring', 'feature', 'maps', 'encourages', 'different', 'feature', 'maps', 'specialize', 'pushes', 'apart', 'forcing', 'explore', 'wider', 'range', 'features', '.', 'It', 'typically', 'used', 'lower', 'layers', 'larger', 'pool', 'low-level', 'features', 'upper', 'layers', 'build', 'upon', '.', '6.The', 'main', 'innovations', 'AlexNet', 'compared', 'LeNet-5', '1', 'much', 'larger', 'deeper', '2', 'stacks', 'convolutional', 'layers', 'directly', 'top', 'instead', 'stacking', 'pooling', 'layer', 'top', 'convolutional', 'layer', '.', 'The', 'main', 'innovation', 'GoogLeNet', 'introduction', 'inception', 'modules', 'make', 'itpossible', 'much', 'deeper', 'net', 'previous', 'CNN', 'architectures', 'fewer', 'parameters', '.', 'Finally', 'ResNet‡s', 'main', 'innovation', 'introduction', 'skip', 'connec…', 'tions', 'make', 'possible', 'go', 'well', 'beyond', '100', 'layers', '.', 'Arguably', 'simplic…', 'ity', 'consistency', 'also', 'rather', 'innovative', '.', 'For', 'solutions', 'exercises', '7', '8', '9', '10', 'please', 'see', 'Jupyter', 'notebooks', 'avail…', 'able', 'https', '//github.com/ageron/handson-ml', '.490', '|', 'Appendix', 'A', 'Exercise', 'Solutions', 'Chapter', '14', 'Recurrent', 'Neural', 'Networks1.Here', 'RNN', 'applications', '‹For', 'sequence-to-sequence', 'RNN', 'predicting', 'weather', 'time', 'series', 'machine', 'translation', 'using', 'encoder–decoder', 'architecture', 'video', 'captioning', 'speech', 'text', 'music', 'generation', 'sequence', 'generation', 'identifying', 'chords', 'song', '.', '‹For', 'sequence-to-vector', 'RNN', 'classifying', 'music', 'samples', 'music', 'genre', 'ana…', 'lyzing', 'sentiment', 'book', 'review', 'predicting', 'word', 'aphasic', 'patient', 'thinking', 'based', 'readings', 'brain', 'implants', 'predicting', 'probabil…', 'ity', 'user', 'want', 'watch', 'movie', 'based', 'watch', 'history', 'one', 'many', 'possible', 'implementations', 'collaborative', '†ltering', '.‹For', 'vector-to-sequence', 'RNN', 'image', 'captioning', 'creating', 'music', 'playlist', 'based', 'embedding', 'current', 'artist', 'generating', 'melody', 'based', 'set', 'parameters', 'locating', 'pedestrians', 'picture', 'e.g.', 'video', 'frame', 'self-driving', 'car‡s', 'camera', '.', '2.In', 'general', 'translate', 'sentence', 'one', 'word', 'time', 'result', 'terri…', 'ble', '.', 'For', 'example', 'French', 'sentence', 'ƒJe', 'vous', 'en', 'prie⁄', 'means', 'ƒYou', 'welcome', '⁄', 'translate', 'one', 'word', 'time', 'get', 'ƒI', 'pray.⁄', 'Huh', '?', 'It', 'much', 'better', 'read', 'whole', 'sentence', 'first', 'translate', '.', 'A', 'plain', 'sequence-to-', 'sequence', 'RNN', 'would', 'start', 'translating', 'sentence', 'immediately', 'reading', 'first', 'word', 'encoder–decoder', 'RNN', 'first', 'read', 'whole', 'sentence', 'translate', '.', 'That', 'said', 'one', 'could', 'imagine', 'plain', 'sequence-to-sequence', 'RNN', 'would', 'output', 'silence', 'whenever', 'unsure', 'say', 'next', 'like', 'human', 'translators', 'must', 'translate', 'live', 'broadcast', '.', '3.To', 'classify', 'videos', 'based', 'visual', 'content', 'one', 'possible', 'architecture', 'could', 'take', 'say', 'one', 'frame', 'per', 'second', 'run', 'frame', 'convolutional', 'neural', 'network', 'feed', 'output', 'CNN', 'sequence-to-vector', 'RNN', 'andfinally', 'run', 'output', 'softmax', 'layer', 'giving', 'class', 'probabili…', 'ties', '.', 'For', 'training', 'would', 'use', 'cross', 'entropy', 'cost', 'function', '.', 'If', 'wanted', 'use', 'audio', 'classification', 'well', 'could', 'convert', 'every', 'second', 'audio', 'spectrograph', 'feed', 'spectrograph', 'CNN', 'feed', 'output', 'CNN', 'RNN', 'along', 'corresponding', 'output', 'otherCNN', '.4.Building', 'RNN', 'using', 'dynamic_rnn', 'rather', 'static_rnn', 'offers', 'several', 'advantages', '‹It', 'based', 'while_loop', 'operation', 'able', 'swap', 'GPU‡s', 'memory', 'CPU‡s', 'memory', 'backpropagation', 'avoiding', 'out-of-memory', 'errors.Exercise', 'Solutions', '|', '491', '‹It', 'arguably', 'easier', 'use', 'directly', 'take', 'single', 'tensor', 'input', 'output', 'covering', 'time', 'steps', 'rather', 'list', 'tensors', 'one', 'per', 'time', 'step', '.', 'No', 'need', 'stack', 'unstack', 'transpose', '.', '‹It', 'generates', 'smaller', 'graph', 'easier', 'visualize', 'TensorBoard', '.', '5.To', 'handle', 'variable', 'length', 'input', 'sequences', 'simplest', 'option', 'set', 'sequence_length', 'parameter', 'calling', 'static_rnn', 'dynamic_rnn', 'functions', '.', 'Another', 'option', 'pad', 'smaller', 'inputs', 'e.g.', 'zeros', 'make', 'size', 'largest', 'input', 'may', 'faster', 'first', 'option', 'input', 'sequences', 'similar', 'lengths', '.', 'To', 'handle', 'variable-length', 'out…', 'put', 'sequences', 'know', 'advance', 'length', 'output', 'sequence', 'youcan', 'use', 'sequence_length', 'parameter', 'example', 'consider', 'sequence-to-', 'sequence', 'RNN', 'labels', 'every', 'frame', 'video', 'violence', 'score', 'output', 'sequence', 'exactly', 'length', 'input', 'sequence', '.', 'If', 'don‡t', 'know', 'advance', 'length', 'output', 'sequence', 'use', 'paddingtrick', 'always', 'output', 'size', 'sequence', 'ignore', 'outputs', 'come', 'end-of-sequence', 'token', 'ignoring', 'computing', 'cost', 'function', '.6.To', 'distribute', 'training', 'execution', 'deep', 'RNN', 'across', 'multiple', 'GPUs', 'common', 'technique', 'simply', 'place', 'layer', 'different', 'GPU', 'see', 'Chap…', 'ter', '12', '.For', 'solutions', 'exercises', '7', '8', '9', 'please', 'see', 'Jupyter', 'notebooks', 'available', 'https', '//github.com/ageron/handson-ml', '.Chapter', '15', 'Autoencoders1.Here', 'main', 'tasks', 'autoencoders', 'used', '‹Feature', 'extraction', '‹Unsupervised', 'pretraining', '‹Dimensionality', 'reduction‹Generative', 'models', '‹Anomaly', 'detection', 'autoencoder', 'generally', 'bad', 'reconstructing', 'outliers', '2.If', 'want', 'train', 'classifier', 'plenty', 'unlabeled', 'training', 'data', 'thousand', 'labeled', 'instances', 'could', 'first', 'train', 'deepautoencoder', 'full', 'dataset', 'labeled', '+', 'unlabeled', 'reuse', 'lower', 'half', 'classifier', 'i.e.', 'reuse', 'layers', 'codings', 'layer', 'included', 'train', 'classifier', 'using', 'labeled', 'data', '.', 'If', 'little', 'labeled', 'data', 'probably', 'want', 'freeze', 'reused', 'layers', 'training', 'classifier', '.', '492', '|', 'Appendix', 'A', 'Exercise', 'Solutions', '3.The', 'fact', 'autoencoder', 'perfectly', 'reconstructs', 'inputs', 'necessarily', 'mean', 'good', 'autoencoder', 'perhaps', 'simply', 'overcomplete', 'autoen…', 'coder', 'learned', 'copy', 'inputs', 'codings', 'layer', 'outputs', '.', 'In', 'fact', 'even', 'codings', 'layer', 'contained', 'single', 'neuron', 'would', 'possible', 'deep', 'autoencoder', 'learn', 'map', 'training', 'instance', 'different', 'coding', 'e.g.', 'first', 'instance', 'could', 'mapped', '0.001', 'second', '0.002', 'third', '0.003', 'could', 'learn', 'ƒby', 'heart⁄', 'reconstruct', 'right', 'training', 'instance', 'coding', '.', 'It', 'would', 'perfectly', 'reconstruct', 'inputs', 'without', 'really', 'learning', 'useful', 'pattern', 'data', '.', 'In', 'practice', 'mapping', 'unlikely', 'happen', 'illustrates', 'fact', 'perfect', 'reconstructions', 'guarantee', 'autoencoder', 'learned', 'anything', 'useful', '.', 'However', 'produces', 'bad', 'reconstructions', 'almost', 'guaranteed', 'bad', 'autoencoder', '.', 'To', 'evaluate', 'performance', 'autoencoder', 'one', 'option', 'measure', 'reconstruction', 'loss', 'e.g.', 'compute', 'MSE', 'mean', 'square', 'outputs', 'minus', 'inputs', '.', 'Again', 'high', 'reconstruction', 'loss', 'good', 'sign', 'autoencoder', 'bad', 'low', 'reconstruction', 'loss', 'guarantee', 'good', '.', 'You', 'also', 'evaluate', 'autoencoder', 'according', 'used', '.', 'For', 'example', 'using', 'unsupervised', 'pretraining', 'classifier', 'also', 'evaluate', 'classifier‡s', 'performance', '.', '4.An', 'undercomplete', 'autoencoder', 'one', 'whose', 'codings', 'layer', 'smaller', 'input', 'output', 'layers', '.', 'If', 'larger', 'overcomplete', 'autoencoder', '.', 'The', 'main', 'risk', 'excessively', 'undercomplete', 'autoencoder', 'may', 'fail', 'reconstruct', 'inputs', '.', 'The', 'main', 'risk', 'overcomplete', 'autoencoder', 'may', 'copy', 'inputs', 'outputs', 'without', 'learning', 'useful', 'feature', '.', '5.To', 'tie', 'weights', 'encoder', 'layer', 'corresponding', 'decoder', 'layer', 'simply', 'make', 'decoder', 'weights', 'equal', 'transpose', 'encoder', 'weights', '.', 'This', 'reduces', 'number', 'parameters', 'model', 'half', 'often', 'making', 'train…', 'ing', 'converge', 'faster', 'less', 'training', 'data', 'reducing', 'risk', 'overfitting', 'training', 'set.6.To', 'visualize', 'features', 'learned', 'lower', 'layer', 'stacked', 'autoencoder', 'common', 'technique', 'simply', 'plot', 'weights', 'neuron', 'reshaping', 'weight', 'vector', 'size', 'input', 'image', 'e.g.', 'MNIST', 'reshaping', 'weight', 'vector', 'shape', '784', '28', '28', '.', 'To', 'visualize', 'features', 'learned', 'higher', 'layers', 'one', 'technique', 'display', 'training', 'instances', 'activate', 'neuron.7.A', 'generative', 'model', 'model', 'capable', 'randomly', 'generating', 'outputs', 'resemble', 'training', 'instances', '.', 'For', 'example', 'trained', 'successfully', 'MNIST', 'dataset', 'generative', 'model', 'used', 'randomly', 'generate', 'realistic', 'images', 'digits', '.', 'The', 'output', 'distribution', 'typically', 'similar', 'training', 'data', '.', 'For', 'example', 'since', 'MNIST', 'contains', 'many', 'images', 'digit', 'generative', 'model', 'would', 'output', 'roughly', 'number', 'images', 'digit', '.', 'Some', 'Exercise', 'Solutions', '|', '493', 'generative', 'models', 'parametrized›for', 'example', 'generate', 'kinds', 'outputs', '.', 'An', 'example', 'generative', 'autoencoder', 'variational', 'autoencoder', '.', 'For', 'solutions', 'exercises', '8', '9', '10', 'please', 'see', 'Jupyter', 'notebooks', 'available', 'https', '//github.com/ageron/handson-ml', '.Chapter', '16', 'Reinforcement', 'Learning1.Reinforcement', 'Learning', 'area', 'Machine', 'Learning', 'aimed', 'creating', 'agents', 'capable', 'taking', 'actions', 'environment', 'way', 'maximizes', 'rewards', 'time', '.', 'There', 'many', 'differences', 'RL', 'regular', 'supervised', 'unsupervised', 'learning', '.', 'Here', '‹In', 'supervised', 'unsupervised', 'learning', 'goal', 'generally', 'find', 'patterns', 'data', '.', 'In', 'Reinforcement', 'Learning', 'goal', 'find', 'good', 'policy', '.', '‹Unlike', 'supervised', 'learning', 'agent', 'explicitly', 'given', 'ƒright⁄', 'answer', '.', 'It', 'must', 'learn', 'trial', 'error', '.', '‹Unlike', 'unsupervised', 'learning', 'form', 'supervision', 'rewards', '.', 'We', 'tell', 'agent', 'perform', 'task', 'tell', 'making', 'progress', 'failing.‹A', 'Reinforcement', 'Learning', 'agent', 'needs', 'find', 'right', 'balance', 'exploring', 'environment', 'looking', 'new', 'ways', 'getting', 'rewards', 'exploiting', 'sources', 'rewards', 'already', 'knows', '.', 'In', 'contrast', 'supervised', 'unsupervised', 'learning', 'systems', 'generally', 'don‡t', 'need', 'worry', 'explora…', 'tion', 'feed', 'training', 'data', 'given', '.', '‹In', 'supervised', 'unsupervised', 'learning', 'training', 'instances', 'typically', 'inde…', 'pendent', 'fact', 'generally', 'shuffled', '.', 'In', 'Reinforcement', 'Learning', 'con…', 'secutive', 'observations', 'generally', 'independent', '.', 'An', 'agent', 'may', 'remain', 'region', 'environment', 'moves', 'consecu…', 'tive', 'observations', 'correlated', '.', 'In', 'cases', 'replay', 'memory', 'used', 'ensure', 'training', 'algorithm', 'gets', 'fairly', 'independent', 'observa…', 'tions.2.Here', 'possible', 'applications', 'Reinforcement', 'Learning', 'mentioned', 'Chapter', '16', 'Music', 'personalization', 'The', 'environment', 'user‡s', 'personalized', 'web', 'radio', '.', 'The', 'agent', 'software', 'deciding', 'song', 'play', 'next', 'user', '.', 'Its', 'possible', 'actions', 'play', 'song', 'catalog', 'must', 'try', 'choose', 'song', 'user', 'enjoy', 'play', 'advertisement', 'must', 'try', 'choose', 'ad', 'user', 'inter…', '494', '|', 'Appendix', 'A', 'Exercise', 'Solutions', 'ested', '.', 'It', 'gets', 'small', 'reward', 'every', 'time', 'user', 'listens', 'song', 'larger', 'reward', 'every', 'time', 'user', 'listens', 'ad', 'negative', 'reward', 'user', 'skips', 'song', 'ad', 'negative', 'reward', 'user', 'leaves', '.', 'Marketing', 'The', 'environment', 'company‡s', 'marketing', 'department', '.', 'The', 'agent', 'software', 'defines', 'customers', 'mailing', 'campaign', 'sent', 'given', 'profile', 'purchase', 'history', 'customer', 'two', 'possi…', 'ble', 'actions', 'send', 'don‡t', 'send', '.', 'It', 'gets', 'negative', 'reward', 'cost', 'mailing', 'campaign', 'positive', 'reward', 'estimated', 'revenue', 'generated', 'campaign', '.', 'Product', 'delivery', 'Let', 'agent', 'control', 'fleet', 'delivery', 'trucks', 'deciding', 'pick', 'depots', 'go', 'drop', '.', 'They', 'would', 'get', 'positive', 'rewards', 'product', 'delivered', 'time', 'negative', 'rewards', 'late', 'deliveries', '.', '3.When', 'estimating', 'value', 'action', 'Reinforcement', 'Learning', 'algorithms', 'typ…', 'ically', 'sum', 'rewards', 'action', 'led', 'giving', 'weight', 'immediate', 'rewards', 'less', 'weight', 'later', 'rewards', 'considering', 'action', 'influence', 'near', 'future', 'distant', 'future', '.', 'To', 'model', 'discount', 'rate', 'typically', 'applied', 'time', 'step', '.', 'For', 'example', 'discount', 'rate', '0.9', 'reward', '100', 'received', 'two', 'time', 'steps', 'later', 'counted', '0.9', '2', '‰', '100=', '81', 'estimating', 'value', 'action', '.', 'You', 'think', 'dis…', 'count', 'rate', 'measure', 'much', 'future', 'valued', 'relative', 'present', 'close', '1', 'future', 'valued', 'almost', 'much', 'present', '.', 'If', 'close', '0', 'immediate', 'rewards', 'matter', '.', 'Of', 'course', 'impacts', 'optimal', 'policy', 'tremendously', 'value', 'future', 'may', 'willing', 'put', 'lot', 'immediate', 'pain', 'prospect', 'eventual', 'rewards', 'don‡t', 'value', 'future', 'grab', 'immediate', 'reward', 'find', 'never', 'investing', 'future', '.', '4.To', 'measure', 'performance', 'Reinforcement', 'Learning', 'agent', 'simply', 'sum', 'rewards', 'gets', '.', 'In', 'simulated', 'environment', 'run', 'many', 'epi…', 'sodes', 'look', 'total', 'rewards', 'gets', 'average', 'possibly', 'look', 'min', 'max', 'standard', 'deviation', '.', '5.The', 'credit', 'assignment', 'problem', 'fact', 'Reinforcement', 'Learning', 'agent', 'receives', 'reward', 'direct', 'way', 'knowing', 'previous', 'actions', 'contributed', 'reward', '.', 'It', 'typically', 'occurs', 'large', 'delay', 'action', 'resulting', 'rewards', 'e.g.', 'game', 'Atari‡s', 'Pong', 'may', 'dozen', 'time', 'steps', 'moment', 'agent', 'hits', 'ball', 'moment', 'wins', 'point', '.', 'One', 'way', 'alleviate', 'provide', 'agent', 'shorter-term', 'rewards', 'possible', '.', 'This', 'usually', 'requires', 'prior', 'knowledge', 'Exercise', 'Solutions', '|', '495', 'task', '.', 'For', 'example', 'want', 'build', 'agent', 'learn', 'play', 'chess', 'instead', 'giving', 'reward', 'wins', 'game', 'could', 'give', 'areward', 'every', 'time', 'captures', 'one', 'opponent‡s', 'pieces', '.', '6.An', 'agent', 'often', 'remain', 'region', 'environment', 'experiences', 'similar', 'period', 'time', '.', 'This', 'intro…', 'duce', 'bias', 'learning', 'algorithm', '.', 'It', 'may', 'tune', 'policy', 'region', 'environment', 'perform', 'well', 'soon', 'moves', 'region', '.', 'To', 'solve', 'problem', 'use', 'replay', 'memory', 'instead', 'using', 'immediate', 'experiences', 'learning', 'agent', 'learn', 'based', 'buffer', 'past', 'experiences', 'recent', 'recent', 'perhaps', 'dream', 'night', 'replay', 'experiences', 'day', 'better', 'learn', '?', '.', '7.An', 'off-policy', 'RL', 'algorithm', 'learns', 'value', 'optimal', 'policy', 'i.e.', 'sum', 'ofdiscounted', 'rewards', 'expected', 'state', 'agent', 'acts', 'opti…', 'mally', 'independently', 'agent', 'actually', 'acts', '.', 'Q-Learning', 'good', 'exam…', 'ple', 'algorithm', '.', 'In', 'contrast', 'on-policy', 'algorithm', 'learns', 'value', 'policy', 'agent', 'actually', 'executes', 'including', 'exploration', 'exploi…', 'tation', '.', 'For', 'solutions', 'exercises', '8', '9', '10', 'please', 'see', 'Jupyter', 'notebooks', 'available', 'https', '//github.com/ageron/handson-ml', '.496', '|', 'Appendix', 'A', 'Exercise', 'Solutions', 'APPENDIX', 'BMachine', 'Learning', 'Project', 'ChecklistThis', 'checklist', 'guide', 'Machine', 'Learning', 'projects', '.', 'There', 'eight', 'main', 'steps', '1.Frame', 'problem', 'look', 'big', 'picture', '.', '2.Get', 'data', '.', '3.Explore', 'data', 'gain', 'insights', '.', '4.Prepare', 'data', 'better', 'expose', 'underlying', 'data', 'patterns', 'Machine', 'Learn…', 'ing', 'algorithms.5.Explore', 'many', 'different', 'models', 'short-list', 'best', 'ones', '.', '6.Fine-tune', 'models', 'combine', 'great', 'solution', '.', '7.Present', 'solution', '.', '8.Launch', 'monitor', 'maintain', 'system', '.', 'Obviously', 'feel', 'free', 'adapt', 'checklist', 'needs', '.', 'Frame', 'Problem', 'Look', 'Big', 'Picture1.Define', 'objective', 'business', 'terms.2.How', 'solution', 'used', '?', '3.What', 'current', 'solutions/workarounds', '?', '4.How', 'frame', 'problem', 'supervised/unsupervised', 'online/offline', 'etc', '.', '?', '5.How', 'performance', 'measured', '?', '6.Is', 'performance', 'measure', 'aligned', 'business', 'objective', '?', '4977.What', 'would', 'minimum', 'performance', 'needed', 'reach', 'business', 'objec…', 'tive', '?', '8.What', 'comparable', 'problems', '?', 'Can', 'reuse', 'experience', 'tools', '?', '9.Is', 'human', 'expertise', 'available', '?', '10.How', 'would', 'solve', 'problem', 'manually', '?', '11.List', 'assumptions', 'others', 'made', 'far', '.', '12.Verify', 'assumptions', 'possible', '.', 'Get', 'DataNote', 'automate', 'much', 'possible', 'easily', 'get', 'fresh', 'data', '.', '1.List', 'data', 'need', 'much', 'need', '.', '2.Find', 'document', 'get', 'data', '.', '3.Check', 'much', 'space', 'take', '.', '4.Check', 'legal', 'obligations', 'get', 'authorization', 'necessary', '.', '5.Get', 'access', 'authorizations', '.', '6.Create', 'workspace', 'enough', 'storage', 'space', '.', '7.Get', 'data', '.', '8.Convert', 'data', 'format', 'easily', 'manipulate', 'without', 'changing', 'data', '.', '9.Ensure', 'sensitive', 'information', 'deleted', 'protected', 'e.g.', 'anonymized', '.', '10.Check', 'size', 'type', 'data', 'time', 'series', 'sample', 'geographical', 'etc.', '.', '11.Sample', 'test', 'set', 'put', 'aside', 'never', 'look', 'data', 'snooping', '!', '.', 'Explore', 'DataNote', 'try', 'get', 'insights', 'field', 'expert', 'steps', '.', '1.Create', 'copy', 'data', 'exploration', 'sampling', 'manageable', 'size', 'necessary', '.', '2.Create', 'Jupyter', 'notebook', 'keep', 'record', 'data', 'exploration', '.', '3.Study', 'attribute', 'characteristics', '‹Name', '‹Type', 'categorical', 'int/float', 'bounded/unbounded', 'text', 'structured', 'etc', '.', '498', '|', 'Appendix', 'B', 'Machine', 'Learning', 'Project', 'Checklist', '‹', '%', 'missing', 'values‹Noisiness', 'type', 'noise', 'stochastic', 'outliers', 'rounding', 'errors', 'etc', '.', '‹Possibly', 'useful', 'task', '?', '‹Type', 'distribution', 'Gaussian', 'uniform', 'logarithmic', 'etc', '.', '4.For', 'supervised', 'learning', 'tasks', 'identify', 'target', 'attribute', '.', '5.Visualize', 'data', '.', '6.Study', 'correlations', 'attributes', '.', '7.Study', 'would', 'solve', 'problem', 'manually', '.', '8.Identify', 'promising', 'transformations', 'may', 'want', 'apply', '.', '9.Identify', 'extra', 'data', 'would', 'useful', 'go', 'back', 'ƒGet', 'Data⁄', 'page', '498', '.10.Document', 'learned', '.', 'Prepare', 'DataNotes', '‹Work', 'copies', 'data', 'keep', 'original', 'dataset', 'intact', '.', '‹Write', 'functions', 'data', 'transformations', 'apply', 'five', 'reasons', '›So', 'easily', 'prepare', 'data', 'next', 'time', 'get', 'fresh', 'dataset', '›So', 'apply', 'transformations', 'future', 'projects', '›To', 'clean', 'prepare', 'test', 'set', '›To', 'clean', 'prepare', 'new', 'data', 'instances', 'solution', 'live', '›To', 'make', 'easy', 'treat', 'preparation', 'choices', 'hyperparameters', '1.Data', 'cleaning', '‹Fix', 'remove', 'outliers', 'optional', '.‹Fill', 'missing', 'values', 'e.g.', 'zero', 'mean', 'medianµ', 'drop', 'rows', 'columns', '.2.Feature', 'selection', 'optional', '‹Drop', 'attributes', 'provide', 'useful', 'information', 'task', '.', '3.Feature', 'engineering', 'appropriate', '‹Discretize', 'continuous', 'features', '.', 'Machine', 'Learning', 'Project', 'Checklist', '|', '499', '‹Decompose', 'features', 'e.g.', 'categorical', 'date/time', 'etc.', '.', '‹Add', 'promising', 'transformations', 'features', 'e.g.', 'log', 'x', 'sqrt', 'x', 'x^2', 'etc.', '.', '‹Aggregate', 'features', 'promising', 'new', 'features', '.', '4.Feature', 'scaling', 'standardize', 'normalize', 'features', '.', 'Short-List', 'Promising', 'ModelsNotes', '‹If', 'data', 'huge', 'may', 'want', 'sample', 'smaller', 'training', 'sets', 'train', 'many', 'different', 'models', 'reasonable', 'time', 'aware', 'penalizes', 'complex', 'models', 'large', 'neural', 'nets', 'Random', 'Forests', '.', '‹Once', 'try', 'automate', 'steps', 'much', 'possible', '.', '1.Train', 'many', 'quick', 'dirty', 'models', 'different', 'categories', 'e.g.', 'linear', 'naive', 'Bayes', 'SVM', 'Random', 'Forests', 'neural', 'net', 'etc', '.', 'using', 'standard', 'parameters', '.', '2.Measure', 'compare', 'performance', '.', '‹For', 'model', 'use', 'N-fold', 'cross-validation', 'compute', 'mean', 'stan…', 'dard', 'deviation', 'performance', 'measure', 'N', 'folds.3.Analyze', 'significant', 'variables', 'algorithm', '.', '4.Analyze', 'types', 'errors', 'models', 'make.‹What', 'data', 'would', 'human', 'used', 'avoid', 'errors', '?', '5.Have', 'quick', 'round', 'feature', 'selection', 'engineering', '.', '6.Have', 'one', 'two', 'quick', 'iterations', 'five', 'previous', 'steps', '.', '7.Short-list', 'top', 'three', 'five', 'promising', 'models', 'preferring', 'models', 'make', 'different', 'types', 'errors', '.', 'Fine-Tune', 'SystemNotes', '‹You', 'want', 'use', 'much', 'data', 'possible', 'step', 'especially', 'move', 'toward', 'end', 'fine-tuning.‹As', 'always', 'automate', '.', '500', '|', 'Appendix', 'B', 'Machine', 'Learning', 'Project', 'Checklist', '1ƒPractical', 'Bayesian', 'Optimization', 'Machine', 'Learning', 'Algorithms', '⁄', 'J.', 'Snoek', 'H.', 'Larochelle', 'R.', 'Adams', '2012', '.', '1.Fine-tune', 'hyperparameters', 'using', 'cross-validation', '.', '‹Treat', 'data', 'transformation', 'choices', 'hyperparameters', 'especially', 'sure', 'e.g.', 'I', 'replace', 'missing', 'values', 'zero', 'orwith', 'median', 'value', '?', 'Or', 'drop', 'rows', '?', '.‹Unless', 'hyperparameter', 'values', 'explore', 'prefer', 'random', 'search', 'grid', 'search', '.', 'If', 'training', 'long', 'may', 'prefer', 'Bayesian', 'optimization', 'approach', 'e.g.', 'using', 'Gaussian', 'process', 'priors', 'described', 'byJasper', 'Snoek', 'Hugo', 'Larochelle', 'Ryan', 'Adams', '.12.Try', 'Ensemble', 'methods', '.', 'Combining', 'best', 'models', 'often', 'perform', 'better', 'running', 'individually', '.', '3.Once', 'confident', 'final', 'model', 'measure', 'performance', 'test', 'set', 'estimate', 'generalization', 'error', '.', 'Don‡t', 'tweak', 'model', 'measuring', 'generalization', 'error', 'would', 'start', 'overfitting', 'test', 'set.Present', 'Your', 'Solution1.Document', 'done', '.', '2.Create', 'nice', 'presentation', '.', '‹Make', 'sure', 'highlight', 'big', 'picture', 'first', '.', '3.Explain', 'solution', 'achieves', 'business', 'objective', '.', '4.Don‡t', 'forget', 'present', 'interesting', 'points', 'noticed', 'along', 'way', '.', '‹Describe', 'worked', '.', '‹List', 'assumptions', 'system‡s', 'limitations', '.', '5.Ensure', 'key', 'findings', 'communicated', 'beautiful', 'visualizations', 'easy-to-remember', 'statements', 'e.g.', 'ƒthe', 'median', 'income', 'number-one', 'pre…', 'dictor', 'housing', 'prices⁄', '.', 'Machine', 'Learning', 'Project', 'Checklist', '|', '501', 'Launch', '!', '1.Get', 'solution', 'ready', 'production', 'plug', 'production', 'data', 'inputs', 'write', 'unit', 'tests', 'etc', '.', '.2.Write', 'monitoring', 'code', 'check', 'system‡s', 'live', 'performance', 'regular', 'inter…', 'vals', 'trigger', 'alerts', 'drops.‹Beware', 'slow', 'degradation', 'models', 'tend', 'ƒrot⁄', 'data', 'evolves', '.', '‹Measuring', 'performance', 'may', 'require', 'human', 'pipeline', 'e.g.', 'via', 'crowdsourc…', 'ing', 'service', '.', '‹Also', 'monitor', 'inputs‡', 'quality', 'e.g.', 'malfunctioning', 'sensor', 'sending', 'ran…', 'dom', 'values', 'another', 'team‡s', 'output', 'becoming', 'stale', '.', 'This', 'particularly', 'important', 'online', 'learning', 'systems', '.', '3.Retrain', 'models', 'regular', 'basis', 'fresh', 'data', 'automate', 'much', 'possi…', 'ble', '.502', '|', 'Appendix', 'B', 'Machine', 'Learning', 'Project', 'Checklist', 'APPENDIX', 'CSVM', 'Dual', 'ProblemTo', 'understand', 'duality', 'first', 'need', 'understand', 'Lagrange', 'multipliers', 'method.The', 'general', 'idea', 'transform', 'constrained', 'optimization', 'objective', 'uncon…', 'strained', 'one', 'moving', 'constraints', 'objective', 'function', '.', 'Let‡s', 'look', 'simple', 'example', '.', 'Suppose', 'want', 'find', 'values', 'x', 'minimize', 'function', 'f', 'x', '=', 'x2', '+', '2y', 'subject', 'equality', 'constraint', '3x', '+', '2y', '+', '1', '=', '0', '.', 'Using', 'Lagrange', 'multipliers', 'method', 'start', 'defining', 'new', 'function', 'called', 'Lagran…', 'gian', 'Lagrange', 'function', 'g', 'x', '‰', '=', 'f', 'x', '–', '‰', '3x', '+', '2y', '+', '1', '.', 'Each', 'constraint', 'case', 'one', 'subtracted', 'original', 'objective', 'multiplied', 'new', 'vari…', 'able', 'called', 'Lagrange', 'multiplier', '.', 'Joseph-Louis', 'Lagrange', 'showed', 'x', 'solution', 'constrained', 'optimiza…tion', 'problem', 'must', 'exist', '‰', 'x', '‰', 'stationary', 'point', 'Lagrangian', 'stationary', 'point', 'point', 'partial', 'derivatives', 'equal', 'zero', '.', 'In', 'words', 'compute', 'partial', 'derivatives', 'g', 'x', '‰', 'regardsto', 'x', '‰', 'find', 'points', 'derivatives', 'equal', 'zero', 'solutions', 'constrained', 'optimization', 'problem', 'exist', 'must', 'among', 'stationary', 'points', '.', 'In', 'example', 'partial', 'derivatives', 'ﬂﬂxgx', '‰=2', 'x', '”', '3', '‰ﬂﬂygx', '‰=2', '”', '2', '‰ﬂﬂ‰gx', '‰=', '”', '3', 'x', '”', '2', '”', '1', 'When', 'partial', 'derivatives', 'equal', '0', 'find', '2x', '”', '3', '‰=2', '”', '2', '‰=', '”', '3', 'x', '”', '2', '”', '1=0', 'easily', 'find', 'x=32', 'y=', '”', '114', '‰=1', '.', 'This', 'stationary', 'point', 'respects', 'con…', 'straint', 'must', 'solution', 'constrained', 'optimization', 'problem', '.', '503However', 'method', 'applies', 'equality', 'constraints', '.', 'Fortunately', 'regularity', 'conditions', 'respected', 'SVM', 'objectives', 'method', 'canbe', 'generalized', 'inequality', 'constraints', 'well', 'e.g.', '3x', '+', '2y', '+', '1', 'Š', '0', '.', 'The', 'generalized', 'Lagrangian', 'hard', 'margin', 'problem', 'given', 'Equation', 'C-1', '‰', 'vari…', 'ables', 'called', 'Karush', '‘', 'Kuhn', '‘', 'Tucker', 'KKT', 'multipliers', 'must', 'greater', 'equal', 'zero', '.', 'Equation', 'C-1', '.', 'Generalized', 'Lagrangian', 'hard', 'margin', 'problem', 'b', '‰=12T', '’', '”', '“', 'i=1', 'm‰itiT', '’', 'i+b', '”', '1', 'with‰iŠ0for', 'i=1,2', 'mJust', 'like', 'Lagrange', 'multipliers', 'method', 'compute', 'partial', 'deriva…', 'tives', 'locate', 'stationary', 'points', '.', 'If', 'solution', 'necessarily', 'among', 'stationary', 'points', 'b', '‰', 'respect', 'KKT', 'conditions', '‹Respect', 'problem‡s', 'constraints', 'tiT', '’', 'i+bŠ1for', 'i=1,2', '‹Verify', '‰iŠ0for', 'i=1,2', '‹Either', '‰i=0', 'ith', 'constraint', 'must', 'active', 'constraint', 'meaning', 'must', 'hold', 'equality', 'tiT', '’', 'i+b=1', '.', 'This', 'condition', 'called', 'complemen…', 'tary', 'slackness', 'condition', '.', 'It', 'implies', 'either', '‰i=0', 'ith', 'instance', 'lies', 'boundary', 'support', 'vector', '.', 'Note', 'KKT', 'conditions', 'necessary', 'conditions', 'stationary', 'point', 'solution', 'constrained', 'optimization', 'problem', '.', 'Under', 'conditions', 'also', 'sufficient', 'conditions', '.', 'Luckily', 'SVM', 'optimization', 'problem', 'happens', 'meet', 'conditions', 'stationary', 'point', 'meets', 'KKT', 'conditions', 'guaranteed', 'solution', 'constrained', 'optimization', 'problem', '.', 'We', 'compute', 'partial', 'derivatives', 'generalized', 'Lagrangian', 'regards', 'w', 'b', 'Equation', 'C-2', '.Equation', 'C-2', '.', 'Partial', 'derivatives', 'generalized', 'Lagrangian', 'b', '‰=', '”', '“', 'i=1', 'm‰itiiﬂﬂb', 'b', '‰=', '”', '“', 'i=1', 'm‰iti504', '|', 'Appendix', 'C', 'SVM', 'Dual', 'Problem', 'When', 'partial', 'derivatives', 'equal', '0', 'Equation', 'C-3', '.Equation', 'C-3', '.', 'Properties', 'stationary', 'points', '=', '“', 'i=1', 'm‰itii', '“', 'i=1', 'm‰iti=0', 'If', 'plug', 'results', 'definition', 'generalized', 'Lagrangian', 'terms', 'disappear', 'find', 'Equation', 'C-4', '.Equation', 'C-4', '.', 'Dual', 'form', 'SVM', 'problem', 'b', '‰=12', '“', 'i=1', '“', 'j=1', 'm‰i‰jtitjiT', '’', 'j', '”', '“', 'i=1', 'm‰iwith‰iŠ0for', 'i=1,2', 'mThe', 'goal', 'find', 'vector', '‰', 'minimizes', 'function', '‰iŠ0', 'instances', '.', 'This', 'constrained', 'optimization', 'problem', 'dual', 'problem', 'look…', 'ing', '.', 'Once', 'find', 'optimal', '‰', 'compute', 'using', 'first', 'line', 'Equation', 'C-3', '.To', 'compute', 'b', 'use', 'fact', 'support', 'vector', 'verifies', 'wT', '’', 'x', '+', 'b', '=', '1', 'kth', 'instance', 'support', 'vector', 'i.e.', '‰k', '>', '0', 'use', 'compute', 'b=1', '”', 'tkT', '’', 'k', '.', 'However', 'often', 'prefered', 'compute', 'average', 'support', 'vectors', 'get', 'stable', 'precise', 'value', 'Equation', 'C-5', '.Equation', 'C-5', '.', 'Bias', 'term', 'estimation', 'using', 'dual', 'form', 'b=1ns', '“', 'i=1', '‰i', '>', '0', 'm1', '”', 'tiT', '’', 'iSVM', 'Dual', 'Problem', '|', '505', 'APPENDIX', 'DAutodi†This', 'appendix', 'explains', 'TensorFlow‡s', 'autodiff', 'feature', 'works', 'com…', 'pares', 'solutions.Suppose', 'define', 'function', 'f', 'x', '=', 'x2y', '+', '+', '2', 'need', 'partial', 'derivatives', 'ﬂfﬂx', 'ﬂfﬂy', 'typically', 'perform', 'Gradient', 'Descent', 'optimization', 'algo…', 'rithm', '.', 'Your', 'main', 'options', 'manual', 'differentiation', 'symbolic', 'differentiation', 'numerical', 'differentiation', 'forward-mode', 'autodiff', 'finally', 'reverse-mode', 'autodiff', '.', 'TensorFlow', 'implements', 'last', 'option', '.', 'Let‡s', 'go', 'options', '.', 'Manual', 'Di†erentiationThe', 'first', 'approach', 'pick', 'pencil', 'piece', 'paper', 'use', 'calculus', 'knowledge', 'derive', 'partial', 'derivatives', 'manually', '.', 'For', 'function', 'f', 'x', 'justdefined', 'hard', 'need', 'use', 'five', 'rules', '‹The', 'derivative', 'constant', '0', '.', '‹The', 'derivative', 'Œx', 'Œ', 'Œ', 'constant', '.', '‹The', 'derivative', 'xÓ', 'ŒxŒ', '–', '1', 'derivative', 'x2', '2x.‹The', 'derivative', 'sum', 'functions', 'sum', 'functions‡', 'derivatives', '.', '‹The', 'derivative', 'Œ', 'times', 'function', 'Œ', 'times', 'derivative', '.', '507From', 'rules', 'derive', 'Equation', 'D-1', 'Equation', 'D-1', '.', 'Partial', 'derivatives', 'f', 'x', 'ﬂfﬂx=ﬂx2yﬂx+ﬂyﬂx+ﬂ2ﬂx=yﬂx2ﬂx+0+0=2', 'xy', 'ﬂfﬂy=ﬂx2yﬂy+ﬂyﬂy+ﬂ2ﬂy=x2+1+0=', 'x2+1', 'This', 'approach', 'become', 'tedious', 'complex', 'functions', 'run', 'risk', 'making', 'mistakes', '.', 'The', 'good', 'news', 'deriving', 'mathematical', 'equations', 'partial', 'derivatives', 'like', 'automated', 'process', 'called', 'symbolic', 'di›erentiation.Symbolic', 'Di†erentiationFigure', 'D-1', 'shows', 'symbolic', 'differentiation', 'works', 'even', 'simpler', 'function', 'g', 'x', '=', '5', '+', 'xy', '.', 'The', 'graph', 'function', 'represented', 'left', '.', 'After', 'symbolic', 'differentiation', 'get', 'graph', 'right', 'represents', 'partial', 'derivative', 'ﬂgﬂx=0+', '0‰', 'x+y‰1', '=y', 'could', 'similarly', 'obtain', 'partial', 'derivative', 'regards', '.Figure', 'D-1', '.', 'Symbolic', 'di›erentiationThe', 'algorithm', 'starts', 'getting', 'partial', 'derivative', 'leaf', 'nodes', '.', 'The', 'constant', 'node', '5', 'returns', 'constant', '0', 'since', 'derivative', 'constant', 'always', '0', '.', 'The', '508', '|', 'Appendix', 'D', 'Autodi†variable', 'x', 'returns', 'constant', '1', 'since', 'ﬂxﬂx=1', 'variable', 'returns', 'constant', '0', 'since', 'ﬂyﬂx=0', 'looking', 'partial', 'derivative', 'regards', 'wouldbe', 'reverse', '.Now', 'need', 'move', 'graph', 'multiplication', 'node', 'function', 'g.', 'Calculus', 'tells', 'us', 'derivative', 'product', 'two', 'functions', 'u', 'v', 'isﬂu‰vﬂx=ﬂvﬂx‰u+ﬂuﬂx‰u', '.', 'We', 'therefore', 'construct', 'large', 'part', 'graph', 'right', 'representing', '0', '‰', 'x', '+', '‰', '1.Finally', 'go', 'addition', 'node', 'function', 'g.', 'As', 'mentioned', 'derivative', 'sum', 'functions', 'sum', 'functions‡', 'derivatives', '.', 'So', 'need', 'create', 'addition', 'node', 'connect', 'parts', 'graph', 'already', 'com…', 'puted', '.', 'We', 'get', 'correct', 'partial', 'derivative', 'ﬂgﬂx=0+', '0‰', 'x+y‰1', '.However', 'simplified', 'lot', '.', 'A', 'trivial', 'pruning', 'steps', 'applied', 'graph', 'get', 'rid', 'unnecessary', 'operations', 'get', 'much', 'smaller', 'graph', 'one', 'node', 'ﬂgﬂx=y.In', 'case', 'simplification', 'fairly', 'easy', 'complex', 'function', 'symbolic', 'differentiation', 'produce', 'huge', 'graph', 'may', 'tough', 'simplify', 'lead', 'suboptimal', 'performance', '.', 'Most', 'importantly', 'symbolic', 'differentiation', 'deal', 'functions', 'defined', 'arbitrary', 'code›for', 'example', 'following', 'function', 'discussedin', 'Chapter', '9', 'def', 'my_func', 'b', 'z', '=', '0', 'range', '100', 'z', '=', '*', 'np.cos', 'z', '+', '+', 'z', '*', 'np.sin', 'b', '-', 'return', 'zNumerical', 'Di†erentiationThe', 'simplest', 'solution', 'compute', 'approximation', 'derivatives', 'numerically', '.', 'Recall', 'derivative', 'h', 'x0', 'function', 'h', 'x', 'point', 'x0', 'slope', 'func…tion', 'point', 'precisely', 'Equation', 'D-2', '.Equation', 'D-2', '.', 'Derivative', 'function', 'h', 'x', 'point', 'x', '0hx=lim', 'xx0hx', '”', 'hx0x', '”', 'x0=lim', '0hx0+', '”', 'hx0Autodi†', '|', '509', 'So', 'want', 'calculate', 'partial', 'derivative', 'f', 'x', 'regards', 'x', 'x', '=', '3', '=', '4', 'simply', 'compute', 'f', '3', '+', '4', '–', 'f', '3', '4', 'divide', 'result', 'using', 'avery', 'small', 'value', '.', 'That‡s', 'exactly', 'following', 'code', 'def', 'f', 'x', 'return', 'x**2*y', '+', '+', '2def', 'derivative', 'f', 'x', 'x_eps', 'y_eps', 'return', 'f', 'x', '+', 'x_eps', '+', 'y_eps', '-', 'f', 'x', '/', 'x_eps', '+', 'y_eps', 'df_dx', '=', 'derivative', 'f', '3', '4', '0.00001', '0', 'df_dy', '=', 'derivative', 'f', '3', '4', '0', '0.00001', 'Unfortunately', 'result', 'imprecise', 'gets', 'worse', 'complex', 'functions', '.', 'The', 'correct', 'results', 'respectively', '24', '10', 'instead', 'get', '>', '>', '>', 'print', 'df_dx', '24.000039999805264', '>', '>', '>', 'print', 'df_dy', '10.000000000331966Notice', 'compute', 'partial', 'derivatives', 'call', 'f', 'least', 'three', 'times', 'called', 'four', 'times', 'preceding', 'code', 'could', 'optimized', '.', 'If', 'therewere', '1,000', 'parameters', 'would', 'need', 'call', 'f', 'least', '1,001', 'times', '.', 'When', 'dealing', 'large', 'neural', 'networks', 'makes', 'numerical', 'differentiation', 'way', 'inefficient', '.', 'However', 'numerical', 'differentiation', 'simple', 'implement', 'great', 'tool', 'check', 'methods', 'implemented', 'correctly', '.', 'For', 'example', 'disagrees', 'manually', 'derived', 'function', 'function', 'probably', 'contains', 'mis…', 'take.Forward-Mode', 'Autodi†Forward-mode', 'autodi›', 'neither', 'numerical', 'differentiation', 'symbolic', 'differentia…', 'tion', 'ways', 'love', 'child', '.', 'It', 'relies', 'dual', 'numbers', 'weird', 'fascinating', 'numbers', 'form', '+', 'b', 'b', 'real', 'numbers', 'infinitesimal', 'number', '2', '=', '0', 'ł', '0', '.', 'You', 'think', 'dual', 'number', '42', '+', '24', 'something', 'akin', '42.0000000024', 'infinite', 'num…', 'ber', '0s', 'course', 'simplified', 'give', 'idea', 'dual', 'num…', 'bers', '.', 'A', 'dual', 'number', 'represented', 'memory', 'pair', 'floats', '.', 'For', 'example', '42', '+', '24', 'represented', 'pair', '42.0', '24.0', '.', '510', '|', 'Appendix', 'D', 'Autodi†Dual', 'numbers', 'added', 'multiplied', 'shown', 'Equation', 'D-3', '.Equation', 'D-3', '.', 'A', 'operations', 'dual', 'numbers', 'Œa+b=Œa+Œba+b+c+d=a+c+b+da+b‰c+d=ac+ad+bc+bd2=ac+ad+bcMost', 'importantly', 'shown', 'h', '+', 'b', '=', 'h', '+', 'b', '‰', 'h', 'computing', 'h', '+', 'gives', 'h', 'derivative', 'h', 'one', 'shot', '.', 'Figure', 'D-2shows', 'forward-mode', 'autodiff', 'computes', 'partial', 'derivative', 'f', 'x', 'withregards', 'x', 'x', '=', '3', '=', '4', '.', 'All', 'need', 'compute', 'f', '3', '+', '4', 'willoutput', 'dual', 'number', 'whose', 'first', 'component', 'equal', 'f', '3', '4', 'whose', 'secondcomponent', 'equal', 'ﬂfﬂx3,4', '.Figure', 'D-2', '.', 'Forward-mode', 'autodi›Autodi†', '|', '511', 'To', 'compute', 'ﬂfﬂy3,4', 'would', 'go', 'graph', 'time', 'x', '=', '3', '=', '4', '+', '.So', 'forward-mode', 'autodiff', 'much', 'accurate', 'numerical', 'differentiation', 'suffers', 'major', 'flaw', '1,000', 'parameters', 'would', 'require', '1,000', 'passes', 'graph', 'compute', 'partial', 'derivatives', '.', 'This', 'reverse-mode', 'autodiff', 'shines', 'compute', 'two', 'passes', 'throughthe', 'graph', '.', 'Reverse-Mode', 'Autodi†Reverse-mode', 'autodiff', 'solution', 'implemented', 'TensorFlow', '.', 'It', 'first', 'goes', 'graph', 'forward', 'direction', 'i.e.', 'inputs', 'output', 'compute', 'value', 'node', '.', 'Then', 'second', 'pass', 'time', 'reverse', 'direction', 'i.e.', 'output', 'inputs', 'compute', 'partial', 'derivatives', '.', 'Figure', 'D-3', 'represents', 'second', 'pass', '.', 'During', 'first', 'pass', 'node', 'values', 'computed', 'starting', 'x', '=', '3', '=', '4', '.', 'You', 'see', 'values', 'bottom', 'right', 'node', 'e.g.', 'x', '‰', 'x', '=', '9', '.', 'The', 'nodes', 'labeled', 'n1', 'n7', 'clarity', '.', 'The', 'output', 'node', 'n7', 'f', '3,4', '=', 'n7', '=', '42.Figure', 'D-3', '.', 'Reverse-mode', 'autodi›512', '|', 'Appendix', 'D', 'Autodi†The', 'idea', 'gradually', 'go', 'graph', 'computing', 'partial', 'derivative', 'f', 'x', 'regards', 'consecutive', 'node', 'reach', 'variable', 'nodes', '.', 'For', 'reverse-mode', 'autodiff', 'relies', 'heavily', 'chain', 'rule', 'shown', 'Equation', 'D-4', '.Equation', 'D-4', '.', 'Chain', 'rule', 'ﬂfﬂx=ﬂfﬂni‰ﬂniﬂxSince', 'n7', 'output', 'node', 'f', '=', 'n7', 'trivially', 'ﬂfﬂn7=1', '.Let‡s', 'continue', 'graph', 'n5', 'much', 'f', 'vary', 'n5', 'varies', '?', 'Theanswer', 'ﬂfﬂn5=ﬂfﬂn7‰ﬂn7ﬂn5', '.', 'We', 'already', 'know', 'ﬂfﬂn7=1', 'need', 'ﬂn7ﬂn5', '.', 'Sincen7', 'simply', 'performs', 'sum', 'n5', '+', 'n6', 'find', 'ﬂn7ﬂn5=1', 'ﬂfﬂn5=1‰1=1', '.Now', 'proceed', 'node', 'n4', 'much', 'f', 'vary', 'n4', 'varies', '?', 'The', 'answer', 'isﬂfﬂn4=ﬂfﬂn5‰ﬂn5ﬂn4', '.', 'Since', 'n5', '=', 'n4', '‰', 'n2', 'find', 'ﬂn5ﬂn4=n2', 'ﬂfﬂn4=1‰', 'n2=4', '.The', 'process', 'continues', 'reach', 'bottom', 'graph', '.', 'At', 'point', 'calculated', 'partial', 'derivatives', 'f', 'x', 'point', 'x', '=', '3', '=', '4', '.', 'In', 'thisexample', 'find', 'ﬂfﬂx=24', 'ﬂfﬂy=10', '.', 'Sounds', 'right', '!', 'Reverse-mode', 'autodiff', 'powerful', 'accurate', 'technique', 'especially', 'many', 'inputs', 'outputs', 'since', 'requires', 'one', 'forward', 'pass', 'plus', 'one', 'reverse', 'pass', 'per', 'output', 'compute', 'partial', 'derivatives', 'outputs', 'regards', 'inputs', '.', 'Most', 'importantly', 'deal', 'functions', 'defined', 'arbi…', 'trary', 'code', '.', 'It', 'also', 'handle', 'functions', 'entirely', 'differentiable', 'long', 'ask', 'compute', 'partial', 'derivatives', 'points', 'differentiable', '.', 'If', 'implement', 'new', 'type', 'operation', 'TensorFlow', 'want', 'make', 'compatible', 'autodiff', 'need', 'provide', 'function', 'builds', 'subgraph', 'compute', 'partial', 'derivatives', 'regards', 'inputs', '.', 'For', 'example', 'suppose', 'implement', 'function', 'computes', 'square', 'input', 'f', 'x', '=', 'x2', '.', 'In', 'case', 'would', 'need', 'provide', 'corresponding', 'derivative', 'function', 'f', 'x', '=', '2x', '.', 'Note', 'function', 'compute', 'numerical', 'result', 'instead', 'builds', 'subgraph', 'later', 'compute', 'result', '.', 'This', 'useful', 'means', 'compute', 'gradients', 'gradients', 'compute', 'second-order', 'derivatives', 'even', 'higher-order', 'derivatives', '.', 'Autodi†', '|', '513', 'APPENDIX', 'EOther', 'Popular', 'ANN', 'ArchitecturesIn', 'appendix', 'give', 'quick', 'overview', 'historically', 'important', 'neural', 'network', 'architectures', 'much', 'less', 'used', 'today', 'deep', 'Multi-Layer', 'Percep…', 'trons', 'Chapter', '10', 'convolutional', 'neural', 'networks', 'Chapter', '13', 'recurrent', 'neural', 'networks', 'Chapter', '14', 'autoencoders', 'Chapter', '15', '.', 'They', 'often', 'mentioned', 'literature', 'still', 'used', 'many', 'applications', 'worth', 'knowing', '.', 'Moreover', 'discuss', 'deep', 'belief', 'nets', 'DBNs', 'state', 'art', 'Deep', 'Learning', 'early', '2010s', '.', 'They', 'still', 'subject', 'active', 'research', 'may', 'well', 'come', 'back', 'vengeance', 'near', 'future', '.', 'Hop•eld', 'NetworksHop†eld', 'networks', 'first', 'introduced', 'W.', 'A', '.', 'Little', '1974', 'popularized', 'J.', 'Hopfield', '1982', '.', 'They', 'associative', 'memory', 'networks', 'first', 'teach', 'somepatterns', 'see', 'new', 'pattern', 'hopefully', 'output', 'closest', 'learned', 'pattern', '.', 'This', 'made', 'useful', 'particular', 'character', 'recognition', 'outperformed', 'approaches', '.', 'You', 'first', 'train', 'network', 'showing', 'examples', 'character', 'images', 'binary', 'pixel', 'maps', 'one', 'neuron', 'show', 'new', 'character', 'image', 'iterations', 'outputs', 'closest', 'learned', 'character', '.', 'They', 'fully', 'connected', 'graphs', 'see', 'Figure', 'E-1', 'every', 'neuron', 'connected', 'every', 'neuron', '.', 'Note', 'diagram', 'images', '6', '‰', '6', 'pixels', 'neural', 'network', 'left', 'contain', '36', 'neurons', '648', 'connections', 'visual', 'clarity', 'much', 'smaller', 'network', 'represented', '.', '515Figure', 'E-1', '.', 'Hop†eld', 'network', 'The', 'training', 'algorithm', 'works', 'using', 'Hebb‡s', 'rule', 'training', 'image', 'weight', 'two', 'neurons', 'increased', 'corresponding', 'pixels', 'decreased', 'one', 'pixel', 'off.To', 'show', 'new', 'image', 'network', 'activate', 'neurons', 'correspond', 'active', 'pixels', '.', 'The', 'network', 'computes', 'output', 'every', 'neuron', 'gives', 'new', 'image', '.', 'You', 'take', 'new', 'image', 'repeat', 'whole', 'process', '.', 'After', 'network', 'reaches', 'stable', 'state', '.', 'Generally', 'corresponds', 'training', 'image', 'resembles', 'input', 'image', '.', 'A', 'so-called', 'energy', 'function', 'associated', 'Hopfield', 'nets', '.', 'At', 'iteration', 'energy', 'decreases', 'network', 'guaranteed', 'eventually', 'stabilize', 'low-energy', 'state', '.', 'The', 'training', 'algorithm', 'tweaks', 'weights', 'way', 'decreases', 'energy', 'level', 'training', 'patterns', 'network', 'likely', 'stabilize', 'one', 'low-', 'energy', 'configurations', '.', 'Unfortunately', 'patterns', 'training', 'set', 'also', 'end', 'low', 'energy', 'network', 'sometimes', 'stabilizes', 'configuration', 'learned', '.', 'These', 'called', 'spurious', 'patterns', '.Another', 'major', 'flaw', 'Hopfield', 'nets', 'don‡t', 'scale', 'well›their', 'mem…', 'ory', 'capacity', 'roughly', 'equal', '14', '%', 'number', 'neurons', '.', 'For', 'example', 'clas…', 'sify', '28', '‰', '28', 'images', 'would', 'need', 'Hopfield', 'net', '784', 'fully', 'connected', 'neurons', '306,936', 'weights', '.', 'Such', 'network', 'would', 'able', 'learn', '110', 'different', 'characters', '14', '%', '784', '.', 'That‡s', 'lot', 'parameters', 'small', 'memory', '.', 'Boltzmann', 'MachinesBoltzmann', 'machines', 'invented', '1985', 'Geoffrey', 'Hinton', 'Terrence', 'Sejnow…', 'ski', '.', 'Just', 'like', 'Hopfield', 'nets', 'fully', 'connected', 'ANNs', 'based', 'sto…', '516', '|', 'Appendix', 'E', 'Other', 'Popular', 'ANN', 'Architectures', 'chastic', 'neurons', 'instead', 'using', 'deterministic', 'step', 'function', 'decide', 'value', 'output', 'neurons', 'output', '1', 'probability', '0', 'otherwise', '.', 'The', 'probabil…', 'ity', 'function', 'ANNs', 'use', 'based', 'Boltzmann', 'distribution', 'used', 'statistical', 'mechanics', 'hence', 'name', '.', 'Equation', 'E-1', 'gives', 'probability', 'par…', 'ticular', 'neuron', 'output', '1.Equation', 'E-1', '.', 'Probability', 'th', 'neuron', 'output', '1', 'psinextstep', '=1', '=„', '“', 'j=1', 'Nwi', 'jsj+biT‹sj', 'jth', 'neuron‡s', 'state', '0', '1', '.', '‹wi', 'j', 'connection', 'weight', 'th', 'jth', 'neurons', '.', 'Note', 'wi', '=', '0.‹bi', 'ith', 'neuron‡s', 'bias', 'term', '.', 'We', 'implement', 'term', 'adding', 'bias', 'neu…', 'ron', 'network.‹N', 'number', 'neurons', 'network', '.', '‹T', 'number', 'called', 'network‡s', 'temperature', 'higher', 'temperature', 'random', 'output', 'i.e.', 'probability', 'approaches', '50', '%', '.', '‹„', 'logistic', 'function.Neurons', 'Boltzmann', 'machines', 'separated', 'two', 'groups', 'visible', 'units', 'hid…', 'den', 'units', 'see', 'Figure', 'E-2', '.', 'All', 'neurons', 'work', 'stochastic', 'way', 'visi…', 'ble', 'units', 'ones', 'receive', 'inputs', 'outputs', 'read', '.', 'Figure', 'E-2', '.', 'Boltzmann', 'machine', 'Other', 'Popular', 'ANN', 'Architectures', '|', '517', 'Because', 'stochastic', 'nature', 'Boltzmann', 'machine', 'never', 'stabilize', 'fixed', 'configuration', 'instead', 'keep', 'switching', 'many', 'configurations', '.', 'If', 'left', 'running', 'sufficiently', 'long', 'time', 'probability', 'observing', 'particular', 'con…', 'figuration', 'function', 'connection', 'weights', 'bias', 'terms', 'original', 'configuration', 'similarly', 'shuffle', 'deck', 'cards', 'long', 'enough', 'configuration', 'deck', 'depend', 'initial', 'state', '.', 'When', 'network', 'reaches', 'state', 'original', 'configuration', 'ƒforgotten', '⁄', 'said', 'inthermal', 'equilibrium', 'although', 'configuration', 'keeps', 'changing', 'time', '.', 'By', 'set…', 'ting', 'network', 'parameters', 'appropriately', 'letting', 'network', 'reach', 'thermal', 'equili…', 'brium', 'observing', 'state', 'simulate', 'wide', 'range', 'probability', 'distributions', '.', 'This', 'called', 'generative', 'model', '.Training', 'Boltzmann', 'machine', 'means', 'finding', 'parameters', 'make', 'net…', 'work', 'approximate', 'training', 'set‡s', 'probability', 'distribution', '.', 'For', 'example', 'three', 'visible', 'neurons', 'training', 'set', 'contains', '75', '%', '0', '1', '1', 'triplets', '10', '%', '0', '0', '1', 'triplets', '15', '%', '1', '1', '1', 'triplets', 'training', 'Boltzmann', 'machine', 'could', 'use', 'generate', 'random', 'binary', 'triplets', 'probability', 'distribu…', 'tion', '.', 'For', 'example', '75', '%', 'time', 'would', 'output', '0', '1', '1', 'triplet', '.', 'Such', 'generative', 'model', 'used', 'variety', 'ways', '.', 'For', 'example', 'trained', 'images', 'provide', 'incomplete', 'noisy', 'image', 'network', 'automatically', 'ƒrepair⁄', 'image', 'reasonable', 'way', '.', 'You', 'also', 'use', 'generative', 'model', 'classification', '.', 'Just', 'add', 'visible', 'neurons', 'encode', 'training', 'image‡s', 'class', 'e.g.', 'add', '10', 'visible', 'neurons', 'turn', 'fifth', 'neuron', 'trainingimage', 'represents', '5', '.', 'Then', 'given', 'new', 'image', 'network', 'automatically', 'turn', 'appropriate', 'visible', 'neurons', 'indicating', 'image‡s', 'class', 'e.g.', 'turn', 'fifth', 'visible', 'neuron', 'image', 'represents', '5', '.', 'Unfortunately', 'efficient', 'technique', 'train', 'Boltzmann', 'machines', '.', 'However', 'fairly', 'efficient', 'algorithms', 'developed', 'train', 'restricted', 'Boltzmann', 'machines', 'RBM', '.Restricted', 'Boltzmann', 'MachinesAn', 'RBM', 'simply', 'Boltzmann', 'machine', 'connections', 'visible', 'units', 'hidden', 'units', 'visible', 'hidden', 'units', '.', 'For', 'example', 'Figure', 'E-3', 'represents', 'RBM', 'three', 'visible', 'units', 'four', 'hidden', 'units.518', '|', 'Appendix', 'E', 'Other', 'Popular', 'ANN', 'Architectures', '1ƒOn', 'Contrastive', 'Divergence', 'Learning', '⁄', 'M.', 'Ô.', 'Carreira-PerpiÕÖn', 'G.', 'Hinton', '2005', '.', 'Figure', 'E-3', '.', 'Restricted', 'Boltzmann', 'machine', 'A', 'efficient', 'training', 'algorithm', 'called', 'Contrastive', 'Divergence', 'introduced', '2005', 'Miguel', 'Ô.', 'Carreira-PerpiÕÖn', 'Geoffrey', 'Hinton', '.1', 'Here', 'works', 'training', 'instance', 'x', 'algorithm', 'starts', 'feeding', 'network', 'settingthe', 'state', 'visible', 'units', 'x1', 'x2', 'xn', '.', 'Then', 'compute', 'state', 'hidden', 'units', 'applying', 'stochastic', 'equation', 'described', 'Equation', 'E-1', '.', 'This', 'givesyou', 'hidden', 'vector', 'h', 'hi', 'equal', 'state', 'th', 'unit', '.', 'Next', 'compute', 'state', 'visible', 'units', 'applying', 'stochastic', 'equation', '.', 'This', 'gives', 'vector', 'ı', '.', 'Then', 'compute', 'state', 'hidden', 'units', 'gives', 'vector', 'ı', '.', 'Now', 'update', 'connection', 'weight', 'applying', 'rule', 'Equation', 'E-2', '.Equation', 'E-2', '.', 'Contrastive', 'divergence', 'weight', 'update', 'wi', 'jnextstep', '=wi', 'j+−', 'T', '”', 'ııTThe', 'great', 'benefit', 'algorithm', 'require', 'waiting', 'network', 'reach', 'thermal', 'equilibrium', 'goes', 'forward', 'backward', 'forward', 'that‡s', '.', 'This', 'makes', 'incomparably', 'efficient', 'previous', 'algorithms', 'key', 'ingredient', 'first', 'success', 'Deep', 'Learning', 'based', 'multiple', 'stacked', 'RBMs.Deep', 'Belief', 'NetsSeveral', 'layers', 'RBMs', 'stacked', 'hidden', 'units', 'first-level', 'RBM', 'serves', 'visible', 'units', 'second-layer', 'RBM', '.', 'Such', 'RBM', 'stack', 'called', 'deep', 'belief', 'net', 'DBN', '.Yee-Whye', 'Teh', 'one', 'Geoffrey', 'Hinton‡s', 'students', 'observed', 'possible', 'train', 'DBNs', 'one', 'layer', 'time', 'using', 'Contrastive', 'Divergence', 'starting', 'lower', 'Other', 'Popular', 'ANN', 'Architectures', '|', '519', '2ƒA', 'Fast', 'Learning', 'Algorithm', 'Deep', 'Belief', 'Nets', '⁄', 'G.', 'Hinton', 'S.', 'Osindero', 'Y.', 'Teh', '2006', '.', 'layers', 'gradually', 'moving', 'top', 'layers', '.', 'This', 'led', 'groundbreakingarticle', 'kickstarted', 'Deep', 'Learning', 'tsunami', '2006', '.2Just', 'like', 'RBMs', 'DBNs', 'learn', 'reproduce', 'probability', 'distribution', 'inputs', 'without', 'supervision', '.', 'However', 'much', 'better', 'reason', 'deep', 'neural', 'networks', 'powerful', 'shallow', 'ones', 'real-world', 'data', 'often', 'organized', 'hierarchical', 'patterns', 'DBNs', 'take', 'advantage', '.', 'Their', 'lower', 'lay…', 'ers', 'learn', 'low-level', 'features', 'input', 'data', 'higher', 'layers', 'learn', 'high-level', 'fea…', 'tures.Just', 'like', 'RBMs', 'DBNs', 'fundamentally', 'unsupervised', 'also', 'train', 'supervised', 'manner', 'adding', 'visible', 'units', 'represent', 'labels', '.', 'More…', 'one', 'great', 'feature', 'DBNs', 'trained', 'semisupervised', 'fash…', 'ion', '.', 'Figure', 'E-4', 'represents', 'DBN', 'configured', 'semisupervised', 'learning', '.', 'Figure', 'E-4', '.', 'A', 'deep', 'belief', 'network', 'con†gured', 'semisupervised', 'learning', 'First', 'RBM', '1', 'trained', 'without', 'supervision', '.', 'It', 'learns', 'low-level', 'features', 'training', 'data', '.', 'Then', 'RBM', '2', 'trained', 'RBM', '1‡s', 'hidden', 'units', 'inputs', 'without', 'supervision', 'learns', 'higher-level', 'features', 'note', 'RBM', '2‡s', 'hidden', 'units', 'include', 'three', 'rightmost', 'units', 'label', 'units', '.', 'Several', 'RBMs', 'could', 'stacked', 'way', 'get', 'idea', '.', 'So', 'far', 'training', '100', '%', 'unsupervised', '.', '520', '|', 'Appendix', 'E', 'Other', 'Popular', 'ANN', 'Architectures', '3See', 'video', 'Geoffrey', 'Hinton', 'details', 'demo', 'http', '//goo.gl/7Z5QiS', '.Lastly', 'RBM', '3', 'trained', 'using', 'RBM', '2‡s', 'hidden', 'units', 'inputs', 'well', 'extra', 'visible', 'units', 'used', 'represent', 'target', 'labels', 'e.g.', 'one-hot', 'vector', 'representing', 'instance', 'class', '.', 'It', 'learns', 'associate', 'high-level', 'features', 'training', 'labels', '.', 'This', 'supervised', 'step', '.', 'At', 'end', 'training', 'feed', 'RBM', '1', 'new', 'instance', 'signal', 'propagate', 'RBM', '2', 'top', 'RBM', '3', 'back', 'label', 'units', 'hope…fully', 'appropriate', 'label', 'light', '.', 'This', 'DBN', 'used', 'classifica…', 'tion.One', 'great', 'benefit', 'semisupervised', 'approach', 'don‡t', 'need', 'much', 'labeled', 'training', 'data', '.', 'If', 'unsupervised', 'RBMs', 'good', 'enough', 'job', 'small', 'amount', 'labeled', 'training', 'instances', 'per', 'class', 'necessary', '.', 'Similarly', 'baby', 'learns', 'recognize', 'objects', 'without', 'supervision', 'point', 'chair', 'say', 'ƒchair', '⁄', 'baby', 'associate', 'word', 'ƒchair⁄', 'class', 'objects', 'already', 'learned', 'recognize', '.', 'You', 'don‡t', 'need', 'point', 'every', 'single', 'chair', 'say', 'ƒchair⁄', 'examples', 'suffice', 'enough', 'baby', 'sure', 'indeed', 'referring', 'chair', 'color', 'one', 'chair‡s', 'parts', '.', 'Quite', 'amazingly', 'DBNs', 'also', 'work', 'reverse', '.', 'If', 'activate', 'one', 'label', 'units', 'signal', 'propagate', 'hidden', 'units', 'RBM', '3', 'RBM', '2', 'RBM', '1', 'new', 'instance', 'output', 'visible', 'units', 'RBM', '1', '.', 'Thisnew', 'instance', 'usually', 'look', 'like', 'regular', 'instance', 'class', 'whose', 'label', 'unit', 'youactivated', '.', 'This', 'generative', 'capability', 'DBNs', 'quite', 'powerful', '.', 'For', 'example', 'used', 'automatically', 'generate', 'captions', 'images', 'vice', 'versa', 'first', 'DBN', 'trained', 'without', 'supervision', 'learn', 'features', 'images', 'another', 'DBN', 'trained', 'without', 'supervision', 'learn', 'features', 'sets', 'captions', 'e.g.', 'ƒcar⁄', 'often', 'comes', 'ƒautomobile⁄', '.', 'Then', 'RBM', 'stacked', 'top', 'DBNs', 'trained', 'set', 'images', 'along', 'captions', 'learns', 'associate', 'high-level', 'features', 'images', 'high-level', 'features', 'captions', '.', 'Next', 'feed', 'image', 'DBN', 'image', 'car', 'signal', 'propagate', 'network', 'top-level', 'RBM', 'back', 'bottom', 'caption', 'DBN', 'producing', 'caption', '.', 'Due', 'stochastic', 'nature', 'RBMs', 'DBNs', 'caption', 'keep', 'changing', 'randomly', 'generally', 'appropriate', 'image', '.', 'If', 'generate', 'hundred', 'cap…', 'tions', 'frequently', 'generated', 'ones', 'likely', 'good', 'description', 'image.3Self-Organizing', 'MapsSelf-organizing', 'maps', 'SOM', 'quite', 'different', 'types', 'neural', 'net…', 'works', 'discussed', 'far', '.', 'They', 'used', 'produce', 'low-dimensional', 'repre…', 'Other', 'Popular', 'ANN', 'Architectures', '|', '521', 'sentation', 'high-dimensional', 'dataset', 'generally', 'visualization', 'clustering', 'classification', '.', 'The', 'neurons', 'spread', 'across', 'map', 'typically', '2D', 'visualization', 'number', 'dimensions', 'want', 'shown', 'Figure', 'E-5', 'eachneuron', 'weighted', 'connection', 'every', 'input', 'note', 'diagram', 'shows', 'two', 'inputs', 'typically', 'large', 'number', 'since', 'whole', 'point', 'SOMs', 'reduce', 'dimensionality', '.Figure', 'E-5', '.', 'Self-organizing', 'maps', 'Once', 'network', 'trained', 'feed', 'new', 'instance', 'activate', 'one', 'neuron', 'i.e.', 'hence', 'one', 'point', 'map', 'neuron', 'whose', 'weight', 'vector', 'closest', 'input', 'vector', '.', 'In', 'general', 'instances', 'nearby', 'original', 'input', 'space', 'activate', 'neurons', 'nearby', 'map', '.', 'This', 'makes', 'SOMs', 'useful', 'visualization', 'particular', 'easily', 'identify', 'clusters', 'map', 'also', 'applications', 'like', 'speech', 'recognition', '.', 'For', 'example', 'instance', 'represents', 'audio', 'recording', 'person', 'pronouncing', 'vowel', 'different', 'pronunciations', 'vowel', 'ƒa⁄', 'activate', 'neurons', 'area', 'map', 'instances', 'vowel', 'ƒe⁄', 'activate', 'neurons', 'another', 'area', 'intermediate', 'sounds', 'gener…', 'ally', 'activate', 'intermediate', 'neurons', 'map', '.', 'One', 'important', 'difference', 'dimensionality', 'reduction', 'techniques', 'discussed', 'Chapter', '8', 'instances', 'get', 'mapped', 'discrete', 'number', 'points', 'low-dimensional', 'space', 'one', 'point', 'per', 'neuron', '.', 'When', 'neurons', 'techni…', 'que', 'better', 'described', 'clustering', 'rather', 'dimensionality', 'reduction.522', '|', 'Appendix', 'E', 'Other', 'Popular', 'ANN', 'Architectures', '4You', 'imagine', 'class', 'young', 'children', 'roughly', 'similar', 'skills', '.', 'One', 'child', 'happens', 'slightly', 'better', 'basketball', '.', 'This', 'motivates', 'practice', 'especially', 'friends', '.', 'After', 'group', 'friends', 'gets', 'good', 'basketball', 'kids', 'compete', '.', 'But', 'that‡s', 'okay', 'kids', 'spe…', 'cialize', 'topics', '.', 'After', 'class', 'full', 'little', 'specialized', 'groups.The', 'training', 'algorithm', 'unsupervised', '.', 'It', 'works', 'neurons', 'compete', '.', 'First', 'weights', 'initialized', 'randomly', '.', 'Then', 'training', 'instance', 'picked', 'randomly', 'fed', 'network', '.', 'All', 'neurons', 'compute', 'dis…', 'tance', 'weight', 'vector', 'input', 'vector', 'different', 'artificial', 'neurons', 'seen', 'far', '.', 'The', 'neuron', 'measures', 'smallest', 'dis…', 'tance', 'wins', 'tweaks', 'weight', 'vector', 'even', 'slightly', 'closer', 'input', 'vector', 'making', 'likely', 'win', 'future', 'competitions', 'inputs', 'similar', 'one', '.', 'It', 'also', 'recruits', 'neighboring', 'neurons', 'update', 'weight', 'vector', 'slightly', 'closer', 'input', 'vector', 'don‡t', 'update', 'weights', 'much', 'winner', 'neuron', '.', 'Then', 'algorithm', 'picks', 'another', 'training', 'instance', 'repeats', 'process', '.', 'This', 'algorithm', 'tends', 'make', 'nearby', 'neurons', 'graduallyspecialize', 'similar', 'inputs', '.', '4Other', 'Popular', 'ANN', 'Architectures', '|', '523', 'IndexSymbols__call__', '385Ò-greedy', 'policy', '459', '464Ò-insensitive', '155×', '2', 'test', 'see', 'chi', 'square', 'test', '—', '0', 'norm', '39—', '1', '—', '2', 'regularization', '303-304—', '1', 'norm', '39', '130', '139', '300', '303—', '2', 'norm', '39', '128-130', '139', '142', '303', '307—', 'k', 'norm', '39—', '‚', 'norm', '39Aaccuracy', '4', '83-84actions', 'evaluating', '447-448activation', 'functions', '262-264active', 'constraints', '504actors', '463actual', 'class', '85AdaBoost', '192-195Adagrad', '296-298Adam', 'optimization', '293', '298-300adaptive', 'learning', 'rate', '297adaptive', 'moment', 'optimization', '298agents', '438AlexNet', 'architecture', '367-368algorithmspreparing', 'data', '59-68AlphaGo', '14', '253', '437', '453Anaconda', '41anomaly', 'detection', '12Apple‡s', 'Siri', '253apply_gradients', '286', '450area', 'curve', 'AUC', '92arg_scope', '285array_split', '217artificial', 'neural', 'networks', 'ANNs', '253-274Boltzmann', 'Machines', '516-518deep', 'belief', 'networks', 'DBNs', '519-521evolution', '254Hopfield', 'Networks', '515-516hyperparameter', 'fine-tuning', '270-272overview', '253-255Perceptrons', '257-264self-organizing', 'maps', '521-523training', 'DNN', 'TensorFlow', '265-270artificial', 'neuron', '256', 'see', 'also', 'artificial', 'neural', 'network', 'ANN', 'assign', '237association', 'rule', 'learning', '12associative', 'memory', 'networks', '515assumptions', 'checking', '40asynchronous', 'updates', '348-349asynchrous', 'communication', '329-334atrous_conv2d', '376attention', 'mechanism', '409attributes', '9', '45-48', 'see', 'also', 'data', 'structure', 'combinations', '58-59preprocessed', '48target', '48autodiff', '238-239', '507-513forward-mode', '510-512manual', 'differentiation', '507numerical', 'differentiation', '509reverse-mode', '512-513symbolic', 'differentiation', '508-509autoencoders', '411-435525adversarial', '433contractive', '432denoising', '424-425efficient', 'data', 'representations', '412generative', 'stochastic', 'network', 'GSN', '433overcomplete', '424PCA', 'undercomplete', 'linear', 'autoen…', 'coder', '413reconstructions', '413sparse', '426-428stacked', '415-424stacked', 'convolutional', '433undercomplete', '413variational', '428-432visualizing', 'features', '421-422winner-take-all', 'WTA', '433automatic', 'differentiating', '231autonomous', 'driving', 'systems', '379Average', 'Absolute', 'Deviation', '39average', 'pooling', 'layer', '364avg_pool', '364Bbackpropagation', '261-262', '275', '291', '422backpropagation', 'time', 'BPTT', '389bagging', 'pasting', '185-188out-of-bag', 'evaluation', '187-188in', 'Scikit-Learn', '186-187bandwidth', 'saturation', '349-351BasicLSTMCell', '401BasicRNNCell', '397-398Batch', 'Gradient', 'Descent', '114-117', '130batch', 'learning', '14-15Batch', 'Normalization', '282-286', '374operation', 'summary', '282with', 'TensorFlow', '284-286batch', '341batch_join', '341batch_norm', '284-285Bellman', 'Optimality', 'Equation', '455between-graph', 'replication', '344bias', 'neurons', '258bias', 'term', '106bias/variance', 'tradeoff', '126biases', '267binary', 'classifiers', '82', '134biological', 'neurons', '254-256black', 'box', 'models', '170blending', '200-203Boltzmann', 'Machines', '516-518', 'see', 'also', 'restricted', 'Boltzman', 'machines', 'RBMs', 'boosting', '191-200AdaBoost', '192-195Gradient', 'Boosting', '195-200bootstrap', 'aggregation', 'see', 'bagging', 'bootstrapping', '72', '185', '442', '469bottleneck', 'layers', '369brew', '202CCaffe', 'model', 'zoo', '291call__', '398CART', 'Classification', 'Regression', 'Tree', 'algorithm', '170-171', '176categorical', 'attributes', '62-64cell', 'wrapper', '392chi', 'square', 'test', '174classification', 'versus', 'regression', '8', '101classifiersbinary', '82error', 'analysis', '96-99evaluating', '96MNIST', 'dataset', '79-81multiclass', '93-96multilabel', '100-101multioutput', '101-102performance', 'measures', '82-93precision', '85voting', '181-184clip_by_value', '286closed-form', 'equation', '105', '128', '136cluster', 'specification', '324clustering', 'algorithms', '10clusters', '323coding', 'space', '429codings', '411complementary', 'slackness', 'condition', '504components_', '214computational', 'complexity', '110', '153', '172compute_gradients', '286', '449concat', '369config.gpu_options', '318ConfigProto', '317confusion', 'matrix', '84-86', '96-99connectionism', '260constrained', 'optimization', '158', '503Contrastive', 'Divergence', '519526', '|', 'Index', 'control', 'dependencies', '323conv1d', '376conv2d_transpose', '376conv3d', '376convergence', 'rate', '117convex', 'function', '113convolution', 'kernels', '357', '365', '370convolutional', 'neural', 'networks', 'CNNs', '353-378architectures', '365-376AlexNet', '367-368GoogleNet', '368-372LeNet5', '366-367ResNet', '372-375convolutional', 'layer', '355-363', '370', '376feature', 'maps', '358-360filters', '357memory', 'requirement', '362-363evolution', '354pooling', 'layer', '363-365TensorFlow', 'implementation', '360-362Coordinator', 'class', '338-340correlation', 'coefficient', '55-58correlations', 'finding', '55-58cost', 'function', '20', '39in', 'AdaBoost', '193in', 'adagrad', '297in', 'artificial', 'neural', 'networks', '264', '267-268in', 'autodiff', '238in', 'batch', 'normalization', '285cross', 'entropy', '367deep', 'Q-Learning', '465in', 'Elastic', 'Net', '132in', 'Gradient', 'Descent', '105', '111-112', '114,117-119', '200', '275in', 'Logistic', 'Regression', '135-136in', 'PG', 'algorithms', '449in', 'variational', 'autoencoders', '430in', 'Lasso', 'Regression', '130-131in', 'Linear', 'Regression', '108', '113in', 'Momentum', 'optimization', '294-295in', 'pretrained', 'layers', 'reuse', '293in', 'ridge', 'regression', '127-129in', 'RNNs', '389', '393stale', 'gradients', '349creative', 'sequences', '396credit', 'assignment', 'problem', '447-448critics', '463cross', 'entropy', '140-141', '264', '428', '449cross-validation', '30', '69-71', '83-84CUDA', 'library', '315cuDNN', 'library', '315curse', 'dimensionality', '205-207', 'see', 'also', 'dimensionality', 'reduction', 'custom', 'transformers', '64-65Ddata', '30', 'see', 'also', 'test', 'data', 'training', 'data', 'creating', 'workspace', '40-43downloading', '43-45finding', 'correlations', '55-58making', 'assumptions', '30preparing', 'Machine', 'Learning', 'algorithms', '59-68test-set', 'creation', '49-53working', 'real', 'data', '33data', 'augmentation', '309-310data', 'cleaning', '60-62data', 'mining', '6data', 'parallelism', '347-351asynchronous', 'updates', '348-349bandwidth', 'saturation', '349-351synchronous', 'updates', '348TensorFlow', 'implementation', '351data', 'pipeline', '36data', 'snooping', 'bias', '49data', 'structure', '45-48data', 'visualization', '53-55DataFrame', '60dataquest', 'xvidecay', '284decision', 'boundaries', '136-139', '142', '170decision', 'function', '87', '156-157Decision', 'Stumps', '195decision', 'threshold', '87Decision', 'Trees', '69-70', '167-179', '181binary', 'trees', '170class', 'probability', 'estimates', '171computational', 'complexity', '172decision', 'boundaries', '170GINI', 'impurity', '172instability', '177-178numbers', 'children', '170predictions', '169-171Random', 'Forests', 'see', 'Random', 'Forests', 'regression', 'tasks', '175-176regularization', 'hyperparameters', '173-174Index', '|', '527', 'training', 'visualizing', '167-169decoder', '412deconvolutional', 'layer', '376deep', 'autoencoders', 'see', 'stacked', 'autoencoders', 'deep', 'belief', 'networks', 'DBNs', '13', '519-521Deep', 'Learning', '437', 'see', 'also', 'Reinforcement', 'Learning', 'Tensor…', 'Flow', 'xiii', 'xvilibraries', '230-231deep', 'neural', 'networks', 'DNNs', '261', '275-312', 'see', 'also', 'Multi-Layer', 'Perceptrons', 'MLP', 'faster', 'optimizers', '293-302regularization', '302-310reusing', 'pretrained', 'layers', '286-293training', 'guidelines', 'overview', '310training', 'TensorFlow', '265-270training', 'TF.Learn', '264unstable', 'gradients', '276vanishing', 'exploding', 'gradients', '275-286Deep', 'Q-Learning', '460-469Ms', '.', 'Pac', 'Man', 'example', '460-469deep', 'Q-network', '460deep', 'RNNs', '396-400applying', 'dropout', '399distributing', 'across', 'multiple', 'GPUs', '397long', 'sequence', 'difficulties', '400truncated', 'backpropagation', 'time', '400DeepMind', '14', '253', '437', '460degrees', 'freedom', '27', '126denoising', 'autoencoders', '424-425depth', 'concat', 'layer', '369depth', 'radius', '368depthwise_conv2d', '376dequeue', '332dequeue_many', '332', '334dequeue_up_to', '333-334dequeuing', 'data', '331describe', '46device', 'blocks', '327device', '319dimensionality', 'reduction', '12', '205-225', '411approaches', 'Manifold', 'Learning', '210projection', '207-209choosing', 'right', 'number', 'dimensions', '215curse', 'dimensionality', '205-207and', 'data', 'visualization', '205Isomap', '224LLE', 'Locally', 'Linear', 'Embedding', '221-223Multidimensional', 'Scaling', '223-224PCA', 'Principal', 'Component', 'Analysis', '211-218t-Distributed', 'Stochastic', 'Neighbor', 'Embed…', 'ding', 't-SNE', '224discount', 'rate', '447distributed', 'computing', '229distributed', 'sessions', '328-329DNNClassifier', '264drop', '60dropconnect', '307dropna', '60dropout', '272', '399dropout', 'rate', '304dropout', '306DropoutWrapper', '399DRY', 'Don‡t', 'Repeat', 'Yourself', '247Dual', 'Averaging', '300dual', 'numbers', '510dual', 'problem', '160duality', '503dying', 'ReLUs', '279dynamic', 'placements', '320dynamic', 'placer', '318Dynamic', 'Programming', '456dynamic', 'unrolling', 'time', '387dynamic_rnn', '387', '398', '409Eearly', 'stopping', '133-134', '198', '272', '303Elastic', 'Net', '132embedded', 'device', 'blocks', '327Embedded', 'Reber', 'grammars', '410embeddings', '405-407embedding_lookup', '406encoder', '412Encoder–Decoder', '383end-of-sequence', 'EOS', 'token', '388energy', 'functions', '516enqueuing', 'data', '330Ensemble', 'Learning', '70', '74', '181-203bagging', 'pasting', '185-188boosting', '191-200in-graph', 'versus', 'between-graph', 'replication', '343-345Random', 'Forests', '189-191528', '|', 'Index', 'see', 'also', 'Random', 'Forests', 'random', 'patches', 'random', 'subspaces', '188stacking', '200-202entropy', 'impurity', 'measure', '172environments', 'reinforcement', 'learning', '438-447', '459', '464episodes', 'RL', '444', '448-449', '451-452', '469epochs', '118Ò-insensitive', '155equality', 'contraints', '504error', 'analysis', '96-99estimators', '61Euclidian', 'norm', '39eval', '240evaluating', 'models', '29-31explained', 'variance', '215explained', 'variance', 'ratio', '214exploding', 'gradients', '276', 'see', 'also', 'gradients', 'vanishing', 'explod…', 'ing', 'exploration', 'policies', '459exponential', 'decay', '284exponential', 'linear', 'unit', 'ELU', '280-281exponential', 'scheduling', '301Extra-Trees', '190FF-1', 'score', '86-87face-recognition', '100fake', 'X', 'server', '443false', 'positive', 'rate', 'FPR', '91-93fan-in', '277', '279fan-out', '277', '279feature', 'detection', '411feature', 'engineering', '25feature', 'extraction', '12feature', 'importance', '190-191feature', 'maps', '220', '357-360', '374feature', 'scaling', '65feature', 'selection', '26', '74', '130', '191', '499feature', 'space', '218', '220feature', 'vector', '39', '107', '156', '237features', '9FeatureUnion', '66feedforward', 'neural', 'network', 'FNN', '263feed_dict', '240FIFOQueue', '330', '333fillna', '60first-in', 'first-out', 'FIFO', 'queues', '330first-order', 'partial', 'derivatives', 'Jacobians', '300fit', '61', '66', '217fitness', 'function', '20fit_inverse_transform=', '221fit_transform', '61', '66folds', '69', '81', '83-84Follow', 'The', 'Regularized', 'Leader', 'FTRL', '300forget', 'gate', '402forward-mode', 'autodiff', '510-512framing', 'problem', '35-37frozen', 'layers', '289-290fully_connected', '267', '278', '284-285', '417Ggame', 'play', 'see', 'reinforcement', 'learning', 'gamma', 'value', '152gate', 'controllers', '402Gaussian', 'distribution', '37', '429', '431Gaussian', 'RBF', '151Gaussian', 'RBF', 'kernel', '152-153', '163generalization', 'error', '29generalized', 'Lagrangian', '504-505generative', 'autoencoders', '428generative', 'models', '411', '518genetic', 'algorithms', '440geodesic', 'distance', '224get_variable', '249-250GINI', 'impurity', '169', '172global', 'average', 'pooling', '372global_step', '466global_variables', '308global_variables_initializer', '233Glorot', 'initialization', '276-279Google', '230Google', 'Images', '253Google', 'Photos', '13GoogleNet', 'architecture', '368-372gpu_options.per_process_gpu_memory_frac…', 'tion', '317gradient', 'ascent', '441Gradient', 'Boosted', 'Regression', 'Trees', 'GBRT', '195Gradient', 'Boosting', '195-200Gradient', 'Descent', 'GD', '105', '111-121', '164', '275,294', '296algorithm', 'comparisons', '119-121automatically', 'computing', 'gradients', '238-239Batch', 'GD', '114-117', '130defining', '111Index', '|', '529', 'local', 'minimum', 'versus', 'global', 'minimum', '112manually', 'computing', 'gradients', '237Mini-batch', 'GD', '119-121', '239-241optimizer', '239Stochastic', 'GD', '117-119', '148with', 'TensorFlow', '237-239Gradient', 'Tree', 'Boosting', '195GradientDescentOptimizer', '268gradients', '238gradients', 'vanishing', 'exploding', '275-286,400Batch', 'Normalization', '282-286Glorot', 'He', 'initialization', '276-279gradient', 'clipping', '286nonsaturating', 'activation', 'functions', '279-281graphviz', '168greedy', 'algorithm', '172grid', 'search', '71-74', '151group', '464GRU', 'Gated', 'Recurrent', 'Unit', 'cell', '404-405Hhailstone', 'sequence', '412hard', 'margin', 'classification', '146-147hard', 'voting', 'classifiers', '181-184harmonic', 'mean', '86He', 'initialization', '276-279Heaviside', 'step', 'function', '257HebbØs', 'rule', '258', '516Hebbian', 'learning', '259hidden', 'layers', '261hierarchical', 'clustering', '10hinge', 'loss', 'function', '164histograms', '47-48hold-out', 'sets', '200', 'see', 'also', 'blenders', 'Hopfield', 'Networks', '515-516hyperbolic', 'tangent', 'htan', 'activation', 'function', '262', '272', '276', '278', '381hyperparameters', '28', '65', '72-74', '76', '111', '151,154', '270', 'see', 'also', 'neural', 'network', 'hyperparameters', 'hyperplane', '157', '210-211', '213', '224hypothesis', '39manifold', '210hypothesis', 'boosting', 'see', 'boosting', 'hypothesis', 'function', '107hypothesis', 'null', '174Iidentity', 'matrix', '128', '160ILSVRC', 'ImageNet', 'challenge', '365image', 'classification', '365impurity', 'measures', '169', '172in-graph', 'replication', '343inception', 'modules', '369Inception-v4', '375incremental', 'learning', '16', '217inequality', 'constraints', '504inference', '22', '311', '363', '408info', '45information', 'gain', '173information', 'theory', '172init', 'node', '241input', 'gate', '402input', 'neurons', '258input_put_keep_prob', '399instance-based', 'learning', '17', '21InteractiveSession', '233intercept', 'term', '106Internal', 'Covariate', 'Shift', 'problem', '282inter_op_parallelism_threads', '322intra_op_parallelism_threads', '322inverse_transform', '221in_top_k', '268irreducible', 'error', '127isolated', 'environment', '41-42Isomap', '224is_training', '284-285', '399Jjobs', '323join', '325', '339Jupyter', '40', '42', '48KK-fold', 'cross-validation', '69-71', '83k-Nearest', 'Neighbors', '21', '100Karush–Kuhn–Tucker', 'KKT', 'conditions', '504keep', 'probability', '306Keras', '231Kernel', 'PCA', 'kPCA', '218-221kernel', 'trick', '150', '152', '161-164', '218kernelized', 'SVM', '161-164kernels', '150-153', '321Kullback–Leibler', 'divergence', '141', '426530', '|', 'Index', 'Ll1_l2_regularizer', '303LabelBinarizer', '66labels', '8', '37Lagrange', 'function', '504-505Lagrange', 'multiplier', '503landmarks', '151-152large', 'margin', 'classification', '145-146Lasso', 'Regression', '130-132latent', 'loss', '430latent', 'space', '429law', 'large', 'numbers', '183leaky', 'ReLU', '279learning', 'rate', '16', '111', '115-118learning', 'rate', 'scheduling', '118', '300-302LeNet-5', 'architecture', '355', '366-367Levenshtein', 'distance', '153liblinear', 'library', '153libsvm', 'library', '154Linear', 'Discriminant', 'Analysis', 'LDA', '224linear', 'modelsearly', 'stopping', '133-134Elastic', 'Net', '132Lasso', 'Regression', '130-132Linear', 'Regression', 'see', 'Linear', 'Regression', 'regression', 'see', 'Linear', 'Regression', 'Ridge', 'Regression', '127-129', '132SVM', '145-148Linear', 'Regression', '20', '68', '105-121', '132computational', 'complexity', '110Gradient', 'Descent', '111-121learning', 'curves', '123-127Normal', 'Equation', '108-110regularizing', 'models', 'see', 'regularization', 'using', 'Stochastic', 'Gradient', 'Descent', 'SGD', '119with', 'TensorFlow', '235-236linear', 'SVM', 'classification', '145-148linear', 'threshold', 'units', 'LTUs', '257Lipschitz', 'continuous', '113LLE', 'Locally', 'Linear', 'Embedding', '221-223load_sample_images', '360local', 'receptive', 'field', '354local', 'response', 'normalization', '368local', 'sessions', '328location', 'invariance', '363log', 'loss', '136logging', 'placements', '320-320logistic', 'function', '134Logistic', 'Regression', '9', '134-142decision', 'boundaries', '136-139estimating', 'probablities', '134-135Softmax', 'Regression', 'model', '139-142training', 'cost', 'function', '135-136log_device_placement', '320LSTM', 'Long', 'Short-Term', 'Memory', 'cell', '401-405Mmachine', 'control', 'see', 'reinforcement', 'learning', 'Machine', 'Learning', 'large-scale', 'projects', 'see', 'TensorFlow', 'notations', '38-39process', 'example', '33-77project', 'checklist', '35', '497-502resources', 'xvi-xviiuses', 'xiii-xivMachine', 'Learning', 'basics', 'attributes', '9challenges', '22-29algorithm', 'problems', '26-28training', 'data', 'problems', '25definition', '4features', '9overview', '3reasons', 'using', '4-7spam', 'filter', 'example', '4-6summary', '28testing', 'validating', '29-31types', 'systems', '7-22batch', 'online', 'learning', '14-17instance-based', 'versus', 'model-basedlearning', '17-22supervised/unsupervised', 'learning', '8-14workflow', 'example', '18-22machine', 'translation', 'see', 'natural', 'language', 'pro…', 'cessing', 'NLP', 'make', '442Manhattan', 'norm', '39manifold', 'assumption/hypothesis', '210Manifold', 'Learning', '210', '221', 'see', 'also', 'LLE', 'Locally', 'Linear', 'Embedding', 'MapReduce', '37margin', 'violations', '147Markov', 'chains', '453Markov', 'decision', 'processes', '453-457master', 'service', '325Matplotlib', '40', '48', '91', '97Index', '|', '531', 'max', 'margin', 'learning', '293max', 'pooling', 'layer', '363max-norm', 'regularization', '307-308max_norm', '308max_norm_regularizer', '308max_pool', '364Mean', 'Absolute', 'Error', 'MAE', '39-40mean', 'coding', '429Mean', 'Square', 'Error', 'MSE', '107', '237', '426measure', 'similarity', '17memmap', '217memory', 'cells', '346', '382MercerØs', 'theorem', '163meta', 'learner', 'see', 'blending', 'min-max', 'scaling', '65Mini-batch', 'Gradient', 'Descent', '119-121', '136,239-241mini-batches', '15minimize', '286', '289', '449', '466min_after_dequeue', '333MNIST', 'dataset', '79-81model', 'parallelism', '345-347model', 'parameters', '114', '116', '133', '156', '159', '234,268', '389defining', '19model', 'selection', '19model', 'zoos', '291model-based', 'learning', '18-22modelsanalyzing', '74-75evaluating', 'test', 'set', '75-76moments', '298Momentum', 'optimization', '294-295Monte', 'Carlo', 'tree', 'search', '453Multi-Layer', 'Perceptrons', 'MLP', '253', '260-263,446training', 'TF.Learn', '264multiclass', 'classifiers', '93-96Multidimensional', 'Scaling', 'MDS', '223multilabel', 'classifiers', '100-101Multinomial', 'Logistic', 'Regression', 'see', 'Softmax', 'Regression', 'multinomial', '446multioutput', 'classifiers', '101-102MultiRNNCell', '398multithreaded', 'readers', '338-340multivariate', 'regression', '37Nnaive', 'Bayes', 'classifiers', '94name', 'scopes', '245natural', 'language', 'processing', 'NLP', '379,405-410encoder-decoder', 'network', 'machine', 'translation', '407-410TensorFlow', 'tutorials', '405', '408word', 'embeddings', '405-407Nesterov', 'Accelerated', 'Gradient', 'NAG', '295-296Nesterov', 'momentum', 'optimization', '295-296network', 'topology', '270neural', 'network', 'hyperparameters', '270-272activation', 'functions', '272neurons', 'per', 'hidden', 'layer', '272number', 'hidden', 'layers', '270-271neural', 'network', 'policies', '444-447neuronsbiological', '254-256logical', 'computations', '256neuron_layer', '267next_batch', '269No', 'Free', 'Lunch', 'theorem', '30node', 'edges', '244nonlinear', 'dimensionality', 'reduction', 'NLDR', ',221', 'see', 'also', 'Kernel', 'PCA', 'LLE', 'Locally', 'Linear', 'Embedding', 'nonlinear', 'SVM', 'classification', '149-154computational', 'complexity', '153Gaussian', 'RBF', 'kernel', '152-153with', 'polynomial', 'features', '149-150polynomial', 'kernel', '150-151similarity', 'features', 'adding', '151-152nonparametric', 'models', '173nonresponse', 'bias', '25nonsaturating', 'activation', 'functions', '279-281normal', 'distribution', 'see', 'Gaussian', 'distribution', 'Normal', 'Equation', '108-110normalization', '65normalized', 'exponential', '139norms', '39notations', '38-39NP-Complete', 'problems', '172null', 'hypothesis', '174numerical', 'differentiation', '509NumPy', '40NumPy', 'arrays', '63NVidia', 'Compute', 'Capability', '314532', '|', 'Index', 'nvidia-smi', '318n_components', '215Oobservation', 'space', '446off-policy', 'algorithm', '459offline', 'learning', '14one-hot', 'encoding', '63one-versus-all', 'OvA', 'strategy', '94', '141', '165one-versus-one', 'OvO', 'strategy', '94online', 'learning', '15-17online', 'SVMs', '164-165OpenAI', 'Gym', '441-444operation_timeout_in_ms', '345Optical', 'Character', 'Recognition', 'OCR', '3optimal', 'state', 'value', '455optimizers', '293-302AdaGrad', '296-298Adam', 'optimization', '293', '298-300Gradient', 'Descent', 'see', 'Gradient', 'Descent', 'optimizer', 'learning', 'rate', 'scheduling', '300-302Momentum', 'optimization', '294-295Nesterov', 'Accelerated', 'Gradient', 'NAG', '295-296RMSProp', '298out-of-bag', 'evaluation', '187-188out-of-core', 'learning', '16out-of-memory', 'OOM', 'errors', '386out-of-sample', 'error', '29OutOfRangeError', '337', '339output', 'gate', '402output', 'layer', '261OutputProjectionWrapper', '392-395output_put_keep_prob', '399overcomplete', 'autoencoder', '424overfitting', '26-28', '49', '147', '152', '173', '176', '272avoiding', 'regularization', '302-310Pp-value', '174PaddingFIFOQueue', '334Pandas', '40', '44scatter_matrix', '56-57parallel', 'distributed', 'computing', '313-352data', 'parallelism', '347-351in-graph', 'versus', 'between-graph', 'replication', '343-345model', 'parallelism', '345-347multiple', 'devices', 'across', 'multiple', 'servers', '323-342asynchronous', 'communication', 'using', 'queues', '329-334loading', 'training', 'data', '335-342master', 'worker', 'services', '325opening', 'session', '325pinning', 'operations', 'across', 'tasks', '326sharding', 'variables', '327sharing', 'state', 'across', 'sessions', '328-329multiple', 'devices', 'single', 'machine', '314-323control', 'dependencies', '323installation', '314-316managing', 'GPU', 'RAM', '317-318parallel', 'execution', '321-322placing', 'operations', 'devices', '318-321one', 'neural', 'network', 'per', 'device', '342-343parameter', 'efficiency', '271parameter', 'matrix', '139parameter', 'server', 'ps', '324parameter', 'space', '114parameter', 'vector', '107', '111', '135', '139parametric', 'models', '173partial', 'derivative', '114partial_fit', '217PearsonØs', 'r', '55peephole', 'connections', '403penalties', 'see', 'rewards', 'RL', 'percentiles', '46Perceptron', 'convergence', 'theorem', '259Perceptrons', '257-264versus', 'Logistic', 'Regression', '260training', '258-259performance', 'measures', '37-40confusion', 'matrix', '84-86cross-validation', '83-84precision', 'recall', '86-90ROC', 'receiver', 'operating', 'characteristic', 'curve', '91-93performance', 'scheduling', '301permutation', '49PG', 'algorithms', '448photo-hosting', 'services', '13pinning', 'operations', '326pip', '41Pipeline', 'constructor', '66-68pipelines', '36placeholder', 'nodes', '239Index', '|', '533', 'placers', 'see', 'simple', 'placer', 'dynamic', 'placer', 'policy', '440policy', 'gradients', '441', 'see', 'PG', 'algorithms', 'policy', 'space', '440polynomial', 'features', 'adding', '149-150polynomial', 'kernel', '150-151', '162Polynomial', 'Regression', '106', '121-123learning', 'curves', '123-127pooling', 'kernel', '363pooling', 'layer', '363-365power', 'scheduling', '301precision', '85precision', 'recall', '86-90F-1', 'score', '86-87precision/recall', 'PR', 'curve', '92precision/recall', 'tradeoff', '87-90predetermined', 'piecewise', 'constant', 'learning', 'rate', '301predict', '62predicted', 'class', '85predictions', '84-86', '156-157', '169-171predictors', '8', '62preloading', 'training', 'data', '335PReLU', 'parametric', 'leaky', 'ReLU', '279preprocessed', 'attributes', '48pretrained', 'layers', 'reuse', '286-293auxiliary', 'task', '292-293caching', 'frozen', 'layers', '290freezing', 'lower', 'layers', '289model', 'zoos', '291other', 'frameworks', '288TensorFlow', 'model', '287-288unsupervised', 'pretraining', '291-292upper', 'layers', '290Pretty', 'Tensor', '231primal', 'problem', '160principal', 'component', '212Principal', 'Component', 'Analysis', 'PCA', '211-218explained', 'variance', 'ratios', '214finding', 'principal', 'components', '212-213for', 'compression', '216-217Incremental', 'PCA', '217-218Kernel', 'PCA', 'kPCA', '218-221projecting', 'dimensions', '213Randomized', 'PCA', '218Scikit', 'Learn', '214variance', 'preserving', '211-212probabilistic', 'autoencoders', '428probabilities', 'estimating', '134-135', '171producer', 'functions', '341projection', '207-209propositional', 'logic', '254pruning', '174', '509Pythonisolated', 'environment', '41-42notebooks', '42-43pickle', '71pip', '41QQ-Learning', 'algorithm', '458-469approximate', 'Q-Learning', '460deep', 'Q-Learning', '460-469Q-Value', 'Iteration', 'Algorithm', '456Q-Values', '456Quadratic', 'Programming', 'QP', 'Problems', '159-160quantizing', '351queries', 'per', 'second', 'QPS', '343QueueRunner', '338-340queues', '329-334closing', '333dequeuing', 'data', '331enqueuing', 'data', '330first-in', 'first-out', 'FIFO', '330of', 'tuples', '332PaddingFIFOQueue', '334RandomShuffleQueue', '333q_network', '463RRadial', 'Basis', 'Function', 'RBF', '151Random', 'Forests', '70-72', '94', '167', '178', '181,189-191Extra-Trees', '190feature', 'importance', '190-191random', 'initialization', '111', '116', '118', '276Random', 'Patches', 'Random', 'Subspaces', '188randomized', 'leaky', 'ReLU', 'RReLU', '279Randomized', 'PCA', '218randomized', 'search', '74', '270RandomShuffleQueue', '333', '337random_uniform', '237reader', 'operations', '335recall', '85recognition', 'network', '412reconstruction', 'error', '216reconstruction', 'loss', '413', '428', '430534', '|', 'Index', 'reconstruction', 'pre-image', '220reconstructions', '413recurrent', 'neural', 'networks', 'RNNs', '379-410deep', 'RNNs', '396-400exploration', 'policies', '459GRU', 'cell', '404-405input', 'output', 'sequences', '382-383LSTM', 'cell', '401-405natural', 'language', 'processing', 'NLP', '405-410in', 'TensorFlow', '384-388dynamic', 'unrolling', 'time', '387static', 'unrolling', 'time', '385-386variable', 'length', 'input', 'sequences', '387variable', 'length', 'output', 'sequences', '388training', '389-396backpropagation', 'time', 'BPTT', '389creative', 'sequences', '396sequence', 'classifiers', '389-391time', 'series', 'predictions', '392-396recurrent', 'neurons', '380-383memory', 'cells', '382reduce_mean', '268reduce_sum', '427-428', '430', '466regression', '8Decision', 'Trees', '175-176regression', 'modelslinear', '68regression', 'versus', 'classification', '101regularization', '27-28', '30', '127-134data', 'augmentation', '309-310Decision', 'Trees', '173-174dropout', '304-307early', 'stopping', '133-134', '303Elastic', 'Net', '132Lasso', 'Regression', '130-132max-norm', '307-308Ridge', 'Regression', '127-129shrinkage', '197—', '1', '—', '2', 'regularization', '303-304REINFORCE', 'algorithms', '448Reinforcement', 'Learning', 'RL', '13-14', '437-470actions', '447-448credit', 'assignment', 'problem', '447-448discount', 'rate', '447examples', '438Markov', 'decision', 'processes', '453-457neural', 'network', 'policies', '444-447OpenAI', 'gym', '441-444PG', 'algorithms', '448-453policy', 'search', '440-441Q-Learning', 'algorithm', '458-469rewards', 'learning', 'optimize', '438-439Temporal', 'Difference', 'TD', 'Learning', '457-458ReLU', 'rectified', 'linear', 'units', '246-248ReLU', 'activation', '374ReLU', 'function', '262', '272', '278-281relu', 'z', '266render', '442replay', 'memory', '464replica_device_setter', '327request_stop', '339reset', '442reset_default_graph', '234reshape', '395residual', 'errors', '195-196residual', 'learning', '372residual', 'network', 'ResNet', '291', '372-375residual', 'units', '373ResNet', '372-375resource', 'containers', '328-329restore', '241restricted', 'Boltzmann', 'machines', 'RBMs', '13,291', '518reuse_variables', '249reverse-mode', 'autodiff', '512-513rewards', 'RL', '438-439rgb_array', '443Ridge', 'Regression', '127-129', '132RMSProp', '298ROC', 'receiver', 'operating', 'characteristic', 'curve', '91-93Root', 'Mean', 'Square', 'Error', 'RMSE', '37-40', '107RReLU', 'randomized', 'leaky', 'ReLU', '279run', '233', '345SSampled', 'Softmax', '409sampling', 'bias', '24-25', '51sampling', 'noise', '24save', '241Saver', 'node', '241Scikit', 'Flow', '231Scikit-Learn', '40about', 'xivbagging', 'pasting', '186-187CART', 'algorithm', '170-171', '176Index', '|', '535', 'cross-validation', '69-71design', 'principles', '61-62imputer', '60-62LinearSVR', 'class', '156MinMaxScaler', '65min_', 'max_', 'hyperparameters', '173PCA', 'implementation', '214Perceptron', 'class', '259Pipeline', 'constructor', '66-68', '149Randomized', 'PCA', '218Ridge', 'Regression', '129SAMME', '195SGDClassifier', '82', '87-88', '94SGDRegressor', '119sklearn.base.BaseEstimator', '64', '67', '84sklearn.base.clone', '83', '133sklearn.base.TransformerMixin', '64', '67sklearn.datasets.fetch_california_housing', '236sklearn.datasets.fetch_mldata', '79sklearn.datasets.load_iris', '137', '148', '167,190', '259sklearn.datasets.load_sample_images', '360-361sklearn.datasets.make_moons', '149', '178sklearn.decomposition.IncrementalPCA', '217sklearn.decomposition.KernelPCA', '218-219', '221sklearn.decomposition.PCA', '214sklearn.ensemble.AdaBoostClassifier', '195sklearn.ensemble.BaggingClassifier', '186-189sklearn.ensemble.GradientBoostingRegres…', 'sor', '196', '198-199sklearn.ensemble.RandomForestClassifier', '92', '95', '184sklearn.ensemble.RandomForestRegressor', '70', '72-74', '189-190', '196sklearn.ensemble.VotingClassifier', '184sklearn.externals.joblib', '71sklearn.linear_model.ElasticNet', '132sklearn.linear_model.Lasso', '132sklearn.linear_model.LinearRegression,20-21', '62', '68', '110', '120', '122', '124-125sklearn.linear_model.LogisticRegression,137', '139', '141', '184', '219sklearn.linear_model.Perceptron', '259sklearn.linear_model.Ridge', '129sklearn.linear_model.SGDClassifier', '82sklearn.linear_model.SGDRegressor,119-120', '129', '132-133sklearn.manifold.LocallyLinearEmbedding,221-222sklearn.metrics.accuracy_score', '184', '188,264sklearn.metrics.confusion_matrix', '85', '96sklearn.metrics.f1_score', '87', '100sklearn.metrics.mean_squared_error', ',68-69', '76', '124', '133', '198-199', '221sklearn.metrics.precision_recall_curve', '88sklearn.metrics.precision_score', '86', '90sklearn.metrics.recall_score', '86', '90sklearn.metrics.roc_auc_score', '92-93sklearn.metrics.roc_curve', '91-92sklearn.model_selection.cross_val_pre…dict', '84', '88', '92', '96', '100sklearn.model_selection.cross_val_score', ',69-70', '83-84sklearn.model_selection.GridSearchCV,72-74', '77', '96', '179', '219sklearn.model_selection.StratifiedKFold', '83sklearn.model_selection.StratifiedShuffleS…', 'plit', '52sklearn.model_selection.train_test_split', ',50', '69', '124', '178', '198sklearn.multiclass.OneVsOneClassifier', '95sklearn.neighbors.KNeighborsClassifier', '100', '102sklearn.neighbors.KNeighborsRegressor', '22sklearn.pipeline.FeatureUnion', '66sklearn.pipeline.Pipeline', '66', '125', '148-149,219sklearn.preprocessing.Imputer', '60', '66sklearn.preprocessing.LabelBinarizer', '64', '66sklearn.preprocessing.LabelEncoder', '62sklearn.preprocessing.OneHotEncoder', '63sklearn.preprocessing.PolynomialFeatures', '122-123', '125', '128', '149sklearn.preprocessing.StandardScaler,65-66', '96', '114', '128', '146', '148-150', '152,237', '264sklearn.svm.LinearSVC', '147-149', '153-154,156', '165sklearn.svm.LinearSVR', '155-156sklearn.svm.SVC', '148', '150', '152-154', '156,165', '184sklearn.svm.SVR', '77', '156536', '|', 'Index', 'sklearn.tree.DecisionTreeClassifier', '173,179', '186-187', '189', '195sklearn.tree.DecisionTreeRegressor', '69', '167,175', '195-196sklearn.tree.export_graphviz', '168StandardScaler', '114', '237', '264SVM', 'classification', 'classes', '154TF.Learn', '231user', 'guide', 'xviscore', '62search', 'space', '74', '270second-order', 'partial', 'derivatives', 'Hessians', '300self-organizing', 'maps', 'SOMs', '521-523semantic', 'hashing', '434semisupervised', 'learning', '13sensitivity', '85', '91sentiment', 'analysis', '379separable_conv2d', '376sequences', '379sequence_length', '387-388', '409ShannonØs', 'information', 'theory', '172shortcut', 'connections', '372show', '48show_graph', '245shrinkage', '197shuffle_batch', '341shuffle_batch_join', '341sigmoid', 'function', '134sigmoid_cross_entropy_with_logits', '428similarity', 'function', '151-152simulated', 'annealing', '118simulated', 'environments', '442', 'see', 'also', 'OpenAI', 'Gym', 'Singular', 'Value', 'Decomposition', 'SVD', '213skewed', 'datasets', '84skip', 'connections', '310', '372slack', 'variable', '158smoothing', 'terms', '283', '297', '299', '430soft', 'margin', 'classification', '146-148soft', 'placements', '321soft', 'voting', '184softmax', 'function', '139', '263', '264Softmax', 'Regression', '139-142source', 'ops', '236', '322spam', 'filters', '3-6', '8sparse', 'autoencoders', '426-428sparse', 'matrix', '63sparse', 'models', '130', '300sparse_softmax_cross_entropy_with_logits', '268sparsity', 'loss', '426specificity', '91speech', 'recognition', '6spurious', 'patterns', '516stack', '385stacked', 'autoencoders', '415-424TensorFlow', 'implementation', '416training', 'one-at-a-time', '418-420tying', 'weights', '417-418unsupervised', 'pretraining', '422-424visualizing', 'reconstructions', '420-421stacked', 'denoising', 'autoencoders', '422', '424stacked', 'denoising', 'encoders', '424stacked', 'generalization', 'see', 'stacking', 'stacking', '200-202stale', 'gradients', '348standard', 'correlation', 'coefficient', '55standard', 'deviation', '37standardization', '65StandardScaler', '66', '237', '264state-action', 'values', '456states', 'tensor', '388state_is_tuple', '398', '401static', 'unrolling', 'time', '385-386static_rnn', '385-386', '409stationary', 'point', '503-505statistical', 'mode', '185statistical', 'significance', '174stemming', '103step', 'functions', '257step', '443Stochastic', 'Gradient', 'Boosting', '199Stochastic', 'Gradient', 'Descent', 'SGD', '117-119,148', '260training', '136Stochastic', 'Gradient', 'Descent', 'SGD', 'classifier', '82', '129stochastic', 'neurons', '516stochastic', 'policy', '440stratified', 'sampling', '51-53', '83stride', '357string', 'kernels', '153string_input_producer', '341strong', 'learners', '182subderivatives', '164subgradient', 'vector', '131subsample', '199', '363Index', '|', '537', 'supervised', 'learning', '8-9Support', 'Vector', 'Machines', 'SVMs', '94', '145-166decision', 'function', 'predictions', '156-157dual', 'problem', '503-505kernelized', 'SVM', '161-164linear', 'classification', '145-148mechanics', '156-165nonlinear', 'classification', '149-154online', 'SVMs', '164-165Quadratic', 'Programming', 'QP', 'problems', '159-160SVM', 'regression', '154-165the', 'dual', 'problem', '160training', 'objective', '157-159support', 'vectors', '146svd', '213symbolic', 'differentiation', '238', '508-509synchronous', 'updates', '348Tt-Distributed', 'Stochastic', 'Neighbor', 'Embedding', 't-SNE', '224tail', 'heavy', '48target', 'attributes', '48target_weights', '409tasks', '323Temporal', 'Difference', 'TD', 'Learning', '457-458tensor', 'processing', 'units', 'TPUs', '315TensorBoard', '231TensorFlow', '229-252about', 'xivautodiff', '238-239', '507-513Batch', 'Normalization', '284-286construction', 'phase', '234control', 'dependencies', '323convenience', 'functions', '341convolutional', 'layers', '376convolutional', 'neural', 'networks', '360-362data', 'parallelism', '351denoising', 'autoencoders', '425-425dropout', '306dynamic', 'placer', '318execution', 'phase', '234feeding', 'data', 'training', 'algorithm', '239-241Gradient', 'Descent', '237-239graphs', 'managing', '234initial', 'graph', 'creation', 'session', 'run', '232-234installation', '232l1', 'l2', 'regularization', '303learning', 'schedules', '302Linear', 'Regression', '235-236max', 'pooling', 'layer', '364max-norm', 'regularization', '307model', 'zoo', '291modularity', '246-248Momentum', 'optimization', '295name', 'scopes', '245neural', 'network', 'policies', '446NLP', 'tutorials', '405', '408node', 'value', 'lifecycle', '235operations', 'ops', '235optimizer', '239overview', '229-231parallel', 'distributed', 'computing', 'see', 'parallel', 'distributed', 'computing', 'TensorFlow', 'Python', 'APIconstruction', '265-269execution', '269using', 'neural', 'network', '270queues', 'see', 'queues', 'reusing', 'pretrained', 'layers', '287-288RNNs', '384-388', 'see', 'also', 'recurrent', 'neural', 'networks', 'RNNs', 'saving', 'restoring', 'models', '241-242sharing', 'variables', '248-251simple', 'placer', '318sklearn.metrics.accuracy_score', '286sparse', 'autoencoders', '427and', 'stacked', 'autoencoders', '416TensorBoard', '242-245tf.abs', '303tf.add', '246', '303-304tf.add_n', '247-248', '250-251tf.add_to_collection', '308tf.assign', '237', '288', '307-308', '482tf.bfloat16', '350tf.bool', '284', '306tf.cast', '268', '391tf.clip_by_norm', '307-308tf.clip_by_value', '286tf.concat', '312', '369', '446', '450tf.ConfigProto', '317', '320-321', '345', '487tf.constant', '235-237', '319-320', '323,325-326tf.constant_initializer', '249-251538', '|', 'Index', 'tf.container', '328-330', '351-352', '481tf.contrib.framework.arg_scope', '285', '416,430tf.contrib.layers.batch_norm', '284-285tf.contrib.layers.convolution2d', '463tf.contrib.layers.fully_connected', '267tf.contrib.layers.l1_regularizer', '303', '308tf.contrib.layers.l2_regularizer', '303,416-417tf.contrib.layers.variance_scaling_initial…', 'izer', '278-279', '391', '416-417', '430', '446,450', '463tf.contrib.learn.DNNClassifier', '264tf.contrib.learn.infer_real_valued_col…', 'umns_from_input', '264tf.contrib.rnn.BasicLSTMCell', '401', '403tf.contrib.rnn.BasicRNNCell', '385-387', '390,392-393', '395', '397-399', '401tf.contrib.rnn.DropoutWrapper', '399tf.contrib.rnn.GRUCell', '405tf.contrib.rnn.LSTMCell', '403tf.contrib.rnn.MultiRNNCell', '397-399tf.contrib.rnn.OutputProjectionWrapper', '392-394tf.contrib.rnn.RNNCell', '398tf.contrib.rnn.static_rnn', '385-387,409-410', '491-492tf.contrib.slim', 'module', '231', '377tf.contrib.slim.nets', 'module', 'nets', '377tf.control_dependencies', '323tf.decode_csv', '336', '340tf.device', '319-321', '326-327', '397-398tf.exp', '430-431tf.FIFOQueue', '330', '332-333', '336', '340tf.float32', '236', '482tf.get_collection', '288-289', '304', '308', '416,463tf.get_default_graph', '234', '242tf.get_default_session', '233tf.get_variable', '249-251', '288', '303-308tf.global_variables', '308tf.global_variables_initializer', '233', '237tf.gradients', '238tf.Graph', '232', '234', '242', '335', '343tf.GraphKeys.REGULARIZATION_LOS…', 'SES', '304', '416tf.GraphKeys.TRAINABLE_VARIABLES', '288-289', '463tf.group', '464tf.int32', '321-332', '337', '387', '390', '406', '466tf.int64', '265tf.InteractiveSession', '233TF.Learn', '264tf.log', '427', '430', '446', '450tf.matmul', '236-237', '246', '265', '384', '417,420', '425', '427-428tf.matrix_inverse', '236tf.maximum', '246', '248-251', '281tf.multinomial', '446', '450tf.name_scope', '245', '248-249', '265,267-268', '419-420tf.nn.conv2d', '360-361tf.nn.dynamic_rnn', '386-387', '390', '392,395', '397-399', '409-410', '491-492tf.nn.elu', '281', '416-417', '430', '446', '450tf.nn.embedding_lookup', '406tf.nn.in_top_k', '268', '391tf.nn.max_pool', '364-365tf.nn.relu', '265', '392-393', '395', '463tf.nn.sigmoid_cross_entropy_with_logits', '428', '431', '449-450tf.nn.sparse_soft…max_cross_entropy_with_logits', '267-268', '390tf.one_hot', '466tf.PaddingFIFOQueue', '334tf.placeholder', '239-240', '482tf.placeholder_with_default', '425tf.RandomShuffleQueue', '333', '337-338,340-341tf.random_normal', '246', '384', '425', '430tf.random_uniform', '237', '241', '406', '482tf.reduce_mean', '237', '245', '267-268', '303,390-391', '414', '416', '418', '420', '425', '427,466tf.reduce_sum', '303', '427-428', '430-431,465-466tf.reset_default_graph', '234tf.reshape', '395', '463tf.RunOptions', '345tf.Session', '233', '482tf.shape', '425', '430tf.square', '237', '245', '393', '414', '416', '418', '420,425', '427', '430-431', '466tf.stack', '336', '340', '386tf.string', '336', '340tf.summary.FileWriter', '242-243tf.summary.scalar', '242Index', '|', '539', 'tf.tanh', '384tf.TextLineReader', '336', '340tf.to_float', '449-450tf.train.AdamOptimizer', '293', '299', '390', '393,414', '416-417', '419', '427', '431', '449-450', '466tf.train.ClusterSpec', '324tf.train.Coordinator', '338-340tf.train.exponential_decay', '302tf.train.GradientDescentOptimizer', '239,268', '286', '293', '295tf.train.MomentumOptimizer', '239', '295-296,302', '311', '351', '485-486tf.train.QueueRunner', '338-341tf.train.replica_device_setter', '327-328tf.train.RMSPropOptimizer', '298tf.train.Saver', '241-242', '268', '377', '399', '450,466tf.train.Server', '324tf.train.start_queue_runners', '341tf.transpose', '236-237', '386', '417tf.truncated_normal', '265tf.unstack', '385-387', '395', '492tf.Variable', '232', '482tf.variable_scope', '249-251', '288', '307-308,328', '391', '463tf.zeros', '265', '384', '417truncated', 'backpropagation', 'time', '400visualizing', 'graph', 'training', 'curves', '242-245TensorFlow', 'Serving', '343tensorflow.contrib', '267test', 'set', '29', '49-53', '81testing', 'validating', '29-31text', 'attributes', '62-64TextLineReader', '336TF-slim', '231TF.Learn', '231', '264thermal', 'equilibrium', '518thread', 'pools', 'inter-op/intra-op', 'TensorFlow', '322threshold', 'variable', '248-251Tikhonov', 'regularization', '127time', 'series', 'data', '379toarray', '63tolerance', 'hyperparameter', '154trainable', '288training', 'data', '4insufficient', 'quantities', '22irrelevant', 'features', '25loading', '335-342nonrepresentative', '24overfitting', '26-28poor', 'quality', '25underfitting', '28training', 'instance', '4training', 'models', '20', '105-143learning', 'curves', '123-127Linear', 'Regression', '105', '106-121Logistic', 'Regression', '134-142overview', '105-106Polynomial', 'Regression', '106', '121-123training', 'objectives', '157-159training', 'set', '4', '29', '53', '60', '68-69cost', 'function', '135-136shuffling', '81transfer', 'learning', '286-293', 'see', 'also', 'pretrained', 'layers', 'reuse', 'transform', '61', '66transformation', 'pipelines', '66-68transformers', '61transformers', 'custom', '64-65transpose', '385true', 'negative', 'rate', 'TNR', '91true', 'positive', 'rate', 'TPR', '85', '91truncated', 'backpropagation', 'time', '400tuples', '332tying', 'weights', '417Uunderfitting', '28', '68', '152univariate', 'regression', '37unstack', '385unsupervised', 'learning', '10-12anomaly', 'detection', '12association', 'rule', 'learning', '10', '12clustering', '10dimensionality', 'reduction', 'algorithm', '12visualization', 'algorithms', '11unsupervised', 'pretraining', '291-292', '422-424upsampling', '376utility', 'function', '20Vvalidation', 'set', '30Value', 'Iteration', '455value_counts', '46vanishing', 'gradients', '276540', '|', 'Index', 'see', 'also', 'gradients', 'vanishing', 'explod…', 'ing', 'variables', 'sharing', '248-251variable_scope', '249-250variancebias/variance', 'tradeoff', '126variance', 'preservation', '211-212variance_scaling_initializer', '278variational', 'autoencoders', '428-432VGGNet', '375visual', 'cortex', '354visualization', '242-245visualization', 'algorithms', '11-12voice', 'recognition', '353voting', 'classifiers', '181-184Wwarmup', 'phase', '349weak', 'learners', '182weight-tying', '417weights', '267', '288freezing', '289while_loop', '387white', 'box', 'models', '170worker', '324worker', 'service', '325worker_device', '327workspace', 'directory', '40-43XXavier', 'initialization', '276-279YYouTube', '253Zzero', 'padding', '356', '361Index', '|', '541', 'About', 'AuthorAur•lien', 'G•ron', 'Machine', 'Learning', 'consultant', '.', 'A', 'former', 'Googler', 'led', 'You…', 'Tube', 'video', 'classification', 'team', '2013', '2016', '.', 'He', 'also', 'founder', 'CTO', 'Wifirst', '2002', '2012', 'leading', 'Wireless', 'ISP', 'France', 'founder', 'CTO', 'Polyconseil', '2001', 'firm', 'manages', 'electric', 'car', 'sharing', 'service', 'Autolib‡', '.', 'Before', 'worked', 'engineer', 'variety', 'domains', 'finance', 'JP', 'Morgan', 'Soci•t•', 'G•n•rale', 'defense', 'Canada‡s', 'DOD', 'healthcare', 'blood', 'transfusion', '.', 'He', 'published', 'technical', 'books', 'C++', 'WiFi', 'internet', 'architectures', 'Computer', 'Science', 'lecturer', 'French', 'engineering', 'school', '.', 'A', 'fun', 'facts', 'taught', 'three', 'children', 'count', 'binary', 'fingers', '1023', 'studied', 'microbiology', 'evolutionary', 'genetics', 'going', 'soft…', 'ware', 'engineering', 'parachute', 'didn‡t', 'open', 'second', 'jump', '.', 'ColophonThe', 'animal', 'cover', 'Hands-On', 'Machine', 'Learning', 'Scikit-Learn', 'Ten…', 'sorFlow', 'far', 'eastern', 'fire', 'salamander', 'Salamandra', 'infraimmaculata', 'amphib…', 'ian', 'found', 'Middle', 'East', '.', 'They', 'black', 'skin', 'featuring', 'large', 'yellow', 'spots', 'back', 'head', '.', 'These', 'spots', 'warning', 'coloration', 'meant', 'keep', 'predators', 'bay', '.', 'Full-grown', 'salamanders', 'foot', 'length', '.', 'Far', 'eastern', 'fire', 'salamanders', 'live', 'subtropical', 'shrubland', 'forests', 'near', 'rivers', 'freshwater', 'bodies', '.', 'They', 'spend', 'life', 'land', 'lay', 'eggs', 'water', '.', 'They', 'subsist', 'mostly', 'diet', 'insects', 'worms', 'small', 'crustaceans', 'occasionally', 'eat', 'salamanders', '.', 'Males', 'species', 'known', 'live', '23', 'years', 'females', 'live', '21', 'years.Although', 'yet', 'endangered', 'far', 'eastern', 'fire', 'salamander', 'population', 'decline', '.', 'Primary', 'threats', 'include', 'damming', 'rivers', 'disrupts', 'salamander‡s', 'breed…', 'ing', 'pollution', '.', 'They', 'also', 'threatened', 'recent', 'introduction', 'predatory', 'fish', 'mosquitofish', '.', 'These', 'fish', 'intended', 'control', 'mosquito', 'pop…', 'ulation', 'also', 'feed', 'young', 'salamanders', '.', 'Many', 'animals', 'O‡Reilly', 'covers', 'endangered', 'important', 'world', '.', 'To', 'learn', 'help', 'go', 'animals.oreilly.com', '.The', 'cover', 'image', 'Wood‹s', 'Illustrated', 'Natural', 'History', '.', 'The', 'cover', 'fonts', 'URW', 'Typewriter', 'Guardian', 'Sans', '.', 'The', 'text', 'font', 'Adobe', 'Minion', 'Pro', 'heading', 'font', 'Adobe', 'Myriad', 'Condensed', 'code', 'font', 'Dalton', 'Maag‡s', 'Ubuntu', 'Mono', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PauKG7j05cfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo9_6ICt5cc5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVQsj0Py5cag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rWbmM4-5cYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGsG3mMG5cW_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMJAx5aB5cUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CePU4XuP5cQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJuFtkfp5cMo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kt2SK_65cKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}